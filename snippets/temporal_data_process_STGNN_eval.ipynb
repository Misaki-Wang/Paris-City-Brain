{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "# device = torch.device('cuda:1')\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"Available CUDA devices:\", torch.cuda.device_count())\n",
    "#     for i in range(torch.cuda.device_count()):\n",
    "#         print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "# else:\n",
    "#     print(\"CUDA is not available.\")\n",
    "# Directory for data and logs\n",
    "node_num = 4634\n",
    "filter_num = 0\n",
    "# filter_num = 1500\n",
    "\n",
    "inputdir = '../data/'\n",
    "spatial_data = '../data/processed/graph_data.npz'\n",
    "precesseddir = '../data/STGNN_data/'\n",
    "constructed_dir = '../data/constructed/'\n",
    "if not os.path.exists(constructed_dir):\n",
    "    os.makedirs(constructed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace iu_ac with adj_matrix id\n",
    "def replace_iu_ac(data):    \n",
    "    graph_data = np.load(spatial_data, allow_pickle=True)\n",
    "\n",
    "    keys = graph_data['values']\n",
    "    values = graph_data['keys']\n",
    "    index_to_iu_ac = {key: value for key, value in zip(keys, values)}\n",
    "    \n",
    "    data['iu_ac'] = data['iu_ac'].map(index_to_iu_ac)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_add_constructed_features(data):\n",
    "    data['constructed'] = (data['target'] == -1).astype(int)\n",
    "    return data\n",
    "\n",
    "def add_constructed_features(data):\n",
    "    data['constructed'] = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def construct_missing_data(t_1h, timestep_data, node_num):\n",
    "    all_iu_ac = set(range(node_num))\n",
    "    existing_iu_ac = set(timestep_data['iu_ac'])\n",
    "    missing_iu_ac = all_iu_ac - existing_iu_ac\n",
    "    \n",
    "    new_data_rows = []\n",
    "    \n",
    "    for iu_ac in missing_iu_ac:\n",
    "        new_row = {\n",
    "            'iu_ac': iu_ac,\n",
    "            't_1h': timestep_data['t_1h'].iloc[0],\n",
    "            'etat_barre': 0,\n",
    "            'constructed': 1,\n",
    "            'q': -1\n",
    "        }\n",
    "        new_data_rows.append(new_row)\n",
    "    \n",
    "    return new_data_rows\n",
    "\n",
    "def timestep_construct_optimized(data):\n",
    "    if 'id' in data.columns:\n",
    "        data.drop('id', axis=1, inplace=True)\n",
    "    data.sort_values(by=['t_1h', 'iu_ac'], inplace=True)\n",
    "    \n",
    "    counts = data.groupby('t_1h').size()\n",
    "    \n",
    "    valid_time_steps = counts[counts >= filter_num].index\n",
    "    print(f\"数量大于{filter_num}的时间步总数:\", valid_time_steps.size)\n",
    "    \n",
    "    valid_rows = data[data['t_1h'].isin(valid_time_steps)]\n",
    "    valid_time_steps_df = pd.DataFrame(valid_rows)\n",
    "\n",
    "    new_data_rows = []\n",
    "    print(f\"Number of nodes: {node_num}\")\n",
    "    total = node_num*valid_time_steps.size\n",
    "    print(f\"total number of processed data should be: {total}\")\n",
    "    \n",
    "    # 并行计算每个时间步的数据构造过程\n",
    "    new_data_rows = Parallel(n_jobs=-1)(\n",
    "        delayed(construct_missing_data)(t_1h, data[data['t_1h'] == t_1h], node_num)\n",
    "        for t_1h in tqdm(valid_time_steps, desc=\"Processing time steps\")\n",
    "    )\n",
    "    # 将并行计算得到的数据合并为一个列表\n",
    "    new_data_rows = [row for sublist in new_data_rows for row in sublist]\n",
    "    \n",
    "    new_data = pd.DataFrame(new_data_rows)\n",
    "    valid_time_steps_df['constructed'] = 0    \n",
    "    \n",
    "    combined_data = pd.concat([valid_time_steps_df, new_data], ignore_index=True)\n",
    "    combined_data.sort_values(by=['t_1h', 'iu_ac'], inplace=True)\n",
    "    \n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(data):\n",
    "    data['t_1h'] = pd.to_datetime(data['t_1h'])\n",
    "    return data\n",
    "\n",
    "def encode_categorical(data, column_name, num_classes):\n",
    "    categories = torch.tensor(data[column_name].values)\n",
    "    return torch.nn.functional.one_hot(categories, num_classes=num_classes).float()\n",
    "\n",
    "def process_time_features(data, time_column='t_1h'):\n",
    "    # 提取周期性时间特征\n",
    "    data['hour'] = data[time_column].dt.hour\n",
    "    data['day_of_week'] = data[time_column].dt.weekday\n",
    "    data['month'] = data[time_column].dt.month\n",
    "\n",
    "    # 应用正弦和余弦变换来捕获时间的周期性\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)\n",
    "    data['day_of_week_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)\n",
    "    data['day_of_week_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_to_tensor(data, train=1):\n",
    "    to_datetime(data)\n",
    "    print(\"Number of rows in data:\", len(data))\n",
    "    if 'id' in data.columns:\n",
    "        data.drop('id', axis=1, inplace=True)\n",
    "    \n",
    "    # One-hot encode 'etat_barre'\n",
    "    etat_barre_encoded = encode_categorical(data, 'etat_barre', 4)\n",
    "    # Process time features\n",
    "    data = process_time_features(data, 't_1h')\n",
    "    data['t_1h'] = data['t_1h'].to_numpy()\n",
    "    # Combine features into a single tensor\n",
    "    features = torch.cat([\n",
    "        torch.tensor(data[['iu_ac', 't_1h', 'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos', 'month_sin', 'month_cos', 'constructed']].values).float(),\n",
    "        etat_barre_encoded\n",
    "    ], dim=1)\n",
    "\n",
    "    if train:\n",
    "        targets = torch.tensor(data['q'].values).float()\n",
    "        return TensorDataset(features, targets)\n",
    "    return TensorDataset(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_dataset_to_dataframe(tensor_dataset, feature_names):\n",
    "    # 从 TensorDataset 中提取特征和目标张量\n",
    "    features = tensor_dataset.tensors[0]\n",
    "    if len(tensor_dataset.tensors) > 1:\n",
    "        targets = tensor_dataset.tensors[1]\n",
    "    else:\n",
    "        targets = None\n",
    "    \n",
    "    \n",
    "    # 将特征张量转换为 DataFrame\n",
    "    features_df = pd.DataFrame(features.numpy(), columns=feature_names)\n",
    "    \n",
    "    # 如果存在目标，添加到 DataFrame\n",
    "    if targets is not None:\n",
    "        features_df['target'] = targets.numpy()\n",
    "    \n",
    "    print(features_df.shape())\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "\n",
    "feature_names = [\n",
    "    'iu_ac','t_1h',\n",
    "    'hour_sin', 'hour_cos', \n",
    "    'day_of_week_sin', 'day_of_week_cos', \n",
    "    'month_sin', 'month_cos',\n",
    "    'etat_barre_0', 'etat_barre_1', 'etat_barre_2', 'etat_barre_3',\n",
    "    'constructed'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_data():\n",
    "    # train_data = pd.read_csv(f'{inputdir}loop_sensor_train.csv')\n",
    "    # train_data = replace_iu_ac(train_data)\n",
    "    # train_data = timestep_construct_optimized(train_data)\n",
    "    # train_dataset = data_to_tensor(train_data)\n",
    "    # train_df = tensor_dataset_to_dataframe(train_dataset, feature_names)\n",
    "    # train_df = test_and_add_constructed_features(train_data)\n",
    "    # train_df.to_csv(f'{precesseddir}train_dataset_stgnn.csv', index=False)\n",
    "    # train_df.to_csv(f'{constructed_dir}train_dataset_constructed_{filter_num}.csv', index=False)    \n",
    "     \n",
    "    # eval_data = pd.read_csv(f'{inputdir}loop_sensor_eval.csv')\n",
    "    # eval_data = replace_iu_ac(eval_data)\n",
    "    # eval_data = timestep_construct_optimized(eval_data)\n",
    "    # eval_dataset = data_to_tensor(eval_data)   \n",
    "    # eval_df = tensor_dataset_to_dataframe(eval_dataset, feature_names)    \n",
    "    # eval_df.to_csv(f'{constructed_dir}eval_dataset_constructed.csv', index=False)     \n",
    "    \n",
    "    eval_data = pd.read_csv(f'{inputdir}loop_sensor_eval.csv')\n",
    "    eval_data = replace_iu_ac(eval_data)\n",
    "    eval_data = timestep_construct_optimized(eval_data)\n",
    "    eval_dataset = data_to_tensor(eval_data)   \n",
    "    eval_df = tensor_dataset_to_dataframe(eval_dataset, feature_names)    \n",
    "    eval_df.to_csv(f'{constructed_dir}eval_dataset_constructed_t_1h.csv', index=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数量大于0的时间步总数: 307\n",
      "Number of nodes: 4634\n",
      "total number of processed data should be: 1422638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing time steps: 100%|██████████| 307/307 [00:03<00:00, 96.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in data: 1422638\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_and_save_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 21\u001b[0m, in \u001b[0;36mprocess_and_save_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m eval_data \u001b[38;5;241m=\u001b[39m replace_iu_ac(eval_data)\n\u001b[1;32m     20\u001b[0m eval_data \u001b[38;5;241m=\u001b[39m timestep_construct_optimized(eval_data)\n\u001b[0;32m---> 21\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdata_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     22\u001b[0m eval_df \u001b[38;5;241m=\u001b[39m tensor_dataset_to_dataframe(eval_dataset, feature_names)    \n\u001b[1;32m     23\u001b[0m eval_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstructed_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124meval_dataset_constructed_t_1h.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[26], line 38\u001b[0m, in \u001b[0;36mdata_to_tensor\u001b[0;34m(data, train)\u001b[0m\n\u001b[1;32m     35\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_1h\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_1h\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Combine features into a single tensor\u001b[39;00m\n\u001b[1;32m     37\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miu_ac\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt_1h\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhour_sin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhour_cos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mday_of_week_sin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mday_of_week_cos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmonth_sin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmonth_cos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconstructed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m     39\u001b[0m     etat_barre_encoded\n\u001b[1;32m     40\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[1;32m     43\u001b[0m     targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "process_and_save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # replace iu_ac with adj_matrix id\n",
    "# def resign_iu_ac(data):    \n",
    "#     graph_data = np.load(spatial_data, allow_pickle=True)\n",
    "\n",
    "#     values = graph_data['values']\n",
    "#     keys = graph_data['keys']\n",
    "#     index_to_iu_ac = {key: value for key, value in zip(keys, values)}\n",
    "    \n",
    "#     data['iu_ac'] = data['iu_ac'].map(index_to_iu_ac)\n",
    "    \n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def submission_transform():\n",
    "#     sub_data = pd.read_csv(f'{inputdir}loop_sensor_test_x.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
