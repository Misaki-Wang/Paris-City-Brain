{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "# device = torch.device('cuda:1')\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"Available CUDA devices:\", torch.cuda.device_count())\n",
    "#     for i in range(torch.cuda.device_count()):\n",
    "#         print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "# else:\n",
    "#     print(\"CUDA is not available.\")\n",
    "# Directory for data and logs\n",
    "node_num = 4634\n",
    "filter_num = 1250\n",
    "# filter_num = 1500\n",
    "\n",
    "inputdir = '../data/'\n",
    "spatial_data = '../data/processed/graph_data.npz'\n",
    "precesseddir = '../data/STGNN_data/'\n",
    "constructed_dir = '../data/constructed/'\n",
    "if not os.path.exists(constructed_dir):\n",
    "    os.makedirs(constructed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace iu_ac with adj_matrix id\n",
    "def replace_iu_ac(data):    \n",
    "    graph_data = np.load(spatial_data, allow_pickle=True)\n",
    "\n",
    "    keys = graph_data['values']\n",
    "    values = graph_data['keys']\n",
    "    index_to_iu_ac = {key: value for key, value in zip(keys, values)}\n",
    "    \n",
    "    data['iu_ac'] = data['iu_ac'].map(index_to_iu_ac)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_add_constructed_features(data):\n",
    "    data['constructed'] = (data['target'] == -1).astype(int)\n",
    "    return data\n",
    "\n",
    "def add_constructed_features(data):\n",
    "    data['constructed'] = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def construct_missing_data(t_1h, timestep_data, node_num):\n",
    "    all_iu_ac = set(range(node_num))\n",
    "    existing_iu_ac = set(timestep_data['iu_ac'])\n",
    "    missing_iu_ac = all_iu_ac - existing_iu_ac\n",
    "    \n",
    "    new_data_rows = []\n",
    "    \n",
    "    for iu_ac in missing_iu_ac:\n",
    "        new_row = {\n",
    "            'iu_ac': iu_ac,\n",
    "            't_1h': timestep_data['t_1h'].iloc[0],\n",
    "            'etat_barre': 0,\n",
    "            'constructed': 1,\n",
    "            'q': -1\n",
    "        }\n",
    "        new_data_rows.append(new_row)\n",
    "    \n",
    "    return new_data_rows\n",
    "\n",
    "def timestep_construct_optimized(data):\n",
    "    if 'id' in data.columns:\n",
    "        data.drop('id', axis=1, inplace=True)\n",
    "    data.sort_values(by=['t_1h', 'iu_ac'], inplace=True)\n",
    "    \n",
    "    counts = data.groupby('t_1h').size()\n",
    "    \n",
    "    valid_time_steps = counts[counts >= filter_num].index\n",
    "    print(f\"数量大于{filter_num}的时间步总数:\", valid_time_steps.size)\n",
    "    \n",
    "    valid_rows = data[data['t_1h'].isin(valid_time_steps)]\n",
    "    valid_time_steps_df = pd.DataFrame(valid_rows)\n",
    "\n",
    "    new_data_rows = []\n",
    "    print(f\"Number of nodes: {node_num}\")\n",
    "    total = node_num*valid_time_steps.size\n",
    "    print(f\"total number of processed data should be: {total}\")\n",
    "    \n",
    "    # 并行计算每个时间步的数据构造过程\n",
    "    new_data_rows = Parallel(n_jobs=-1)(\n",
    "        delayed(construct_missing_data)(t_1h, data[data['t_1h'] == t_1h], node_num)\n",
    "        for t_1h in tqdm(valid_time_steps, desc=\"Processing time steps\")\n",
    "    )\n",
    "    # 将并行计算得到的数据合并为一个列表\n",
    "    new_data_rows = [row for sublist in new_data_rows for row in sublist]\n",
    "    \n",
    "    new_data = pd.DataFrame(new_data_rows)\n",
    "    valid_time_steps_df['constructed'] = 0    \n",
    "    \n",
    "    combined_data = pd.concat([valid_time_steps_df, new_data], ignore_index=True)\n",
    "    combined_data.sort_values(by=['t_1h', 'iu_ac'], inplace=True)\n",
    "    \n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(data):\n",
    "    data['t_1h'] = pd.to_datetime(data['t_1h'])\n",
    "    return data\n",
    "\n",
    "def encode_categorical(data, column_name, num_classes):\n",
    "    categories = torch.tensor(data[column_name].values)\n",
    "    return torch.nn.functional.one_hot(categories, num_classes=num_classes).float()\n",
    "\n",
    "def process_time_features(data, time_column='t_1h'):\n",
    "    # 提取周期性时间特征\n",
    "    data['hour'] = data[time_column].dt.hour\n",
    "    data['day_of_week'] = data[time_column].dt.weekday\n",
    "    data['month'] = data[time_column].dt.month\n",
    "\n",
    "    # 应用正弦和余弦变换来捕获时间的周期性\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)\n",
    "    data['day_of_week_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)\n",
    "    data['day_of_week_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_to_tensor(data, train=1):\n",
    "    to_datetime(data)\n",
    "    print(\"Number of rows in data:\", len(data))\n",
    "    if 'id' in data.columns:\n",
    "        data.drop('id', axis=1, inplace=True)\n",
    "    \n",
    "    # One-hot encode 'etat_barre'\n",
    "    etat_barre_encoded = encode_categorical(data, 'etat_barre', 4)\n",
    "    # Process time features\n",
    "    data = process_time_features(data, 't_1h')\n",
    "    \n",
    "    # Combine features into a single tensor\n",
    "    features = torch.cat([\n",
    "        torch.tensor(data[['iu_ac', 'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos', 'month_sin', 'month_cos', 'constructed']].values).float(),\n",
    "        etat_barre_encoded\n",
    "    ], dim=1)\n",
    "\n",
    "    if train:\n",
    "        targets = torch.tensor(data['q'].values).float()\n",
    "        return TensorDataset(features, targets)\n",
    "    return TensorDataset(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_dataset_to_dataframe(tensor_dataset, feature_names):\n",
    "    # 从 TensorDataset 中提取特征和目标张量\n",
    "    features = tensor_dataset.tensors[0]\n",
    "    if len(tensor_dataset.tensors) > 1:\n",
    "        targets = tensor_dataset.tensors[1]\n",
    "    else:\n",
    "        targets = None\n",
    "    \n",
    "    # 将特征张量转换为 DataFrame\n",
    "    features_df = pd.DataFrame(features.numpy(), columns=feature_names)\n",
    "    \n",
    "    # 如果存在目标，添加到 DataFrame\n",
    "    if targets is not None:\n",
    "        features_df['target'] = targets.numpy()\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "\n",
    "feature_names = [\n",
    "    'iu_ac',\n",
    "    'hour_sin', 'hour_cos', \n",
    "    'day_of_week_sin', 'day_of_week_cos', \n",
    "    'month_sin', 'month_cos',\n",
    "    'etat_barre_0', 'etat_barre_1', 'etat_barre_2', 'etat_barre_3',\n",
    "    'constructed'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_data():\n",
    "    train_data = pd.read_csv(f'{inputdir}loop_sensor_train.csv')\n",
    "    train_data = replace_iu_ac(train_data)\n",
    "    train_data = timestep_construct_optimized(train_data)\n",
    "    train_dataset = data_to_tensor(train_data)\n",
    "    train_df = tensor_dataset_to_dataframe(train_dataset, feature_names)\n",
    "    # train_df = test_and_add_constructed_features(train_data)\n",
    "    # train_df.to_csv(f'{precesseddir}train_dataset_stgnn.csv', index=False)\n",
    "    train_df.to_csv(f'{constructed_dir}train_dataset_constructed_{filter_num}.csv', index=False)    \n",
    "     \n",
    "    # eval_data = pd.read_csv(f'{inputdir}loop_sensor_eval.csv')\n",
    "    # eval_data = replace_iu_ac(eval_data)\n",
    "    # eval_dataset = data_to_tensor(eval_data)   \n",
    "    # eval_df = tensor_dataset_to_dataframe(eval_dataset, feature_names)    \n",
    "    # eval_df.to_csv(f'{precesseddir}eval_dataset_stgnn.csv', index=False)    \n",
    "    \n",
    "    # test_data_x = pd.read_csv(f'{inputdir}loop_sensor_test_x.csv')\n",
    "    # test_data_x = replace_iu_ac(test_data_x)\n",
    "    # test_dataset_x = data_to_tensor(test_data_x, train=0)\n",
    "    # test_df = tensor_dataset_to_dataframe(test_dataset_x, feature_names)   \n",
    "    # test_df.to_csv(f'{precesseddir}test_dataset_stgnn_x.csv', index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数量大于1250的时间步总数: 15029\n",
      "Number of nodes: 4634\n",
      "total number of processed data should be: 69644386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing time steps: 100%|██████████| 15029/15029 [2:06:10<00:00,  1.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in data: 69644386\n"
     ]
    }
   ],
   "source": [
    "process_and_save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def constructed_data():\n",
    "#     # eval_data = pd.read_csv(f'{precesseddir}eval_dataset_stgnn.csv')\n",
    "#     # eval_df = add_constructed_features(eval_data)\n",
    "#     # eval_df.to_csv(f'{constructed_dir}eval_dataset_constructed.csv', index=False)\n",
    "    \n",
    "#     # test_data_x = pd.read_csv(f'{precesseddir}test_dataset_stgnn_x.csv')\n",
    "#     # test_df = add_constructed_features(test_data_x)\n",
    "#     # test_df.to_csv(f'{constructed_dir}test_dataset_constructed_x.csv', index=False)\n",
    "    \n",
    "#     train_data = pd.read_csv(f'{precesseddir}train_dataset_stgnn.csv')\n",
    "#     train_df = test_and_add_constructed_features(train_data)\n",
    "#     train_df.to_csv(f'{constructed_dir}train_dataset_constructed.csv', index=False)    \n",
    "    \n",
    "# constructed_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
