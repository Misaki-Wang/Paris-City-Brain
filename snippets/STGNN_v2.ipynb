{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as l1_loss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CUDA devices: 2\n",
      "Device 0: NVIDIA GeForce RTX 4090\n",
      "Device 1: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "inputdir = '../data/constructed/'\n",
    "resultdir = '../data/result/'\n",
    "num_nodes = 4634\n",
    "batchsize = 1024\n",
    "# [53, 1730], [7006,1500]\n",
    "# total_num_timesteps = 7006\n",
    "num_timesteps_train =  7006\n",
    "num_timesteps_eval = 307\n",
    "\n",
    "feature_names = [\n",
    "    'iu_ac',\n",
    "    'hour_sin', 'hour_cos', \n",
    "    'day_of_week_sin', 'day_of_week_cos', \n",
    "    'month_sin', 'month_cos',\n",
    "    'etat_barre_0', 'etat_barre_1', 'etat_barre_2', 'etat_barre_3',\n",
    "    'constructed'\n",
    "]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Available CUDA devices:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "if not os.path.exists(resultdir):\n",
    "    os.makedirs(resultdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_dataset(filepath, feature_names, target_name='target', batch_size=batchsize, shuffle=False, train=True):\n",
    "def load_dataset(filepath, feature_names, num_timesteps ,target_name='target', shuffle=False, train=True):\n",
    "    data = pd.read_csv(filepath)\n",
    "    features = data[feature_names].values\n",
    "    print(features.shape)\n",
    "    features = features.reshape(-1, num_timesteps, num_nodes,  len(feature_names))\n",
    "    batchSize = features.shape[0]\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "    print(features_tensor.shape)\n",
    "\n",
    "    if train:\n",
    "        targets = data[target_name].values\n",
    "        targets = targets.reshape(-1, num_timesteps, num_nodes, 1)\n",
    "        targets_tensor = torch.tensor(targets, dtype=torch.float32)\n",
    "        dataset = TensorDataset(features_tensor, targets_tensor)\n",
    "    else:\n",
    "        dataset = TensorDataset(features_tensor)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batchSize, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_data(filename):\n",
    "    data = np.load(filename)\n",
    "    adjacency_matrix = data['adjacency_matrix']\n",
    "    keys = data['keys']\n",
    "    values = data['values']\n",
    "    index_to_iu_ac = {key: value for key, value in zip(keys, values)}\n",
    "    return adjacency_matrix, index_to_iu_ac\n",
    "\n",
    "def symmetric_normalize_adjacency(adjacency_matrix):\n",
    "    D = np.diag(np.sqrt(np.sum(adjacency_matrix, axis=1)) + 1e-6)\n",
    "    D_inv = np.linalg.inv(D)\n",
    "    normalized_adjacency = np.dot(np.dot(D_inv, adjacency_matrix), D_inv)\n",
    "    return normalized_adjacency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32465804, 12)\n",
      "torch.Size([7006, 1, 4634, 12])\n"
     ]
    }
   ],
   "source": [
    "# train_loader = load_dataset(f'{inputdir}train_dataset_constructed.csv', feature_names)\n",
    "train_loader = load_dataset(f'{inputdir}train_dataset_constructed_1500.csv', feature_names, num_timesteps = num_timesteps_train)\n",
    "# eval_loader = load_dataset(f'{inputdir}eval_dataset_constructed.csv', feature_names)\n",
    "# test_loader = load_dataset(f'{inputdir}test_dataset_constructed_x.csv', feature_names, train=False)\n",
    "\n",
    "# 加载图数据\n",
    "adjacency_matrix, index_to_iu_ac = load_graph_data(f\"{inputdir}graph_data.npz\")\n",
    "# 归一化邻接矩阵\n",
    "normalized_adjacency = symmetric_normalize_adjacency(adjacency_matrix)\n",
    "adjacency_tensor = torch.from_numpy(normalized_adjacency)\n",
    "adjacency_tensor = adjacency_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / np.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        output = torch.matmul(adj, support)\n",
    "        return output + self.bias\n",
    "\n",
    "class STGNN(nn.Module):\n",
    "    def __init__(self, num_features, num_nodes, hidden_size, num_layers,  num_timesteps=num_timesteps_train, dropout=0.5):\n",
    "        super(STGNN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(num_features, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # Assuming the target is per node per timestep\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # Apply the graph convolution only once per batch\n",
    "        x = x.mean(dim=1)  # Average over time to get a single representation per node\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        # Repeat the graph convolution output for each timestep\n",
    "        x = x.unsqueeze(1).repeat(1, num_timesteps, 1)\n",
    "        # Process sequences using LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, adjacency_matrix, criterion, optimizer, num_epochs, log_interval=10, checkpoint_dir='./logs/stgnn/',TBlog_dir='./runs/stgnn/'):\n",
    "    model.to(device)\n",
    "    \n",
    "    # 初始化 TensorBoard 记录器\n",
    "    if not os.path.exists(TBlog_dir):\n",
    "        os.makedirs(TBlog_dir)\n",
    "    writer = SummaryWriter(TBlog_dir)\n",
    "    start_epoch = 1\n",
    "    flag = True\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')  # 指定检查点文件名\n",
    "    \n",
    "    # 确保检查点目录存在\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    # 检查是否存在检查点\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1  # 从下一个周期开始\n",
    "        \n",
    "    model.train()\n",
    "    for epoch in range(start_epoch, num_epochs + start_epoch):\n",
    "        # for batch_idx, (inputs, targets) in tqdm(enumerate(train_loader), desc=\"Training\"):\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if flag:\n",
    "                print(next(model.parameters()).is_cuda)  # 确认模型参数是否在 GPU 上\n",
    "                print(inputs.is_cuda)  # 确认输入是否在 GPU 上\n",
    "                flag = False\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs, adjacency_matrix)\n",
    "            # print(f'output: {output.shape}, target:{targets.shape}')\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 在 TensorBoard 中记录损失\n",
    "            if batch_idx % log_interval == 0:\n",
    "                writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "            \n",
    "            # print(f'Epoch {epoch}, Batch {batch_idx+1}, Loss: {loss.item()}')\n",
    "            # 定期保存检查点\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx+1}, Loss: {loss.item()}')\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                }, checkpoint_path)\n",
    "    # 关闭 TensorBoard 记录器\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = STGNN(len(feature_names), num_nodes=num_nodes, hidden_size=64, num_layers=2).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "True\n",
      "True\n",
      "Epoch 109, Batch 1, Loss: 193.0497589111328\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, adjacency_tensor, criterion, optimizer, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1422638, 12)\n",
      "torch.Size([307, 1, 4634, 12])\n"
     ]
    }
   ],
   "source": [
    "eval_loader = load_dataset(f'{inputdir}eval_dataset_constructed_new.csv', feature_names, num_timesteps=num_timesteps_eval)\n",
    "# # test_loader = load_dataset(f'{inputdir}test_dataset_constructed_x.csv', feature_names, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "matmul(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m     average_mae \u001b[38;5;241m=\u001b[39m total_mae \u001b[38;5;241m/\u001b[39m total_count\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m average_mae\n\u001b[0;32m---> 18\u001b[0m average_mae \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_mae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage MAE on evaluation data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_mae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m, in \u001b[0;36mcalculate_mae\u001b[0;34m(model, data_loader, adjacency_matrix, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m      8\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjacency_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     mae \u001b[38;5;241m=\u001b[39m l1_loss(output, targets, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m     total_mae \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mae\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/media/ssd4t/anaconda3/envs/traffic/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ssd4t/anaconda3/envs/traffic/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m, in \u001b[0;36mSTGNN.forward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     30\u001b[0m batch_size, num_timesteps, num_nodes, num_features \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m num_timesteps, num_nodes, num_features)\n\u001b[0;32m---> 32\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, num_timesteps, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten all node features into the feature dimension\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Process sequences using LSTM\u001b[39;00m\n",
      "File \u001b[0;32m/media/ssd4t/anaconda3/envs/traffic/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ssd4t/anaconda3/envs/traffic/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mGraphConvolution.forward\u001b[0;34m(self, input, adj)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, adj):\n\u001b[1;32m     16\u001b[0m     support \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m---> 17\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msupport\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[0;31mTypeError\u001b[0m: matmul(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "def calculate_mae(model, data_loader,adjacency_matrix= adjacency_matrix, device=device):\n",
    "    model.eval()  \n",
    "    total_mae = 0.0\n",
    "    total_count = 0\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            output = model(inputs, adjacency_matrix)\n",
    "            mae = l1_loss(output, targets, reduction='sum')\n",
    "            total_mae += mae.item()\n",
    "            total_count += targets.size(0)\n",
    "    \n",
    "    average_mae = total_mae / total_count\n",
    "    return average_mae\n",
    "\n",
    "\n",
    "average_mae = calculate_mae(model, eval_loader)\n",
    "print(f'Average MAE on evaluation data: {average_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, loader, adjacency_matrix= adjacency_matrix, device=device):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     with torch.no_grad():\n",
    "#         for features, _ in loader:\n",
    "#             features = features[0].to(device)\n",
    "#             output = model(features, adjacency_matrix)\n",
    "#             # predictions.extend(outputs.cpu().numpy())\n",
    "#             predictions.extend(output.round().cpu().numpy())\n",
    "#     return predictions\n",
    "\n",
    "# predictions = evaluate_model(model, eval_loader)\n",
    "\n",
    "# predictions_df = pd.DataFrame(predictions)\n",
    "# predictions_df.index = predictions_df.index + 1  # Adjust index to start from 1\n",
    "# print(len(predictions_df))\n",
    "# # Save the predictions to a CSV file\n",
    "# predictions_df.to_csv(f'{resultdir}stgnn_predictions.csv', index_label='id')\n",
    "\n",
    "# print(\"Predictions saved to 'stgnn_predictions.csv', with IDs starting from 1.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
