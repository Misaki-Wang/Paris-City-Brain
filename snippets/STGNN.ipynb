{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as l1_loss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CUDA devices: 2\n",
      "Device 0: NVIDIA GeForce RTX 4090\n",
      "Device 1: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "inputdir = '../data/constructed/'\n",
    "resultdir = '../data/result/'\n",
    "num_nodes = 4634\n",
    "batchsize = 1024\n",
    "# [53, 1730], [7006,1500]\n",
    "# total_num_timesteps = 7006\n",
    "num_timesteps_train =  7006\n",
    "num_timesteps_eval = 307\n",
    "\n",
    "feature_names = [\n",
    "    'iu_ac',\n",
    "    'hour_sin', 'hour_cos', \n",
    "    'day_of_week_sin', 'day_of_week_cos', \n",
    "    'month_sin', 'month_cos',\n",
    "    'etat_barre_0', 'etat_barre_1', 'etat_barre_2', 'etat_barre_3',\n",
    "    'constructed'\n",
    "]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Available CUDA devices:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "if not os.path.exists(resultdir):\n",
    "    os.makedirs(resultdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_dataset(filepath, feature_names, target_name='target', batch_size=batchsize, shuffle=False, train=True):\n",
    "def load_dataset(filepath, feature_names, num_timesteps ,target_name='target', shuffle=False, train=True):\n",
    "    data = pd.read_csv(filepath)\n",
    "    features = data[feature_names].values\n",
    "    print(features.shape)\n",
    "    features = features.reshape(-1, num_timesteps, num_nodes,  len(feature_names))\n",
    "    batchSize = features.shape[0]\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "    print(features_tensor.shape)\n",
    "\n",
    "    if train:\n",
    "        targets = data[target_name].values\n",
    "        targets = targets.reshape(-1, num_timesteps, num_nodes, 1)\n",
    "        targets_tensor = torch.tensor(targets, dtype=torch.float32)\n",
    "        dataset = TensorDataset(features_tensor, targets_tensor)\n",
    "    else:\n",
    "        dataset = TensorDataset(features_tensor)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batchSize, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_data(filename):\n",
    "    data = np.load(filename)\n",
    "    adjacency_matrix = data['adjacency_matrix']\n",
    "    keys = data['keys']\n",
    "    values = data['values']\n",
    "    index_to_iu_ac = {key: value for key, value in zip(keys, values)}\n",
    "    return adjacency_matrix, index_to_iu_ac\n",
    "\n",
    "def symmetric_normalize_adjacency(adjacency_matrix):\n",
    "    D = np.diag(np.sqrt(np.sum(adjacency_matrix, axis=1)) + 1e-6)\n",
    "    D_inv = np.linalg.inv(D)\n",
    "    normalized_adjacency = np.dot(np.dot(D_inv, adjacency_matrix), D_inv)\n",
    "    return normalized_adjacency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32465804, 12)\n",
      "torch.Size([7006, 1, 4634, 12])\n"
     ]
    }
   ],
   "source": [
    "# train_loader = load_dataset(f'{inputdir}train_dataset_constructed.csv', feature_names)\n",
    "train_loader = load_dataset(f'{inputdir}train_dataset_constructed_1500.csv', feature_names, num_timesteps = num_timesteps_train)\n",
    "# eval_loader = load_dataset(f'{inputdir}eval_dataset_constructed.csv', feature_names)\n",
    "# test_loader = load_dataset(f'{inputdir}test_dataset_constructed_x.csv', feature_names, train=False)\n",
    "\n",
    "# 加载图数据\n",
    "adjacency_matrix, index_to_iu_ac = load_graph_data(f\"{inputdir}graph_data.npz\")\n",
    "# 归一化邻接矩阵\n",
    "normalized_adjacency = symmetric_normalize_adjacency(adjacency_matrix)\n",
    "adjacency_tensor = torch.from_numpy(normalized_adjacency)\n",
    "adjacency_tensor = adjacency_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / np.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        output = torch.matmul(adj, support)\n",
    "        return output + self.bias\n",
    "\n",
    "class STGNN(nn.Module):\n",
    "    def __init__(self, num_features, num_nodes, hidden_size, num_layers, num_timesteps=num_timesteps_train ,dropout=0.5):\n",
    "        super(STGNN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(num_features, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=num_nodes * hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_nodes * num_timesteps * 1)  \n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        batch_size, num_timesteps, num_nodes, num_features = x.shape\n",
    "        x = x.view(batch_size * num_timesteps, num_nodes, num_features)\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = x.view(batch_size, num_timesteps, -1) \n",
    "        x, _ = self.lstm(x) \n",
    "        x = self.fc(x[:, -1, :])\n",
    "        x = x.view(batch_size, num_timesteps, num_nodes, 1)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, adjacency_matrix, criterion, optimizer, num_epochs, log_interval=10, checkpoint_dir='./logs/stgnn/',TBlog_dir='./runs/stgnn/'):\n",
    "    model.to(device)\n",
    "    \n",
    "    # 初始化 TensorBoard 记录器\n",
    "    if not os.path.exists(TBlog_dir):\n",
    "        os.makedirs(TBlog_dir)\n",
    "    writer = SummaryWriter(TBlog_dir)\n",
    "    start_epoch = 1\n",
    "    flag = True\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')  # 指定检查点文件名\n",
    "    \n",
    "    # 确保检查点目录存在\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    # 检查是否存在检查点\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1  # 从下一个周期开始\n",
    "        \n",
    "    model.train()\n",
    "    for epoch in range(start_epoch, num_epochs + start_epoch):\n",
    "        # for batch_idx, (inputs, targets) in tqdm(enumerate(train_loader), desc=\"Training\"):\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if flag:\n",
    "                print(next(model.parameters()).is_cuda)  # 确认模型参数是否在 GPU 上\n",
    "                print(inputs.is_cuda)  # 确认输入是否在 GPU 上\n",
    "                flag = False\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs, adjacency_matrix)\n",
    "            # print(f'output: {output.shape}, target:{targets.shape}')\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 在 TensorBoard 中记录损失\n",
    "            if batch_idx % log_interval == 0:\n",
    "                writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "            \n",
    "            # print(f'Epoch {epoch}, Batch {batch_idx+1}, Loss: {loss.item()}')\n",
    "            # 定期保存检查点\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx+1}, Loss: {loss.item()}')\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                }, checkpoint_path)\n",
    "    # 关闭 TensorBoard 记录器\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = STGNN(len(feature_names), num_nodes=num_nodes, hidden_size=64, num_layers=2).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "True\n",
      "True\n",
      "Epoch 109, Batch 1, Loss: 193.0497589111328\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, adjacency_tensor, criterion, optimizer, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1422638, 12)\n",
      "torch.Size([307, 1, 4634, 12])\n"
     ]
    }
   ],
   "source": [
    "eval_loader = load_dataset(f'{inputdir}eval_dataset_constructed_new.csv', feature_names, num_timesteps=num_timesteps_eval)\n",
    "# # test_loader = load_dataset(f'{inputdir}test_dataset_constructed_x.csv', feature_names, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae(model, data_loader,adjacency_matrix= adjacency_matrix, device=device):\n",
    "    model.eval()  \n",
    "    total_mae = 0.0\n",
    "    total_count = 0\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            output = model(inputs, adjacency_matrix)\n",
    "            mae = l1_loss(output, targets, reduction='sum')\n",
    "            total_mae += mae.item()\n",
    "            total_count += targets.size(0)\n",
    "    \n",
    "    average_mae = total_mae / total_count\n",
    "    return average_mae\n",
    "\n",
    "\n",
    "average_mae = calculate_mae(model, eval_loader)\n",
    "print(f'Average MAE on evaluation data: {average_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, loader, adjacency_matrix= adjacency_matrix, device=device):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     with torch.no_grad():\n",
    "#         for features, _ in loader:\n",
    "#             features = features[0].to(device)\n",
    "#             output = model(features, adjacency_matrix)\n",
    "#             # predictions.extend(outputs.cpu().numpy())\n",
    "#             predictions.extend(output.round().cpu().numpy())\n",
    "#     return predictions\n",
    "\n",
    "# predictions = evaluate_model(model, eval_loader)\n",
    "\n",
    "# predictions_df = pd.DataFrame(predictions)\n",
    "# predictions_df.index = predictions_df.index + 1  # Adjust index to start from 1\n",
    "# print(len(predictions_df))\n",
    "# # Save the predictions to a CSV file\n",
    "# predictions_df.to_csv(f'{resultdir}stgnn_predictions.csv', index_label='id')\n",
    "\n",
    "# print(\"Predictions saved to 'stgnn_predictions.csv', with IDs starting from 1.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
