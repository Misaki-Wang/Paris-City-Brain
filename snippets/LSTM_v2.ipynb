{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.functional import l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CUDA devices: 2\n",
      "Device 0: NVIDIA GeForce RTX 4090\n",
      "Device 1: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Available CUDA devices:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "    \n",
    "device = torch.device('cuda:1')\n",
    "inputdir = '../data/constructed/'\n",
    "resultdir = '../data/result/'\n",
    "BatchSize = 2030\n",
    "# 特征和文件路径配置\n",
    "feature_names = [\n",
    "    'hour_sin', 'hour_cos', \n",
    "    'day_of_week_sin', 'day_of_week_cos', \n",
    "    'month_sin', 'month_cos',\n",
    "    'etat_barre_0', 'etat_barre_1', 'etat_barre_2', 'etat_barre_3',\n",
    "    'constructed'\n",
    "]\n",
    "\n",
    "if not os.path.exists(resultdir):\n",
    "    os.makedirs(resultdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath, feature_names, target_name='target', batch_size=BatchSize, shuffle=True, train=True):\n",
    "    data = pd.read_csv(filepath)\n",
    "    features = data[feature_names].values\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(1)\n",
    "    print(features_tensor.shape)\n",
    "\n",
    "    if train:\n",
    "        targets = data[target_name].values\n",
    "        targets_tensor = torch.tensor(targets, dtype=torch.float32).unsqueeze(1)\n",
    "        dataset = TensorDataset(features_tensor, targets_tensor)\n",
    "    else:\n",
    "        dataset = TensorDataset(features_tensor)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1422638, 1, 11])\n"
     ]
    }
   ],
   "source": [
    "# train_loader = load_dataset(f'{inputdir}train_dataset_constructed.csv', feature_names)\n",
    "eval_loader = load_dataset(f'{inputdir}eval_dataset_constructed.csv', feature_names)\n",
    "# test_loader = load_dataset(f'{inputdir}test_dataset_constructed_x.csv', feature_names, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers  \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 取最后一个时间步\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "True\n",
      "True\n",
      "Epoch 62, Batch 1, Loss: 169.9705047607422\n",
      "Epoch 62, Batch 2, Loss: 169.2726287841797\n",
      "Epoch 62, Batch 3, Loss: 189.20648193359375\n",
      "Epoch 62, Batch 4, Loss: 190.93142700195312\n",
      "Epoch 62, Batch 5, Loss: 159.04183959960938\n",
      "Epoch 62, Batch 6, Loss: 185.5626220703125\n",
      "Epoch 62, Batch 7, Loss: 157.0928955078125\n",
      "Epoch 62, Batch 8, Loss: 185.16195678710938\n",
      "Epoch 62, Batch 9, Loss: 167.30276489257812\n",
      "Epoch 62, Batch 10, Loss: 180.39268493652344\n",
      "Epoch 62, Batch 11, Loss: 178.6160125732422\n",
      "Epoch 62, Batch 12, Loss: 169.9308624267578\n",
      "Epoch 62, Batch 13, Loss: 176.42127990722656\n",
      "Epoch 62, Batch 14, Loss: 180.1195526123047\n",
      "Epoch 62, Batch 15, Loss: 177.68218994140625\n",
      "Epoch 62, Batch 16, Loss: 174.66014099121094\n",
      "Epoch 62, Batch 17, Loss: 179.386962890625\n",
      "Epoch 62, Batch 18, Loss: 176.37574768066406\n",
      "Epoch 62, Batch 19, Loss: 177.43934631347656\n",
      "Epoch 62, Batch 20, Loss: 180.46414184570312\n",
      "Epoch 62, Batch 21, Loss: 193.04234313964844\n",
      "Epoch 62, Batch 22, Loss: 175.88258361816406\n",
      "Epoch 62, Batch 23, Loss: 167.58448791503906\n",
      "Epoch 62, Batch 24, Loss: 174.17391967773438\n",
      "Epoch 62, Batch 25, Loss: 176.54507446289062\n",
      "Epoch 62, Batch 26, Loss: 175.12142944335938\n",
      "Epoch 62, Batch 27, Loss: 187.36399841308594\n",
      "Epoch 62, Batch 28, Loss: 170.85801696777344\n",
      "Epoch 62, Batch 29, Loss: 174.73365783691406\n",
      "Epoch 62, Batch 30, Loss: 158.19354248046875\n",
      "Epoch 62, Batch 31, Loss: 168.64480590820312\n",
      "Epoch 62, Batch 32, Loss: 179.3831329345703\n",
      "Epoch 62, Batch 33, Loss: 179.55323791503906\n",
      "Epoch 62, Batch 34, Loss: 176.326416015625\n",
      "Epoch 62, Batch 35, Loss: 183.13914489746094\n",
      "Epoch 62, Batch 36, Loss: 174.0870361328125\n",
      "Epoch 62, Batch 37, Loss: 173.91046142578125\n",
      "Epoch 62, Batch 38, Loss: 182.64813232421875\n",
      "Epoch 62, Batch 39, Loss: 165.42221069335938\n",
      "Epoch 62, Batch 40, Loss: 160.91592407226562\n",
      "Epoch 62, Batch 41, Loss: 185.28582763671875\n",
      "Epoch 62, Batch 42, Loss: 170.3827362060547\n",
      "Epoch 62, Batch 43, Loss: 176.22805786132812\n",
      "Epoch 62, Batch 44, Loss: 166.51571655273438\n",
      "Epoch 62, Batch 45, Loss: 166.5932159423828\n",
      "Epoch 62, Batch 46, Loss: 173.388427734375\n",
      "Epoch 62, Batch 47, Loss: 183.3614044189453\n",
      "Epoch 62, Batch 48, Loss: 189.5614013671875\n",
      "Epoch 62, Batch 49, Loss: 162.0233154296875\n",
      "Epoch 62, Batch 50, Loss: 171.7098388671875\n",
      "Epoch 62, Batch 51, Loss: 187.24705505371094\n",
      "Epoch 62, Batch 52, Loss: 196.9022216796875\n",
      "Epoch 62, Batch 53, Loss: 174.91615295410156\n",
      "Epoch 62, Batch 54, Loss: 183.99325561523438\n",
      "Epoch 62, Batch 55, Loss: 176.65835571289062\n",
      "Epoch 62, Batch 56, Loss: 170.9124755859375\n",
      "Epoch 62, Batch 57, Loss: 177.04293823242188\n",
      "Epoch 62, Batch 58, Loss: 184.63113403320312\n",
      "Epoch 62, Batch 59, Loss: 189.0422821044922\n",
      "Epoch 62, Batch 60, Loss: 172.80557250976562\n",
      "Epoch 62, Batch 61, Loss: 176.89869689941406\n",
      "Epoch 62, Batch 62, Loss: 175.96200561523438\n",
      "Epoch 62, Batch 63, Loss: 176.89291381835938\n",
      "Epoch 62, Batch 64, Loss: 171.18087768554688\n",
      "Epoch 62, Batch 65, Loss: 181.8445587158203\n",
      "Epoch 62, Batch 66, Loss: 176.46580505371094\n",
      "Epoch 62, Batch 67, Loss: 171.63648986816406\n",
      "Epoch 62, Batch 68, Loss: 181.29627990722656\n",
      "Epoch 62, Batch 69, Loss: 178.6522216796875\n",
      "Epoch 62, Batch 70, Loss: 191.9295654296875\n",
      "Epoch 62, Batch 71, Loss: 176.08306884765625\n",
      "Epoch 62, Batch 72, Loss: 177.52867126464844\n",
      "Epoch 62, Batch 73, Loss: 198.06394958496094\n",
      "Epoch 62, Batch 74, Loss: 192.24172973632812\n",
      "Epoch 62, Batch 75, Loss: 176.93881225585938\n",
      "Epoch 62, Batch 76, Loss: 170.37132263183594\n",
      "Epoch 62, Batch 77, Loss: 165.34507751464844\n",
      "Epoch 62, Batch 78, Loss: 172.06326293945312\n",
      "Epoch 62, Batch 79, Loss: 174.92227172851562\n",
      "Epoch 62, Batch 80, Loss: 192.86697387695312\n",
      "Epoch 62, Batch 81, Loss: 183.8837127685547\n",
      "Epoch 62, Batch 82, Loss: 179.59800720214844\n",
      "Epoch 62, Batch 83, Loss: 178.3754425048828\n",
      "Epoch 62, Batch 84, Loss: 181.1492919921875\n",
      "Epoch 62, Batch 85, Loss: 168.84567260742188\n",
      "Epoch 62, Batch 86, Loss: 166.92767333984375\n",
      "Epoch 62, Batch 87, Loss: 169.55958557128906\n",
      "Epoch 62, Batch 88, Loss: 172.47169494628906\n",
      "Epoch 62, Batch 89, Loss: 157.42178344726562\n",
      "Epoch 62, Batch 90, Loss: 178.94557189941406\n",
      "Epoch 62, Batch 91, Loss: 169.3904571533203\n",
      "Epoch 62, Batch 92, Loss: 157.8599853515625\n",
      "Epoch 62, Batch 93, Loss: 180.3313751220703\n",
      "Epoch 62, Batch 94, Loss: 172.0201873779297\n",
      "Epoch 62, Batch 95, Loss: 185.8409881591797\n",
      "Epoch 62, Batch 96, Loss: 177.7406768798828\n",
      "Epoch 62, Batch 97, Loss: 185.35972595214844\n",
      "Epoch 62, Batch 98, Loss: 164.14523315429688\n",
      "Epoch 62, Batch 99, Loss: 159.62916564941406\n",
      "Epoch 62, Batch 100, Loss: 191.6184539794922\n",
      "Epoch 62, Batch 101, Loss: 155.25347900390625\n",
      "Epoch 62, Batch 102, Loss: 171.8437957763672\n",
      "Epoch 62, Batch 103, Loss: 171.71151733398438\n",
      "Epoch 62, Batch 104, Loss: 170.38931274414062\n",
      "Epoch 62, Batch 105, Loss: 181.65228271484375\n",
      "Epoch 62, Batch 106, Loss: 173.1329803466797\n",
      "Epoch 62, Batch 107, Loss: 173.23248291015625\n",
      "Epoch 62, Batch 108, Loss: 176.68975830078125\n",
      "Epoch 62, Batch 109, Loss: 172.90052795410156\n",
      "Epoch 62, Batch 110, Loss: 188.30471801757812\n",
      "Epoch 62, Batch 111, Loss: 160.46861267089844\n",
      "Epoch 62, Batch 112, Loss: 182.96058654785156\n",
      "Epoch 62, Batch 113, Loss: 173.48187255859375\n",
      "Epoch 62, Batch 114, Loss: 187.88037109375\n",
      "Epoch 62, Batch 115, Loss: 171.98309326171875\n",
      "Epoch 62, Batch 116, Loss: 174.8104705810547\n",
      "Epoch 62, Batch 117, Loss: 161.96290588378906\n",
      "Epoch 62, Batch 118, Loss: 177.961669921875\n",
      "Epoch 62, Batch 119, Loss: 175.8801727294922\n",
      "Epoch 62, Batch 120, Loss: 165.36099243164062\n",
      "Epoch 62, Batch 121, Loss: 176.4066162109375\n",
      "Epoch 62, Batch 122, Loss: 177.52099609375\n",
      "Epoch 62, Batch 123, Loss: 167.94703674316406\n",
      "Epoch 62, Batch 124, Loss: 170.31585693359375\n",
      "Epoch 62, Batch 125, Loss: 175.22731018066406\n",
      "Epoch 62, Batch 126, Loss: 167.03448486328125\n",
      "Epoch 62, Batch 127, Loss: 163.37692260742188\n",
      "Epoch 62, Batch 128, Loss: 183.0338134765625\n",
      "Epoch 62, Batch 129, Loss: 171.3824005126953\n",
      "Epoch 62, Batch 130, Loss: 173.31259155273438\n",
      "Epoch 62, Batch 131, Loss: 171.81727600097656\n",
      "Epoch 62, Batch 132, Loss: 166.16659545898438\n",
      "Epoch 62, Batch 133, Loss: 195.3244171142578\n",
      "Epoch 62, Batch 134, Loss: 159.73878479003906\n",
      "Epoch 62, Batch 135, Loss: 170.10533142089844\n",
      "Epoch 62, Batch 136, Loss: 170.11294555664062\n",
      "Epoch 62, Batch 137, Loss: 179.0684814453125\n",
      "Epoch 62, Batch 138, Loss: 168.3399200439453\n",
      "Epoch 62, Batch 139, Loss: 163.88941955566406\n",
      "Epoch 62, Batch 140, Loss: 166.01754760742188\n",
      "Epoch 62, Batch 141, Loss: 162.5317840576172\n",
      "Epoch 62, Batch 142, Loss: 162.90037536621094\n",
      "Epoch 62, Batch 143, Loss: 171.70046997070312\n",
      "Epoch 62, Batch 144, Loss: 160.00015258789062\n",
      "Epoch 62, Batch 145, Loss: 181.2854766845703\n",
      "Epoch 62, Batch 146, Loss: 186.89556884765625\n",
      "Epoch 62, Batch 147, Loss: 174.07418823242188\n",
      "Epoch 62, Batch 148, Loss: 162.66213989257812\n",
      "Epoch 62, Batch 149, Loss: 167.86976623535156\n",
      "Epoch 62, Batch 150, Loss: 168.99745178222656\n",
      "Epoch 62, Batch 151, Loss: 175.15988159179688\n",
      "Epoch 62, Batch 152, Loss: 172.89599609375\n",
      "Epoch 62, Batch 153, Loss: 177.17929077148438\n",
      "Epoch 62, Batch 154, Loss: 175.70343017578125\n",
      "Epoch 62, Batch 155, Loss: 167.8124237060547\n",
      "Epoch 62, Batch 156, Loss: 170.82435607910156\n",
      "Epoch 62, Batch 157, Loss: 177.9190673828125\n",
      "Epoch 62, Batch 158, Loss: 178.818115234375\n",
      "Epoch 62, Batch 159, Loss: 167.29046630859375\n",
      "Epoch 62, Batch 160, Loss: 180.12196350097656\n",
      "Epoch 62, Batch 161, Loss: 165.95123291015625\n",
      "Epoch 62, Batch 162, Loss: 174.0927276611328\n",
      "Epoch 62, Batch 163, Loss: 189.01087951660156\n",
      "Epoch 62, Batch 164, Loss: 169.85926818847656\n",
      "Epoch 62, Batch 165, Loss: 189.8557586669922\n",
      "Epoch 62, Batch 166, Loss: 172.7545623779297\n",
      "Epoch 62, Batch 167, Loss: 166.7923126220703\n",
      "Epoch 62, Batch 168, Loss: 185.30836486816406\n",
      "Epoch 62, Batch 169, Loss: 176.08596801757812\n",
      "Epoch 62, Batch 170, Loss: 187.491455078125\n",
      "Epoch 62, Batch 171, Loss: 176.25631713867188\n",
      "Epoch 62, Batch 172, Loss: 162.5244598388672\n",
      "Epoch 62, Batch 173, Loss: 170.37841796875\n",
      "Epoch 62, Batch 174, Loss: 186.2295379638672\n",
      "Epoch 62, Batch 175, Loss: 163.40980529785156\n",
      "Epoch 62, Batch 176, Loss: 170.3242950439453\n",
      "Epoch 62, Batch 177, Loss: 178.6315155029297\n",
      "Epoch 62, Batch 178, Loss: 180.9241180419922\n",
      "Epoch 62, Batch 179, Loss: 188.069580078125\n",
      "Epoch 62, Batch 180, Loss: 165.5653076171875\n",
      "Epoch 62, Batch 181, Loss: 168.5591583251953\n",
      "Epoch 62, Batch 182, Loss: 157.40896606445312\n",
      "Epoch 62, Batch 183, Loss: 177.96084594726562\n",
      "Epoch 62, Batch 184, Loss: 176.7822723388672\n",
      "Epoch 62, Batch 185, Loss: 170.99301147460938\n",
      "Epoch 62, Batch 186, Loss: 179.59642028808594\n",
      "Epoch 62, Batch 187, Loss: 186.95147705078125\n",
      "Epoch 62, Batch 188, Loss: 182.4137725830078\n",
      "Epoch 62, Batch 189, Loss: 185.15760803222656\n",
      "Epoch 62, Batch 190, Loss: 178.78793334960938\n",
      "Epoch 62, Batch 191, Loss: 169.8456268310547\n",
      "Epoch 62, Batch 192, Loss: 148.66322326660156\n",
      "Epoch 62, Batch 193, Loss: 174.2406005859375\n",
      "Epoch 62, Batch 194, Loss: 167.3755645751953\n",
      "Epoch 62, Batch 195, Loss: 165.07708740234375\n",
      "Epoch 62, Batch 196, Loss: 166.1195831298828\n",
      "Epoch 62, Batch 197, Loss: 190.15699768066406\n",
      "Epoch 62, Batch 198, Loss: 182.4529266357422\n",
      "Epoch 62, Batch 199, Loss: 169.7741241455078\n",
      "Epoch 62, Batch 200, Loss: 170.82199096679688\n",
      "Epoch 62, Batch 201, Loss: 166.04864501953125\n",
      "Epoch 62, Batch 202, Loss: 177.93409729003906\n",
      "Epoch 62, Batch 203, Loss: 188.4705352783203\n",
      "Epoch 62, Batch 204, Loss: 169.72389221191406\n",
      "Epoch 62, Batch 205, Loss: 163.57701110839844\n",
      "Epoch 62, Batch 206, Loss: 182.96856689453125\n",
      "Epoch 62, Batch 207, Loss: 159.61619567871094\n",
      "Epoch 62, Batch 208, Loss: 182.6304168701172\n",
      "Epoch 62, Batch 209, Loss: 152.16720581054688\n",
      "Epoch 62, Batch 210, Loss: 176.43084716796875\n",
      "Epoch 62, Batch 211, Loss: 156.63153076171875\n",
      "Epoch 62, Batch 212, Loss: 165.71022033691406\n",
      "Epoch 62, Batch 213, Loss: 165.0791778564453\n",
      "Epoch 62, Batch 214, Loss: 180.7446746826172\n",
      "Epoch 62, Batch 215, Loss: 176.7738800048828\n",
      "Epoch 62, Batch 216, Loss: 165.07034301757812\n",
      "Epoch 62, Batch 217, Loss: 174.098388671875\n",
      "Epoch 62, Batch 218, Loss: 194.59686279296875\n",
      "Epoch 62, Batch 219, Loss: 177.73880004882812\n",
      "Epoch 62, Batch 220, Loss: 148.02325439453125\n",
      "Epoch 62, Batch 221, Loss: 172.0046844482422\n",
      "Epoch 62, Batch 222, Loss: 162.13177490234375\n",
      "Epoch 62, Batch 223, Loss: 157.41481018066406\n",
      "Epoch 62, Batch 224, Loss: 168.9705047607422\n",
      "Epoch 62, Batch 225, Loss: 160.3365478515625\n",
      "Epoch 62, Batch 226, Loss: 172.819580078125\n",
      "Epoch 62, Batch 227, Loss: 181.60499572753906\n",
      "Epoch 62, Batch 228, Loss: 173.18446350097656\n",
      "Epoch 62, Batch 229, Loss: 181.61151123046875\n",
      "Epoch 62, Batch 230, Loss: 191.614501953125\n",
      "Epoch 62, Batch 231, Loss: 179.59104919433594\n",
      "Epoch 62, Batch 232, Loss: 186.2584686279297\n",
      "Epoch 62, Batch 233, Loss: 162.2930908203125\n",
      "Epoch 62, Batch 234, Loss: 176.749267578125\n",
      "Epoch 62, Batch 235, Loss: 170.68753051757812\n",
      "Epoch 62, Batch 236, Loss: 155.80462646484375\n",
      "Epoch 62, Batch 237, Loss: 170.31649780273438\n",
      "Epoch 62, Batch 238, Loss: 167.68365478515625\n",
      "Epoch 62, Batch 239, Loss: 165.2639923095703\n",
      "Epoch 62, Batch 240, Loss: 173.55862426757812\n",
      "Epoch 62, Batch 241, Loss: 166.95677185058594\n",
      "Epoch 62, Batch 242, Loss: 185.09295654296875\n",
      "Epoch 62, Batch 243, Loss: 175.760009765625\n",
      "Epoch 62, Batch 244, Loss: 157.32162475585938\n",
      "Epoch 62, Batch 245, Loss: 185.32041931152344\n",
      "Epoch 62, Batch 246, Loss: 172.38186645507812\n",
      "Epoch 62, Batch 247, Loss: 189.95932006835938\n",
      "Epoch 62, Batch 248, Loss: 174.45071411132812\n",
      "Epoch 62, Batch 249, Loss: 156.06900024414062\n",
      "Epoch 62, Batch 250, Loss: 155.604248046875\n",
      "Epoch 62, Batch 251, Loss: 177.0\n",
      "Epoch 62, Batch 252, Loss: 170.52365112304688\n",
      "Epoch 62, Batch 253, Loss: 159.6333465576172\n",
      "Epoch 62, Batch 254, Loss: 171.12937927246094\n",
      "Epoch 62, Batch 255, Loss: 171.4037628173828\n",
      "Epoch 62, Batch 256, Loss: 167.32553100585938\n",
      "Epoch 62, Batch 257, Loss: 171.62448120117188\n",
      "Epoch 62, Batch 258, Loss: 150.9341583251953\n",
      "Epoch 62, Batch 259, Loss: 169.78123474121094\n",
      "Epoch 62, Batch 260, Loss: 167.2124786376953\n",
      "Epoch 62, Batch 261, Loss: 161.84393310546875\n",
      "Epoch 62, Batch 262, Loss: 156.0525360107422\n",
      "Epoch 62, Batch 263, Loss: 175.93592834472656\n",
      "Epoch 62, Batch 264, Loss: 167.66165161132812\n",
      "Epoch 62, Batch 265, Loss: 168.36163330078125\n",
      "Epoch 62, Batch 266, Loss: 190.2069091796875\n",
      "Epoch 62, Batch 267, Loss: 162.5411376953125\n",
      "Epoch 62, Batch 268, Loss: 169.7830352783203\n",
      "Epoch 62, Batch 269, Loss: 187.03091430664062\n",
      "Epoch 62, Batch 270, Loss: 173.63523864746094\n",
      "Epoch 62, Batch 271, Loss: 181.59559631347656\n",
      "Epoch 62, Batch 272, Loss: 165.4073944091797\n",
      "Epoch 62, Batch 273, Loss: 193.1636505126953\n",
      "Epoch 62, Batch 274, Loss: 149.6116180419922\n",
      "Epoch 62, Batch 275, Loss: 162.66078186035156\n",
      "Epoch 62, Batch 276, Loss: 173.20809936523438\n",
      "Epoch 62, Batch 277, Loss: 181.56971740722656\n",
      "Epoch 62, Batch 278, Loss: 166.59127807617188\n",
      "Epoch 62, Batch 279, Loss: 168.36630249023438\n",
      "Epoch 62, Batch 280, Loss: 180.10061645507812\n",
      "Epoch 62, Batch 281, Loss: 175.83367919921875\n",
      "Epoch 62, Batch 282, Loss: 177.16001892089844\n",
      "Epoch 62, Batch 283, Loss: 161.42318725585938\n",
      "Epoch 62, Batch 284, Loss: 167.63861083984375\n",
      "Epoch 62, Batch 285, Loss: 162.06338500976562\n",
      "Epoch 62, Batch 286, Loss: 162.47479248046875\n",
      "Epoch 62, Batch 287, Loss: 179.1701202392578\n",
      "Epoch 62, Batch 288, Loss: 184.67666625976562\n",
      "Epoch 62, Batch 289, Loss: 198.7288818359375\n",
      "Epoch 62, Batch 290, Loss: 182.65206909179688\n",
      "Epoch 62, Batch 291, Loss: 177.94200134277344\n",
      "Epoch 62, Batch 292, Loss: 164.2236785888672\n",
      "Epoch 62, Batch 293, Loss: 167.24813842773438\n",
      "Epoch 62, Batch 294, Loss: 178.12901306152344\n",
      "Epoch 62, Batch 295, Loss: 175.1711883544922\n",
      "Epoch 62, Batch 296, Loss: 172.9386749267578\n",
      "Epoch 62, Batch 297, Loss: 176.79864501953125\n",
      "Epoch 62, Batch 298, Loss: 170.30250549316406\n",
      "Epoch 62, Batch 299, Loss: 147.52049255371094\n",
      "Epoch 62, Batch 300, Loss: 171.64356994628906\n",
      "Epoch 62, Batch 301, Loss: 166.9135284423828\n",
      "Epoch 62, Batch 302, Loss: 174.21420288085938\n",
      "Epoch 62, Batch 303, Loss: 170.09674072265625\n",
      "Epoch 62, Batch 304, Loss: 175.0753631591797\n",
      "Epoch 62, Batch 305, Loss: 176.77044677734375\n",
      "Epoch 62, Batch 306, Loss: 186.85678100585938\n",
      "Epoch 62, Batch 307, Loss: 181.14556884765625\n",
      "Epoch 62, Batch 308, Loss: 172.70945739746094\n",
      "Epoch 62, Batch 309, Loss: 182.01315307617188\n",
      "Epoch 62, Batch 310, Loss: 170.93109130859375\n",
      "Epoch 62, Batch 311, Loss: 166.90582275390625\n",
      "Epoch 62, Batch 312, Loss: 165.35218811035156\n",
      "Epoch 62, Batch 313, Loss: 171.52127075195312\n",
      "Epoch 62, Batch 314, Loss: 174.80284118652344\n",
      "Epoch 62, Batch 315, Loss: 164.3961944580078\n",
      "Epoch 62, Batch 316, Loss: 165.7895965576172\n",
      "Epoch 62, Batch 317, Loss: 182.0225830078125\n",
      "Epoch 62, Batch 318, Loss: 174.1889190673828\n",
      "Epoch 62, Batch 319, Loss: 171.1634979248047\n",
      "Epoch 62, Batch 320, Loss: 177.37648010253906\n",
      "Epoch 62, Batch 321, Loss: 174.61062622070312\n",
      "Epoch 62, Batch 322, Loss: 178.02874755859375\n",
      "Epoch 62, Batch 323, Loss: 172.60879516601562\n",
      "Epoch 62, Batch 324, Loss: 160.11854553222656\n",
      "Epoch 62, Batch 325, Loss: 175.39566040039062\n",
      "Epoch 62, Batch 326, Loss: 166.935302734375\n",
      "Epoch 62, Batch 327, Loss: 177.75216674804688\n",
      "Epoch 62, Batch 328, Loss: 174.9241485595703\n",
      "Epoch 62, Batch 329, Loss: 191.1668701171875\n",
      "Epoch 62, Batch 330, Loss: 169.43759155273438\n",
      "Epoch 62, Batch 331, Loss: 166.98716735839844\n",
      "Epoch 62, Batch 332, Loss: 181.1416778564453\n",
      "Epoch 62, Batch 333, Loss: 189.33724975585938\n",
      "Epoch 62, Batch 334, Loss: 182.22210693359375\n",
      "Epoch 62, Batch 335, Loss: 153.68373107910156\n",
      "Epoch 62, Batch 336, Loss: 174.84439086914062\n",
      "Epoch 62, Batch 337, Loss: 187.34310913085938\n",
      "Epoch 62, Batch 338, Loss: 162.66510009765625\n",
      "Epoch 62, Batch 339, Loss: 176.31671142578125\n",
      "Epoch 62, Batch 340, Loss: 160.6386260986328\n",
      "Epoch 62, Batch 341, Loss: 182.2536163330078\n",
      "Epoch 62, Batch 342, Loss: 166.81393432617188\n",
      "Epoch 62, Batch 343, Loss: 161.60629272460938\n",
      "Epoch 62, Batch 344, Loss: 156.93470764160156\n",
      "Epoch 62, Batch 345, Loss: 180.50384521484375\n",
      "Epoch 62, Batch 346, Loss: 180.8888702392578\n",
      "Epoch 62, Batch 347, Loss: 176.1360321044922\n",
      "Epoch 62, Batch 348, Loss: 167.98171997070312\n",
      "Epoch 62, Batch 349, Loss: 154.2998809814453\n",
      "Epoch 62, Batch 350, Loss: 184.13584899902344\n",
      "Epoch 62, Batch 351, Loss: 191.06275939941406\n",
      "Epoch 62, Batch 352, Loss: 167.08021545410156\n",
      "Epoch 62, Batch 353, Loss: 175.42645263671875\n",
      "Epoch 62, Batch 354, Loss: 183.4894256591797\n",
      "Epoch 62, Batch 355, Loss: 172.4394073486328\n",
      "Epoch 62, Batch 356, Loss: 175.75946044921875\n",
      "Epoch 62, Batch 357, Loss: 188.1548309326172\n",
      "Epoch 62, Batch 358, Loss: 167.12245178222656\n",
      "Epoch 62, Batch 359, Loss: 176.77212524414062\n",
      "Epoch 62, Batch 360, Loss: 176.41912841796875\n",
      "Epoch 62, Batch 361, Loss: 200.3035430908203\n",
      "Epoch 62, Batch 362, Loss: 160.62429809570312\n",
      "Epoch 62, Batch 363, Loss: 174.6480255126953\n",
      "Epoch 62, Batch 364, Loss: 172.59262084960938\n",
      "Epoch 62, Batch 365, Loss: 165.48634338378906\n",
      "Epoch 62, Batch 366, Loss: 173.3663787841797\n",
      "Epoch 62, Batch 367, Loss: 174.85751342773438\n",
      "Epoch 62, Batch 368, Loss: 157.66883850097656\n",
      "Epoch 62, Batch 369, Loss: 177.542236328125\n",
      "Epoch 62, Batch 370, Loss: 192.7032470703125\n",
      "Epoch 62, Batch 371, Loss: 178.1290283203125\n",
      "Epoch 62, Batch 372, Loss: 176.9593048095703\n",
      "Epoch 62, Batch 373, Loss: 170.50535583496094\n",
      "Epoch 62, Batch 374, Loss: 183.07485961914062\n",
      "Epoch 62, Batch 375, Loss: 161.84510803222656\n",
      "Epoch 62, Batch 376, Loss: 177.09381103515625\n",
      "Epoch 62, Batch 377, Loss: 172.47842407226562\n",
      "Epoch 62, Batch 378, Loss: 158.95298767089844\n",
      "Epoch 62, Batch 379, Loss: 179.23020935058594\n",
      "Epoch 62, Batch 380, Loss: 158.52093505859375\n",
      "Epoch 62, Batch 381, Loss: 174.70120239257812\n",
      "Epoch 62, Batch 382, Loss: 173.912109375\n",
      "Epoch 62, Batch 383, Loss: 184.01817321777344\n",
      "Epoch 62, Batch 384, Loss: 174.79678344726562\n",
      "Epoch 62, Batch 385, Loss: 163.27786254882812\n",
      "Epoch 62, Batch 386, Loss: 168.5952911376953\n",
      "Epoch 62, Batch 387, Loss: 159.45608520507812\n",
      "Epoch 62, Batch 388, Loss: 176.2476806640625\n",
      "Epoch 62, Batch 389, Loss: 177.73583984375\n",
      "Epoch 62, Batch 390, Loss: 177.45761108398438\n",
      "Epoch 62, Batch 391, Loss: 208.52430725097656\n",
      "Epoch 62, Batch 392, Loss: 184.73880004882812\n",
      "Epoch 62, Batch 393, Loss: 161.69219970703125\n",
      "Epoch 62, Batch 394, Loss: 182.40789794921875\n",
      "Epoch 62, Batch 395, Loss: 163.4266357421875\n",
      "Epoch 62, Batch 396, Loss: 167.49351501464844\n",
      "Epoch 62, Batch 397, Loss: 178.6405487060547\n",
      "Epoch 62, Batch 398, Loss: 164.22488403320312\n",
      "Epoch 62, Batch 399, Loss: 193.2898406982422\n",
      "Epoch 62, Batch 400, Loss: 187.61557006835938\n",
      "Epoch 62, Batch 401, Loss: 165.52027893066406\n",
      "Epoch 62, Batch 402, Loss: 178.33763122558594\n",
      "Epoch 62, Batch 403, Loss: 192.70970153808594\n",
      "Epoch 62, Batch 404, Loss: 167.9111328125\n",
      "Epoch 62, Batch 405, Loss: 174.74002075195312\n",
      "Epoch 62, Batch 406, Loss: 172.2458953857422\n",
      "Epoch 62, Batch 407, Loss: 154.65634155273438\n",
      "Epoch 62, Batch 408, Loss: 194.09176635742188\n",
      "Epoch 62, Batch 409, Loss: 173.14208984375\n",
      "Epoch 62, Batch 410, Loss: 187.37384033203125\n",
      "Epoch 62, Batch 411, Loss: 157.42137145996094\n",
      "Epoch 62, Batch 412, Loss: 179.31678771972656\n",
      "Epoch 62, Batch 413, Loss: 175.60238647460938\n",
      "Epoch 62, Batch 414, Loss: 158.6019287109375\n",
      "Epoch 62, Batch 415, Loss: 170.77928161621094\n",
      "Epoch 62, Batch 416, Loss: 165.6360321044922\n",
      "Epoch 62, Batch 417, Loss: 175.56813049316406\n",
      "Epoch 62, Batch 418, Loss: 174.36544799804688\n",
      "Epoch 62, Batch 419, Loss: 170.1234588623047\n",
      "Epoch 62, Batch 420, Loss: 179.8909149169922\n",
      "Epoch 62, Batch 421, Loss: 173.2401580810547\n",
      "Epoch 62, Batch 422, Loss: 177.5946502685547\n",
      "Epoch 62, Batch 423, Loss: 168.96734619140625\n",
      "Epoch 62, Batch 424, Loss: 172.46749877929688\n",
      "Epoch 62, Batch 425, Loss: 156.2738037109375\n",
      "Epoch 62, Batch 426, Loss: 171.1634979248047\n",
      "Epoch 62, Batch 427, Loss: 169.58082580566406\n",
      "Epoch 62, Batch 428, Loss: 180.6773223876953\n",
      "Epoch 62, Batch 429, Loss: 172.02125549316406\n",
      "Epoch 62, Batch 430, Loss: 159.55445861816406\n",
      "Epoch 62, Batch 431, Loss: 173.49822998046875\n",
      "Epoch 62, Batch 432, Loss: 189.1592559814453\n",
      "Epoch 62, Batch 433, Loss: 183.65576171875\n",
      "Epoch 62, Batch 434, Loss: 164.74563598632812\n",
      "Epoch 62, Batch 435, Loss: 168.31503295898438\n",
      "Epoch 62, Batch 436, Loss: 166.73516845703125\n",
      "Epoch 62, Batch 437, Loss: 177.91049194335938\n",
      "Epoch 62, Batch 438, Loss: 176.32273864746094\n",
      "Epoch 62, Batch 439, Loss: 174.1964874267578\n",
      "Epoch 62, Batch 440, Loss: 183.5343017578125\n",
      "Epoch 62, Batch 441, Loss: 171.10250854492188\n",
      "Epoch 62, Batch 442, Loss: 173.88369750976562\n",
      "Epoch 62, Batch 443, Loss: 169.89218139648438\n",
      "Epoch 62, Batch 444, Loss: 168.9599609375\n",
      "Epoch 62, Batch 445, Loss: 196.17764282226562\n",
      "Epoch 62, Batch 446, Loss: 157.3494873046875\n",
      "Epoch 62, Batch 447, Loss: 167.58013916015625\n",
      "Epoch 62, Batch 448, Loss: 179.20465087890625\n",
      "Epoch 62, Batch 449, Loss: 190.25283813476562\n",
      "Epoch 62, Batch 450, Loss: 165.02467346191406\n",
      "Epoch 62, Batch 451, Loss: 172.79998779296875\n",
      "Epoch 62, Batch 452, Loss: 164.0282440185547\n",
      "Epoch 62, Batch 453, Loss: 168.87026977539062\n",
      "Epoch 62, Batch 454, Loss: 170.28968811035156\n",
      "Epoch 62, Batch 455, Loss: 188.5852813720703\n",
      "Epoch 62, Batch 456, Loss: 160.6760711669922\n",
      "Epoch 62, Batch 457, Loss: 179.4026336669922\n",
      "Epoch 62, Batch 458, Loss: 163.18408203125\n",
      "Epoch 62, Batch 459, Loss: 179.75653076171875\n",
      "Epoch 62, Batch 460, Loss: 174.84725952148438\n",
      "Epoch 62, Batch 461, Loss: 175.6174774169922\n",
      "Epoch 62, Batch 462, Loss: 170.21714782714844\n",
      "Epoch 62, Batch 463, Loss: 177.75930786132812\n",
      "Epoch 62, Batch 464, Loss: 173.10911560058594\n",
      "Epoch 62, Batch 465, Loss: 169.71792602539062\n",
      "Epoch 62, Batch 466, Loss: 176.08782958984375\n",
      "Epoch 62, Batch 467, Loss: 164.44264221191406\n",
      "Epoch 62, Batch 468, Loss: 180.6886444091797\n",
      "Epoch 62, Batch 469, Loss: 177.40167236328125\n",
      "Epoch 62, Batch 470, Loss: 165.74012756347656\n",
      "Epoch 62, Batch 471, Loss: 176.70266723632812\n",
      "Epoch 62, Batch 472, Loss: 164.45416259765625\n",
      "Epoch 62, Batch 473, Loss: 167.78651428222656\n",
      "Epoch 62, Batch 474, Loss: 164.4051055908203\n",
      "Epoch 62, Batch 475, Loss: 163.46229553222656\n",
      "Epoch 62, Batch 476, Loss: 169.74314880371094\n",
      "Epoch 62, Batch 477, Loss: 177.86825561523438\n",
      "Epoch 62, Batch 478, Loss: 164.2689208984375\n",
      "Epoch 62, Batch 479, Loss: 195.96768188476562\n",
      "Epoch 62, Batch 480, Loss: 183.5795440673828\n",
      "Epoch 62, Batch 481, Loss: 174.9296417236328\n",
      "Epoch 62, Batch 482, Loss: 183.30628967285156\n",
      "Epoch 62, Batch 483, Loss: 178.32347106933594\n",
      "Epoch 62, Batch 484, Loss: 177.87255859375\n",
      "Epoch 62, Batch 485, Loss: 167.87762451171875\n",
      "Epoch 62, Batch 486, Loss: 179.07498168945312\n",
      "Epoch 62, Batch 487, Loss: 170.82432556152344\n",
      "Epoch 62, Batch 488, Loss: 189.21151733398438\n",
      "Epoch 62, Batch 489, Loss: 166.84483337402344\n",
      "Epoch 62, Batch 490, Loss: 178.8549346923828\n",
      "Epoch 62, Batch 491, Loss: 172.97470092773438\n",
      "Epoch 62, Batch 492, Loss: 176.48912048339844\n",
      "Epoch 62, Batch 493, Loss: 154.61180114746094\n",
      "Epoch 62, Batch 494, Loss: 171.4508056640625\n",
      "Epoch 62, Batch 495, Loss: 181.9156494140625\n",
      "Epoch 62, Batch 496, Loss: 183.38221740722656\n",
      "Epoch 62, Batch 497, Loss: 180.05072021484375\n",
      "Epoch 62, Batch 498, Loss: 169.19329833984375\n",
      "Epoch 62, Batch 499, Loss: 175.46678161621094\n",
      "Epoch 62, Batch 500, Loss: 160.02041625976562\n",
      "Epoch 62, Batch 501, Loss: 181.2008056640625\n",
      "Epoch 62, Batch 502, Loss: 152.57667541503906\n",
      "Epoch 62, Batch 503, Loss: 190.2536163330078\n",
      "Epoch 62, Batch 504, Loss: 152.0007781982422\n",
      "Epoch 62, Batch 505, Loss: 185.46731567382812\n",
      "Epoch 62, Batch 506, Loss: 177.48350524902344\n",
      "Epoch 62, Batch 507, Loss: 168.53271484375\n",
      "Epoch 62, Batch 508, Loss: 173.986328125\n",
      "Epoch 62, Batch 509, Loss: 167.97923278808594\n",
      "Epoch 62, Batch 510, Loss: 183.73895263671875\n",
      "Epoch 62, Batch 511, Loss: 184.4899139404297\n",
      "Epoch 62, Batch 512, Loss: 179.53811645507812\n",
      "Epoch 62, Batch 513, Loss: 161.45285034179688\n",
      "Epoch 62, Batch 514, Loss: 161.41468811035156\n",
      "Epoch 62, Batch 515, Loss: 162.1697998046875\n",
      "Epoch 62, Batch 516, Loss: 156.28306579589844\n",
      "Epoch 62, Batch 517, Loss: 190.9295196533203\n",
      "Epoch 62, Batch 518, Loss: 163.24954223632812\n",
      "Epoch 62, Batch 519, Loss: 196.57876586914062\n",
      "Epoch 62, Batch 520, Loss: 171.3577880859375\n",
      "Epoch 62, Batch 521, Loss: 165.32351684570312\n",
      "Epoch 62, Batch 522, Loss: 154.70924377441406\n",
      "Epoch 62, Batch 523, Loss: 178.31478881835938\n",
      "Epoch 62, Batch 524, Loss: 197.53826904296875\n",
      "Epoch 62, Batch 525, Loss: 165.71713256835938\n",
      "Epoch 62, Batch 526, Loss: 164.824951171875\n",
      "Epoch 62, Batch 527, Loss: 188.50074768066406\n",
      "Epoch 62, Batch 528, Loss: 157.88392639160156\n",
      "Epoch 62, Batch 529, Loss: 180.193115234375\n",
      "Epoch 62, Batch 530, Loss: 177.20762634277344\n",
      "Epoch 62, Batch 531, Loss: 187.5478057861328\n",
      "Epoch 62, Batch 532, Loss: 174.6492919921875\n",
      "Epoch 62, Batch 533, Loss: 173.0293731689453\n",
      "Epoch 62, Batch 534, Loss: 180.37216186523438\n",
      "Epoch 62, Batch 535, Loss: 172.6988067626953\n",
      "Epoch 62, Batch 536, Loss: 167.4768524169922\n",
      "Epoch 62, Batch 537, Loss: 155.194580078125\n",
      "Epoch 62, Batch 538, Loss: 154.1250762939453\n",
      "Epoch 62, Batch 539, Loss: 166.49783325195312\n",
      "Epoch 62, Batch 540, Loss: 176.66921997070312\n",
      "Epoch 62, Batch 541, Loss: 183.7916717529297\n",
      "Epoch 62, Batch 542, Loss: 177.3301544189453\n",
      "Epoch 62, Batch 543, Loss: 180.98667907714844\n",
      "Epoch 62, Batch 544, Loss: 164.9158172607422\n",
      "Epoch 62, Batch 545, Loss: 176.91175842285156\n",
      "Epoch 62, Batch 546, Loss: 163.68502807617188\n",
      "Epoch 62, Batch 547, Loss: 165.69049072265625\n",
      "Epoch 62, Batch 548, Loss: 176.1075439453125\n",
      "Epoch 62, Batch 549, Loss: 179.85166931152344\n",
      "Epoch 62, Batch 550, Loss: 169.36842346191406\n",
      "Epoch 62, Batch 551, Loss: 172.9313507080078\n",
      "Epoch 62, Batch 552, Loss: 170.0570831298828\n",
      "Epoch 62, Batch 553, Loss: 171.59243774414062\n",
      "Epoch 62, Batch 554, Loss: 172.32542419433594\n",
      "Epoch 62, Batch 555, Loss: 161.3276824951172\n",
      "Epoch 62, Batch 556, Loss: 189.08517456054688\n",
      "Epoch 62, Batch 557, Loss: 192.40982055664062\n",
      "Epoch 62, Batch 558, Loss: 166.94207763671875\n",
      "Epoch 62, Batch 559, Loss: 155.80572509765625\n",
      "Epoch 62, Batch 560, Loss: 169.36331176757812\n",
      "Epoch 62, Batch 561, Loss: 163.7050323486328\n",
      "Epoch 62, Batch 562, Loss: 182.3136749267578\n",
      "Epoch 62, Batch 563, Loss: 188.34295654296875\n",
      "Epoch 62, Batch 564, Loss: 186.44085693359375\n",
      "Epoch 62, Batch 565, Loss: 167.88909912109375\n",
      "Epoch 62, Batch 566, Loss: 150.71499633789062\n",
      "Epoch 62, Batch 567, Loss: 153.64080810546875\n",
      "Epoch 62, Batch 568, Loss: 177.3682403564453\n",
      "Epoch 62, Batch 569, Loss: 183.83224487304688\n",
      "Epoch 62, Batch 570, Loss: 169.80230712890625\n",
      "Epoch 62, Batch 571, Loss: 203.47927856445312\n",
      "Epoch 62, Batch 572, Loss: 171.4892578125\n",
      "Epoch 62, Batch 573, Loss: 169.93865966796875\n",
      "Epoch 62, Batch 574, Loss: 160.20106506347656\n",
      "Epoch 62, Batch 575, Loss: 188.93310546875\n",
      "Epoch 62, Batch 576, Loss: 173.96414184570312\n",
      "Epoch 62, Batch 577, Loss: 172.68190002441406\n",
      "Epoch 62, Batch 578, Loss: 176.2329864501953\n",
      "Epoch 62, Batch 579, Loss: 174.43569946289062\n",
      "Epoch 62, Batch 580, Loss: 167.9924774169922\n",
      "Epoch 62, Batch 581, Loss: 179.92044067382812\n",
      "Epoch 62, Batch 582, Loss: 166.2652130126953\n",
      "Epoch 62, Batch 583, Loss: 176.9700164794922\n",
      "Epoch 62, Batch 584, Loss: 172.270751953125\n",
      "Epoch 62, Batch 585, Loss: 194.80172729492188\n",
      "Epoch 62, Batch 586, Loss: 176.15589904785156\n",
      "Epoch 62, Batch 587, Loss: 153.1572265625\n",
      "Epoch 62, Batch 588, Loss: 181.8142852783203\n",
      "Epoch 62, Batch 589, Loss: 161.84364318847656\n",
      "Epoch 62, Batch 590, Loss: 169.32398986816406\n",
      "Epoch 62, Batch 591, Loss: 176.2588348388672\n",
      "Epoch 62, Batch 592, Loss: 166.2160186767578\n",
      "Epoch 62, Batch 593, Loss: 180.14471435546875\n",
      "Epoch 62, Batch 594, Loss: 172.1710968017578\n",
      "Epoch 62, Batch 595, Loss: 169.25877380371094\n",
      "Epoch 62, Batch 596, Loss: 166.96913146972656\n",
      "Epoch 62, Batch 597, Loss: 180.03941345214844\n",
      "Epoch 62, Batch 598, Loss: 174.06321716308594\n",
      "Epoch 62, Batch 599, Loss: 172.68875122070312\n",
      "Epoch 62, Batch 600, Loss: 185.44308471679688\n",
      "Epoch 62, Batch 601, Loss: 186.1312255859375\n",
      "Epoch 62, Batch 602, Loss: 175.97984313964844\n",
      "Epoch 62, Batch 603, Loss: 164.05419921875\n",
      "Epoch 62, Batch 604, Loss: 181.8433837890625\n",
      "Epoch 62, Batch 605, Loss: 146.33958435058594\n",
      "Epoch 62, Batch 606, Loss: 174.86866760253906\n",
      "Epoch 62, Batch 607, Loss: 170.54977416992188\n",
      "Epoch 62, Batch 608, Loss: 175.31724548339844\n",
      "Epoch 62, Batch 609, Loss: 170.1943359375\n",
      "Epoch 62, Batch 610, Loss: 176.08212280273438\n",
      "Epoch 62, Batch 611, Loss: 162.372314453125\n",
      "Epoch 62, Batch 612, Loss: 186.12799072265625\n",
      "Epoch 62, Batch 613, Loss: 182.05003356933594\n",
      "Epoch 62, Batch 614, Loss: 167.4830780029297\n",
      "Epoch 62, Batch 615, Loss: 176.13711547851562\n",
      "Epoch 62, Batch 616, Loss: 176.54771423339844\n",
      "Epoch 62, Batch 617, Loss: 168.36619567871094\n",
      "Epoch 62, Batch 618, Loss: 167.29031372070312\n",
      "Epoch 62, Batch 619, Loss: 181.9781494140625\n",
      "Epoch 62, Batch 620, Loss: 162.74449157714844\n",
      "Epoch 62, Batch 621, Loss: 170.05563354492188\n",
      "Epoch 62, Batch 622, Loss: 182.70956420898438\n",
      "Epoch 62, Batch 623, Loss: 160.41049194335938\n",
      "Epoch 62, Batch 624, Loss: 177.81863403320312\n",
      "Epoch 62, Batch 625, Loss: 188.18142700195312\n",
      "Epoch 62, Batch 626, Loss: 173.1186065673828\n",
      "Epoch 62, Batch 627, Loss: 173.74978637695312\n",
      "Epoch 62, Batch 628, Loss: 145.4638671875\n",
      "Epoch 62, Batch 629, Loss: 166.21066284179688\n",
      "Epoch 62, Batch 630, Loss: 175.0531463623047\n",
      "Epoch 62, Batch 631, Loss: 176.2829132080078\n",
      "Epoch 62, Batch 632, Loss: 156.5421600341797\n",
      "Epoch 62, Batch 633, Loss: 173.5561981201172\n",
      "Epoch 62, Batch 634, Loss: 161.16542053222656\n",
      "Epoch 62, Batch 635, Loss: 167.75341796875\n",
      "Epoch 62, Batch 636, Loss: 187.77597045898438\n",
      "Epoch 62, Batch 637, Loss: 172.47850036621094\n",
      "Epoch 62, Batch 638, Loss: 160.26321411132812\n",
      "Epoch 62, Batch 639, Loss: 166.0320281982422\n",
      "Epoch 62, Batch 640, Loss: 173.98121643066406\n",
      "Epoch 62, Batch 641, Loss: 176.82960510253906\n",
      "Epoch 62, Batch 642, Loss: 175.7278289794922\n",
      "Epoch 62, Batch 643, Loss: 161.43511962890625\n",
      "Epoch 62, Batch 644, Loss: 174.6334686279297\n",
      "Epoch 62, Batch 645, Loss: 181.13052368164062\n",
      "Epoch 62, Batch 646, Loss: 174.6630859375\n",
      "Epoch 62, Batch 647, Loss: 178.92881774902344\n",
      "Epoch 62, Batch 648, Loss: 189.1782989501953\n",
      "Epoch 62, Batch 649, Loss: 159.45127868652344\n",
      "Epoch 62, Batch 650, Loss: 162.56204223632812\n",
      "Epoch 62, Batch 651, Loss: 192.73670959472656\n",
      "Epoch 62, Batch 652, Loss: 167.20367431640625\n",
      "Epoch 62, Batch 653, Loss: 164.551513671875\n",
      "Epoch 62, Batch 654, Loss: 162.61038208007812\n",
      "Epoch 62, Batch 655, Loss: 186.29200744628906\n",
      "Epoch 62, Batch 656, Loss: 185.9799041748047\n",
      "Epoch 62, Batch 657, Loss: 184.175048828125\n",
      "Epoch 62, Batch 658, Loss: 166.29843139648438\n",
      "Epoch 62, Batch 659, Loss: 177.26300048828125\n",
      "Epoch 62, Batch 660, Loss: 163.62799072265625\n",
      "Epoch 62, Batch 661, Loss: 168.6686248779297\n",
      "Epoch 62, Batch 662, Loss: 168.69808959960938\n",
      "Epoch 62, Batch 663, Loss: 164.38397216796875\n",
      "Epoch 62, Batch 664, Loss: 167.454833984375\n",
      "Epoch 62, Batch 665, Loss: 167.03738403320312\n",
      "Epoch 62, Batch 666, Loss: 172.6500244140625\n",
      "Epoch 62, Batch 667, Loss: 182.32557678222656\n",
      "Epoch 62, Batch 668, Loss: 172.8550567626953\n",
      "Epoch 62, Batch 669, Loss: 170.27577209472656\n",
      "Epoch 62, Batch 670, Loss: 172.84947204589844\n",
      "Epoch 62, Batch 671, Loss: 160.72654724121094\n",
      "Epoch 62, Batch 672, Loss: 184.62997436523438\n",
      "Epoch 62, Batch 673, Loss: 197.328369140625\n",
      "Epoch 62, Batch 674, Loss: 178.68780517578125\n",
      "Epoch 62, Batch 675, Loss: 172.05145263671875\n",
      "Epoch 62, Batch 676, Loss: 175.2473907470703\n",
      "Epoch 62, Batch 677, Loss: 155.32156372070312\n",
      "Epoch 62, Batch 678, Loss: 172.21954345703125\n",
      "Epoch 62, Batch 679, Loss: 188.26490783691406\n",
      "Epoch 62, Batch 680, Loss: 171.50901794433594\n",
      "Epoch 62, Batch 681, Loss: 172.8844757080078\n",
      "Epoch 62, Batch 682, Loss: 178.43087768554688\n",
      "Epoch 62, Batch 683, Loss: 198.28086853027344\n",
      "Epoch 62, Batch 684, Loss: 180.5994873046875\n",
      "Epoch 62, Batch 685, Loss: 177.43032836914062\n",
      "Epoch 62, Batch 686, Loss: 176.1106414794922\n",
      "Epoch 62, Batch 687, Loss: 171.52023315429688\n",
      "Epoch 62, Batch 688, Loss: 184.6427764892578\n",
      "Epoch 62, Batch 689, Loss: 183.40994262695312\n",
      "Epoch 62, Batch 690, Loss: 177.12620544433594\n",
      "Epoch 62, Batch 691, Loss: 175.48895263671875\n",
      "Epoch 62, Batch 692, Loss: 161.20806884765625\n",
      "Epoch 62, Batch 693, Loss: 172.39181518554688\n",
      "Epoch 62, Batch 694, Loss: 182.57139587402344\n",
      "Epoch 62, Batch 695, Loss: 165.3556365966797\n",
      "Epoch 62, Batch 696, Loss: 189.95684814453125\n",
      "Epoch 62, Batch 697, Loss: 168.57838439941406\n",
      "Epoch 62, Batch 698, Loss: 188.6957244873047\n",
      "Epoch 62, Batch 699, Loss: 176.06893920898438\n",
      "Epoch 62, Batch 700, Loss: 174.48245239257812\n",
      "Epoch 62, Batch 701, Loss: 171.52413940429688\n",
      "Epoch 62, Batch 702, Loss: 171.36607360839844\n",
      "Epoch 62, Batch 703, Loss: 186.37484741210938\n",
      "Epoch 62, Batch 704, Loss: 170.59925842285156\n",
      "Epoch 62, Batch 705, Loss: 177.2244110107422\n",
      "Epoch 62, Batch 706, Loss: 164.21983337402344\n",
      "Epoch 62, Batch 707, Loss: 164.68307495117188\n",
      "Epoch 62, Batch 708, Loss: 179.401123046875\n",
      "Epoch 62, Batch 709, Loss: 160.88363647460938\n",
      "Epoch 62, Batch 710, Loss: 164.8038330078125\n",
      "Epoch 62, Batch 711, Loss: 185.57374572753906\n",
      "Epoch 62, Batch 712, Loss: 168.9052276611328\n",
      "Epoch 62, Batch 713, Loss: 169.81031799316406\n",
      "Epoch 62, Batch 714, Loss: 177.46542358398438\n",
      "Epoch 62, Batch 715, Loss: 181.5152130126953\n",
      "Epoch 62, Batch 716, Loss: 185.50633239746094\n",
      "Epoch 62, Batch 717, Loss: 172.5352783203125\n",
      "Epoch 62, Batch 718, Loss: 171.128662109375\n",
      "Epoch 62, Batch 719, Loss: 184.79122924804688\n",
      "Epoch 62, Batch 720, Loss: 182.0882568359375\n",
      "Epoch 62, Batch 721, Loss: 165.57199096679688\n",
      "Epoch 62, Batch 722, Loss: 164.29562377929688\n",
      "Epoch 62, Batch 723, Loss: 182.50975036621094\n",
      "Epoch 62, Batch 724, Loss: 163.7047119140625\n",
      "Epoch 62, Batch 725, Loss: 165.11819458007812\n",
      "Epoch 62, Batch 726, Loss: 181.93975830078125\n",
      "Epoch 62, Batch 727, Loss: 181.01284790039062\n",
      "Epoch 62, Batch 728, Loss: 177.9234619140625\n",
      "Epoch 62, Batch 729, Loss: 173.24147033691406\n",
      "Epoch 62, Batch 730, Loss: 190.70751953125\n",
      "Epoch 62, Batch 731, Loss: 187.22735595703125\n",
      "Epoch 62, Batch 732, Loss: 183.58291625976562\n",
      "Epoch 62, Batch 733, Loss: 177.4336395263672\n",
      "Epoch 62, Batch 734, Loss: 184.5090789794922\n",
      "Epoch 62, Batch 735, Loss: 165.5678253173828\n",
      "Epoch 62, Batch 736, Loss: 172.22447204589844\n",
      "Epoch 62, Batch 737, Loss: 162.59185791015625\n",
      "Epoch 62, Batch 738, Loss: 180.50543212890625\n",
      "Epoch 62, Batch 739, Loss: 176.05511474609375\n",
      "Epoch 62, Batch 740, Loss: 178.06866455078125\n",
      "Epoch 62, Batch 741, Loss: 167.95816040039062\n",
      "Epoch 62, Batch 742, Loss: 178.2410430908203\n",
      "Epoch 62, Batch 743, Loss: 173.0991973876953\n",
      "Epoch 62, Batch 744, Loss: 158.70025634765625\n",
      "Epoch 62, Batch 745, Loss: 194.29920959472656\n",
      "Epoch 62, Batch 746, Loss: 159.72010803222656\n",
      "Epoch 62, Batch 747, Loss: 180.8509979248047\n",
      "Epoch 62, Batch 748, Loss: 169.69207763671875\n",
      "Epoch 62, Batch 749, Loss: 169.60760498046875\n",
      "Epoch 62, Batch 750, Loss: 174.06114196777344\n",
      "Epoch 62, Batch 751, Loss: 164.20401000976562\n",
      "Epoch 62, Batch 752, Loss: 185.6080322265625\n",
      "Epoch 62, Batch 753, Loss: 183.93414306640625\n",
      "Epoch 62, Batch 754, Loss: 166.6715850830078\n",
      "Epoch 62, Batch 755, Loss: 169.76487731933594\n",
      "Epoch 62, Batch 756, Loss: 180.74978637695312\n",
      "Epoch 62, Batch 757, Loss: 179.5447998046875\n",
      "Epoch 62, Batch 758, Loss: 177.7685089111328\n",
      "Epoch 62, Batch 759, Loss: 178.60012817382812\n",
      "Epoch 62, Batch 760, Loss: 167.95677185058594\n",
      "Epoch 62, Batch 761, Loss: 183.8523712158203\n",
      "Epoch 62, Batch 762, Loss: 180.5690155029297\n",
      "Epoch 62, Batch 763, Loss: 160.41580200195312\n",
      "Epoch 62, Batch 764, Loss: 167.96287536621094\n",
      "Epoch 62, Batch 765, Loss: 188.41851806640625\n",
      "Epoch 62, Batch 766, Loss: 179.5332794189453\n",
      "Epoch 62, Batch 767, Loss: 175.79676818847656\n",
      "Epoch 62, Batch 768, Loss: 165.4111785888672\n",
      "Epoch 62, Batch 769, Loss: 203.22337341308594\n",
      "Epoch 62, Batch 770, Loss: 173.17059326171875\n",
      "Epoch 62, Batch 771, Loss: 151.4172821044922\n",
      "Epoch 62, Batch 772, Loss: 161.7163848876953\n",
      "Epoch 62, Batch 773, Loss: 160.58094787597656\n",
      "Epoch 62, Batch 774, Loss: 159.2189178466797\n",
      "Epoch 62, Batch 775, Loss: 168.92193603515625\n",
      "Epoch 62, Batch 776, Loss: 184.19581604003906\n",
      "Epoch 62, Batch 777, Loss: 166.53697204589844\n",
      "Epoch 62, Batch 778, Loss: 174.82717895507812\n",
      "Epoch 62, Batch 779, Loss: 165.90536499023438\n",
      "Epoch 62, Batch 780, Loss: 180.60430908203125\n",
      "Epoch 62, Batch 781, Loss: 174.11642456054688\n",
      "Epoch 62, Batch 782, Loss: 178.00115966796875\n",
      "Epoch 62, Batch 783, Loss: 169.28306579589844\n",
      "Epoch 62, Batch 784, Loss: 182.9384002685547\n",
      "Epoch 62, Batch 785, Loss: 174.4829559326172\n",
      "Epoch 62, Batch 786, Loss: 166.60684204101562\n",
      "Epoch 62, Batch 787, Loss: 174.2291259765625\n",
      "Epoch 62, Batch 788, Loss: 192.78245544433594\n",
      "Epoch 62, Batch 789, Loss: 162.08790588378906\n",
      "Epoch 62, Batch 790, Loss: 164.1836395263672\n",
      "Epoch 62, Batch 791, Loss: 166.33660888671875\n",
      "Epoch 62, Batch 792, Loss: 186.20489501953125\n",
      "Epoch 62, Batch 793, Loss: 154.08868408203125\n",
      "Epoch 62, Batch 794, Loss: 172.00096130371094\n",
      "Epoch 62, Batch 795, Loss: 167.1796112060547\n",
      "Epoch 62, Batch 796, Loss: 186.34011840820312\n",
      "Epoch 62, Batch 797, Loss: 179.0697784423828\n",
      "Epoch 62, Batch 798, Loss: 166.31130981445312\n",
      "Epoch 62, Batch 799, Loss: 168.84812927246094\n",
      "Epoch 62, Batch 800, Loss: 155.0058135986328\n",
      "Epoch 62, Batch 801, Loss: 191.52206420898438\n",
      "Epoch 62, Batch 802, Loss: 166.2891082763672\n",
      "Epoch 62, Batch 803, Loss: 176.37200927734375\n",
      "Epoch 62, Batch 804, Loss: 169.7307891845703\n",
      "Epoch 62, Batch 805, Loss: 177.81573486328125\n",
      "Epoch 62, Batch 806, Loss: 157.1528778076172\n",
      "Epoch 62, Batch 807, Loss: 167.84642028808594\n",
      "Epoch 62, Batch 808, Loss: 174.38433837890625\n",
      "Epoch 62, Batch 809, Loss: 173.14816284179688\n",
      "Epoch 62, Batch 810, Loss: 172.30262756347656\n",
      "Epoch 62, Batch 811, Loss: 175.5492401123047\n",
      "Epoch 62, Batch 812, Loss: 182.54583740234375\n",
      "Epoch 62, Batch 813, Loss: 168.9014129638672\n",
      "Epoch 62, Batch 814, Loss: 177.07269287109375\n",
      "Epoch 62, Batch 815, Loss: 154.73880004882812\n",
      "Epoch 62, Batch 816, Loss: 175.1326141357422\n",
      "Epoch 62, Batch 817, Loss: 182.85885620117188\n",
      "Epoch 62, Batch 818, Loss: 175.60137939453125\n",
      "Epoch 62, Batch 819, Loss: 162.45372009277344\n",
      "Epoch 62, Batch 820, Loss: 168.93264770507812\n",
      "Epoch 62, Batch 821, Loss: 154.6094207763672\n",
      "Epoch 62, Batch 822, Loss: 180.9969482421875\n",
      "Epoch 62, Batch 823, Loss: 161.54660034179688\n",
      "Epoch 62, Batch 824, Loss: 165.6723175048828\n",
      "Epoch 62, Batch 825, Loss: 179.97891235351562\n",
      "Epoch 62, Batch 826, Loss: 158.1834716796875\n",
      "Epoch 62, Batch 827, Loss: 180.0208282470703\n",
      "Epoch 62, Batch 828, Loss: 189.03053283691406\n",
      "Epoch 62, Batch 829, Loss: 179.18017578125\n",
      "Epoch 62, Batch 830, Loss: 170.45486450195312\n",
      "Epoch 62, Batch 831, Loss: 173.7239227294922\n",
      "Epoch 62, Batch 832, Loss: 178.66441345214844\n",
      "Epoch 62, Batch 833, Loss: 171.134521484375\n",
      "Epoch 62, Batch 834, Loss: 178.62957763671875\n",
      "Epoch 62, Batch 835, Loss: 163.07460021972656\n",
      "Epoch 62, Batch 836, Loss: 165.32557678222656\n",
      "Epoch 62, Batch 837, Loss: 170.8859100341797\n",
      "Epoch 62, Batch 838, Loss: 178.39035034179688\n",
      "Epoch 62, Batch 839, Loss: 177.09341430664062\n",
      "Epoch 62, Batch 840, Loss: 171.97512817382812\n",
      "Epoch 62, Batch 841, Loss: 175.47195434570312\n",
      "Epoch 62, Batch 842, Loss: 170.69989013671875\n",
      "Epoch 62, Batch 843, Loss: 186.62713623046875\n",
      "Epoch 62, Batch 844, Loss: 161.17213439941406\n",
      "Epoch 62, Batch 845, Loss: 185.71310424804688\n",
      "Epoch 62, Batch 846, Loss: 152.0323944091797\n",
      "Epoch 62, Batch 847, Loss: 169.39468383789062\n",
      "Epoch 62, Batch 848, Loss: 165.5411376953125\n",
      "Epoch 62, Batch 849, Loss: 179.32962036132812\n",
      "Epoch 62, Batch 850, Loss: 177.5045166015625\n",
      "Epoch 62, Batch 851, Loss: 173.01007080078125\n",
      "Epoch 62, Batch 852, Loss: 169.98138427734375\n",
      "Epoch 62, Batch 853, Loss: 165.4741973876953\n",
      "Epoch 62, Batch 854, Loss: 186.7406005859375\n",
      "Epoch 62, Batch 855, Loss: 157.84283447265625\n",
      "Epoch 62, Batch 856, Loss: 177.20388793945312\n",
      "Epoch 62, Batch 857, Loss: 162.32980346679688\n",
      "Epoch 62, Batch 858, Loss: 180.1161651611328\n",
      "Epoch 62, Batch 859, Loss: 188.48818969726562\n",
      "Epoch 62, Batch 860, Loss: 171.5432586669922\n",
      "Epoch 62, Batch 861, Loss: 186.94464111328125\n",
      "Epoch 62, Batch 862, Loss: 173.75299072265625\n",
      "Epoch 62, Batch 863, Loss: 185.60018920898438\n",
      "Epoch 62, Batch 864, Loss: 161.01144409179688\n",
      "Epoch 62, Batch 865, Loss: 163.5139923095703\n",
      "Epoch 62, Batch 866, Loss: 158.70657348632812\n",
      "Epoch 62, Batch 867, Loss: 165.67730712890625\n",
      "Epoch 62, Batch 868, Loss: 154.27552795410156\n",
      "Epoch 62, Batch 869, Loss: 178.4503631591797\n",
      "Epoch 62, Batch 870, Loss: 186.94590759277344\n",
      "Epoch 62, Batch 871, Loss: 160.6893768310547\n",
      "Epoch 62, Batch 872, Loss: 168.42872619628906\n",
      "Epoch 62, Batch 873, Loss: 167.59776306152344\n",
      "Epoch 62, Batch 874, Loss: 169.2581329345703\n",
      "Epoch 62, Batch 875, Loss: 180.8728485107422\n",
      "Epoch 62, Batch 876, Loss: 168.6750946044922\n",
      "Epoch 62, Batch 877, Loss: 182.56553649902344\n",
      "Epoch 62, Batch 878, Loss: 168.8809814453125\n",
      "Epoch 62, Batch 879, Loss: 161.3324737548828\n",
      "Epoch 62, Batch 880, Loss: 173.937744140625\n",
      "Epoch 62, Batch 881, Loss: 174.59840393066406\n",
      "Epoch 62, Batch 882, Loss: 159.71568298339844\n",
      "Epoch 62, Batch 883, Loss: 164.0672149658203\n",
      "Epoch 62, Batch 884, Loss: 183.47805786132812\n",
      "Epoch 62, Batch 885, Loss: 174.81509399414062\n",
      "Epoch 62, Batch 886, Loss: 184.03427124023438\n",
      "Epoch 62, Batch 887, Loss: 187.02757263183594\n",
      "Epoch 62, Batch 888, Loss: 173.5805206298828\n",
      "Epoch 62, Batch 889, Loss: 164.24501037597656\n",
      "Epoch 62, Batch 890, Loss: 178.31114196777344\n",
      "Epoch 62, Batch 891, Loss: 177.61000061035156\n",
      "Epoch 62, Batch 892, Loss: 171.0398712158203\n",
      "Epoch 62, Batch 893, Loss: 146.30003356933594\n",
      "Epoch 62, Batch 894, Loss: 178.9901885986328\n",
      "Epoch 62, Batch 895, Loss: 181.7879638671875\n",
      "Epoch 62, Batch 896, Loss: 173.1111297607422\n",
      "Epoch 62, Batch 897, Loss: 171.75003051757812\n",
      "Epoch 62, Batch 898, Loss: 180.5849151611328\n",
      "Epoch 62, Batch 899, Loss: 176.1880645751953\n",
      "Epoch 62, Batch 900, Loss: 186.62026977539062\n",
      "Epoch 62, Batch 901, Loss: 161.58114624023438\n",
      "Epoch 62, Batch 902, Loss: 181.77365112304688\n",
      "Epoch 62, Batch 903, Loss: 172.77830505371094\n",
      "Epoch 62, Batch 904, Loss: 180.82534790039062\n",
      "Epoch 62, Batch 905, Loss: 170.7080078125\n",
      "Epoch 62, Batch 906, Loss: 176.6056671142578\n",
      "Epoch 62, Batch 907, Loss: 197.3203887939453\n",
      "Epoch 62, Batch 908, Loss: 176.4495849609375\n",
      "Epoch 62, Batch 909, Loss: 160.47686767578125\n",
      "Epoch 62, Batch 910, Loss: 171.26759338378906\n",
      "Epoch 62, Batch 911, Loss: 167.79766845703125\n",
      "Epoch 62, Batch 912, Loss: 174.8438720703125\n",
      "Epoch 62, Batch 913, Loss: 181.79537963867188\n",
      "Epoch 62, Batch 914, Loss: 170.2345428466797\n",
      "Epoch 62, Batch 915, Loss: 174.5642547607422\n",
      "Epoch 62, Batch 916, Loss: 163.35511779785156\n",
      "Epoch 62, Batch 917, Loss: 177.63587951660156\n",
      "Epoch 62, Batch 918, Loss: 168.38682556152344\n",
      "Epoch 62, Batch 919, Loss: 189.81570434570312\n",
      "Epoch 62, Batch 920, Loss: 166.14385986328125\n",
      "Epoch 62, Batch 921, Loss: 172.49839782714844\n",
      "Epoch 62, Batch 922, Loss: 165.97019958496094\n",
      "Epoch 62, Batch 923, Loss: 174.2460174560547\n",
      "Epoch 62, Batch 924, Loss: 153.80625915527344\n",
      "Epoch 62, Batch 925, Loss: 165.9003448486328\n",
      "Epoch 62, Batch 926, Loss: 162.838623046875\n",
      "Epoch 62, Batch 927, Loss: 171.91680908203125\n",
      "Epoch 62, Batch 928, Loss: 186.08242797851562\n",
      "Epoch 62, Batch 929, Loss: 169.83544921875\n",
      "Epoch 62, Batch 930, Loss: 189.86546325683594\n",
      "Epoch 62, Batch 931, Loss: 180.62847900390625\n",
      "Epoch 62, Batch 932, Loss: 184.72950744628906\n",
      "Epoch 62, Batch 933, Loss: 173.15908813476562\n",
      "Epoch 62, Batch 934, Loss: 159.8311767578125\n",
      "Epoch 62, Batch 935, Loss: 169.63839721679688\n",
      "Epoch 62, Batch 936, Loss: 174.47177124023438\n",
      "Epoch 62, Batch 937, Loss: 182.27955627441406\n",
      "Epoch 62, Batch 938, Loss: 156.53982543945312\n",
      "Epoch 62, Batch 939, Loss: 169.8182830810547\n",
      "Epoch 62, Batch 940, Loss: 162.05857849121094\n",
      "Epoch 62, Batch 941, Loss: 201.20034790039062\n",
      "Epoch 62, Batch 942, Loss: 173.49766540527344\n",
      "Epoch 62, Batch 943, Loss: 185.75709533691406\n",
      "Epoch 62, Batch 944, Loss: 183.5747528076172\n",
      "Epoch 62, Batch 945, Loss: 178.05508422851562\n",
      "Epoch 62, Batch 946, Loss: 155.12841796875\n",
      "Epoch 62, Batch 947, Loss: 161.4679718017578\n",
      "Epoch 62, Batch 948, Loss: 169.2478790283203\n",
      "Epoch 62, Batch 949, Loss: 172.34884643554688\n",
      "Epoch 62, Batch 950, Loss: 189.59092712402344\n",
      "Epoch 62, Batch 951, Loss: 153.9695587158203\n",
      "Epoch 62, Batch 952, Loss: 164.38558959960938\n",
      "Epoch 62, Batch 953, Loss: 174.90283203125\n",
      "Epoch 62, Batch 954, Loss: 190.17584228515625\n",
      "Epoch 62, Batch 955, Loss: 196.10047912597656\n",
      "Epoch 62, Batch 956, Loss: 173.25564575195312\n",
      "Epoch 62, Batch 957, Loss: 176.76329040527344\n",
      "Epoch 62, Batch 958, Loss: 169.6510467529297\n",
      "Epoch 62, Batch 959, Loss: 176.84837341308594\n",
      "Epoch 62, Batch 960, Loss: 170.81497192382812\n",
      "Epoch 62, Batch 961, Loss: 167.97177124023438\n",
      "Epoch 62, Batch 962, Loss: 154.51988220214844\n",
      "Epoch 62, Batch 963, Loss: 177.57330322265625\n",
      "Epoch 62, Batch 964, Loss: 172.53855895996094\n",
      "Epoch 62, Batch 965, Loss: 167.69525146484375\n",
      "Epoch 62, Batch 966, Loss: 172.40682983398438\n",
      "Epoch 62, Batch 967, Loss: 177.964599609375\n",
      "Epoch 62, Batch 968, Loss: 181.77484130859375\n",
      "Epoch 62, Batch 969, Loss: 170.5830841064453\n",
      "Epoch 62, Batch 970, Loss: 180.55984497070312\n",
      "Epoch 62, Batch 971, Loss: 186.42617797851562\n",
      "Epoch 62, Batch 972, Loss: 184.4051513671875\n",
      "Epoch 62, Batch 973, Loss: 170.49769592285156\n",
      "Epoch 62, Batch 974, Loss: 176.5748291015625\n",
      "Epoch 62, Batch 975, Loss: 174.80203247070312\n",
      "Epoch 62, Batch 976, Loss: 202.0672149658203\n",
      "Epoch 62, Batch 977, Loss: 178.24049377441406\n",
      "Epoch 62, Batch 978, Loss: 170.99705505371094\n",
      "Epoch 62, Batch 979, Loss: 173.5385284423828\n",
      "Epoch 62, Batch 980, Loss: 170.48611450195312\n",
      "Epoch 62, Batch 981, Loss: 168.86514282226562\n",
      "Epoch 62, Batch 982, Loss: 168.6807098388672\n",
      "Epoch 62, Batch 983, Loss: 177.89840698242188\n",
      "Epoch 62, Batch 984, Loss: 182.17173767089844\n",
      "Epoch 62, Batch 985, Loss: 162.84510803222656\n",
      "Epoch 62, Batch 986, Loss: 183.47955322265625\n",
      "Epoch 62, Batch 987, Loss: 182.21548461914062\n",
      "Epoch 62, Batch 988, Loss: 175.7487030029297\n",
      "Epoch 62, Batch 989, Loss: 171.89573669433594\n",
      "Epoch 62, Batch 990, Loss: 165.59913635253906\n",
      "Epoch 62, Batch 991, Loss: 167.2472381591797\n",
      "Epoch 62, Batch 992, Loss: 169.58737182617188\n",
      "Epoch 62, Batch 993, Loss: 154.97337341308594\n",
      "Epoch 62, Batch 994, Loss: 156.62086486816406\n",
      "Epoch 62, Batch 995, Loss: 156.56605529785156\n",
      "Epoch 62, Batch 996, Loss: 165.85594177246094\n",
      "Epoch 62, Batch 997, Loss: 168.11582946777344\n",
      "Epoch 62, Batch 998, Loss: 186.0732421875\n",
      "Epoch 62, Batch 999, Loss: 177.94078063964844\n",
      "Epoch 62, Batch 1000, Loss: 177.2487030029297\n",
      "Epoch 62, Batch 1001, Loss: 172.14898681640625\n",
      "Epoch 62, Batch 1002, Loss: 182.14846801757812\n",
      "Epoch 62, Batch 1003, Loss: 175.1848602294922\n",
      "Epoch 62, Batch 1004, Loss: 180.94070434570312\n",
      "Epoch 62, Batch 1005, Loss: 173.09410095214844\n",
      "Epoch 62, Batch 1006, Loss: 179.3756561279297\n",
      "Epoch 62, Batch 1007, Loss: 177.4627685546875\n",
      "Epoch 62, Batch 1008, Loss: 171.03070068359375\n",
      "Epoch 62, Batch 1009, Loss: 165.8557891845703\n",
      "Epoch 62, Batch 1010, Loss: 172.24366760253906\n",
      "Epoch 62, Batch 1011, Loss: 171.09286499023438\n",
      "Epoch 62, Batch 1012, Loss: 164.22097778320312\n",
      "Epoch 62, Batch 1013, Loss: 156.15467834472656\n",
      "Epoch 62, Batch 1014, Loss: 183.5088348388672\n",
      "Epoch 62, Batch 1015, Loss: 171.08119201660156\n",
      "Epoch 62, Batch 1016, Loss: 155.10694885253906\n",
      "Epoch 62, Batch 1017, Loss: 159.7662353515625\n",
      "Epoch 62, Batch 1018, Loss: 159.2265625\n",
      "Epoch 62, Batch 1019, Loss: 170.362548828125\n",
      "Epoch 62, Batch 1020, Loss: 177.8240509033203\n",
      "Epoch 62, Batch 1021, Loss: 173.75308227539062\n",
      "Epoch 62, Batch 1022, Loss: 182.59231567382812\n",
      "Epoch 62, Batch 1023, Loss: 157.132080078125\n",
      "Epoch 62, Batch 1024, Loss: 185.82879638671875\n",
      "Epoch 62, Batch 1025, Loss: 177.33016967773438\n",
      "Epoch 62, Batch 1026, Loss: 182.48670959472656\n",
      "Epoch 62, Batch 1027, Loss: 171.7224884033203\n",
      "Epoch 62, Batch 1028, Loss: 156.2184295654297\n",
      "Epoch 62, Batch 1029, Loss: 171.91725158691406\n",
      "Epoch 62, Batch 1030, Loss: 175.85252380371094\n",
      "Epoch 62, Batch 1031, Loss: 184.22642517089844\n",
      "Epoch 62, Batch 1032, Loss: 179.40875244140625\n",
      "Epoch 62, Batch 1033, Loss: 161.9372100830078\n",
      "Epoch 62, Batch 1034, Loss: 183.66810607910156\n",
      "Epoch 62, Batch 1035, Loss: 178.41969299316406\n",
      "Epoch 62, Batch 1036, Loss: 171.65333557128906\n",
      "Epoch 62, Batch 1037, Loss: 188.98583984375\n",
      "Epoch 62, Batch 1038, Loss: 181.59320068359375\n",
      "Epoch 62, Batch 1039, Loss: 165.06126403808594\n",
      "Epoch 62, Batch 1040, Loss: 182.72146606445312\n",
      "Epoch 62, Batch 1041, Loss: 182.83587646484375\n",
      "Epoch 62, Batch 1042, Loss: 163.99591064453125\n",
      "Epoch 62, Batch 1043, Loss: 176.0915985107422\n",
      "Epoch 62, Batch 1044, Loss: 180.68658447265625\n",
      "Epoch 62, Batch 1045, Loss: 165.61770629882812\n",
      "Epoch 62, Batch 1046, Loss: 186.80801391601562\n",
      "Epoch 62, Batch 1047, Loss: 174.52294921875\n",
      "Epoch 62, Batch 1048, Loss: 176.3308868408203\n",
      "Epoch 62, Batch 1049, Loss: 170.19224548339844\n",
      "Epoch 62, Batch 1050, Loss: 173.11032104492188\n",
      "Epoch 62, Batch 1051, Loss: 192.39581298828125\n",
      "Epoch 62, Batch 1052, Loss: 179.78257751464844\n",
      "Epoch 62, Batch 1053, Loss: 173.80885314941406\n",
      "Epoch 62, Batch 1054, Loss: 177.45849609375\n",
      "Epoch 62, Batch 1055, Loss: 182.73995971679688\n",
      "Epoch 62, Batch 1056, Loss: 195.12599182128906\n",
      "Epoch 62, Batch 1057, Loss: 173.77870178222656\n",
      "Epoch 62, Batch 1058, Loss: 155.1299285888672\n",
      "Epoch 62, Batch 1059, Loss: 171.31520080566406\n",
      "Epoch 62, Batch 1060, Loss: 172.57443237304688\n",
      "Epoch 62, Batch 1061, Loss: 197.68846130371094\n",
      "Epoch 62, Batch 1062, Loss: 172.62509155273438\n",
      "Epoch 62, Batch 1063, Loss: 181.78175354003906\n",
      "Epoch 62, Batch 1064, Loss: 169.2995147705078\n",
      "Epoch 62, Batch 1065, Loss: 179.51705932617188\n",
      "Epoch 62, Batch 1066, Loss: 174.7760772705078\n",
      "Epoch 62, Batch 1067, Loss: 168.57275390625\n",
      "Epoch 62, Batch 1068, Loss: 176.9989013671875\n",
      "Epoch 62, Batch 1069, Loss: 180.31900024414062\n",
      "Epoch 62, Batch 1070, Loss: 175.5940399169922\n",
      "Epoch 62, Batch 1071, Loss: 179.5889129638672\n",
      "Epoch 62, Batch 1072, Loss: 195.94754028320312\n",
      "Epoch 62, Batch 1073, Loss: 158.53590393066406\n",
      "Epoch 62, Batch 1074, Loss: 176.32168579101562\n",
      "Epoch 62, Batch 1075, Loss: 172.992919921875\n",
      "Epoch 62, Batch 1076, Loss: 168.2158660888672\n",
      "Epoch 62, Batch 1077, Loss: 174.03976440429688\n",
      "Epoch 62, Batch 1078, Loss: 158.97781372070312\n",
      "Epoch 62, Batch 1079, Loss: 162.2267608642578\n",
      "Epoch 62, Batch 1080, Loss: 165.0637969970703\n",
      "Epoch 62, Batch 1081, Loss: 198.12823486328125\n",
      "Epoch 62, Batch 1082, Loss: 186.4648895263672\n",
      "Epoch 62, Batch 1083, Loss: 175.0582733154297\n",
      "Epoch 62, Batch 1084, Loss: 154.59799194335938\n",
      "Epoch 62, Batch 1085, Loss: 167.09051513671875\n",
      "Epoch 62, Batch 1086, Loss: 171.73599243164062\n",
      "Epoch 62, Batch 1087, Loss: 168.10635375976562\n",
      "Epoch 62, Batch 1088, Loss: 167.9394073486328\n",
      "Epoch 62, Batch 1089, Loss: 183.83033752441406\n",
      "Epoch 62, Batch 1090, Loss: 167.46438598632812\n",
      "Epoch 62, Batch 1091, Loss: 176.6726531982422\n",
      "Epoch 62, Batch 1092, Loss: 175.54103088378906\n",
      "Epoch 62, Batch 1093, Loss: 161.2853546142578\n",
      "Epoch 62, Batch 1094, Loss: 145.33319091796875\n",
      "Epoch 62, Batch 1095, Loss: 167.36241149902344\n",
      "Epoch 62, Batch 1096, Loss: 196.8525848388672\n",
      "Epoch 62, Batch 1097, Loss: 186.20462036132812\n",
      "Epoch 62, Batch 1098, Loss: 171.2939910888672\n",
      "Epoch 62, Batch 1099, Loss: 174.28550720214844\n",
      "Epoch 62, Batch 1100, Loss: 172.4814910888672\n",
      "Epoch 62, Batch 1101, Loss: 173.5828399658203\n",
      "Epoch 62, Batch 1102, Loss: 163.54541015625\n",
      "Epoch 62, Batch 1103, Loss: 164.46261596679688\n",
      "Epoch 62, Batch 1104, Loss: 181.0223846435547\n",
      "Epoch 62, Batch 1105, Loss: 178.9703369140625\n",
      "Epoch 62, Batch 1106, Loss: 193.60064697265625\n",
      "Epoch 62, Batch 1107, Loss: 172.98956298828125\n",
      "Epoch 62, Batch 1108, Loss: 163.90005493164062\n",
      "Epoch 62, Batch 1109, Loss: 199.4635467529297\n",
      "Epoch 62, Batch 1110, Loss: 183.91859436035156\n",
      "Epoch 62, Batch 1111, Loss: 167.88796997070312\n",
      "Epoch 62, Batch 1112, Loss: 176.25979614257812\n",
      "Epoch 62, Batch 1113, Loss: 182.6253204345703\n",
      "Epoch 62, Batch 1114, Loss: 161.2156219482422\n",
      "Epoch 62, Batch 1115, Loss: 158.29283142089844\n",
      "Epoch 62, Batch 1116, Loss: 181.02316284179688\n",
      "Epoch 62, Batch 1117, Loss: 168.711669921875\n",
      "Epoch 62, Batch 1118, Loss: 160.86305236816406\n",
      "Epoch 62, Batch 1119, Loss: 181.2093963623047\n",
      "Epoch 62, Batch 1120, Loss: 164.05828857421875\n",
      "Epoch 62, Batch 1121, Loss: 175.0773468017578\n",
      "Epoch 62, Batch 1122, Loss: 186.3310089111328\n",
      "Epoch 62, Batch 1123, Loss: 186.9893798828125\n",
      "Epoch 62, Batch 1124, Loss: 166.8186492919922\n",
      "Epoch 62, Batch 1125, Loss: 183.59646606445312\n",
      "Epoch 62, Batch 1126, Loss: 169.80752563476562\n",
      "Epoch 62, Batch 1127, Loss: 160.2511749267578\n",
      "Epoch 62, Batch 1128, Loss: 166.99794006347656\n",
      "Epoch 62, Batch 1129, Loss: 170.08941650390625\n",
      "Epoch 62, Batch 1130, Loss: 184.37493896484375\n",
      "Epoch 62, Batch 1131, Loss: 171.72935485839844\n",
      "Epoch 62, Batch 1132, Loss: 164.0304718017578\n",
      "Epoch 62, Batch 1133, Loss: 195.2550506591797\n",
      "Epoch 62, Batch 1134, Loss: 186.50350952148438\n",
      "Epoch 62, Batch 1135, Loss: 169.379638671875\n",
      "Epoch 62, Batch 1136, Loss: 147.759765625\n",
      "Epoch 62, Batch 1137, Loss: 165.09072875976562\n",
      "Epoch 62, Batch 1138, Loss: 156.76727294921875\n",
      "Epoch 62, Batch 1139, Loss: 173.9626007080078\n",
      "Epoch 62, Batch 1140, Loss: 194.43365478515625\n",
      "Epoch 62, Batch 1141, Loss: 166.37808227539062\n",
      "Epoch 62, Batch 1142, Loss: 169.99615478515625\n",
      "Epoch 62, Batch 1143, Loss: 166.3170166015625\n",
      "Epoch 62, Batch 1144, Loss: 174.662109375\n",
      "Epoch 62, Batch 1145, Loss: 155.98947143554688\n",
      "Epoch 62, Batch 1146, Loss: 175.44203186035156\n",
      "Epoch 62, Batch 1147, Loss: 166.77264404296875\n",
      "Epoch 62, Batch 1148, Loss: 170.40536499023438\n",
      "Epoch 62, Batch 1149, Loss: 160.91598510742188\n",
      "Epoch 62, Batch 1150, Loss: 171.0782928466797\n",
      "Epoch 62, Batch 1151, Loss: 177.916259765625\n",
      "Epoch 62, Batch 1152, Loss: 182.70361328125\n",
      "Epoch 62, Batch 1153, Loss: 183.62831115722656\n",
      "Epoch 62, Batch 1154, Loss: 171.20541381835938\n",
      "Epoch 62, Batch 1155, Loss: 175.89450073242188\n",
      "Epoch 62, Batch 1156, Loss: 154.30540466308594\n",
      "Epoch 62, Batch 1157, Loss: 190.04965209960938\n",
      "Epoch 62, Batch 1158, Loss: 164.0816192626953\n",
      "Epoch 62, Batch 1159, Loss: 168.270751953125\n",
      "Epoch 62, Batch 1160, Loss: 166.2303924560547\n",
      "Epoch 62, Batch 1161, Loss: 179.46878051757812\n",
      "Epoch 62, Batch 1162, Loss: 170.86740112304688\n",
      "Epoch 62, Batch 1163, Loss: 179.73626708984375\n",
      "Epoch 62, Batch 1164, Loss: 161.6719970703125\n",
      "Epoch 62, Batch 1165, Loss: 181.16416931152344\n",
      "Epoch 62, Batch 1166, Loss: 178.43177795410156\n",
      "Epoch 62, Batch 1167, Loss: 170.4729461669922\n",
      "Epoch 62, Batch 1168, Loss: 174.22927856445312\n",
      "Epoch 62, Batch 1169, Loss: 164.7102813720703\n",
      "Epoch 62, Batch 1170, Loss: 190.4079132080078\n",
      "Epoch 62, Batch 1171, Loss: 179.34182739257812\n",
      "Epoch 62, Batch 1172, Loss: 171.58103942871094\n",
      "Epoch 62, Batch 1173, Loss: 178.1175537109375\n",
      "Epoch 62, Batch 1174, Loss: 186.09979248046875\n",
      "Epoch 62, Batch 1175, Loss: 160.9037628173828\n",
      "Epoch 62, Batch 1176, Loss: 182.205322265625\n",
      "Epoch 62, Batch 1177, Loss: 178.41864013671875\n",
      "Epoch 62, Batch 1178, Loss: 179.569091796875\n",
      "Epoch 62, Batch 1179, Loss: 174.83880615234375\n",
      "Epoch 62, Batch 1180, Loss: 161.4506072998047\n",
      "Epoch 62, Batch 1181, Loss: 168.09840393066406\n",
      "Epoch 62, Batch 1182, Loss: 173.47491455078125\n",
      "Epoch 62, Batch 1183, Loss: 174.25399780273438\n",
      "Epoch 62, Batch 1184, Loss: 183.19578552246094\n",
      "Epoch 62, Batch 1185, Loss: 178.8794708251953\n",
      "Epoch 62, Batch 1186, Loss: 176.43675231933594\n",
      "Epoch 62, Batch 1187, Loss: 149.86004638671875\n",
      "Epoch 62, Batch 1188, Loss: 156.90049743652344\n",
      "Epoch 62, Batch 1189, Loss: 186.2392578125\n",
      "Epoch 62, Batch 1190, Loss: 175.47982788085938\n",
      "Epoch 62, Batch 1191, Loss: 176.22715759277344\n",
      "Epoch 62, Batch 1192, Loss: 168.56423950195312\n",
      "Epoch 62, Batch 1193, Loss: 177.18731689453125\n",
      "Epoch 62, Batch 1194, Loss: 174.9806365966797\n",
      "Epoch 62, Batch 1195, Loss: 170.98281860351562\n",
      "Epoch 62, Batch 1196, Loss: 181.2371063232422\n",
      "Epoch 62, Batch 1197, Loss: 174.7926483154297\n",
      "Epoch 62, Batch 1198, Loss: 200.50344848632812\n",
      "Epoch 62, Batch 1199, Loss: 179.1420135498047\n",
      "Epoch 62, Batch 1200, Loss: 181.20098876953125\n",
      "Epoch 62, Batch 1201, Loss: 181.4158172607422\n",
      "Epoch 62, Batch 1202, Loss: 163.602783203125\n",
      "Epoch 62, Batch 1203, Loss: 163.2060089111328\n",
      "Epoch 62, Batch 1204, Loss: 192.73301696777344\n",
      "Epoch 62, Batch 1205, Loss: 172.79653930664062\n",
      "Epoch 62, Batch 1206, Loss: 191.2314453125\n",
      "Epoch 62, Batch 1207, Loss: 179.2292022705078\n",
      "Epoch 62, Batch 1208, Loss: 180.69439697265625\n",
      "Epoch 62, Batch 1209, Loss: 173.80152893066406\n",
      "Epoch 62, Batch 1210, Loss: 158.14549255371094\n",
      "Epoch 62, Batch 1211, Loss: 166.14752197265625\n",
      "Epoch 62, Batch 1212, Loss: 162.56893920898438\n",
      "Epoch 62, Batch 1213, Loss: 174.23184204101562\n",
      "Epoch 62, Batch 1214, Loss: 158.5781707763672\n",
      "Epoch 62, Batch 1215, Loss: 153.96310424804688\n",
      "Epoch 62, Batch 1216, Loss: 171.429931640625\n",
      "Epoch 62, Batch 1217, Loss: 169.77740478515625\n",
      "Epoch 62, Batch 1218, Loss: 183.6831512451172\n",
      "Epoch 62, Batch 1219, Loss: 170.76303100585938\n",
      "Epoch 62, Batch 1220, Loss: 184.98548889160156\n",
      "Epoch 62, Batch 1221, Loss: 168.06219482421875\n",
      "Epoch 62, Batch 1222, Loss: 175.3266143798828\n",
      "Epoch 62, Batch 1223, Loss: 176.02513122558594\n",
      "Epoch 62, Batch 1224, Loss: 176.8805389404297\n",
      "Epoch 62, Batch 1225, Loss: 169.39108276367188\n",
      "Epoch 62, Batch 1226, Loss: 173.7419891357422\n",
      "Epoch 62, Batch 1227, Loss: 189.64039611816406\n",
      "Epoch 62, Batch 1228, Loss: 165.0009307861328\n",
      "Epoch 62, Batch 1229, Loss: 181.98614501953125\n",
      "Epoch 62, Batch 1230, Loss: 164.17294311523438\n",
      "Epoch 62, Batch 1231, Loss: 176.86289978027344\n",
      "Epoch 62, Batch 1232, Loss: 151.79074096679688\n",
      "Epoch 62, Batch 1233, Loss: 176.0104522705078\n",
      "Epoch 62, Batch 1234, Loss: 184.58717346191406\n",
      "Epoch 62, Batch 1235, Loss: 160.25222778320312\n",
      "Epoch 62, Batch 1236, Loss: 180.68496704101562\n",
      "Epoch 62, Batch 1237, Loss: 173.43408203125\n",
      "Epoch 62, Batch 1238, Loss: 188.6856231689453\n",
      "Epoch 62, Batch 1239, Loss: 172.65960693359375\n",
      "Epoch 62, Batch 1240, Loss: 179.60459899902344\n",
      "Epoch 62, Batch 1241, Loss: 171.93136596679688\n",
      "Epoch 62, Batch 1242, Loss: 173.6992645263672\n",
      "Epoch 62, Batch 1243, Loss: 191.22291564941406\n",
      "Epoch 62, Batch 1244, Loss: 159.28953552246094\n",
      "Epoch 62, Batch 1245, Loss: 183.50241088867188\n",
      "Epoch 62, Batch 1246, Loss: 179.25997924804688\n",
      "Epoch 62, Batch 1247, Loss: 167.1709442138672\n",
      "Epoch 62, Batch 1248, Loss: 176.4068145751953\n",
      "Epoch 62, Batch 1249, Loss: 162.9423370361328\n",
      "Epoch 62, Batch 1250, Loss: 174.4593963623047\n",
      "Epoch 62, Batch 1251, Loss: 184.7750244140625\n",
      "Epoch 62, Batch 1252, Loss: 189.9176025390625\n",
      "Epoch 62, Batch 1253, Loss: 170.23101806640625\n",
      "Epoch 62, Batch 1254, Loss: 155.0609893798828\n",
      "Epoch 62, Batch 1255, Loss: 185.11509704589844\n",
      "Epoch 62, Batch 1256, Loss: 179.4510498046875\n",
      "Epoch 62, Batch 1257, Loss: 186.86380004882812\n",
      "Epoch 62, Batch 1258, Loss: 177.0196533203125\n",
      "Epoch 62, Batch 1259, Loss: 186.84864807128906\n",
      "Epoch 62, Batch 1260, Loss: 148.66836547851562\n",
      "Epoch 62, Batch 1261, Loss: 179.00271606445312\n",
      "Epoch 62, Batch 1262, Loss: 150.0646209716797\n",
      "Epoch 62, Batch 1263, Loss: 154.07147216796875\n",
      "Epoch 62, Batch 1264, Loss: 182.22251892089844\n",
      "Epoch 62, Batch 1265, Loss: 164.9646453857422\n",
      "Epoch 62, Batch 1266, Loss: 161.66928100585938\n",
      "Epoch 62, Batch 1267, Loss: 189.9598846435547\n",
      "Epoch 62, Batch 1268, Loss: 172.2304229736328\n",
      "Epoch 62, Batch 1269, Loss: 182.5969696044922\n",
      "Epoch 62, Batch 1270, Loss: 176.7627410888672\n",
      "Epoch 62, Batch 1271, Loss: 177.29153442382812\n",
      "Epoch 62, Batch 1272, Loss: 186.7139892578125\n",
      "Epoch 62, Batch 1273, Loss: 177.8096466064453\n",
      "Epoch 62, Batch 1274, Loss: 186.89073181152344\n",
      "Epoch 62, Batch 1275, Loss: 169.80841064453125\n",
      "Epoch 62, Batch 1276, Loss: 175.54611206054688\n",
      "Epoch 62, Batch 1277, Loss: 170.85015869140625\n",
      "Epoch 62, Batch 1278, Loss: 179.75489807128906\n",
      "Epoch 62, Batch 1279, Loss: 190.6356658935547\n",
      "Epoch 62, Batch 1280, Loss: 158.839599609375\n",
      "Epoch 62, Batch 1281, Loss: 182.81912231445312\n",
      "Epoch 62, Batch 1282, Loss: 156.94091796875\n",
      "Epoch 62, Batch 1283, Loss: 159.53759765625\n",
      "Epoch 62, Batch 1284, Loss: 167.40899658203125\n",
      "Epoch 62, Batch 1285, Loss: 180.85317993164062\n",
      "Epoch 62, Batch 1286, Loss: 175.08865356445312\n",
      "Epoch 62, Batch 1287, Loss: 183.3190460205078\n",
      "Epoch 62, Batch 1288, Loss: 173.22879028320312\n",
      "Epoch 62, Batch 1289, Loss: 174.70840454101562\n",
      "Epoch 62, Batch 1290, Loss: 165.1619873046875\n",
      "Epoch 62, Batch 1291, Loss: 159.60403442382812\n",
      "Epoch 62, Batch 1292, Loss: 181.49021911621094\n",
      "Epoch 62, Batch 1293, Loss: 168.61093139648438\n",
      "Epoch 62, Batch 1294, Loss: 176.52932739257812\n",
      "Epoch 62, Batch 1295, Loss: 186.8604736328125\n",
      "Epoch 62, Batch 1296, Loss: 179.73556518554688\n",
      "Epoch 62, Batch 1297, Loss: 172.56617736816406\n",
      "Epoch 62, Batch 1298, Loss: 173.69329833984375\n",
      "Epoch 62, Batch 1299, Loss: 167.529052734375\n",
      "Epoch 62, Batch 1300, Loss: 183.8273162841797\n",
      "Epoch 62, Batch 1301, Loss: 167.6383514404297\n",
      "Epoch 62, Batch 1302, Loss: 183.08157348632812\n",
      "Epoch 62, Batch 1303, Loss: 175.76138305664062\n",
      "Epoch 62, Batch 1304, Loss: 178.49317932128906\n",
      "Epoch 62, Batch 1305, Loss: 177.73876953125\n",
      "Epoch 62, Batch 1306, Loss: 174.8260040283203\n",
      "Epoch 62, Batch 1307, Loss: 172.96751403808594\n",
      "Epoch 62, Batch 1308, Loss: 181.59681701660156\n",
      "Epoch 62, Batch 1309, Loss: 168.86610412597656\n",
      "Epoch 62, Batch 1310, Loss: 188.21253967285156\n",
      "Epoch 62, Batch 1311, Loss: 158.98452758789062\n",
      "Epoch 62, Batch 1312, Loss: 169.09500122070312\n",
      "Epoch 62, Batch 1313, Loss: 187.28550720214844\n",
      "Epoch 62, Batch 1314, Loss: 181.02529907226562\n",
      "Epoch 62, Batch 1315, Loss: 190.58615112304688\n",
      "Epoch 62, Batch 1316, Loss: 163.09901428222656\n",
      "Epoch 62, Batch 1317, Loss: 165.8696746826172\n",
      "Epoch 62, Batch 1318, Loss: 173.96270751953125\n",
      "Epoch 62, Batch 1319, Loss: 160.54440307617188\n",
      "Epoch 62, Batch 1320, Loss: 182.28199768066406\n",
      "Epoch 62, Batch 1321, Loss: 194.02516174316406\n",
      "Epoch 62, Batch 1322, Loss: 181.69113159179688\n",
      "Epoch 62, Batch 1323, Loss: 161.70664978027344\n",
      "Epoch 62, Batch 1324, Loss: 165.9193572998047\n",
      "Epoch 62, Batch 1325, Loss: 163.07537841796875\n",
      "Epoch 62, Batch 1326, Loss: 161.84092712402344\n",
      "Epoch 62, Batch 1327, Loss: 189.20274353027344\n",
      "Epoch 62, Batch 1328, Loss: 159.4956512451172\n",
      "Epoch 62, Batch 1329, Loss: 189.0566864013672\n",
      "Epoch 62, Batch 1330, Loss: 165.8316192626953\n",
      "Epoch 62, Batch 1331, Loss: 198.8070526123047\n",
      "Epoch 62, Batch 1332, Loss: 186.8193359375\n",
      "Epoch 62, Batch 1333, Loss: 173.3511199951172\n",
      "Epoch 62, Batch 1334, Loss: 172.57626342773438\n",
      "Epoch 62, Batch 1335, Loss: 164.52410888671875\n",
      "Epoch 62, Batch 1336, Loss: 164.15609741210938\n",
      "Epoch 62, Batch 1337, Loss: 164.7277069091797\n",
      "Epoch 62, Batch 1338, Loss: 189.85226440429688\n",
      "Epoch 62, Batch 1339, Loss: 165.27960205078125\n",
      "Epoch 62, Batch 1340, Loss: 186.49049377441406\n",
      "Epoch 62, Batch 1341, Loss: 175.38624572753906\n",
      "Epoch 62, Batch 1342, Loss: 171.1522216796875\n",
      "Epoch 62, Batch 1343, Loss: 173.04258728027344\n",
      "Epoch 62, Batch 1344, Loss: 174.08848571777344\n",
      "Epoch 62, Batch 1345, Loss: 173.0298309326172\n",
      "Epoch 62, Batch 1346, Loss: 157.99215698242188\n",
      "Epoch 62, Batch 1347, Loss: 163.47161865234375\n",
      "Epoch 62, Batch 1348, Loss: 175.9339599609375\n",
      "Epoch 62, Batch 1349, Loss: 173.04144287109375\n",
      "Epoch 62, Batch 1350, Loss: 159.32940673828125\n",
      "Epoch 62, Batch 1351, Loss: 190.10873413085938\n",
      "Epoch 62, Batch 1352, Loss: 156.73654174804688\n",
      "Epoch 62, Batch 1353, Loss: 158.5120391845703\n",
      "Epoch 62, Batch 1354, Loss: 150.32215881347656\n",
      "Epoch 62, Batch 1355, Loss: 176.65444946289062\n",
      "Epoch 62, Batch 1356, Loss: 163.83111572265625\n",
      "Epoch 62, Batch 1357, Loss: 180.6522979736328\n",
      "Epoch 62, Batch 1358, Loss: 163.8558807373047\n",
      "Epoch 62, Batch 1359, Loss: 170.8568115234375\n",
      "Epoch 62, Batch 1360, Loss: 170.5849151611328\n",
      "Epoch 62, Batch 1361, Loss: 186.80589294433594\n",
      "Epoch 62, Batch 1362, Loss: 161.10995483398438\n",
      "Epoch 62, Batch 1363, Loss: 165.6470184326172\n",
      "Epoch 62, Batch 1364, Loss: 172.0772705078125\n",
      "Epoch 62, Batch 1365, Loss: 158.37034606933594\n",
      "Epoch 62, Batch 1366, Loss: 182.7094268798828\n",
      "Epoch 62, Batch 1367, Loss: 176.98472595214844\n",
      "Epoch 62, Batch 1368, Loss: 150.24868774414062\n",
      "Epoch 62, Batch 1369, Loss: 179.57400512695312\n",
      "Epoch 62, Batch 1370, Loss: 173.21408081054688\n",
      "Epoch 62, Batch 1371, Loss: 187.28997802734375\n",
      "Epoch 62, Batch 1372, Loss: 162.36903381347656\n",
      "Epoch 62, Batch 1373, Loss: 172.64572143554688\n",
      "Epoch 62, Batch 1374, Loss: 172.3061981201172\n",
      "Epoch 62, Batch 1375, Loss: 174.5398712158203\n",
      "Epoch 62, Batch 1376, Loss: 175.70782470703125\n",
      "Epoch 62, Batch 1377, Loss: 173.7042236328125\n",
      "Epoch 62, Batch 1378, Loss: 175.6302490234375\n",
      "Epoch 62, Batch 1379, Loss: 172.2935028076172\n",
      "Epoch 62, Batch 1380, Loss: 175.83021545410156\n",
      "Epoch 62, Batch 1381, Loss: 162.61985778808594\n",
      "Epoch 62, Batch 1382, Loss: 196.2945098876953\n",
      "Epoch 62, Batch 1383, Loss: 172.754150390625\n",
      "Epoch 62, Batch 1384, Loss: 175.23513793945312\n",
      "Epoch 62, Batch 1385, Loss: 179.79782104492188\n",
      "Epoch 62, Batch 1386, Loss: 170.24923706054688\n",
      "Epoch 62, Batch 1387, Loss: 174.80563354492188\n",
      "Epoch 62, Batch 1388, Loss: 171.92257690429688\n",
      "Epoch 62, Batch 1389, Loss: 172.62359619140625\n",
      "Epoch 62, Batch 1390, Loss: 168.33895874023438\n",
      "Epoch 62, Batch 1391, Loss: 186.24119567871094\n",
      "Epoch 62, Batch 1392, Loss: 163.73294067382812\n",
      "Epoch 62, Batch 1393, Loss: 172.08078002929688\n",
      "Epoch 62, Batch 1394, Loss: 174.39393615722656\n",
      "Epoch 62, Batch 1395, Loss: 175.50863647460938\n",
      "Epoch 62, Batch 1396, Loss: 164.52210998535156\n",
      "Epoch 62, Batch 1397, Loss: 157.5865020751953\n",
      "Epoch 62, Batch 1398, Loss: 183.23695373535156\n",
      "Epoch 62, Batch 1399, Loss: 163.50942993164062\n",
      "Epoch 62, Batch 1400, Loss: 187.52352905273438\n",
      "Epoch 62, Batch 1401, Loss: 172.62811279296875\n",
      "Epoch 62, Batch 1402, Loss: 181.87112426757812\n",
      "Epoch 62, Batch 1403, Loss: 169.48953247070312\n",
      "Epoch 62, Batch 1404, Loss: 173.7086944580078\n",
      "Epoch 62, Batch 1405, Loss: 181.3560028076172\n",
      "Epoch 62, Batch 1406, Loss: 179.2637176513672\n",
      "Epoch 62, Batch 1407, Loss: 145.68405151367188\n",
      "Epoch 62, Batch 1408, Loss: 169.52137756347656\n",
      "Epoch 62, Batch 1409, Loss: 171.66229248046875\n",
      "Epoch 62, Batch 1410, Loss: 170.96913146972656\n",
      "Epoch 62, Batch 1411, Loss: 181.2545928955078\n",
      "Epoch 62, Batch 1412, Loss: 176.84048461914062\n",
      "Epoch 62, Batch 1413, Loss: 156.8269805908203\n",
      "Epoch 62, Batch 1414, Loss: 182.81033325195312\n",
      "Epoch 62, Batch 1415, Loss: 169.78857421875\n",
      "Epoch 62, Batch 1416, Loss: 170.2611083984375\n",
      "Epoch 62, Batch 1417, Loss: 178.73574829101562\n",
      "Epoch 62, Batch 1418, Loss: 185.12066650390625\n",
      "Epoch 62, Batch 1419, Loss: 173.25640869140625\n",
      "Epoch 62, Batch 1420, Loss: 173.53009033203125\n",
      "Epoch 62, Batch 1421, Loss: 183.18960571289062\n",
      "Epoch 62, Batch 1422, Loss: 179.45570373535156\n",
      "Epoch 62, Batch 1423, Loss: 181.4843292236328\n",
      "Epoch 62, Batch 1424, Loss: 165.30686950683594\n",
      "Epoch 62, Batch 1425, Loss: 157.44166564941406\n",
      "Epoch 62, Batch 1426, Loss: 163.80043029785156\n",
      "Epoch 62, Batch 1427, Loss: 190.04879760742188\n",
      "Epoch 62, Batch 1428, Loss: 175.0340576171875\n",
      "Epoch 62, Batch 1429, Loss: 170.4489288330078\n",
      "Epoch 62, Batch 1430, Loss: 174.59640502929688\n",
      "Epoch 62, Batch 1431, Loss: 185.62753295898438\n",
      "Epoch 62, Batch 1432, Loss: 171.6365203857422\n",
      "Epoch 62, Batch 1433, Loss: 170.3125\n",
      "Epoch 62, Batch 1434, Loss: 164.46949768066406\n",
      "Epoch 62, Batch 1435, Loss: 167.92457580566406\n",
      "Epoch 62, Batch 1436, Loss: 155.53163146972656\n",
      "Epoch 62, Batch 1437, Loss: 157.459228515625\n",
      "Epoch 62, Batch 1438, Loss: 188.24215698242188\n",
      "Epoch 62, Batch 1439, Loss: 165.80880737304688\n",
      "Epoch 62, Batch 1440, Loss: 186.1607208251953\n",
      "Epoch 62, Batch 1441, Loss: 158.90277099609375\n",
      "Epoch 62, Batch 1442, Loss: 170.51068115234375\n",
      "Epoch 62, Batch 1443, Loss: 172.1566619873047\n",
      "Epoch 62, Batch 1444, Loss: 174.6740264892578\n",
      "Epoch 62, Batch 1445, Loss: 169.97012329101562\n",
      "Epoch 62, Batch 1446, Loss: 172.6284637451172\n",
      "Epoch 62, Batch 1447, Loss: 188.79127502441406\n",
      "Epoch 62, Batch 1448, Loss: 163.70425415039062\n",
      "Epoch 62, Batch 1449, Loss: 173.429931640625\n",
      "Epoch 62, Batch 1450, Loss: 183.34107971191406\n",
      "Epoch 62, Batch 1451, Loss: 176.1580047607422\n",
      "Epoch 62, Batch 1452, Loss: 184.38381958007812\n",
      "Epoch 62, Batch 1453, Loss: 160.40725708007812\n",
      "Epoch 62, Batch 1454, Loss: 161.52928161621094\n",
      "Epoch 62, Batch 1455, Loss: 174.4698944091797\n",
      "Epoch 62, Batch 1456, Loss: 162.7869415283203\n",
      "Epoch 62, Batch 1457, Loss: 165.0641632080078\n",
      "Epoch 62, Batch 1458, Loss: 169.97427368164062\n",
      "Epoch 62, Batch 1459, Loss: 181.54669189453125\n",
      "Epoch 62, Batch 1460, Loss: 167.8922119140625\n",
      "Epoch 62, Batch 1461, Loss: 176.90687561035156\n",
      "Epoch 62, Batch 1462, Loss: 181.81454467773438\n",
      "Epoch 62, Batch 1463, Loss: 161.6707305908203\n",
      "Epoch 62, Batch 1464, Loss: 171.9226837158203\n",
      "Epoch 62, Batch 1465, Loss: 178.78916931152344\n",
      "Epoch 62, Batch 1466, Loss: 166.3413543701172\n",
      "Epoch 62, Batch 1467, Loss: 171.61460876464844\n",
      "Epoch 62, Batch 1468, Loss: 168.00950622558594\n",
      "Epoch 62, Batch 1469, Loss: 174.48507690429688\n",
      "Epoch 62, Batch 1470, Loss: 182.90533447265625\n",
      "Epoch 62, Batch 1471, Loss: 178.55189514160156\n",
      "Epoch 62, Batch 1472, Loss: 175.05751037597656\n",
      "Epoch 62, Batch 1473, Loss: 159.3488006591797\n",
      "Epoch 62, Batch 1474, Loss: 162.90188598632812\n",
      "Epoch 62, Batch 1475, Loss: 159.22854614257812\n",
      "Epoch 62, Batch 1476, Loss: 176.4528045654297\n",
      "Epoch 62, Batch 1477, Loss: 168.73680114746094\n",
      "Epoch 62, Batch 1478, Loss: 193.15283203125\n",
      "Epoch 62, Batch 1479, Loss: 152.63082885742188\n",
      "Epoch 62, Batch 1480, Loss: 177.6754913330078\n",
      "Epoch 62, Batch 1481, Loss: 160.6189727783203\n",
      "Epoch 62, Batch 1482, Loss: 155.99826049804688\n",
      "Epoch 62, Batch 1483, Loss: 182.11903381347656\n",
      "Epoch 62, Batch 1484, Loss: 175.27053833007812\n",
      "Epoch 62, Batch 1485, Loss: 172.04843139648438\n",
      "Epoch 62, Batch 1486, Loss: 170.7447967529297\n",
      "Epoch 62, Batch 1487, Loss: 153.7799835205078\n",
      "Epoch 62, Batch 1488, Loss: 187.85980224609375\n",
      "Epoch 62, Batch 1489, Loss: 177.80430603027344\n",
      "Epoch 62, Batch 1490, Loss: 158.31597900390625\n",
      "Epoch 62, Batch 1491, Loss: 182.78402709960938\n",
      "Epoch 62, Batch 1492, Loss: 162.51210021972656\n",
      "Epoch 62, Batch 1493, Loss: 179.0981903076172\n",
      "Epoch 62, Batch 1494, Loss: 177.4140625\n",
      "Epoch 62, Batch 1495, Loss: 174.66461181640625\n",
      "Epoch 62, Batch 1496, Loss: 173.97512817382812\n",
      "Epoch 62, Batch 1497, Loss: 180.90682983398438\n",
      "Epoch 62, Batch 1498, Loss: 160.68405151367188\n",
      "Epoch 62, Batch 1499, Loss: 187.1966552734375\n",
      "Epoch 62, Batch 1500, Loss: 166.39239501953125\n",
      "Epoch 62, Batch 1501, Loss: 179.5943603515625\n",
      "Epoch 62, Batch 1502, Loss: 185.52963256835938\n",
      "Epoch 62, Batch 1503, Loss: 164.85752868652344\n",
      "Epoch 62, Batch 1504, Loss: 177.70452880859375\n",
      "Epoch 62, Batch 1505, Loss: 168.24339294433594\n",
      "Epoch 62, Batch 1506, Loss: 159.31509399414062\n",
      "Epoch 62, Batch 1507, Loss: 171.92567443847656\n",
      "Epoch 62, Batch 1508, Loss: 157.584228515625\n",
      "Epoch 62, Batch 1509, Loss: 177.0872344970703\n",
      "Epoch 62, Batch 1510, Loss: 175.34336853027344\n",
      "Epoch 62, Batch 1511, Loss: 173.7335205078125\n",
      "Epoch 62, Batch 1512, Loss: 174.5644073486328\n",
      "Epoch 62, Batch 1513, Loss: 162.94796752929688\n",
      "Epoch 62, Batch 1514, Loss: 177.3989715576172\n",
      "Epoch 62, Batch 1515, Loss: 178.80166625976562\n",
      "Epoch 62, Batch 1516, Loss: 172.1622314453125\n",
      "Epoch 62, Batch 1517, Loss: 173.3082275390625\n",
      "Epoch 62, Batch 1518, Loss: 184.77439880371094\n",
      "Epoch 62, Batch 1519, Loss: 185.12794494628906\n",
      "Epoch 62, Batch 1520, Loss: 169.61102294921875\n",
      "Epoch 62, Batch 1521, Loss: 190.69683837890625\n",
      "Epoch 62, Batch 1522, Loss: 185.1707000732422\n",
      "Epoch 62, Batch 1523, Loss: 161.33253479003906\n",
      "Epoch 62, Batch 1524, Loss: 179.33717346191406\n",
      "Epoch 62, Batch 1525, Loss: 183.77093505859375\n",
      "Epoch 62, Batch 1526, Loss: 184.59698486328125\n",
      "Epoch 62, Batch 1527, Loss: 153.8534698486328\n",
      "Epoch 62, Batch 1528, Loss: 188.5848846435547\n",
      "Epoch 62, Batch 1529, Loss: 175.01853942871094\n",
      "Epoch 62, Batch 1530, Loss: 174.45249938964844\n",
      "Epoch 62, Batch 1531, Loss: 174.94332885742188\n",
      "Epoch 62, Batch 1532, Loss: 173.24517822265625\n",
      "Epoch 62, Batch 1533, Loss: 160.56710815429688\n",
      "Epoch 62, Batch 1534, Loss: 167.4448699951172\n",
      "Epoch 62, Batch 1535, Loss: 174.3282470703125\n",
      "Epoch 62, Batch 1536, Loss: 152.06700134277344\n",
      "Epoch 62, Batch 1537, Loss: 166.33538818359375\n",
      "Epoch 62, Batch 1538, Loss: 179.6719207763672\n",
      "Epoch 62, Batch 1539, Loss: 185.53941345214844\n",
      "Epoch 62, Batch 1540, Loss: 176.4986572265625\n",
      "Epoch 62, Batch 1541, Loss: 167.03076171875\n",
      "Epoch 62, Batch 1542, Loss: 180.96148681640625\n",
      "Epoch 62, Batch 1543, Loss: 175.11459350585938\n",
      "Epoch 62, Batch 1544, Loss: 183.99986267089844\n",
      "Epoch 62, Batch 1545, Loss: 182.76611328125\n",
      "Epoch 62, Batch 1546, Loss: 170.38357543945312\n",
      "Epoch 62, Batch 1547, Loss: 172.6798858642578\n",
      "Epoch 62, Batch 1548, Loss: 175.72459411621094\n",
      "Epoch 62, Batch 1549, Loss: 174.52818298339844\n",
      "Epoch 62, Batch 1550, Loss: 155.36805725097656\n",
      "Epoch 62, Batch 1551, Loss: 174.36985778808594\n",
      "Epoch 62, Batch 1552, Loss: 166.4432830810547\n",
      "Epoch 62, Batch 1553, Loss: 163.07955932617188\n",
      "Epoch 62, Batch 1554, Loss: 169.75547790527344\n",
      "Epoch 62, Batch 1555, Loss: 182.2121124267578\n",
      "Epoch 62, Batch 1556, Loss: 164.4088134765625\n",
      "Epoch 62, Batch 1557, Loss: 156.28855895996094\n",
      "Epoch 62, Batch 1558, Loss: 177.20591735839844\n",
      "Epoch 62, Batch 1559, Loss: 163.67922973632812\n",
      "Epoch 62, Batch 1560, Loss: 163.4694061279297\n",
      "Epoch 62, Batch 1561, Loss: 171.06532287597656\n",
      "Epoch 62, Batch 1562, Loss: 178.26251220703125\n",
      "Epoch 62, Batch 1563, Loss: 186.81216430664062\n",
      "Epoch 62, Batch 1564, Loss: 174.1201171875\n",
      "Epoch 62, Batch 1565, Loss: 164.23020935058594\n",
      "Epoch 62, Batch 1566, Loss: 172.88046264648438\n",
      "Epoch 62, Batch 1567, Loss: 170.4078826904297\n",
      "Epoch 62, Batch 1568, Loss: 159.16311645507812\n",
      "Epoch 62, Batch 1569, Loss: 161.28106689453125\n",
      "Epoch 62, Batch 1570, Loss: 171.44862365722656\n",
      "Epoch 62, Batch 1571, Loss: 173.51148986816406\n",
      "Epoch 62, Batch 1572, Loss: 158.6263885498047\n",
      "Epoch 62, Batch 1573, Loss: 196.34629821777344\n",
      "Epoch 62, Batch 1574, Loss: 161.89659118652344\n",
      "Epoch 62, Batch 1575, Loss: 157.27294921875\n",
      "Epoch 62, Batch 1576, Loss: 180.9912567138672\n",
      "Epoch 62, Batch 1577, Loss: 153.87081909179688\n",
      "Epoch 62, Batch 1578, Loss: 166.92079162597656\n",
      "Epoch 62, Batch 1579, Loss: 169.5697784423828\n",
      "Epoch 62, Batch 1580, Loss: 168.43057250976562\n",
      "Epoch 62, Batch 1581, Loss: 183.0750732421875\n",
      "Epoch 62, Batch 1582, Loss: 175.4228515625\n",
      "Epoch 62, Batch 1583, Loss: 175.66246032714844\n",
      "Epoch 62, Batch 1584, Loss: 191.3650665283203\n",
      "Epoch 62, Batch 1585, Loss: 176.0581512451172\n",
      "Epoch 62, Batch 1586, Loss: 179.05506896972656\n",
      "Epoch 62, Batch 1587, Loss: 167.19717407226562\n",
      "Epoch 62, Batch 1588, Loss: 169.20448303222656\n",
      "Epoch 62, Batch 1589, Loss: 174.9990997314453\n",
      "Epoch 62, Batch 1590, Loss: 163.29991149902344\n",
      "Epoch 62, Batch 1591, Loss: 154.24551391601562\n",
      "Epoch 62, Batch 1592, Loss: 182.42555236816406\n",
      "Epoch 62, Batch 1593, Loss: 172.19467163085938\n",
      "Epoch 62, Batch 1594, Loss: 168.0565643310547\n",
      "Epoch 62, Batch 1595, Loss: 169.4769287109375\n",
      "Epoch 62, Batch 1596, Loss: 163.14886474609375\n",
      "Epoch 62, Batch 1597, Loss: 196.23883056640625\n",
      "Epoch 62, Batch 1598, Loss: 176.90216064453125\n",
      "Epoch 62, Batch 1599, Loss: 167.36465454101562\n",
      "Epoch 62, Batch 1600, Loss: 185.3142547607422\n",
      "Epoch 62, Batch 1601, Loss: 170.81814575195312\n",
      "Epoch 62, Batch 1602, Loss: 175.6231689453125\n",
      "Epoch 62, Batch 1603, Loss: 170.0907745361328\n",
      "Epoch 62, Batch 1604, Loss: 175.0165557861328\n",
      "Epoch 62, Batch 1605, Loss: 168.5529327392578\n",
      "Epoch 62, Batch 1606, Loss: 163.1108856201172\n",
      "Epoch 62, Batch 1607, Loss: 169.5839385986328\n",
      "Epoch 62, Batch 1608, Loss: 174.0923309326172\n",
      "Epoch 62, Batch 1609, Loss: 161.1773681640625\n",
      "Epoch 62, Batch 1610, Loss: 168.90576171875\n",
      "Epoch 62, Batch 1611, Loss: 167.42333984375\n",
      "Epoch 62, Batch 1612, Loss: 158.55064392089844\n",
      "Epoch 62, Batch 1613, Loss: 188.04177856445312\n",
      "Epoch 62, Batch 1614, Loss: 158.93482971191406\n",
      "Epoch 62, Batch 1615, Loss: 179.70709228515625\n",
      "Epoch 62, Batch 1616, Loss: 166.8193817138672\n",
      "Epoch 62, Batch 1617, Loss: 160.9705352783203\n",
      "Epoch 62, Batch 1618, Loss: 161.0390625\n",
      "Epoch 62, Batch 1619, Loss: 181.59423828125\n",
      "Epoch 62, Batch 1620, Loss: 180.55844116210938\n",
      "Epoch 62, Batch 1621, Loss: 172.32022094726562\n",
      "Epoch 62, Batch 1622, Loss: 180.16650390625\n",
      "Epoch 62, Batch 1623, Loss: 174.3943634033203\n",
      "Epoch 62, Batch 1624, Loss: 170.2102813720703\n",
      "Epoch 62, Batch 1625, Loss: 164.4767608642578\n",
      "Epoch 62, Batch 1626, Loss: 168.4444580078125\n",
      "Epoch 62, Batch 1627, Loss: 175.02957153320312\n",
      "Epoch 62, Batch 1628, Loss: 168.39865112304688\n",
      "Epoch 62, Batch 1629, Loss: 166.96469116210938\n",
      "Epoch 62, Batch 1630, Loss: 175.73907470703125\n",
      "Epoch 62, Batch 1631, Loss: 168.35000610351562\n",
      "Epoch 62, Batch 1632, Loss: 165.0675506591797\n",
      "Epoch 62, Batch 1633, Loss: 175.27041625976562\n",
      "Epoch 62, Batch 1634, Loss: 156.69163513183594\n",
      "Epoch 62, Batch 1635, Loss: 185.13833618164062\n",
      "Epoch 62, Batch 1636, Loss: 171.39181518554688\n",
      "Epoch 62, Batch 1637, Loss: 155.03965759277344\n",
      "Epoch 62, Batch 1638, Loss: 162.35536193847656\n",
      "Epoch 62, Batch 1639, Loss: 175.52947998046875\n",
      "Epoch 62, Batch 1640, Loss: 174.79873657226562\n",
      "Epoch 62, Batch 1641, Loss: 180.977783203125\n",
      "Epoch 62, Batch 1642, Loss: 173.306640625\n",
      "Epoch 62, Batch 1643, Loss: 180.01283264160156\n",
      "Epoch 62, Batch 1644, Loss: 161.22842407226562\n",
      "Epoch 62, Batch 1645, Loss: 172.50648498535156\n",
      "Epoch 62, Batch 1646, Loss: 186.78277587890625\n",
      "Epoch 62, Batch 1647, Loss: 164.4596405029297\n",
      "Epoch 62, Batch 1648, Loss: 169.29153442382812\n",
      "Epoch 62, Batch 1649, Loss: 166.32261657714844\n",
      "Epoch 62, Batch 1650, Loss: 180.68038940429688\n",
      "Epoch 62, Batch 1651, Loss: 189.67738342285156\n",
      "Epoch 62, Batch 1652, Loss: 172.37109375\n",
      "Epoch 62, Batch 1653, Loss: 170.32020568847656\n",
      "Epoch 62, Batch 1654, Loss: 161.5527801513672\n",
      "Epoch 62, Batch 1655, Loss: 162.4024658203125\n",
      "Epoch 62, Batch 1656, Loss: 167.24407958984375\n",
      "Epoch 62, Batch 1657, Loss: 171.3392333984375\n",
      "Epoch 62, Batch 1658, Loss: 165.9539031982422\n",
      "Epoch 62, Batch 1659, Loss: 160.82411193847656\n",
      "Epoch 62, Batch 1660, Loss: 168.50949096679688\n",
      "Epoch 62, Batch 1661, Loss: 181.88365173339844\n",
      "Epoch 62, Batch 1662, Loss: 174.5335235595703\n",
      "Epoch 62, Batch 1663, Loss: 179.24293518066406\n",
      "Epoch 62, Batch 1664, Loss: 179.33497619628906\n",
      "Epoch 62, Batch 1665, Loss: 167.53736877441406\n",
      "Epoch 62, Batch 1666, Loss: 172.5536346435547\n",
      "Epoch 62, Batch 1667, Loss: 155.09255981445312\n",
      "Epoch 62, Batch 1668, Loss: 172.8316192626953\n",
      "Epoch 62, Batch 1669, Loss: 161.4724578857422\n",
      "Epoch 62, Batch 1670, Loss: 183.75721740722656\n",
      "Epoch 62, Batch 1671, Loss: 174.13311767578125\n",
      "Epoch 62, Batch 1672, Loss: 163.28485107421875\n",
      "Epoch 62, Batch 1673, Loss: 186.73843383789062\n",
      "Epoch 62, Batch 1674, Loss: 184.57618713378906\n",
      "Epoch 62, Batch 1675, Loss: 167.16372680664062\n",
      "Epoch 62, Batch 1676, Loss: 166.11097717285156\n",
      "Epoch 62, Batch 1677, Loss: 179.0146026611328\n",
      "Epoch 62, Batch 1678, Loss: 161.39683532714844\n",
      "Epoch 62, Batch 1679, Loss: 164.0537567138672\n",
      "Epoch 62, Batch 1680, Loss: 178.24891662597656\n",
      "Epoch 62, Batch 1681, Loss: 167.0625\n",
      "Epoch 62, Batch 1682, Loss: 159.6937713623047\n",
      "Epoch 62, Batch 1683, Loss: 174.10528564453125\n",
      "Epoch 62, Batch 1684, Loss: 190.83082580566406\n",
      "Epoch 62, Batch 1685, Loss: 161.48448181152344\n",
      "Epoch 62, Batch 1686, Loss: 157.43162536621094\n",
      "Epoch 62, Batch 1687, Loss: 187.87298583984375\n",
      "Epoch 62, Batch 1688, Loss: 177.2851104736328\n",
      "Epoch 62, Batch 1689, Loss: 175.5096893310547\n",
      "Epoch 62, Batch 1690, Loss: 169.72927856445312\n",
      "Epoch 62, Batch 1691, Loss: 186.5114288330078\n",
      "Epoch 62, Batch 1692, Loss: 173.57684326171875\n",
      "Epoch 62, Batch 1693, Loss: 173.8625030517578\n",
      "Epoch 62, Batch 1694, Loss: 177.0952606201172\n",
      "Epoch 62, Batch 1695, Loss: 170.3438262939453\n",
      "Epoch 62, Batch 1696, Loss: 167.6427764892578\n",
      "Epoch 62, Batch 1697, Loss: 178.0026092529297\n",
      "Epoch 62, Batch 1698, Loss: 173.47427368164062\n",
      "Epoch 62, Batch 1699, Loss: 176.47608947753906\n",
      "Epoch 62, Batch 1700, Loss: 176.73944091796875\n",
      "Epoch 62, Batch 1701, Loss: 178.77281188964844\n",
      "Epoch 62, Batch 1702, Loss: 182.99386596679688\n",
      "Epoch 62, Batch 1703, Loss: 168.38229370117188\n",
      "Epoch 62, Batch 1704, Loss: 182.20115661621094\n",
      "Epoch 62, Batch 1705, Loss: 166.1883087158203\n",
      "Epoch 62, Batch 1706, Loss: 169.57791137695312\n",
      "Epoch 62, Batch 1707, Loss: 157.10150146484375\n",
      "Epoch 62, Batch 1708, Loss: 164.61680603027344\n",
      "Epoch 62, Batch 1709, Loss: 178.98045349121094\n",
      "Epoch 62, Batch 1710, Loss: 179.67417907714844\n",
      "Epoch 62, Batch 1711, Loss: 160.92593383789062\n",
      "Epoch 62, Batch 1712, Loss: 175.1149444580078\n",
      "Epoch 62, Batch 1713, Loss: 167.3392333984375\n",
      "Epoch 62, Batch 1714, Loss: 164.8903045654297\n",
      "Epoch 62, Batch 1715, Loss: 177.43661499023438\n",
      "Epoch 62, Batch 1716, Loss: 177.49203491210938\n",
      "Epoch 62, Batch 1717, Loss: 161.9733123779297\n",
      "Epoch 62, Batch 1718, Loss: 159.39556884765625\n",
      "Epoch 62, Batch 1719, Loss: 168.2373809814453\n",
      "Epoch 62, Batch 1720, Loss: 180.06936645507812\n",
      "Epoch 62, Batch 1721, Loss: 181.02420043945312\n",
      "Epoch 62, Batch 1722, Loss: 168.46206665039062\n",
      "Epoch 62, Batch 1723, Loss: 167.45228576660156\n",
      "Epoch 62, Batch 1724, Loss: 166.93531799316406\n",
      "Epoch 62, Batch 1725, Loss: 183.25909423828125\n",
      "Epoch 62, Batch 1726, Loss: 161.2886962890625\n",
      "Epoch 62, Batch 1727, Loss: 169.67971801757812\n",
      "Epoch 62, Batch 1728, Loss: 173.81158447265625\n",
      "Epoch 62, Batch 1729, Loss: 189.4453887939453\n",
      "Epoch 62, Batch 1730, Loss: 180.9632110595703\n",
      "Epoch 62, Batch 1731, Loss: 186.94534301757812\n",
      "Epoch 62, Batch 1732, Loss: 178.1485137939453\n",
      "Epoch 62, Batch 1733, Loss: 169.46470642089844\n",
      "Epoch 62, Batch 1734, Loss: 178.924560546875\n",
      "Epoch 62, Batch 1735, Loss: 177.90415954589844\n",
      "Epoch 62, Batch 1736, Loss: 173.732666015625\n",
      "Epoch 62, Batch 1737, Loss: 173.1885223388672\n",
      "Epoch 62, Batch 1738, Loss: 171.49954223632812\n",
      "Epoch 62, Batch 1739, Loss: 177.8284912109375\n",
      "Epoch 62, Batch 1740, Loss: 200.4908447265625\n",
      "Epoch 62, Batch 1741, Loss: 178.4429168701172\n",
      "Epoch 62, Batch 1742, Loss: 186.0255889892578\n",
      "Epoch 62, Batch 1743, Loss: 171.48257446289062\n",
      "Epoch 62, Batch 1744, Loss: 162.13938903808594\n",
      "Epoch 62, Batch 1745, Loss: 163.35427856445312\n",
      "Epoch 62, Batch 1746, Loss: 155.64915466308594\n",
      "Epoch 62, Batch 1747, Loss: 182.246337890625\n",
      "Epoch 62, Batch 1748, Loss: 170.50274658203125\n",
      "Epoch 62, Batch 1749, Loss: 160.64405822753906\n",
      "Epoch 62, Batch 1750, Loss: 165.18399047851562\n",
      "Epoch 62, Batch 1751, Loss: 172.0983123779297\n",
      "Epoch 62, Batch 1752, Loss: 163.56045532226562\n",
      "Epoch 62, Batch 1753, Loss: 173.08544921875\n",
      "Epoch 62, Batch 1754, Loss: 179.68922424316406\n",
      "Epoch 62, Batch 1755, Loss: 164.1835479736328\n",
      "Epoch 62, Batch 1756, Loss: 173.6405792236328\n",
      "Epoch 62, Batch 1757, Loss: 173.3059844970703\n",
      "Epoch 62, Batch 1758, Loss: 160.37451171875\n",
      "Epoch 62, Batch 1759, Loss: 188.46139526367188\n",
      "Epoch 62, Batch 1760, Loss: 157.49591064453125\n",
      "Epoch 62, Batch 1761, Loss: 166.95718383789062\n",
      "Epoch 62, Batch 1762, Loss: 183.64266967773438\n",
      "Epoch 62, Batch 1763, Loss: 171.48635864257812\n",
      "Epoch 62, Batch 1764, Loss: 183.98336791992188\n",
      "Epoch 62, Batch 1765, Loss: 163.09153747558594\n",
      "Epoch 62, Batch 1766, Loss: 188.6221160888672\n",
      "Epoch 62, Batch 1767, Loss: 172.26524353027344\n",
      "Epoch 62, Batch 1768, Loss: 176.34906005859375\n",
      "Epoch 62, Batch 1769, Loss: 155.3760528564453\n",
      "Epoch 62, Batch 1770, Loss: 189.73825073242188\n",
      "Epoch 62, Batch 1771, Loss: 183.07081604003906\n",
      "Epoch 62, Batch 1772, Loss: 187.55874633789062\n",
      "Epoch 62, Batch 1773, Loss: 169.36598205566406\n",
      "Epoch 62, Batch 1774, Loss: 167.02288818359375\n",
      "Epoch 62, Batch 1775, Loss: 179.36277770996094\n",
      "Epoch 62, Batch 1776, Loss: 165.92919921875\n",
      "Epoch 62, Batch 1777, Loss: 168.01644897460938\n",
      "Epoch 62, Batch 1778, Loss: 177.10618591308594\n",
      "Epoch 62, Batch 1779, Loss: 178.4285430908203\n",
      "Epoch 62, Batch 1780, Loss: 185.93722534179688\n",
      "Epoch 62, Batch 1781, Loss: 178.30653381347656\n",
      "Epoch 62, Batch 1782, Loss: 161.45591735839844\n",
      "Epoch 62, Batch 1783, Loss: 175.05331420898438\n",
      "Epoch 62, Batch 1784, Loss: 170.61436462402344\n",
      "Epoch 62, Batch 1785, Loss: 185.28689575195312\n",
      "Epoch 62, Batch 1786, Loss: 166.3431396484375\n",
      "Epoch 62, Batch 1787, Loss: 156.53468322753906\n",
      "Epoch 62, Batch 1788, Loss: 171.7218017578125\n",
      "Epoch 62, Batch 1789, Loss: 167.56446838378906\n",
      "Epoch 62, Batch 1790, Loss: 187.97137451171875\n",
      "Epoch 62, Batch 1791, Loss: 163.6294403076172\n",
      "Epoch 62, Batch 1792, Loss: 192.36598205566406\n",
      "Epoch 62, Batch 1793, Loss: 156.8555908203125\n",
      "Epoch 62, Batch 1794, Loss: 179.9697723388672\n",
      "Epoch 62, Batch 1795, Loss: 174.63198852539062\n",
      "Epoch 62, Batch 1796, Loss: 174.40025329589844\n",
      "Epoch 62, Batch 1797, Loss: 161.84686279296875\n",
      "Epoch 62, Batch 1798, Loss: 185.04446411132812\n",
      "Epoch 62, Batch 1799, Loss: 188.31692504882812\n",
      "Epoch 62, Batch 1800, Loss: 172.99234008789062\n",
      "Epoch 62, Batch 1801, Loss: 163.52105712890625\n",
      "Epoch 62, Batch 1802, Loss: 173.3672637939453\n",
      "Epoch 62, Batch 1803, Loss: 165.0797119140625\n",
      "Epoch 62, Batch 1804, Loss: 175.9958953857422\n",
      "Epoch 62, Batch 1805, Loss: 155.76116943359375\n",
      "Epoch 62, Batch 1806, Loss: 178.387939453125\n",
      "Epoch 62, Batch 1807, Loss: 166.9180450439453\n",
      "Epoch 62, Batch 1808, Loss: 174.2468719482422\n",
      "Epoch 62, Batch 1809, Loss: 189.53396606445312\n",
      "Epoch 62, Batch 1810, Loss: 189.630859375\n",
      "Epoch 62, Batch 1811, Loss: 171.88050842285156\n",
      "Epoch 62, Batch 1812, Loss: 176.35406494140625\n",
      "Epoch 62, Batch 1813, Loss: 163.35369873046875\n",
      "Epoch 62, Batch 1814, Loss: 155.35585021972656\n",
      "Epoch 62, Batch 1815, Loss: 171.66221618652344\n",
      "Epoch 62, Batch 1816, Loss: 176.79664611816406\n",
      "Epoch 62, Batch 1817, Loss: 179.51768493652344\n",
      "Epoch 62, Batch 1818, Loss: 177.6802978515625\n",
      "Epoch 62, Batch 1819, Loss: 179.12713623046875\n",
      "Epoch 62, Batch 1820, Loss: 175.0332489013672\n",
      "Epoch 62, Batch 1821, Loss: 172.90419006347656\n",
      "Epoch 62, Batch 1822, Loss: 161.3919677734375\n",
      "Epoch 62, Batch 1823, Loss: 189.89508056640625\n",
      "Epoch 62, Batch 1824, Loss: 167.71641540527344\n",
      "Epoch 62, Batch 1825, Loss: 177.1357421875\n",
      "Epoch 62, Batch 1826, Loss: 191.72640991210938\n",
      "Epoch 62, Batch 1827, Loss: 163.39669799804688\n",
      "Epoch 62, Batch 1828, Loss: 170.24969482421875\n",
      "Epoch 62, Batch 1829, Loss: 181.42930603027344\n",
      "Epoch 62, Batch 1830, Loss: 155.94873046875\n",
      "Epoch 62, Batch 1831, Loss: 170.04933166503906\n",
      "Epoch 62, Batch 1832, Loss: 171.3072509765625\n",
      "Epoch 62, Batch 1833, Loss: 167.5109100341797\n",
      "Epoch 62, Batch 1834, Loss: 157.37249755859375\n",
      "Epoch 62, Batch 1835, Loss: 164.62086486816406\n",
      "Epoch 62, Batch 1836, Loss: 163.3072967529297\n",
      "Epoch 62, Batch 1837, Loss: 175.24447631835938\n",
      "Epoch 62, Batch 1838, Loss: 181.325927734375\n",
      "Epoch 62, Batch 1839, Loss: 159.00502014160156\n",
      "Epoch 62, Batch 1840, Loss: 180.32102966308594\n",
      "Epoch 62, Batch 1841, Loss: 181.52679443359375\n",
      "Epoch 62, Batch 1842, Loss: 185.48635864257812\n",
      "Epoch 62, Batch 1843, Loss: 179.3188934326172\n",
      "Epoch 62, Batch 1844, Loss: 161.10508728027344\n",
      "Epoch 62, Batch 1845, Loss: 171.4212188720703\n",
      "Epoch 62, Batch 1846, Loss: 159.44815063476562\n",
      "Epoch 62, Batch 1847, Loss: 178.19583129882812\n",
      "Epoch 62, Batch 1848, Loss: 175.29733276367188\n",
      "Epoch 62, Batch 1849, Loss: 173.45181274414062\n",
      "Epoch 62, Batch 1850, Loss: 192.5881805419922\n",
      "Epoch 62, Batch 1851, Loss: 164.92279052734375\n",
      "Epoch 62, Batch 1852, Loss: 179.974609375\n",
      "Epoch 62, Batch 1853, Loss: 196.3841552734375\n",
      "Epoch 62, Batch 1854, Loss: 162.23193359375\n",
      "Epoch 62, Batch 1855, Loss: 163.11776733398438\n",
      "Epoch 62, Batch 1856, Loss: 184.45132446289062\n",
      "Epoch 62, Batch 1857, Loss: 170.07945251464844\n",
      "Epoch 62, Batch 1858, Loss: 166.28347778320312\n",
      "Epoch 62, Batch 1859, Loss: 180.44883728027344\n",
      "Epoch 62, Batch 1860, Loss: 161.77500915527344\n",
      "Epoch 62, Batch 1861, Loss: 178.13311767578125\n",
      "Epoch 62, Batch 1862, Loss: 171.79698181152344\n",
      "Epoch 62, Batch 1863, Loss: 181.3042449951172\n",
      "Epoch 62, Batch 1864, Loss: 170.27676391601562\n",
      "Epoch 62, Batch 1865, Loss: 158.6827392578125\n",
      "Epoch 62, Batch 1866, Loss: 177.08888244628906\n",
      "Epoch 62, Batch 1867, Loss: 166.8755340576172\n",
      "Epoch 62, Batch 1868, Loss: 177.54708862304688\n",
      "Epoch 62, Batch 1869, Loss: 168.99436950683594\n",
      "Epoch 62, Batch 1870, Loss: 181.40182495117188\n",
      "Epoch 62, Batch 1871, Loss: 159.17469787597656\n",
      "Epoch 62, Batch 1872, Loss: 171.85977172851562\n",
      "Epoch 62, Batch 1873, Loss: 176.19248962402344\n",
      "Epoch 62, Batch 1874, Loss: 168.77293395996094\n",
      "Epoch 62, Batch 1875, Loss: 186.6830291748047\n",
      "Epoch 62, Batch 1876, Loss: 191.3833465576172\n",
      "Epoch 62, Batch 1877, Loss: 168.70172119140625\n",
      "Epoch 62, Batch 1878, Loss: 174.10079956054688\n",
      "Epoch 62, Batch 1879, Loss: 185.0917205810547\n",
      "Epoch 62, Batch 1880, Loss: 185.11546325683594\n",
      "Epoch 62, Batch 1881, Loss: 174.28460693359375\n",
      "Epoch 62, Batch 1882, Loss: 187.534912109375\n",
      "Epoch 62, Batch 1883, Loss: 201.5578155517578\n",
      "Epoch 62, Batch 1884, Loss: 176.3339385986328\n",
      "Epoch 62, Batch 1885, Loss: 167.1731719970703\n",
      "Epoch 62, Batch 1886, Loss: 168.71046447753906\n",
      "Epoch 62, Batch 1887, Loss: 162.6062774658203\n",
      "Epoch 62, Batch 1888, Loss: 161.63487243652344\n",
      "Epoch 62, Batch 1889, Loss: 183.30987548828125\n",
      "Epoch 62, Batch 1890, Loss: 167.5403594970703\n",
      "Epoch 62, Batch 1891, Loss: 174.80108642578125\n",
      "Epoch 62, Batch 1892, Loss: 182.62283325195312\n",
      "Epoch 62, Batch 1893, Loss: 168.88568115234375\n",
      "Epoch 62, Batch 1894, Loss: 182.8098602294922\n",
      "Epoch 62, Batch 1895, Loss: 170.776611328125\n",
      "Epoch 62, Batch 1896, Loss: 176.70053100585938\n",
      "Epoch 62, Batch 1897, Loss: 172.9217071533203\n",
      "Epoch 62, Batch 1898, Loss: 172.74488830566406\n",
      "Epoch 62, Batch 1899, Loss: 181.55178833007812\n",
      "Epoch 62, Batch 1900, Loss: 153.57435607910156\n",
      "Epoch 62, Batch 1901, Loss: 169.87515258789062\n",
      "Epoch 62, Batch 1902, Loss: 176.59642028808594\n",
      "Epoch 62, Batch 1903, Loss: 160.07505798339844\n",
      "Epoch 62, Batch 1904, Loss: 164.12937927246094\n",
      "Epoch 62, Batch 1905, Loss: 168.19981384277344\n",
      "Epoch 62, Batch 1906, Loss: 177.33279418945312\n",
      "Epoch 62, Batch 1907, Loss: 180.07225036621094\n",
      "Epoch 62, Batch 1908, Loss: 156.87303161621094\n",
      "Epoch 62, Batch 1909, Loss: 150.5392303466797\n",
      "Epoch 62, Batch 1910, Loss: 169.24984741210938\n",
      "Epoch 62, Batch 1911, Loss: 155.9080047607422\n",
      "Epoch 62, Batch 1912, Loss: 182.31324768066406\n",
      "Epoch 62, Batch 1913, Loss: 173.08726501464844\n",
      "Epoch 62, Batch 1914, Loss: 191.45498657226562\n",
      "Epoch 62, Batch 1915, Loss: 172.81004333496094\n",
      "Epoch 62, Batch 1916, Loss: 174.92376708984375\n",
      "Epoch 62, Batch 1917, Loss: 170.41616821289062\n",
      "Epoch 62, Batch 1918, Loss: 187.3098602294922\n",
      "Epoch 62, Batch 1919, Loss: 187.95359802246094\n",
      "Epoch 62, Batch 1920, Loss: 164.76763916015625\n",
      "Epoch 62, Batch 1921, Loss: 178.67764282226562\n",
      "Epoch 62, Batch 1922, Loss: 182.41334533691406\n",
      "Epoch 62, Batch 1923, Loss: 185.1051788330078\n",
      "Epoch 62, Batch 1924, Loss: 181.13137817382812\n",
      "Epoch 62, Batch 1925, Loss: 185.2003173828125\n",
      "Epoch 62, Batch 1926, Loss: 166.05699157714844\n",
      "Epoch 62, Batch 1927, Loss: 167.14248657226562\n",
      "Epoch 62, Batch 1928, Loss: 175.74021911621094\n",
      "Epoch 62, Batch 1929, Loss: 178.45144653320312\n",
      "Epoch 62, Batch 1930, Loss: 174.60723876953125\n",
      "Epoch 62, Batch 1931, Loss: 164.2924346923828\n",
      "Epoch 62, Batch 1932, Loss: 173.55557250976562\n",
      "Epoch 62, Batch 1933, Loss: 180.61265563964844\n",
      "Epoch 62, Batch 1934, Loss: 161.38438415527344\n",
      "Epoch 62, Batch 1935, Loss: 160.0128173828125\n",
      "Epoch 62, Batch 1936, Loss: 174.9189910888672\n",
      "Epoch 62, Batch 1937, Loss: 157.8354949951172\n",
      "Epoch 62, Batch 1938, Loss: 178.46142578125\n",
      "Epoch 62, Batch 1939, Loss: 173.22723388671875\n",
      "Epoch 62, Batch 1940, Loss: 164.55072021484375\n",
      "Epoch 62, Batch 1941, Loss: 184.42974853515625\n",
      "Epoch 62, Batch 1942, Loss: 162.67340087890625\n",
      "Epoch 62, Batch 1943, Loss: 165.13516235351562\n",
      "Epoch 62, Batch 1944, Loss: 173.0360870361328\n",
      "Epoch 62, Batch 1945, Loss: 179.6468048095703\n",
      "Epoch 62, Batch 1946, Loss: 192.67340087890625\n",
      "Epoch 62, Batch 1947, Loss: 162.700927734375\n",
      "Epoch 62, Batch 1948, Loss: 196.761962890625\n",
      "Epoch 62, Batch 1949, Loss: 162.6893768310547\n",
      "Epoch 62, Batch 1950, Loss: 170.0662841796875\n",
      "Epoch 62, Batch 1951, Loss: 171.63002014160156\n",
      "Epoch 62, Batch 1952, Loss: 172.63499450683594\n",
      "Epoch 62, Batch 1953, Loss: 175.05384826660156\n",
      "Epoch 62, Batch 1954, Loss: 174.39695739746094\n",
      "Epoch 62, Batch 1955, Loss: 176.62574768066406\n",
      "Epoch 62, Batch 1956, Loss: 158.86180114746094\n",
      "Epoch 62, Batch 1957, Loss: 203.31155395507812\n",
      "Epoch 62, Batch 1958, Loss: 169.3607177734375\n",
      "Epoch 62, Batch 1959, Loss: 208.86355590820312\n",
      "Epoch 62, Batch 1960, Loss: 176.19601440429688\n",
      "Epoch 62, Batch 1961, Loss: 161.8200225830078\n",
      "Epoch 62, Batch 1962, Loss: 168.2391357421875\n",
      "Epoch 62, Batch 1963, Loss: 162.52052307128906\n",
      "Epoch 62, Batch 1964, Loss: 190.83009338378906\n",
      "Epoch 62, Batch 1965, Loss: 164.7767791748047\n",
      "Epoch 62, Batch 1966, Loss: 151.9198760986328\n",
      "Epoch 62, Batch 1967, Loss: 187.4779052734375\n",
      "Epoch 62, Batch 1968, Loss: 161.58067321777344\n",
      "Epoch 62, Batch 1969, Loss: 168.9728546142578\n",
      "Epoch 62, Batch 1970, Loss: 163.7024383544922\n",
      "Epoch 62, Batch 1971, Loss: 188.13722229003906\n",
      "Epoch 62, Batch 1972, Loss: 184.90951538085938\n",
      "Epoch 62, Batch 1973, Loss: 177.14405822753906\n",
      "Epoch 62, Batch 1974, Loss: 174.73504638671875\n",
      "Epoch 62, Batch 1975, Loss: 151.98171997070312\n",
      "Epoch 62, Batch 1976, Loss: 203.06500244140625\n",
      "Epoch 62, Batch 1977, Loss: 180.8988037109375\n",
      "Epoch 62, Batch 1978, Loss: 179.22488403320312\n",
      "Epoch 62, Batch 1979, Loss: 177.02305603027344\n",
      "Epoch 62, Batch 1980, Loss: 174.4116668701172\n",
      "Epoch 62, Batch 1981, Loss: 160.40512084960938\n",
      "Epoch 62, Batch 1982, Loss: 187.22207641601562\n",
      "Epoch 62, Batch 1983, Loss: 183.64169311523438\n",
      "Epoch 62, Batch 1984, Loss: 180.24966430664062\n",
      "Epoch 62, Batch 1985, Loss: 176.48048400878906\n",
      "Epoch 62, Batch 1986, Loss: 178.21974182128906\n",
      "Epoch 62, Batch 1987, Loss: 166.6454315185547\n",
      "Epoch 62, Batch 1988, Loss: 167.30804443359375\n",
      "Epoch 62, Batch 1989, Loss: 177.7169952392578\n",
      "Epoch 62, Batch 1990, Loss: 181.95416259765625\n",
      "Epoch 62, Batch 1991, Loss: 173.03704833984375\n",
      "Epoch 62, Batch 1992, Loss: 179.41123962402344\n",
      "Epoch 62, Batch 1993, Loss: 177.71267700195312\n",
      "Epoch 62, Batch 1994, Loss: 177.78094482421875\n",
      "Epoch 62, Batch 1995, Loss: 180.0535430908203\n",
      "Epoch 62, Batch 1996, Loss: 165.30548095703125\n",
      "Epoch 62, Batch 1997, Loss: 179.1160888671875\n",
      "Epoch 62, Batch 1998, Loss: 174.54745483398438\n",
      "Epoch 62, Batch 1999, Loss: 168.80096435546875\n",
      "Epoch 62, Batch 2000, Loss: 190.3504180908203\n",
      "Epoch 62, Batch 2001, Loss: 186.98245239257812\n",
      "Epoch 62, Batch 2002, Loss: 175.57017517089844\n",
      "Epoch 62, Batch 2003, Loss: 177.17198181152344\n",
      "Epoch 62, Batch 2004, Loss: 177.50140380859375\n",
      "Epoch 62, Batch 2005, Loss: 161.81259155273438\n",
      "Epoch 62, Batch 2006, Loss: 164.27857971191406\n",
      "Epoch 62, Batch 2007, Loss: 171.60870361328125\n",
      "Epoch 62, Batch 2008, Loss: 174.5198211669922\n",
      "Epoch 62, Batch 2009, Loss: 164.82052612304688\n",
      "Epoch 62, Batch 2010, Loss: 163.82748413085938\n",
      "Epoch 62, Batch 2011, Loss: 174.89840698242188\n",
      "Epoch 62, Batch 2012, Loss: 155.79478454589844\n",
      "Epoch 62, Batch 2013, Loss: 179.8348388671875\n",
      "Epoch 62, Batch 2014, Loss: 179.00157165527344\n",
      "Epoch 62, Batch 2015, Loss: 165.030029296875\n",
      "Epoch 62, Batch 2016, Loss: 184.62362670898438\n",
      "Epoch 62, Batch 2017, Loss: 189.6271209716797\n",
      "Epoch 62, Batch 2018, Loss: 160.8474884033203\n",
      "Epoch 62, Batch 2019, Loss: 174.64157104492188\n",
      "Epoch 62, Batch 2020, Loss: 173.01699829101562\n",
      "Epoch 62, Batch 2021, Loss: 174.17623901367188\n",
      "Epoch 62, Batch 2022, Loss: 166.3125\n",
      "Epoch 62, Batch 2023, Loss: 182.1332244873047\n",
      "Epoch 62, Batch 2024, Loss: 164.53636169433594\n",
      "Epoch 62, Batch 2025, Loss: 172.23765563964844\n",
      "Epoch 62, Batch 2026, Loss: 172.3177032470703\n",
      "Epoch 62, Batch 2027, Loss: 178.66786193847656\n",
      "Epoch 62, Batch 2028, Loss: 177.60926818847656\n",
      "Epoch 62, Batch 2029, Loss: 166.2841796875\n",
      "Epoch 62, Batch 2030, Loss: 165.71438598632812\n",
      "Epoch 62, Batch 2031, Loss: 173.0124053955078\n",
      "Epoch 62, Batch 2032, Loss: 159.75387573242188\n",
      "Epoch 62, Batch 2033, Loss: 193.4488067626953\n",
      "Epoch 62, Batch 2034, Loss: 169.7264862060547\n",
      "Epoch 62, Batch 2035, Loss: 178.4170684814453\n",
      "Epoch 62, Batch 2036, Loss: 163.40028381347656\n",
      "Epoch 62, Batch 2037, Loss: 175.0015106201172\n",
      "Epoch 62, Batch 2038, Loss: 165.63851928710938\n",
      "Epoch 62, Batch 2039, Loss: 176.8388671875\n",
      "Epoch 62, Batch 2040, Loss: 184.35215759277344\n",
      "Epoch 62, Batch 2041, Loss: 168.45037841796875\n",
      "Epoch 62, Batch 2042, Loss: 168.89120483398438\n",
      "Epoch 62, Batch 2043, Loss: 164.1243896484375\n",
      "Epoch 62, Batch 2044, Loss: 168.44252014160156\n",
      "Epoch 62, Batch 2045, Loss: 175.84715270996094\n",
      "Epoch 62, Batch 2046, Loss: 161.02699279785156\n",
      "Epoch 62, Batch 2047, Loss: 174.2401885986328\n",
      "Epoch 62, Batch 2048, Loss: 176.29092407226562\n",
      "Epoch 62, Batch 2049, Loss: 188.69618225097656\n",
      "Epoch 62, Batch 2050, Loss: 175.25779724121094\n",
      "Epoch 62, Batch 2051, Loss: 170.73324584960938\n",
      "Epoch 62, Batch 2052, Loss: 167.8515167236328\n",
      "Epoch 62, Batch 2053, Loss: 176.51358032226562\n",
      "Epoch 62, Batch 2054, Loss: 171.1506805419922\n",
      "Epoch 62, Batch 2055, Loss: 171.83883666992188\n",
      "Epoch 62, Batch 2056, Loss: 187.64462280273438\n",
      "Epoch 62, Batch 2057, Loss: 181.49298095703125\n",
      "Epoch 62, Batch 2058, Loss: 156.83578491210938\n",
      "Epoch 62, Batch 2059, Loss: 157.33103942871094\n",
      "Epoch 62, Batch 2060, Loss: 187.83200073242188\n",
      "Epoch 62, Batch 2061, Loss: 172.77703857421875\n",
      "Epoch 62, Batch 2062, Loss: 179.9816436767578\n",
      "Epoch 62, Batch 2063, Loss: 158.4936065673828\n",
      "Epoch 62, Batch 2064, Loss: 192.21971130371094\n",
      "Epoch 62, Batch 2065, Loss: 179.68946838378906\n",
      "Epoch 62, Batch 2066, Loss: 164.97781372070312\n",
      "Epoch 62, Batch 2067, Loss: 172.5271759033203\n",
      "Epoch 62, Batch 2068, Loss: 172.70489501953125\n",
      "Epoch 62, Batch 2069, Loss: 162.00616455078125\n",
      "Epoch 62, Batch 2070, Loss: 183.0673828125\n",
      "Epoch 62, Batch 2071, Loss: 179.8433380126953\n",
      "Epoch 62, Batch 2072, Loss: 160.6792449951172\n",
      "Epoch 62, Batch 2073, Loss: 183.42498779296875\n",
      "Epoch 62, Batch 2074, Loss: 166.8520965576172\n",
      "Epoch 62, Batch 2075, Loss: 175.2801513671875\n",
      "Epoch 62, Batch 2076, Loss: 164.39120483398438\n",
      "Epoch 62, Batch 2077, Loss: 182.5811004638672\n",
      "Epoch 62, Batch 2078, Loss: 160.5391082763672\n",
      "Epoch 62, Batch 2079, Loss: 169.91766357421875\n",
      "Epoch 62, Batch 2080, Loss: 166.02304077148438\n",
      "Epoch 62, Batch 2081, Loss: 178.07693481445312\n",
      "Epoch 62, Batch 2082, Loss: 159.638916015625\n",
      "Epoch 62, Batch 2083, Loss: 183.85324096679688\n",
      "Epoch 62, Batch 2084, Loss: 176.36346435546875\n",
      "Epoch 62, Batch 2085, Loss: 165.59259033203125\n",
      "Epoch 62, Batch 2086, Loss: 157.7019805908203\n",
      "Epoch 62, Batch 2087, Loss: 176.96759033203125\n",
      "Epoch 62, Batch 2088, Loss: 177.3175811767578\n",
      "Epoch 62, Batch 2089, Loss: 171.57020568847656\n",
      "Epoch 62, Batch 2090, Loss: 170.26087951660156\n",
      "Epoch 62, Batch 2091, Loss: 171.3631134033203\n",
      "Epoch 62, Batch 2092, Loss: 163.56954956054688\n",
      "Epoch 62, Batch 2093, Loss: 171.35394287109375\n",
      "Epoch 62, Batch 2094, Loss: 178.09268188476562\n",
      "Epoch 62, Batch 2095, Loss: 163.9970245361328\n",
      "Epoch 62, Batch 2096, Loss: 179.23468017578125\n",
      "Epoch 62, Batch 2097, Loss: 171.24166870117188\n",
      "Epoch 62, Batch 2098, Loss: 174.658203125\n",
      "Epoch 62, Batch 2099, Loss: 175.1665802001953\n",
      "Epoch 62, Batch 2100, Loss: 164.8588104248047\n",
      "Epoch 62, Batch 2101, Loss: 178.59072875976562\n",
      "Epoch 62, Batch 2102, Loss: 166.61383056640625\n",
      "Epoch 62, Batch 2103, Loss: 182.3802947998047\n",
      "Epoch 62, Batch 2104, Loss: 174.67098999023438\n",
      "Epoch 62, Batch 2105, Loss: 167.5634765625\n",
      "Epoch 62, Batch 2106, Loss: 166.98452758789062\n",
      "Epoch 62, Batch 2107, Loss: 163.0574188232422\n",
      "Epoch 62, Batch 2108, Loss: 171.912353515625\n",
      "Epoch 62, Batch 2109, Loss: 175.4141387939453\n",
      "Epoch 62, Batch 2110, Loss: 169.06185913085938\n",
      "Epoch 62, Batch 2111, Loss: 179.6527862548828\n",
      "Epoch 62, Batch 2112, Loss: 192.51463317871094\n",
      "Epoch 62, Batch 2113, Loss: 184.3758087158203\n",
      "Epoch 62, Batch 2114, Loss: 176.70367431640625\n",
      "Epoch 62, Batch 2115, Loss: 187.0386962890625\n",
      "Epoch 62, Batch 2116, Loss: 170.0062255859375\n",
      "Epoch 62, Batch 2117, Loss: 189.8108673095703\n",
      "Epoch 62, Batch 2118, Loss: 160.49366760253906\n",
      "Epoch 62, Batch 2119, Loss: 158.90077209472656\n",
      "Epoch 62, Batch 2120, Loss: 175.12039184570312\n",
      "Epoch 62, Batch 2121, Loss: 158.4562530517578\n",
      "Epoch 62, Batch 2122, Loss: 177.11802673339844\n",
      "Epoch 62, Batch 2123, Loss: 156.6155242919922\n",
      "Epoch 62, Batch 2124, Loss: 176.42515563964844\n",
      "Epoch 62, Batch 2125, Loss: 162.95346069335938\n",
      "Epoch 62, Batch 2126, Loss: 179.81324768066406\n",
      "Epoch 62, Batch 2127, Loss: 185.0965576171875\n",
      "Epoch 62, Batch 2128, Loss: 179.61776733398438\n",
      "Epoch 62, Batch 2129, Loss: 186.28648376464844\n",
      "Epoch 62, Batch 2130, Loss: 166.36468505859375\n",
      "Epoch 62, Batch 2131, Loss: 166.56504821777344\n",
      "Epoch 62, Batch 2132, Loss: 173.1262664794922\n",
      "Epoch 62, Batch 2133, Loss: 176.21363830566406\n",
      "Epoch 62, Batch 2134, Loss: 178.7936553955078\n",
      "Epoch 62, Batch 2135, Loss: 180.4661865234375\n",
      "Epoch 62, Batch 2136, Loss: 187.48162841796875\n",
      "Epoch 62, Batch 2137, Loss: 180.87232971191406\n",
      "Epoch 62, Batch 2138, Loss: 174.6292724609375\n",
      "Epoch 62, Batch 2139, Loss: 176.20407104492188\n",
      "Epoch 62, Batch 2140, Loss: 160.2883758544922\n",
      "Epoch 62, Batch 2141, Loss: 173.92306518554688\n",
      "Epoch 62, Batch 2142, Loss: 155.00259399414062\n",
      "Epoch 62, Batch 2143, Loss: 183.59042358398438\n",
      "Epoch 62, Batch 2144, Loss: 164.26724243164062\n",
      "Epoch 62, Batch 2145, Loss: 183.12925720214844\n",
      "Epoch 62, Batch 2146, Loss: 164.93356323242188\n",
      "Epoch 62, Batch 2147, Loss: 177.2310791015625\n",
      "Epoch 62, Batch 2148, Loss: 171.32891845703125\n",
      "Epoch 62, Batch 2149, Loss: 185.18048095703125\n",
      "Epoch 62, Batch 2150, Loss: 188.6890869140625\n",
      "Epoch 62, Batch 2151, Loss: 170.97592163085938\n",
      "Epoch 62, Batch 2152, Loss: 193.35025024414062\n",
      "Epoch 62, Batch 2153, Loss: 181.36685180664062\n",
      "Epoch 62, Batch 2154, Loss: 194.62860107421875\n",
      "Epoch 62, Batch 2155, Loss: 156.52044677734375\n",
      "Epoch 62, Batch 2156, Loss: 154.81576538085938\n",
      "Epoch 62, Batch 2157, Loss: 158.4051513671875\n",
      "Epoch 62, Batch 2158, Loss: 168.9088134765625\n",
      "Epoch 62, Batch 2159, Loss: 168.8404083251953\n",
      "Epoch 62, Batch 2160, Loss: 172.73098754882812\n",
      "Epoch 62, Batch 2161, Loss: 168.3951416015625\n",
      "Epoch 62, Batch 2162, Loss: 186.2715301513672\n",
      "Epoch 62, Batch 2163, Loss: 179.75238037109375\n",
      "Epoch 62, Batch 2164, Loss: 182.50030517578125\n",
      "Epoch 62, Batch 2165, Loss: 174.00921630859375\n",
      "Epoch 62, Batch 2166, Loss: 166.19064331054688\n",
      "Epoch 62, Batch 2167, Loss: 184.0142059326172\n",
      "Epoch 62, Batch 2168, Loss: 177.1185302734375\n",
      "Epoch 62, Batch 2169, Loss: 173.62237548828125\n",
      "Epoch 62, Batch 2170, Loss: 178.24603271484375\n",
      "Epoch 62, Batch 2171, Loss: 188.21678161621094\n",
      "Epoch 62, Batch 2172, Loss: 177.4346160888672\n",
      "Epoch 62, Batch 2173, Loss: 172.76748657226562\n",
      "Epoch 62, Batch 2174, Loss: 179.26473999023438\n",
      "Epoch 62, Batch 2175, Loss: 159.45001220703125\n",
      "Epoch 62, Batch 2176, Loss: 169.26771545410156\n",
      "Epoch 62, Batch 2177, Loss: 174.38134765625\n",
      "Epoch 62, Batch 2178, Loss: 164.74990844726562\n",
      "Epoch 62, Batch 2179, Loss: 169.65672302246094\n",
      "Epoch 62, Batch 2180, Loss: 167.09127807617188\n",
      "Epoch 62, Batch 2181, Loss: 164.2042236328125\n",
      "Epoch 62, Batch 2182, Loss: 167.7544403076172\n",
      "Epoch 62, Batch 2183, Loss: 169.04180908203125\n",
      "Epoch 62, Batch 2184, Loss: 175.7388916015625\n",
      "Epoch 62, Batch 2185, Loss: 168.65675354003906\n",
      "Epoch 62, Batch 2186, Loss: 185.11683654785156\n",
      "Epoch 62, Batch 2187, Loss: 179.29257202148438\n",
      "Epoch 62, Batch 2188, Loss: 161.12759399414062\n",
      "Epoch 62, Batch 2189, Loss: 159.8164520263672\n",
      "Epoch 62, Batch 2190, Loss: 178.05079650878906\n",
      "Epoch 62, Batch 2191, Loss: 171.37481689453125\n",
      "Epoch 62, Batch 2192, Loss: 161.5505828857422\n",
      "Epoch 62, Batch 2193, Loss: 168.05601501464844\n",
      "Epoch 62, Batch 2194, Loss: 191.7301025390625\n",
      "Epoch 62, Batch 2195, Loss: 158.89369201660156\n",
      "Epoch 62, Batch 2196, Loss: 176.88818359375\n",
      "Epoch 62, Batch 2197, Loss: 190.03831481933594\n",
      "Epoch 62, Batch 2198, Loss: 168.15089416503906\n",
      "Epoch 62, Batch 2199, Loss: 177.2448272705078\n",
      "Epoch 62, Batch 2200, Loss: 171.36688232421875\n",
      "Epoch 62, Batch 2201, Loss: 165.62423706054688\n",
      "Epoch 62, Batch 2202, Loss: 180.4747314453125\n",
      "Epoch 62, Batch 2203, Loss: 157.67127990722656\n",
      "Epoch 62, Batch 2204, Loss: 153.50790405273438\n",
      "Epoch 62, Batch 2205, Loss: 168.61422729492188\n",
      "Epoch 62, Batch 2206, Loss: 181.615478515625\n",
      "Epoch 62, Batch 2207, Loss: 182.38697814941406\n",
      "Epoch 62, Batch 2208, Loss: 192.40672302246094\n",
      "Epoch 62, Batch 2209, Loss: 168.43145751953125\n",
      "Epoch 62, Batch 2210, Loss: 189.74293518066406\n",
      "Epoch 62, Batch 2211, Loss: 181.7584686279297\n",
      "Epoch 62, Batch 2212, Loss: 172.8583526611328\n",
      "Epoch 62, Batch 2213, Loss: 184.2931671142578\n",
      "Epoch 62, Batch 2214, Loss: 178.5774383544922\n",
      "Epoch 62, Batch 2215, Loss: 179.12669372558594\n",
      "Epoch 62, Batch 2216, Loss: 166.1756134033203\n",
      "Epoch 62, Batch 2217, Loss: 172.53541564941406\n",
      "Epoch 62, Batch 2218, Loss: 202.23123168945312\n",
      "Epoch 62, Batch 2219, Loss: 174.08375549316406\n",
      "Epoch 62, Batch 2220, Loss: 158.27247619628906\n",
      "Epoch 62, Batch 2221, Loss: 165.7324676513672\n",
      "Epoch 62, Batch 2222, Loss: 180.90696716308594\n",
      "Epoch 62, Batch 2223, Loss: 163.8134002685547\n",
      "Epoch 62, Batch 2224, Loss: 195.55398559570312\n",
      "Epoch 62, Batch 2225, Loss: 161.57078552246094\n",
      "Epoch 62, Batch 2226, Loss: 164.33421325683594\n",
      "Epoch 62, Batch 2227, Loss: 181.0574493408203\n",
      "Epoch 62, Batch 2228, Loss: 177.08941650390625\n",
      "Epoch 62, Batch 2229, Loss: 176.84051513671875\n",
      "Epoch 62, Batch 2230, Loss: 166.87057495117188\n",
      "Epoch 62, Batch 2231, Loss: 175.7874755859375\n",
      "Epoch 62, Batch 2232, Loss: 170.03184509277344\n",
      "Epoch 62, Batch 2233, Loss: 179.54043579101562\n",
      "Epoch 62, Batch 2234, Loss: 154.17857360839844\n",
      "Epoch 62, Batch 2235, Loss: 170.12115478515625\n",
      "Epoch 62, Batch 2236, Loss: 165.54876708984375\n",
      "Epoch 62, Batch 2237, Loss: 184.30770874023438\n",
      "Epoch 62, Batch 2238, Loss: 181.44662475585938\n",
      "Epoch 62, Batch 2239, Loss: 161.8196563720703\n",
      "Epoch 62, Batch 2240, Loss: 177.87303161621094\n",
      "Epoch 62, Batch 2241, Loss: 171.24114990234375\n",
      "Epoch 62, Batch 2242, Loss: 172.5814971923828\n",
      "Epoch 62, Batch 2243, Loss: 162.29928588867188\n",
      "Epoch 62, Batch 2244, Loss: 169.89317321777344\n",
      "Epoch 62, Batch 2245, Loss: 175.14393615722656\n",
      "Epoch 62, Batch 2246, Loss: 188.85614013671875\n",
      "Epoch 62, Batch 2247, Loss: 173.0020751953125\n",
      "Epoch 62, Batch 2248, Loss: 157.9423065185547\n",
      "Epoch 62, Batch 2249, Loss: 159.25184631347656\n",
      "Epoch 62, Batch 2250, Loss: 182.34588623046875\n",
      "Epoch 62, Batch 2251, Loss: 179.5006866455078\n",
      "Epoch 62, Batch 2252, Loss: 180.14146423339844\n",
      "Epoch 62, Batch 2253, Loss: 171.43392944335938\n",
      "Epoch 62, Batch 2254, Loss: 164.50732421875\n",
      "Epoch 62, Batch 2255, Loss: 171.3722686767578\n",
      "Epoch 62, Batch 2256, Loss: 169.57398986816406\n",
      "Epoch 62, Batch 2257, Loss: 172.45640563964844\n",
      "Epoch 62, Batch 2258, Loss: 172.7757110595703\n",
      "Epoch 62, Batch 2259, Loss: 167.5631103515625\n",
      "Epoch 62, Batch 2260, Loss: 185.53060913085938\n",
      "Epoch 62, Batch 2261, Loss: 168.43038940429688\n",
      "Epoch 62, Batch 2262, Loss: 163.2960662841797\n",
      "Epoch 62, Batch 2263, Loss: 167.60475158691406\n",
      "Epoch 62, Batch 2264, Loss: 181.3341522216797\n",
      "Epoch 62, Batch 2265, Loss: 188.17578125\n",
      "Epoch 62, Batch 2266, Loss: 168.83547973632812\n",
      "Epoch 62, Batch 2267, Loss: 170.6019744873047\n",
      "Epoch 62, Batch 2268, Loss: 157.15194702148438\n",
      "Epoch 62, Batch 2269, Loss: 183.56724548339844\n",
      "Epoch 62, Batch 2270, Loss: 166.2414093017578\n",
      "Epoch 62, Batch 2271, Loss: 164.13072204589844\n",
      "Epoch 62, Batch 2272, Loss: 189.2824249267578\n",
      "Epoch 62, Batch 2273, Loss: 216.1502227783203\n",
      "Epoch 62, Batch 2274, Loss: 164.8352813720703\n",
      "Epoch 62, Batch 2275, Loss: 172.107666015625\n",
      "Epoch 62, Batch 2276, Loss: 168.85635375976562\n",
      "Epoch 62, Batch 2277, Loss: 190.72048950195312\n",
      "Epoch 62, Batch 2278, Loss: 155.68458557128906\n",
      "Epoch 62, Batch 2279, Loss: 152.22071838378906\n",
      "Epoch 62, Batch 2280, Loss: 174.4043731689453\n",
      "Epoch 62, Batch 2281, Loss: 174.68955993652344\n",
      "Epoch 62, Batch 2282, Loss: 160.95314025878906\n",
      "Epoch 62, Batch 2283, Loss: 176.7305450439453\n",
      "Epoch 62, Batch 2284, Loss: 162.74240112304688\n",
      "Epoch 62, Batch 2285, Loss: 165.9867401123047\n",
      "Epoch 62, Batch 2286, Loss: 171.32957458496094\n",
      "Epoch 62, Batch 2287, Loss: 170.8820343017578\n",
      "Epoch 62, Batch 2288, Loss: 170.06640625\n",
      "Epoch 62, Batch 2289, Loss: 188.76687622070312\n",
      "Epoch 62, Batch 2290, Loss: 170.8580780029297\n",
      "Epoch 62, Batch 2291, Loss: 168.88182067871094\n",
      "Epoch 62, Batch 2292, Loss: 188.68161010742188\n",
      "Epoch 62, Batch 2293, Loss: 184.58273315429688\n",
      "Epoch 62, Batch 2294, Loss: 164.72976684570312\n",
      "Epoch 62, Batch 2295, Loss: 166.4503936767578\n",
      "Epoch 62, Batch 2296, Loss: 163.64239501953125\n",
      "Epoch 62, Batch 2297, Loss: 170.39556884765625\n",
      "Epoch 62, Batch 2298, Loss: 179.21844482421875\n",
      "Epoch 62, Batch 2299, Loss: 177.38816833496094\n",
      "Epoch 62, Batch 2300, Loss: 153.9995880126953\n",
      "Epoch 62, Batch 2301, Loss: 174.7490692138672\n",
      "Epoch 62, Batch 2302, Loss: 177.2760467529297\n",
      "Epoch 62, Batch 2303, Loss: 185.1809844970703\n",
      "Epoch 62, Batch 2304, Loss: 177.0221710205078\n",
      "Epoch 62, Batch 2305, Loss: 188.62109375\n",
      "Epoch 62, Batch 2306, Loss: 179.25698852539062\n",
      "Epoch 62, Batch 2307, Loss: 178.8535614013672\n",
      "Epoch 62, Batch 2308, Loss: 195.80628967285156\n",
      "Epoch 62, Batch 2309, Loss: 167.90711975097656\n",
      "Epoch 62, Batch 2310, Loss: 179.19918823242188\n",
      "Epoch 62, Batch 2311, Loss: 187.20779418945312\n",
      "Epoch 62, Batch 2312, Loss: 168.98388671875\n",
      "Epoch 62, Batch 2313, Loss: 172.65785217285156\n",
      "Epoch 62, Batch 2314, Loss: 173.7934112548828\n",
      "Epoch 62, Batch 2315, Loss: 161.5260772705078\n",
      "Epoch 62, Batch 2316, Loss: 182.6361541748047\n",
      "Epoch 62, Batch 2317, Loss: 173.64927673339844\n",
      "Epoch 62, Batch 2318, Loss: 174.24388122558594\n",
      "Epoch 62, Batch 2319, Loss: 163.24107360839844\n",
      "Epoch 62, Batch 2320, Loss: 174.7039031982422\n",
      "Epoch 62, Batch 2321, Loss: 173.93435668945312\n",
      "Epoch 62, Batch 2322, Loss: 171.32949829101562\n",
      "Epoch 62, Batch 2323, Loss: 183.89524841308594\n",
      "Epoch 62, Batch 2324, Loss: 189.7720184326172\n",
      "Epoch 62, Batch 2325, Loss: 165.90505981445312\n",
      "Epoch 62, Batch 2326, Loss: 181.9236297607422\n",
      "Epoch 62, Batch 2327, Loss: 179.7666473388672\n",
      "Epoch 62, Batch 2328, Loss: 183.285888671875\n",
      "Epoch 62, Batch 2329, Loss: 169.99044799804688\n",
      "Epoch 62, Batch 2330, Loss: 177.2198944091797\n",
      "Epoch 62, Batch 2331, Loss: 162.3230743408203\n",
      "Epoch 62, Batch 2332, Loss: 176.97935485839844\n",
      "Epoch 62, Batch 2333, Loss: 167.6314697265625\n",
      "Epoch 62, Batch 2334, Loss: 158.13079833984375\n",
      "Epoch 62, Batch 2335, Loss: 181.4583282470703\n",
      "Epoch 62, Batch 2336, Loss: 170.4612579345703\n",
      "Epoch 62, Batch 2337, Loss: 153.48011779785156\n",
      "Epoch 62, Batch 2338, Loss: 168.3372039794922\n",
      "Epoch 62, Batch 2339, Loss: 177.52577209472656\n",
      "Epoch 62, Batch 2340, Loss: 152.26364135742188\n",
      "Epoch 62, Batch 2341, Loss: 184.18045043945312\n",
      "Epoch 62, Batch 2342, Loss: 163.4234619140625\n",
      "Epoch 62, Batch 2343, Loss: 190.77879333496094\n",
      "Epoch 62, Batch 2344, Loss: 164.53152465820312\n",
      "Epoch 62, Batch 2345, Loss: 169.18075561523438\n",
      "Epoch 62, Batch 2346, Loss: 169.40065002441406\n",
      "Epoch 62, Batch 2347, Loss: 149.29095458984375\n",
      "Epoch 62, Batch 2348, Loss: 169.00772094726562\n",
      "Epoch 62, Batch 2349, Loss: 168.43038940429688\n",
      "Epoch 62, Batch 2350, Loss: 174.35394287109375\n",
      "Epoch 62, Batch 2351, Loss: 165.1011199951172\n",
      "Epoch 62, Batch 2352, Loss: 156.988525390625\n",
      "Epoch 62, Batch 2353, Loss: 167.29177856445312\n",
      "Epoch 62, Batch 2354, Loss: 162.5326690673828\n",
      "Epoch 62, Batch 2355, Loss: 183.72555541992188\n",
      "Epoch 62, Batch 2356, Loss: 167.51416015625\n",
      "Epoch 62, Batch 2357, Loss: 163.77061462402344\n",
      "Epoch 62, Batch 2358, Loss: 161.67947387695312\n",
      "Epoch 62, Batch 2359, Loss: 175.02980041503906\n",
      "Epoch 62, Batch 2360, Loss: 165.2261505126953\n",
      "Epoch 62, Batch 2361, Loss: 168.8326416015625\n",
      "Epoch 62, Batch 2362, Loss: 178.49945068359375\n",
      "Epoch 62, Batch 2363, Loss: 189.2720184326172\n",
      "Epoch 62, Batch 2364, Loss: 159.11158752441406\n",
      "Epoch 62, Batch 2365, Loss: 158.51947021484375\n",
      "Epoch 62, Batch 2366, Loss: 172.2053680419922\n",
      "Epoch 62, Batch 2367, Loss: 178.69883728027344\n",
      "Epoch 62, Batch 2368, Loss: 155.90809631347656\n",
      "Epoch 62, Batch 2369, Loss: 163.56964111328125\n",
      "Epoch 62, Batch 2370, Loss: 170.06719970703125\n",
      "Epoch 62, Batch 2371, Loss: 177.6643524169922\n",
      "Epoch 62, Batch 2372, Loss: 163.69947814941406\n",
      "Epoch 62, Batch 2373, Loss: 168.67478942871094\n",
      "Epoch 62, Batch 2374, Loss: 168.5160675048828\n",
      "Epoch 62, Batch 2375, Loss: 176.53309631347656\n",
      "Epoch 62, Batch 2376, Loss: 180.2530517578125\n",
      "Epoch 62, Batch 2377, Loss: 166.76268005371094\n",
      "Epoch 62, Batch 2378, Loss: 162.5944061279297\n",
      "Epoch 62, Batch 2379, Loss: 173.28363037109375\n",
      "Epoch 62, Batch 2380, Loss: 176.81314086914062\n",
      "Epoch 62, Batch 2381, Loss: 162.91329956054688\n",
      "Epoch 62, Batch 2382, Loss: 179.8723907470703\n",
      "Epoch 62, Batch 2383, Loss: 163.2313995361328\n",
      "Epoch 62, Batch 2384, Loss: 190.38400268554688\n",
      "Epoch 62, Batch 2385, Loss: 178.72222900390625\n",
      "Epoch 62, Batch 2386, Loss: 164.40176391601562\n",
      "Epoch 62, Batch 2387, Loss: 176.4679718017578\n",
      "Epoch 62, Batch 2388, Loss: 180.6457061767578\n",
      "Epoch 62, Batch 2389, Loss: 153.84567260742188\n",
      "Epoch 62, Batch 2390, Loss: 176.58206176757812\n",
      "Epoch 62, Batch 2391, Loss: 169.37794494628906\n",
      "Epoch 62, Batch 2392, Loss: 163.2735595703125\n",
      "Epoch 62, Batch 2393, Loss: 173.14817810058594\n",
      "Epoch 62, Batch 2394, Loss: 174.58409118652344\n",
      "Epoch 62, Batch 2395, Loss: 184.42373657226562\n",
      "Epoch 62, Batch 2396, Loss: 169.7386474609375\n",
      "Epoch 62, Batch 2397, Loss: 181.5103759765625\n",
      "Epoch 62, Batch 2398, Loss: 167.87054443359375\n",
      "Epoch 62, Batch 2399, Loss: 174.59190368652344\n",
      "Epoch 62, Batch 2400, Loss: 166.6798858642578\n",
      "Epoch 62, Batch 2401, Loss: 185.32424926757812\n",
      "Epoch 62, Batch 2402, Loss: 169.0621337890625\n",
      "Epoch 62, Batch 2403, Loss: 167.2937469482422\n",
      "Epoch 62, Batch 2404, Loss: 173.44615173339844\n",
      "Epoch 62, Batch 2405, Loss: 183.88694763183594\n",
      "Epoch 62, Batch 2406, Loss: 165.1947021484375\n",
      "Epoch 62, Batch 2407, Loss: 171.55323791503906\n",
      "Epoch 62, Batch 2408, Loss: 187.8243408203125\n",
      "Epoch 62, Batch 2409, Loss: 180.3009033203125\n",
      "Epoch 62, Batch 2410, Loss: 168.85885620117188\n",
      "Epoch 62, Batch 2411, Loss: 163.12225341796875\n",
      "Epoch 62, Batch 2412, Loss: 188.49273681640625\n",
      "Epoch 62, Batch 2413, Loss: 155.6660614013672\n",
      "Epoch 62, Batch 2414, Loss: 171.6830291748047\n",
      "Epoch 62, Batch 2415, Loss: 173.5879364013672\n",
      "Epoch 62, Batch 2416, Loss: 180.06117248535156\n",
      "Epoch 62, Batch 2417, Loss: 175.1838836669922\n",
      "Epoch 62, Batch 2418, Loss: 175.72174072265625\n",
      "Epoch 62, Batch 2419, Loss: 178.88365173339844\n",
      "Epoch 62, Batch 2420, Loss: 175.98837280273438\n",
      "Epoch 62, Batch 2421, Loss: 160.57272338867188\n",
      "Epoch 62, Batch 2422, Loss: 181.25494384765625\n",
      "Epoch 62, Batch 2423, Loss: 163.90037536621094\n",
      "Epoch 62, Batch 2424, Loss: 190.8348846435547\n",
      "Epoch 62, Batch 2425, Loss: 181.82774353027344\n",
      "Epoch 62, Batch 2426, Loss: 185.2434844970703\n",
      "Epoch 62, Batch 2427, Loss: 162.65330505371094\n",
      "Epoch 62, Batch 2428, Loss: 174.9951171875\n",
      "Epoch 62, Batch 2429, Loss: 191.51669311523438\n",
      "Epoch 62, Batch 2430, Loss: 198.10671997070312\n",
      "Epoch 62, Batch 2431, Loss: 169.7643585205078\n",
      "Epoch 62, Batch 2432, Loss: 177.54434204101562\n",
      "Epoch 62, Batch 2433, Loss: 169.3992156982422\n",
      "Epoch 62, Batch 2434, Loss: 166.14434814453125\n",
      "Epoch 62, Batch 2435, Loss: 156.44717407226562\n",
      "Epoch 62, Batch 2436, Loss: 175.90000915527344\n",
      "Epoch 62, Batch 2437, Loss: 165.8067169189453\n",
      "Epoch 62, Batch 2438, Loss: 161.00787353515625\n",
      "Epoch 62, Batch 2439, Loss: 168.69976806640625\n",
      "Epoch 62, Batch 2440, Loss: 169.9176483154297\n",
      "Epoch 62, Batch 2441, Loss: 187.2418670654297\n",
      "Epoch 62, Batch 2442, Loss: 164.52005004882812\n",
      "Epoch 62, Batch 2443, Loss: 165.9108123779297\n",
      "Epoch 62, Batch 2444, Loss: 172.13929748535156\n",
      "Epoch 62, Batch 2445, Loss: 174.38824462890625\n",
      "Epoch 62, Batch 2446, Loss: 194.90292358398438\n",
      "Epoch 62, Batch 2447, Loss: 165.42506408691406\n",
      "Epoch 62, Batch 2448, Loss: 172.50213623046875\n",
      "Epoch 62, Batch 2449, Loss: 185.40509033203125\n",
      "Epoch 62, Batch 2450, Loss: 152.216796875\n",
      "Epoch 62, Batch 2451, Loss: 203.1915283203125\n",
      "Epoch 62, Batch 2452, Loss: 180.22134399414062\n",
      "Epoch 62, Batch 2453, Loss: 171.3660430908203\n",
      "Epoch 62, Batch 2454, Loss: 178.361083984375\n",
      "Epoch 62, Batch 2455, Loss: 165.9150848388672\n",
      "Epoch 62, Batch 2456, Loss: 178.86573791503906\n",
      "Epoch 62, Batch 2457, Loss: 173.17822265625\n",
      "Epoch 62, Batch 2458, Loss: 164.06719970703125\n",
      "Epoch 62, Batch 2459, Loss: 175.84901428222656\n",
      "Epoch 62, Batch 2460, Loss: 184.91700744628906\n",
      "Epoch 62, Batch 2461, Loss: 182.30499267578125\n",
      "Epoch 62, Batch 2462, Loss: 175.87506103515625\n",
      "Epoch 62, Batch 2463, Loss: 173.22128295898438\n",
      "Epoch 62, Batch 2464, Loss: 178.74400329589844\n",
      "Epoch 62, Batch 2465, Loss: 164.1278533935547\n",
      "Epoch 62, Batch 2466, Loss: 166.78199768066406\n",
      "Epoch 62, Batch 2467, Loss: 172.42201232910156\n",
      "Epoch 62, Batch 2468, Loss: 166.50157165527344\n",
      "Epoch 62, Batch 2469, Loss: 183.6265869140625\n",
      "Epoch 62, Batch 2470, Loss: 185.85702514648438\n",
      "Epoch 62, Batch 2471, Loss: 172.05508422851562\n",
      "Epoch 62, Batch 2472, Loss: 159.1188201904297\n",
      "Epoch 62, Batch 2473, Loss: 169.36392211914062\n",
      "Epoch 62, Batch 2474, Loss: 167.38047790527344\n",
      "Epoch 62, Batch 2475, Loss: 170.90821838378906\n",
      "Epoch 62, Batch 2476, Loss: 172.99346923828125\n",
      "Epoch 62, Batch 2477, Loss: 186.61468505859375\n",
      "Epoch 62, Batch 2478, Loss: 177.1381072998047\n",
      "Epoch 62, Batch 2479, Loss: 189.08349609375\n",
      "Epoch 62, Batch 2480, Loss: 178.79725646972656\n",
      "Epoch 62, Batch 2481, Loss: 163.27293395996094\n",
      "Epoch 62, Batch 2482, Loss: 173.36734008789062\n",
      "Epoch 62, Batch 2483, Loss: 175.14891052246094\n",
      "Epoch 62, Batch 2484, Loss: 170.7407989501953\n",
      "Epoch 62, Batch 2485, Loss: 168.79994201660156\n",
      "Epoch 62, Batch 2486, Loss: 178.59840393066406\n",
      "Epoch 62, Batch 2487, Loss: 163.2968292236328\n",
      "Epoch 62, Batch 2488, Loss: 175.5697479248047\n",
      "Epoch 62, Batch 2489, Loss: 180.71835327148438\n",
      "Epoch 62, Batch 2490, Loss: 173.79373168945312\n",
      "Epoch 62, Batch 2491, Loss: 176.0782928466797\n",
      "Epoch 62, Batch 2492, Loss: 167.54412841796875\n",
      "Epoch 62, Batch 2493, Loss: 162.34344482421875\n",
      "Epoch 62, Batch 2494, Loss: 164.20648193359375\n",
      "Epoch 62, Batch 2495, Loss: 156.4217987060547\n",
      "Epoch 62, Batch 2496, Loss: 169.60255432128906\n",
      "Epoch 62, Batch 2497, Loss: 169.3612518310547\n",
      "Epoch 62, Batch 2498, Loss: 165.13873291015625\n",
      "Epoch 62, Batch 2499, Loss: 177.0457305908203\n",
      "Epoch 62, Batch 2500, Loss: 171.4132537841797\n",
      "Epoch 62, Batch 2501, Loss: 171.0767059326172\n",
      "Epoch 62, Batch 2502, Loss: 179.8491668701172\n",
      "Epoch 62, Batch 2503, Loss: 172.19970703125\n",
      "Epoch 62, Batch 2504, Loss: 157.4707794189453\n",
      "Epoch 62, Batch 2505, Loss: 151.9729461669922\n",
      "Epoch 62, Batch 2506, Loss: 179.36468505859375\n",
      "Epoch 62, Batch 2507, Loss: 163.14111328125\n",
      "Epoch 62, Batch 2508, Loss: 158.8900909423828\n",
      "Epoch 62, Batch 2509, Loss: 166.46871948242188\n",
      "Epoch 62, Batch 2510, Loss: 167.7129364013672\n",
      "Epoch 62, Batch 2511, Loss: 181.44276428222656\n",
      "Epoch 62, Batch 2512, Loss: 192.6413116455078\n",
      "Epoch 62, Batch 2513, Loss: 164.7677459716797\n",
      "Epoch 62, Batch 2514, Loss: 183.58445739746094\n",
      "Epoch 62, Batch 2515, Loss: 184.96197509765625\n",
      "Epoch 62, Batch 2516, Loss: 175.7458953857422\n",
      "Epoch 62, Batch 2517, Loss: 181.9076690673828\n",
      "Epoch 62, Batch 2518, Loss: 177.63002014160156\n",
      "Epoch 62, Batch 2519, Loss: 156.4648895263672\n",
      "Epoch 62, Batch 2520, Loss: 175.0077667236328\n",
      "Epoch 62, Batch 2521, Loss: 184.66856384277344\n",
      "Epoch 62, Batch 2522, Loss: 153.71682739257812\n",
      "Epoch 62, Batch 2523, Loss: 173.11019897460938\n",
      "Epoch 62, Batch 2524, Loss: 169.64027404785156\n",
      "Epoch 62, Batch 2525, Loss: 164.9371337890625\n",
      "Epoch 62, Batch 2526, Loss: 180.4657745361328\n",
      "Epoch 62, Batch 2527, Loss: 187.05136108398438\n",
      "Epoch 62, Batch 2528, Loss: 178.43338012695312\n",
      "Epoch 62, Batch 2529, Loss: 170.76580810546875\n",
      "Epoch 62, Batch 2530, Loss: 172.57691955566406\n",
      "Epoch 62, Batch 2531, Loss: 169.94003295898438\n",
      "Epoch 62, Batch 2532, Loss: 169.56369018554688\n",
      "Epoch 62, Batch 2533, Loss: 182.06558227539062\n",
      "Epoch 62, Batch 2534, Loss: 178.07958984375\n",
      "Epoch 62, Batch 2535, Loss: 177.99569702148438\n",
      "Epoch 62, Batch 2536, Loss: 155.4652557373047\n",
      "Epoch 62, Batch 2537, Loss: 164.98411560058594\n",
      "Epoch 62, Batch 2538, Loss: 168.18128967285156\n",
      "Epoch 62, Batch 2539, Loss: 176.15562438964844\n",
      "Epoch 62, Batch 2540, Loss: 166.01564025878906\n",
      "Epoch 62, Batch 2541, Loss: 167.2225341796875\n",
      "Epoch 62, Batch 2542, Loss: 176.2623748779297\n",
      "Epoch 62, Batch 2543, Loss: 165.7646026611328\n",
      "Epoch 62, Batch 2544, Loss: 167.62242126464844\n",
      "Epoch 62, Batch 2545, Loss: 187.6465301513672\n",
      "Epoch 62, Batch 2546, Loss: 187.55136108398438\n",
      "Epoch 62, Batch 2547, Loss: 164.8411102294922\n",
      "Epoch 62, Batch 2548, Loss: 186.72218322753906\n",
      "Epoch 62, Batch 2549, Loss: 177.41554260253906\n",
      "Epoch 62, Batch 2550, Loss: 159.2485809326172\n",
      "Epoch 62, Batch 2551, Loss: 176.34640502929688\n",
      "Epoch 62, Batch 2552, Loss: 176.73062133789062\n",
      "Epoch 62, Batch 2553, Loss: 171.2300567626953\n",
      "Epoch 62, Batch 2554, Loss: 188.8723602294922\n",
      "Epoch 62, Batch 2555, Loss: 192.94529724121094\n",
      "Epoch 62, Batch 2556, Loss: 154.6016387939453\n",
      "Epoch 62, Batch 2557, Loss: 167.01954650878906\n",
      "Epoch 62, Batch 2558, Loss: 163.70545959472656\n",
      "Epoch 62, Batch 2559, Loss: 173.6802978515625\n",
      "Epoch 62, Batch 2560, Loss: 170.20155334472656\n",
      "Epoch 62, Batch 2561, Loss: 167.13851928710938\n",
      "Epoch 62, Batch 2562, Loss: 166.0019989013672\n",
      "Epoch 62, Batch 2563, Loss: 174.75726318359375\n",
      "Epoch 62, Batch 2564, Loss: 173.49581909179688\n",
      "Epoch 62, Batch 2565, Loss: 184.12960815429688\n",
      "Epoch 62, Batch 2566, Loss: 186.43063354492188\n",
      "Epoch 62, Batch 2567, Loss: 162.56121826171875\n",
      "Epoch 62, Batch 2568, Loss: 179.65243530273438\n",
      "Epoch 62, Batch 2569, Loss: 169.78146362304688\n",
      "Epoch 62, Batch 2570, Loss: 188.47332763671875\n",
      "Epoch 62, Batch 2571, Loss: 173.4348907470703\n",
      "Epoch 62, Batch 2572, Loss: 192.40560913085938\n",
      "Epoch 62, Batch 2573, Loss: 174.06201171875\n",
      "Epoch 62, Batch 2574, Loss: 168.87124633789062\n",
      "Epoch 62, Batch 2575, Loss: 177.51881408691406\n",
      "Epoch 62, Batch 2576, Loss: 165.44264221191406\n",
      "Epoch 62, Batch 2577, Loss: 187.595458984375\n",
      "Epoch 62, Batch 2578, Loss: 189.59954833984375\n",
      "Epoch 62, Batch 2579, Loss: 171.62033081054688\n",
      "Epoch 62, Batch 2580, Loss: 164.56988525390625\n",
      "Epoch 62, Batch 2581, Loss: 185.84744262695312\n",
      "Epoch 62, Batch 2582, Loss: 195.82456970214844\n",
      "Epoch 62, Batch 2583, Loss: 155.63760375976562\n",
      "Epoch 62, Batch 2584, Loss: 180.32192993164062\n",
      "Epoch 62, Batch 2585, Loss: 174.1779327392578\n",
      "Epoch 62, Batch 2586, Loss: 178.3023223876953\n",
      "Epoch 62, Batch 2587, Loss: 156.2783966064453\n",
      "Epoch 62, Batch 2588, Loss: 169.67926025390625\n",
      "Epoch 62, Batch 2589, Loss: 181.0491485595703\n",
      "Epoch 62, Batch 2590, Loss: 182.67831420898438\n",
      "Epoch 62, Batch 2591, Loss: 169.63949584960938\n",
      "Epoch 62, Batch 2592, Loss: 164.17523193359375\n",
      "Epoch 62, Batch 2593, Loss: 180.42796325683594\n",
      "Epoch 62, Batch 2594, Loss: 181.746826171875\n",
      "Epoch 62, Batch 2595, Loss: 168.3446502685547\n",
      "Epoch 62, Batch 2596, Loss: 180.80172729492188\n",
      "Epoch 62, Batch 2597, Loss: 176.75637817382812\n",
      "Epoch 62, Batch 2598, Loss: 157.54217529296875\n",
      "Epoch 62, Batch 2599, Loss: 175.8303985595703\n",
      "Epoch 62, Batch 2600, Loss: 181.96311950683594\n",
      "Epoch 62, Batch 2601, Loss: 165.852783203125\n",
      "Epoch 62, Batch 2602, Loss: 169.26441955566406\n",
      "Epoch 62, Batch 2603, Loss: 189.04098510742188\n",
      "Epoch 62, Batch 2604, Loss: 193.3942108154297\n",
      "Epoch 62, Batch 2605, Loss: 169.7449493408203\n",
      "Epoch 62, Batch 2606, Loss: 179.58932495117188\n",
      "Epoch 62, Batch 2607, Loss: 171.8860626220703\n",
      "Epoch 62, Batch 2608, Loss: 165.28189086914062\n",
      "Epoch 62, Batch 2609, Loss: 163.57720947265625\n",
      "Epoch 62, Batch 2610, Loss: 173.6453857421875\n",
      "Epoch 62, Batch 2611, Loss: 178.9528350830078\n",
      "Epoch 62, Batch 2612, Loss: 178.86215209960938\n",
      "Epoch 62, Batch 2613, Loss: 163.73045349121094\n",
      "Epoch 62, Batch 2614, Loss: 175.88026428222656\n",
      "Epoch 62, Batch 2615, Loss: 165.65380859375\n",
      "Epoch 62, Batch 2616, Loss: 152.01022338867188\n",
      "Epoch 62, Batch 2617, Loss: 176.26370239257812\n",
      "Epoch 62, Batch 2618, Loss: 179.3326416015625\n",
      "Epoch 62, Batch 2619, Loss: 165.82626342773438\n",
      "Epoch 62, Batch 2620, Loss: 167.82864379882812\n",
      "Epoch 62, Batch 2621, Loss: 176.6683807373047\n",
      "Epoch 62, Batch 2622, Loss: 169.9349822998047\n",
      "Epoch 62, Batch 2623, Loss: 176.93612670898438\n",
      "Epoch 62, Batch 2624, Loss: 161.94012451171875\n",
      "Epoch 62, Batch 2625, Loss: 171.81179809570312\n",
      "Epoch 62, Batch 2626, Loss: 166.35800170898438\n",
      "Epoch 62, Batch 2627, Loss: 184.65762329101562\n",
      "Epoch 62, Batch 2628, Loss: 174.45794677734375\n",
      "Epoch 62, Batch 2629, Loss: 169.12290954589844\n",
      "Epoch 62, Batch 2630, Loss: 158.39248657226562\n",
      "Epoch 62, Batch 2631, Loss: 197.35093688964844\n",
      "Epoch 62, Batch 2632, Loss: 169.32550048828125\n",
      "Epoch 62, Batch 2633, Loss: 183.910400390625\n",
      "Epoch 62, Batch 2634, Loss: 178.51315307617188\n",
      "Epoch 62, Batch 2635, Loss: 174.81820678710938\n",
      "Epoch 62, Batch 2636, Loss: 168.29640197753906\n",
      "Epoch 62, Batch 2637, Loss: 166.51409912109375\n",
      "Epoch 62, Batch 2638, Loss: 181.12123107910156\n",
      "Epoch 62, Batch 2639, Loss: 153.85923767089844\n",
      "Epoch 62, Batch 2640, Loss: 170.83824157714844\n",
      "Epoch 62, Batch 2641, Loss: 163.5015106201172\n",
      "Epoch 62, Batch 2642, Loss: 173.74658203125\n",
      "Epoch 62, Batch 2643, Loss: 180.6488037109375\n",
      "Epoch 62, Batch 2644, Loss: 168.69252014160156\n",
      "Epoch 62, Batch 2645, Loss: 153.79100036621094\n",
      "Epoch 62, Batch 2646, Loss: 172.0818634033203\n",
      "Epoch 62, Batch 2647, Loss: 176.86669921875\n",
      "Epoch 62, Batch 2648, Loss: 185.08509826660156\n",
      "Epoch 62, Batch 2649, Loss: 174.39215087890625\n",
      "Epoch 62, Batch 2650, Loss: 173.63055419921875\n",
      "Epoch 62, Batch 2651, Loss: 178.0656280517578\n",
      "Epoch 62, Batch 2652, Loss: 172.20106506347656\n",
      "Epoch 62, Batch 2653, Loss: 166.72389221191406\n",
      "Epoch 62, Batch 2654, Loss: 168.89947509765625\n",
      "Epoch 62, Batch 2655, Loss: 181.5979766845703\n",
      "Epoch 62, Batch 2656, Loss: 184.37765502929688\n",
      "Epoch 62, Batch 2657, Loss: 169.45938110351562\n",
      "Epoch 62, Batch 2658, Loss: 176.35145568847656\n",
      "Epoch 62, Batch 2659, Loss: 170.89852905273438\n",
      "Epoch 62, Batch 2660, Loss: 169.07440185546875\n",
      "Epoch 62, Batch 2661, Loss: 178.51284790039062\n",
      "Epoch 62, Batch 2662, Loss: 162.14773559570312\n",
      "Epoch 62, Batch 2663, Loss: 164.7758026123047\n",
      "Epoch 62, Batch 2664, Loss: 171.05307006835938\n",
      "Epoch 62, Batch 2665, Loss: 172.94818115234375\n",
      "Epoch 62, Batch 2666, Loss: 170.78799438476562\n",
      "Epoch 62, Batch 2667, Loss: 184.7646026611328\n",
      "Epoch 62, Batch 2668, Loss: 160.09559631347656\n",
      "Epoch 62, Batch 2669, Loss: 189.78553771972656\n",
      "Epoch 62, Batch 2670, Loss: 167.77456665039062\n",
      "Epoch 62, Batch 2671, Loss: 184.70022583007812\n",
      "Epoch 62, Batch 2672, Loss: 161.1469268798828\n",
      "Epoch 62, Batch 2673, Loss: 173.5010528564453\n",
      "Epoch 62, Batch 2674, Loss: 170.6866455078125\n",
      "Epoch 62, Batch 2675, Loss: 181.67198181152344\n",
      "Epoch 62, Batch 2676, Loss: 170.09153747558594\n",
      "Epoch 62, Batch 2677, Loss: 179.62054443359375\n",
      "Epoch 62, Batch 2678, Loss: 156.45712280273438\n",
      "Epoch 62, Batch 2679, Loss: 182.64369201660156\n",
      "Epoch 62, Batch 2680, Loss: 172.03399658203125\n",
      "Epoch 62, Batch 2681, Loss: 167.28482055664062\n",
      "Epoch 62, Batch 2682, Loss: 161.51353454589844\n",
      "Epoch 62, Batch 2683, Loss: 175.4352569580078\n",
      "Epoch 62, Batch 2684, Loss: 171.51588439941406\n",
      "Epoch 62, Batch 2685, Loss: 170.98011779785156\n",
      "Epoch 62, Batch 2686, Loss: 159.38955688476562\n",
      "Epoch 62, Batch 2687, Loss: 166.50904846191406\n",
      "Epoch 62, Batch 2688, Loss: 176.47874450683594\n",
      "Epoch 62, Batch 2689, Loss: 179.4628448486328\n",
      "Epoch 62, Batch 2690, Loss: 182.3216552734375\n",
      "Epoch 62, Batch 2691, Loss: 182.1464080810547\n",
      "Epoch 62, Batch 2692, Loss: 163.22386169433594\n",
      "Epoch 62, Batch 2693, Loss: 170.32199096679688\n",
      "Epoch 62, Batch 2694, Loss: 173.842041015625\n",
      "Epoch 62, Batch 2695, Loss: 171.27301025390625\n",
      "Epoch 62, Batch 2696, Loss: 155.19639587402344\n",
      "Epoch 62, Batch 2697, Loss: 174.07322692871094\n",
      "Epoch 62, Batch 2698, Loss: 175.16354370117188\n",
      "Epoch 62, Batch 2699, Loss: 175.99691772460938\n",
      "Epoch 62, Batch 2700, Loss: 175.9116973876953\n",
      "Epoch 62, Batch 2701, Loss: 187.10592651367188\n",
      "Epoch 62, Batch 2702, Loss: 166.88230895996094\n",
      "Epoch 62, Batch 2703, Loss: 174.50576782226562\n",
      "Epoch 62, Batch 2704, Loss: 184.20233154296875\n",
      "Epoch 62, Batch 2705, Loss: 183.57887268066406\n",
      "Epoch 62, Batch 2706, Loss: 187.7061309814453\n",
      "Epoch 62, Batch 2707, Loss: 190.74630737304688\n",
      "Epoch 62, Batch 2708, Loss: 175.97418212890625\n",
      "Epoch 62, Batch 2709, Loss: 181.63951110839844\n",
      "Epoch 62, Batch 2710, Loss: 182.8831024169922\n",
      "Epoch 62, Batch 2711, Loss: 155.5408935546875\n",
      "Epoch 62, Batch 2712, Loss: 173.01132202148438\n",
      "Epoch 62, Batch 2713, Loss: 161.32257080078125\n",
      "Epoch 62, Batch 2714, Loss: 182.09393310546875\n",
      "Epoch 62, Batch 2715, Loss: 182.04307556152344\n",
      "Epoch 62, Batch 2716, Loss: 178.62901306152344\n",
      "Epoch 62, Batch 2717, Loss: 168.81300354003906\n",
      "Epoch 62, Batch 2718, Loss: 163.59999084472656\n",
      "Epoch 62, Batch 2719, Loss: 152.80885314941406\n",
      "Epoch 62, Batch 2720, Loss: 173.90391540527344\n",
      "Epoch 62, Batch 2721, Loss: 181.45957946777344\n",
      "Epoch 62, Batch 2722, Loss: 159.30535888671875\n",
      "Epoch 62, Batch 2723, Loss: 153.6798095703125\n",
      "Epoch 62, Batch 2724, Loss: 173.51048278808594\n",
      "Epoch 62, Batch 2725, Loss: 180.3727264404297\n",
      "Epoch 62, Batch 2726, Loss: 181.61573791503906\n",
      "Epoch 62, Batch 2727, Loss: 165.37738037109375\n",
      "Epoch 62, Batch 2728, Loss: 173.67796325683594\n",
      "Epoch 62, Batch 2729, Loss: 166.3627166748047\n",
      "Epoch 62, Batch 2730, Loss: 168.01951599121094\n",
      "Epoch 62, Batch 2731, Loss: 185.0887451171875\n",
      "Epoch 62, Batch 2732, Loss: 172.0365447998047\n",
      "Epoch 62, Batch 2733, Loss: 173.71780395507812\n",
      "Epoch 62, Batch 2734, Loss: 186.8488311767578\n",
      "Epoch 62, Batch 2735, Loss: 168.67999267578125\n",
      "Epoch 62, Batch 2736, Loss: 169.8783416748047\n",
      "Epoch 62, Batch 2737, Loss: 172.38072204589844\n",
      "Epoch 62, Batch 2738, Loss: 168.2094268798828\n",
      "Epoch 62, Batch 2739, Loss: 186.1925811767578\n",
      "Epoch 62, Batch 2740, Loss: 169.57275390625\n",
      "Epoch 62, Batch 2741, Loss: 159.17124938964844\n",
      "Epoch 62, Batch 2742, Loss: 177.41856384277344\n",
      "Epoch 62, Batch 2743, Loss: 151.0003662109375\n",
      "Epoch 62, Batch 2744, Loss: 191.50746154785156\n",
      "Epoch 62, Batch 2745, Loss: 170.72915649414062\n",
      "Epoch 62, Batch 2746, Loss: 178.7284393310547\n",
      "Epoch 62, Batch 2747, Loss: 187.43841552734375\n",
      "Epoch 62, Batch 2748, Loss: 161.3737030029297\n",
      "Epoch 62, Batch 2749, Loss: 168.62014770507812\n",
      "Epoch 62, Batch 2750, Loss: 182.7332000732422\n",
      "Epoch 62, Batch 2751, Loss: 167.2979278564453\n",
      "Epoch 62, Batch 2752, Loss: 179.05459594726562\n",
      "Epoch 62, Batch 2753, Loss: 179.19985961914062\n",
      "Epoch 62, Batch 2754, Loss: 176.19442749023438\n",
      "Epoch 62, Batch 2755, Loss: 164.0049285888672\n",
      "Epoch 62, Batch 2756, Loss: 163.0788116455078\n",
      "Epoch 62, Batch 2757, Loss: 165.870849609375\n",
      "Epoch 62, Batch 2758, Loss: 182.32183837890625\n",
      "Epoch 62, Batch 2759, Loss: 162.36361694335938\n",
      "Epoch 62, Batch 2760, Loss: 165.4512176513672\n",
      "Epoch 62, Batch 2761, Loss: 171.39019775390625\n",
      "Epoch 62, Batch 2762, Loss: 170.22084045410156\n",
      "Epoch 62, Batch 2763, Loss: 171.9521484375\n",
      "Epoch 62, Batch 2764, Loss: 153.09646606445312\n",
      "Epoch 62, Batch 2765, Loss: 170.86451721191406\n",
      "Epoch 62, Batch 2766, Loss: 179.39462280273438\n",
      "Epoch 62, Batch 2767, Loss: 184.80982971191406\n",
      "Epoch 62, Batch 2768, Loss: 162.57302856445312\n",
      "Epoch 62, Batch 2769, Loss: 177.4208221435547\n",
      "Epoch 62, Batch 2770, Loss: 172.7689208984375\n",
      "Epoch 62, Batch 2771, Loss: 184.78988647460938\n",
      "Epoch 62, Batch 2772, Loss: 182.38697814941406\n",
      "Epoch 62, Batch 2773, Loss: 172.99684143066406\n",
      "Epoch 62, Batch 2774, Loss: 171.06246948242188\n",
      "Epoch 62, Batch 2775, Loss: 187.1598663330078\n",
      "Epoch 62, Batch 2776, Loss: 169.08131408691406\n",
      "Epoch 62, Batch 2777, Loss: 178.68238830566406\n",
      "Epoch 62, Batch 2778, Loss: 174.1911163330078\n",
      "Epoch 62, Batch 2779, Loss: 180.65261840820312\n",
      "Epoch 62, Batch 2780, Loss: 169.7749481201172\n",
      "Epoch 62, Batch 2781, Loss: 186.18605041503906\n",
      "Epoch 62, Batch 2782, Loss: 184.51113891601562\n",
      "Epoch 62, Batch 2783, Loss: 171.85153198242188\n",
      "Epoch 62, Batch 2784, Loss: 170.6191864013672\n",
      "Epoch 62, Batch 2785, Loss: 167.6150665283203\n",
      "Epoch 62, Batch 2786, Loss: 171.9645538330078\n",
      "Epoch 62, Batch 2787, Loss: 180.14601135253906\n",
      "Epoch 62, Batch 2788, Loss: 178.06497192382812\n",
      "Epoch 62, Batch 2789, Loss: 163.88771057128906\n",
      "Epoch 62, Batch 2790, Loss: 175.55380249023438\n",
      "Epoch 62, Batch 2791, Loss: 173.9480438232422\n",
      "Epoch 62, Batch 2792, Loss: 164.6508026123047\n",
      "Epoch 62, Batch 2793, Loss: 161.85671997070312\n",
      "Epoch 62, Batch 2794, Loss: 188.07713317871094\n",
      "Epoch 62, Batch 2795, Loss: 165.54876708984375\n",
      "Epoch 62, Batch 2796, Loss: 164.08639526367188\n",
      "Epoch 62, Batch 2797, Loss: 172.98846435546875\n",
      "Epoch 62, Batch 2798, Loss: 183.42556762695312\n",
      "Epoch 62, Batch 2799, Loss: 168.06146240234375\n",
      "Epoch 62, Batch 2800, Loss: 183.17897033691406\n",
      "Epoch 62, Batch 2801, Loss: 178.00314331054688\n",
      "Epoch 62, Batch 2802, Loss: 178.19351196289062\n",
      "Epoch 62, Batch 2803, Loss: 177.90538024902344\n",
      "Epoch 62, Batch 2804, Loss: 180.55120849609375\n",
      "Epoch 62, Batch 2805, Loss: 175.0572967529297\n",
      "Epoch 62, Batch 2806, Loss: 163.52896118164062\n",
      "Epoch 62, Batch 2807, Loss: 161.49070739746094\n",
      "Epoch 62, Batch 2808, Loss: 174.23899841308594\n",
      "Epoch 62, Batch 2809, Loss: 165.45449829101562\n",
      "Epoch 62, Batch 2810, Loss: 191.3899688720703\n",
      "Epoch 62, Batch 2811, Loss: 174.23699951171875\n",
      "Epoch 62, Batch 2812, Loss: 162.16558837890625\n",
      "Epoch 62, Batch 2813, Loss: 172.96495056152344\n",
      "Epoch 62, Batch 2814, Loss: 147.82473754882812\n",
      "Epoch 62, Batch 2815, Loss: 159.64224243164062\n",
      "Epoch 62, Batch 2816, Loss: 167.70750427246094\n",
      "Epoch 62, Batch 2817, Loss: 170.88038635253906\n",
      "Epoch 62, Batch 2818, Loss: 167.08750915527344\n",
      "Epoch 62, Batch 2819, Loss: 179.8147430419922\n",
      "Epoch 62, Batch 2820, Loss: 172.8800811767578\n",
      "Epoch 62, Batch 2821, Loss: 170.02198791503906\n",
      "Epoch 62, Batch 2822, Loss: 162.2628173828125\n",
      "Epoch 62, Batch 2823, Loss: 178.176513671875\n",
      "Epoch 62, Batch 2824, Loss: 169.1796875\n",
      "Epoch 62, Batch 2825, Loss: 181.15460205078125\n",
      "Epoch 62, Batch 2826, Loss: 164.3307342529297\n",
      "Epoch 62, Batch 2827, Loss: 184.88462829589844\n",
      "Epoch 62, Batch 2828, Loss: 170.51727294921875\n",
      "Epoch 62, Batch 2829, Loss: 166.36351013183594\n",
      "Epoch 62, Batch 2830, Loss: 163.74435424804688\n",
      "Epoch 62, Batch 2831, Loss: 171.9314422607422\n",
      "Epoch 62, Batch 2832, Loss: 166.20420837402344\n",
      "Epoch 62, Batch 2833, Loss: 178.40982055664062\n",
      "Epoch 62, Batch 2834, Loss: 162.08168029785156\n",
      "Epoch 62, Batch 2835, Loss: 159.9141082763672\n",
      "Epoch 62, Batch 2836, Loss: 156.8497314453125\n",
      "Epoch 62, Batch 2837, Loss: 174.64085388183594\n",
      "Epoch 62, Batch 2838, Loss: 176.34701538085938\n",
      "Epoch 62, Batch 2839, Loss: 175.0340118408203\n",
      "Epoch 62, Batch 2840, Loss: 193.1433563232422\n",
      "Epoch 62, Batch 2841, Loss: 171.05221557617188\n",
      "Epoch 62, Batch 2842, Loss: 190.31607055664062\n",
      "Epoch 62, Batch 2843, Loss: 191.0025634765625\n",
      "Epoch 62, Batch 2844, Loss: 162.74563598632812\n",
      "Epoch 62, Batch 2845, Loss: 187.86505126953125\n",
      "Epoch 62, Batch 2846, Loss: 170.16578674316406\n",
      "Epoch 62, Batch 2847, Loss: 166.32723999023438\n",
      "Epoch 62, Batch 2848, Loss: 178.49957275390625\n",
      "Epoch 62, Batch 2849, Loss: 160.82861328125\n",
      "Epoch 62, Batch 2850, Loss: 178.08111572265625\n",
      "Epoch 62, Batch 2851, Loss: 186.22503662109375\n",
      "Epoch 62, Batch 2852, Loss: 190.72462463378906\n",
      "Epoch 62, Batch 2853, Loss: 177.6157684326172\n",
      "Epoch 62, Batch 2854, Loss: 154.27099609375\n",
      "Epoch 62, Batch 2855, Loss: 171.18492126464844\n",
      "Epoch 62, Batch 2856, Loss: 178.91177368164062\n",
      "Epoch 62, Batch 2857, Loss: 174.36180114746094\n",
      "Epoch 62, Batch 2858, Loss: 172.4127197265625\n",
      "Epoch 62, Batch 2859, Loss: 176.7926788330078\n",
      "Epoch 62, Batch 2860, Loss: 172.8652801513672\n",
      "Epoch 62, Batch 2861, Loss: 157.2895965576172\n",
      "Epoch 62, Batch 2862, Loss: 170.87222290039062\n",
      "Epoch 62, Batch 2863, Loss: 186.46450805664062\n",
      "Epoch 62, Batch 2864, Loss: 174.18350219726562\n",
      "Epoch 62, Batch 2865, Loss: 178.6737060546875\n",
      "Epoch 62, Batch 2866, Loss: 154.5207977294922\n",
      "Epoch 62, Batch 2867, Loss: 176.4944610595703\n",
      "Epoch 62, Batch 2868, Loss: 166.9828643798828\n",
      "Epoch 62, Batch 2869, Loss: 187.63494873046875\n",
      "Epoch 62, Batch 2870, Loss: 189.85594177246094\n",
      "Epoch 62, Batch 2871, Loss: 180.9210968017578\n",
      "Epoch 62, Batch 2872, Loss: 178.018310546875\n",
      "Epoch 62, Batch 2873, Loss: 178.86231994628906\n",
      "Epoch 62, Batch 2874, Loss: 177.97329711914062\n",
      "Epoch 62, Batch 2875, Loss: 173.55747985839844\n",
      "Epoch 62, Batch 2876, Loss: 188.25440979003906\n",
      "Epoch 62, Batch 2877, Loss: 161.49951171875\n",
      "Epoch 62, Batch 2878, Loss: 162.17491149902344\n",
      "Epoch 62, Batch 2879, Loss: 163.3937225341797\n",
      "Epoch 62, Batch 2880, Loss: 158.06036376953125\n",
      "Epoch 62, Batch 2881, Loss: 171.2307891845703\n",
      "Epoch 62, Batch 2882, Loss: 171.23953247070312\n",
      "Epoch 62, Batch 2883, Loss: 173.42294311523438\n",
      "Epoch 62, Batch 2884, Loss: 172.4818115234375\n",
      "Epoch 62, Batch 2885, Loss: 168.39578247070312\n",
      "Epoch 62, Batch 2886, Loss: 168.1184539794922\n",
      "Epoch 62, Batch 2887, Loss: 180.96653747558594\n",
      "Epoch 62, Batch 2888, Loss: 185.73231506347656\n",
      "Epoch 62, Batch 2889, Loss: 164.1020965576172\n",
      "Epoch 62, Batch 2890, Loss: 183.71463012695312\n",
      "Epoch 62, Batch 2891, Loss: 183.9661102294922\n",
      "Epoch 62, Batch 2892, Loss: 169.36180114746094\n",
      "Epoch 62, Batch 2893, Loss: 191.4091796875\n",
      "Epoch 62, Batch 2894, Loss: 163.1422576904297\n",
      "Epoch 62, Batch 2895, Loss: 197.12411499023438\n",
      "Epoch 62, Batch 2896, Loss: 171.80323791503906\n",
      "Epoch 62, Batch 2897, Loss: 171.54190063476562\n",
      "Epoch 62, Batch 2898, Loss: 160.9887237548828\n",
      "Epoch 62, Batch 2899, Loss: 175.2747344970703\n",
      "Epoch 62, Batch 2900, Loss: 173.9966583251953\n",
      "Epoch 62, Batch 2901, Loss: 168.59681701660156\n",
      "Epoch 62, Batch 2902, Loss: 167.38668823242188\n",
      "Epoch 62, Batch 2903, Loss: 182.71722412109375\n",
      "Epoch 62, Batch 2904, Loss: 182.3163299560547\n",
      "Epoch 62, Batch 2905, Loss: 173.7920379638672\n",
      "Epoch 62, Batch 2906, Loss: 171.7467803955078\n",
      "Epoch 62, Batch 2907, Loss: 169.97854614257812\n",
      "Epoch 62, Batch 2908, Loss: 174.27308654785156\n",
      "Epoch 62, Batch 2909, Loss: 178.69288635253906\n",
      "Epoch 62, Batch 2910, Loss: 173.3350372314453\n",
      "Epoch 62, Batch 2911, Loss: 181.59344482421875\n",
      "Epoch 62, Batch 2912, Loss: 189.71890258789062\n",
      "Epoch 62, Batch 2913, Loss: 171.236328125\n",
      "Epoch 62, Batch 2914, Loss: 163.8518829345703\n",
      "Epoch 62, Batch 2915, Loss: 179.66140747070312\n",
      "Epoch 62, Batch 2916, Loss: 158.8144989013672\n",
      "Epoch 62, Batch 2917, Loss: 183.19422912597656\n",
      "Epoch 62, Batch 2918, Loss: 169.7390899658203\n",
      "Epoch 62, Batch 2919, Loss: 175.2511444091797\n",
      "Epoch 62, Batch 2920, Loss: 167.7178192138672\n",
      "Epoch 62, Batch 2921, Loss: 176.21762084960938\n",
      "Epoch 62, Batch 2922, Loss: 157.1051483154297\n",
      "Epoch 62, Batch 2923, Loss: 196.69052124023438\n",
      "Epoch 62, Batch 2924, Loss: 177.32119750976562\n",
      "Epoch 62, Batch 2925, Loss: 162.5556640625\n",
      "Epoch 62, Batch 2926, Loss: 156.1204071044922\n",
      "Epoch 62, Batch 2927, Loss: 164.48414611816406\n",
      "Epoch 62, Batch 2928, Loss: 178.6403045654297\n",
      "Epoch 62, Batch 2929, Loss: 161.09274291992188\n",
      "Epoch 62, Batch 2930, Loss: 175.6216278076172\n",
      "Epoch 62, Batch 2931, Loss: 157.9040069580078\n",
      "Epoch 62, Batch 2932, Loss: 146.61819458007812\n",
      "Epoch 62, Batch 2933, Loss: 178.27003479003906\n",
      "Epoch 62, Batch 2934, Loss: 162.03067016601562\n",
      "Epoch 62, Batch 2935, Loss: 161.18014526367188\n",
      "Epoch 62, Batch 2936, Loss: 165.91998291015625\n",
      "Epoch 62, Batch 2937, Loss: 160.4783935546875\n",
      "Epoch 62, Batch 2938, Loss: 182.03366088867188\n",
      "Epoch 62, Batch 2939, Loss: 200.15591430664062\n",
      "Epoch 62, Batch 2940, Loss: 177.1356964111328\n",
      "Epoch 62, Batch 2941, Loss: 161.05474853515625\n",
      "Epoch 62, Batch 2942, Loss: 177.67201232910156\n",
      "Epoch 62, Batch 2943, Loss: 160.15631103515625\n",
      "Epoch 62, Batch 2944, Loss: 176.23446655273438\n",
      "Epoch 62, Batch 2945, Loss: 153.16932678222656\n",
      "Epoch 62, Batch 2946, Loss: 178.55722045898438\n",
      "Epoch 62, Batch 2947, Loss: 177.39190673828125\n",
      "Epoch 62, Batch 2948, Loss: 194.9281768798828\n",
      "Epoch 62, Batch 2949, Loss: 167.5322723388672\n",
      "Epoch 62, Batch 2950, Loss: 176.622802734375\n",
      "Epoch 62, Batch 2951, Loss: 180.73165893554688\n",
      "Epoch 62, Batch 2952, Loss: 174.95431518554688\n",
      "Epoch 62, Batch 2953, Loss: 170.52198791503906\n",
      "Epoch 62, Batch 2954, Loss: 149.78562927246094\n",
      "Epoch 62, Batch 2955, Loss: 190.53851318359375\n",
      "Epoch 62, Batch 2956, Loss: 184.58763122558594\n",
      "Epoch 62, Batch 2957, Loss: 176.47589111328125\n",
      "Epoch 62, Batch 2958, Loss: 182.41448974609375\n",
      "Epoch 62, Batch 2959, Loss: 171.0787811279297\n",
      "Epoch 62, Batch 2960, Loss: 178.1693572998047\n",
      "Epoch 62, Batch 2961, Loss: 174.95053100585938\n",
      "Epoch 62, Batch 2962, Loss: 177.02520751953125\n",
      "Epoch 62, Batch 2963, Loss: 170.41062927246094\n",
      "Epoch 62, Batch 2964, Loss: 188.56732177734375\n",
      "Epoch 62, Batch 2965, Loss: 169.37741088867188\n",
      "Epoch 62, Batch 2966, Loss: 168.80686950683594\n",
      "Epoch 62, Batch 2967, Loss: 169.76124572753906\n",
      "Epoch 62, Batch 2968, Loss: 154.4962615966797\n",
      "Epoch 62, Batch 2969, Loss: 203.9697723388672\n",
      "Epoch 62, Batch 2970, Loss: 168.42227172851562\n",
      "Epoch 62, Batch 2971, Loss: 176.73170471191406\n",
      "Epoch 62, Batch 2972, Loss: 185.10423278808594\n",
      "Epoch 62, Batch 2973, Loss: 170.54226684570312\n",
      "Epoch 62, Batch 2974, Loss: 157.29661560058594\n",
      "Epoch 62, Batch 2975, Loss: 159.89208984375\n",
      "Epoch 62, Batch 2976, Loss: 160.5513153076172\n",
      "Epoch 62, Batch 2977, Loss: 176.65411376953125\n",
      "Epoch 62, Batch 2978, Loss: 168.4530487060547\n",
      "Epoch 62, Batch 2979, Loss: 188.04649353027344\n",
      "Epoch 62, Batch 2980, Loss: 163.89173889160156\n",
      "Epoch 62, Batch 2981, Loss: 155.3412322998047\n",
      "Epoch 62, Batch 2982, Loss: 180.89794921875\n",
      "Epoch 62, Batch 2983, Loss: 170.51145935058594\n",
      "Epoch 62, Batch 2984, Loss: 159.74200439453125\n",
      "Epoch 62, Batch 2985, Loss: 171.33457946777344\n",
      "Epoch 62, Batch 2986, Loss: 183.92774963378906\n",
      "Epoch 62, Batch 2987, Loss: 170.78956604003906\n",
      "Epoch 62, Batch 2988, Loss: 163.923583984375\n",
      "Epoch 62, Batch 2989, Loss: 168.3165283203125\n",
      "Epoch 62, Batch 2990, Loss: 176.45053100585938\n",
      "Epoch 62, Batch 2991, Loss: 175.48190307617188\n",
      "Epoch 62, Batch 2992, Loss: 180.8709716796875\n",
      "Epoch 62, Batch 2993, Loss: 159.09396362304688\n",
      "Epoch 62, Batch 2994, Loss: 162.21337890625\n",
      "Epoch 62, Batch 2995, Loss: 178.1654052734375\n",
      "Epoch 62, Batch 2996, Loss: 169.31634521484375\n",
      "Epoch 62, Batch 2997, Loss: 177.88479614257812\n",
      "Epoch 62, Batch 2998, Loss: 158.07806396484375\n",
      "Epoch 62, Batch 2999, Loss: 169.38134765625\n",
      "Epoch 62, Batch 3000, Loss: 165.7427978515625\n",
      "Epoch 62, Batch 3001, Loss: 164.1844940185547\n",
      "Epoch 62, Batch 3002, Loss: 166.7371826171875\n",
      "Epoch 62, Batch 3003, Loss: 181.33790588378906\n",
      "Epoch 62, Batch 3004, Loss: 169.08935546875\n",
      "Epoch 62, Batch 3005, Loss: 175.598388671875\n",
      "Epoch 62, Batch 3006, Loss: 162.52398681640625\n",
      "Epoch 62, Batch 3007, Loss: 184.37472534179688\n",
      "Epoch 62, Batch 3008, Loss: 168.2691650390625\n",
      "Epoch 62, Batch 3009, Loss: 172.6958770751953\n",
      "Epoch 62, Batch 3010, Loss: 179.71324157714844\n",
      "Epoch 62, Batch 3011, Loss: 176.58966064453125\n",
      "Epoch 62, Batch 3012, Loss: 182.56932067871094\n",
      "Epoch 62, Batch 3013, Loss: 171.6656951904297\n",
      "Epoch 62, Batch 3014, Loss: 176.92596435546875\n",
      "Epoch 62, Batch 3015, Loss: 173.8692169189453\n",
      "Epoch 62, Batch 3016, Loss: 175.52685546875\n",
      "Epoch 62, Batch 3017, Loss: 155.60708618164062\n",
      "Epoch 62, Batch 3018, Loss: 161.43319702148438\n",
      "Epoch 62, Batch 3019, Loss: 160.26983642578125\n",
      "Epoch 62, Batch 3020, Loss: 175.79208374023438\n",
      "Epoch 62, Batch 3021, Loss: 186.67811584472656\n",
      "Epoch 62, Batch 3022, Loss: 169.46966552734375\n",
      "Epoch 62, Batch 3023, Loss: 173.49842834472656\n",
      "Epoch 62, Batch 3024, Loss: 158.7262725830078\n",
      "Epoch 62, Batch 3025, Loss: 180.578857421875\n",
      "Epoch 62, Batch 3026, Loss: 169.34725952148438\n",
      "Epoch 62, Batch 3027, Loss: 166.90460205078125\n",
      "Epoch 62, Batch 3028, Loss: 180.7051544189453\n",
      "Epoch 62, Batch 3029, Loss: 183.10833740234375\n",
      "Epoch 62, Batch 3030, Loss: 189.78829956054688\n",
      "Epoch 62, Batch 3031, Loss: 172.05523681640625\n",
      "Epoch 62, Batch 3032, Loss: 164.16209411621094\n",
      "Epoch 62, Batch 3033, Loss: 188.986083984375\n",
      "Epoch 62, Batch 3034, Loss: 182.18577575683594\n",
      "Epoch 62, Batch 3035, Loss: 176.25440979003906\n",
      "Epoch 62, Batch 3036, Loss: 185.3865509033203\n",
      "Epoch 62, Batch 3037, Loss: 188.10842895507812\n",
      "Epoch 62, Batch 3038, Loss: 164.0197296142578\n",
      "Epoch 62, Batch 3039, Loss: 165.1575164794922\n",
      "Epoch 62, Batch 3040, Loss: 166.8118896484375\n",
      "Epoch 62, Batch 3041, Loss: 175.25689697265625\n",
      "Epoch 62, Batch 3042, Loss: 164.20542907714844\n",
      "Epoch 62, Batch 3043, Loss: 169.75794982910156\n",
      "Epoch 62, Batch 3044, Loss: 180.4801483154297\n",
      "Epoch 62, Batch 3045, Loss: 190.2174530029297\n",
      "Epoch 62, Batch 3046, Loss: 164.78228759765625\n",
      "Epoch 62, Batch 3047, Loss: 176.6092071533203\n",
      "Epoch 62, Batch 3048, Loss: 172.96420288085938\n",
      "Epoch 62, Batch 3049, Loss: 147.3550262451172\n",
      "Epoch 62, Batch 3050, Loss: 190.94464111328125\n",
      "Epoch 62, Batch 3051, Loss: 181.53790283203125\n",
      "Epoch 62, Batch 3052, Loss: 169.72994995117188\n",
      "Epoch 62, Batch 3053, Loss: 167.02340698242188\n",
      "Epoch 62, Batch 3054, Loss: 156.18116760253906\n",
      "Epoch 62, Batch 3055, Loss: 165.39280700683594\n",
      "Epoch 62, Batch 3056, Loss: 183.7510223388672\n",
      "Epoch 62, Batch 3057, Loss: 175.98545837402344\n",
      "Epoch 62, Batch 3058, Loss: 167.29408264160156\n",
      "Epoch 62, Batch 3059, Loss: 168.81979370117188\n",
      "Epoch 62, Batch 3060, Loss: 169.6727294921875\n",
      "Epoch 62, Batch 3061, Loss: 176.50668334960938\n",
      "Epoch 62, Batch 3062, Loss: 156.34510803222656\n",
      "Epoch 62, Batch 3063, Loss: 171.7736053466797\n",
      "Epoch 62, Batch 3064, Loss: 181.9809112548828\n",
      "Epoch 62, Batch 3065, Loss: 172.6507568359375\n",
      "Epoch 62, Batch 3066, Loss: 171.3232879638672\n",
      "Epoch 62, Batch 3067, Loss: 161.71588134765625\n",
      "Epoch 62, Batch 3068, Loss: 178.60000610351562\n",
      "Epoch 62, Batch 3069, Loss: 181.9610137939453\n",
      "Epoch 62, Batch 3070, Loss: 170.5261688232422\n",
      "Epoch 62, Batch 3071, Loss: 170.31834411621094\n",
      "Epoch 62, Batch 3072, Loss: 177.36888122558594\n",
      "Epoch 62, Batch 3073, Loss: 179.54864501953125\n",
      "Epoch 62, Batch 3074, Loss: 172.35366821289062\n",
      "Epoch 62, Batch 3075, Loss: 179.4324951171875\n",
      "Epoch 62, Batch 3076, Loss: 172.0298309326172\n",
      "Epoch 62, Batch 3077, Loss: 169.80120849609375\n",
      "Epoch 62, Batch 3078, Loss: 168.30233764648438\n",
      "Epoch 62, Batch 3079, Loss: 180.65216064453125\n",
      "Epoch 62, Batch 3080, Loss: 173.5518035888672\n",
      "Epoch 62, Batch 3081, Loss: 164.97845458984375\n",
      "Epoch 62, Batch 3082, Loss: 170.9738006591797\n",
      "Epoch 62, Batch 3083, Loss: 148.87567138671875\n",
      "Epoch 62, Batch 3084, Loss: 163.8926544189453\n",
      "Epoch 62, Batch 3085, Loss: 170.6384735107422\n",
      "Epoch 62, Batch 3086, Loss: 191.53440856933594\n",
      "Epoch 62, Batch 3087, Loss: 183.27706909179688\n",
      "Epoch 62, Batch 3088, Loss: 169.28298950195312\n",
      "Epoch 62, Batch 3089, Loss: 165.1287078857422\n",
      "Epoch 62, Batch 3090, Loss: 175.77867126464844\n",
      "Epoch 62, Batch 3091, Loss: 166.6851348876953\n",
      "Epoch 62, Batch 3092, Loss: 175.28175354003906\n",
      "Epoch 62, Batch 3093, Loss: 180.79583740234375\n",
      "Epoch 62, Batch 3094, Loss: 149.77928161621094\n",
      "Epoch 62, Batch 3095, Loss: 186.90548706054688\n",
      "Epoch 62, Batch 3096, Loss: 158.74476623535156\n",
      "Epoch 62, Batch 3097, Loss: 169.4515838623047\n",
      "Epoch 62, Batch 3098, Loss: 161.249755859375\n",
      "Epoch 62, Batch 3099, Loss: 174.08216857910156\n",
      "Epoch 62, Batch 3100, Loss: 181.32374572753906\n",
      "Epoch 62, Batch 3101, Loss: 164.11195373535156\n",
      "Epoch 62, Batch 3102, Loss: 164.6563262939453\n",
      "Epoch 62, Batch 3103, Loss: 178.47210693359375\n",
      "Epoch 62, Batch 3104, Loss: 174.21392822265625\n",
      "Epoch 62, Batch 3105, Loss: 160.18211364746094\n",
      "Epoch 62, Batch 3106, Loss: 179.68247985839844\n",
      "Epoch 62, Batch 3107, Loss: 167.40977478027344\n",
      "Epoch 62, Batch 3108, Loss: 182.18899536132812\n",
      "Epoch 62, Batch 3109, Loss: 189.2777557373047\n",
      "Epoch 62, Batch 3110, Loss: 174.41477966308594\n",
      "Epoch 62, Batch 3111, Loss: 169.5181884765625\n",
      "Epoch 62, Batch 3112, Loss: 174.90750122070312\n",
      "Epoch 62, Batch 3113, Loss: 177.0259246826172\n",
      "Epoch 62, Batch 3114, Loss: 163.11964416503906\n",
      "Epoch 62, Batch 3115, Loss: 173.69129943847656\n",
      "Epoch 62, Batch 3116, Loss: 182.25527954101562\n",
      "Epoch 62, Batch 3117, Loss: 156.25198364257812\n",
      "Epoch 62, Batch 3118, Loss: 175.1635284423828\n",
      "Epoch 62, Batch 3119, Loss: 179.59603881835938\n",
      "Epoch 62, Batch 3120, Loss: 178.3650360107422\n",
      "Epoch 62, Batch 3121, Loss: 168.63656616210938\n",
      "Epoch 62, Batch 3122, Loss: 162.22299194335938\n",
      "Epoch 62, Batch 3123, Loss: 162.96336364746094\n",
      "Epoch 62, Batch 3124, Loss: 180.51231384277344\n",
      "Epoch 62, Batch 3125, Loss: 160.74411010742188\n",
      "Epoch 62, Batch 3126, Loss: 170.86427307128906\n",
      "Epoch 62, Batch 3127, Loss: 165.70919799804688\n",
      "Epoch 62, Batch 3128, Loss: 179.71487426757812\n",
      "Epoch 62, Batch 3129, Loss: 175.86265563964844\n",
      "Epoch 62, Batch 3130, Loss: 166.63092041015625\n",
      "Epoch 62, Batch 3131, Loss: 168.74024963378906\n",
      "Epoch 62, Batch 3132, Loss: 172.5223388671875\n",
      "Epoch 62, Batch 3133, Loss: 175.88172912597656\n",
      "Epoch 62, Batch 3134, Loss: 168.27394104003906\n",
      "Epoch 62, Batch 3135, Loss: 174.10650634765625\n",
      "Epoch 62, Batch 3136, Loss: 183.95765686035156\n",
      "Epoch 62, Batch 3137, Loss: 170.43511962890625\n",
      "Epoch 62, Batch 3138, Loss: 170.39320373535156\n",
      "Epoch 62, Batch 3139, Loss: 168.9232177734375\n",
      "Epoch 62, Batch 3140, Loss: 153.036865234375\n",
      "Epoch 62, Batch 3141, Loss: 195.2308349609375\n",
      "Epoch 62, Batch 3142, Loss: 170.51019287109375\n",
      "Epoch 62, Batch 3143, Loss: 173.21044921875\n",
      "Epoch 62, Batch 3144, Loss: 167.0408935546875\n",
      "Epoch 62, Batch 3145, Loss: 159.49261474609375\n",
      "Epoch 62, Batch 3146, Loss: 180.17095947265625\n",
      "Epoch 62, Batch 3147, Loss: 167.74624633789062\n",
      "Epoch 62, Batch 3148, Loss: 165.3216094970703\n",
      "Epoch 62, Batch 3149, Loss: 166.67396545410156\n",
      "Epoch 62, Batch 3150, Loss: 172.42694091796875\n",
      "Epoch 62, Batch 3151, Loss: 168.0069580078125\n",
      "Epoch 62, Batch 3152, Loss: 187.86851501464844\n",
      "Epoch 62, Batch 3153, Loss: 170.52134704589844\n",
      "Epoch 62, Batch 3154, Loss: 165.19754028320312\n",
      "Epoch 62, Batch 3155, Loss: 165.8158416748047\n",
      "Epoch 62, Batch 3156, Loss: 174.51597595214844\n",
      "Epoch 62, Batch 3157, Loss: 169.59165954589844\n",
      "Epoch 62, Batch 3158, Loss: 183.1748504638672\n",
      "Epoch 62, Batch 3159, Loss: 178.81053161621094\n",
      "Epoch 62, Batch 3160, Loss: 158.9906463623047\n",
      "Epoch 62, Batch 3161, Loss: 185.89390563964844\n",
      "Epoch 62, Batch 3162, Loss: 170.82505798339844\n",
      "Epoch 62, Batch 3163, Loss: 177.71047973632812\n",
      "Epoch 62, Batch 3164, Loss: 168.96600341796875\n",
      "Epoch 62, Batch 3165, Loss: 176.9630126953125\n",
      "Epoch 62, Batch 3166, Loss: 168.09036254882812\n",
      "Epoch 62, Batch 3167, Loss: 171.1517791748047\n",
      "Epoch 62, Batch 3168, Loss: 169.50767517089844\n",
      "Epoch 62, Batch 3169, Loss: 168.20187377929688\n",
      "Epoch 62, Batch 3170, Loss: 168.0458221435547\n",
      "Epoch 62, Batch 3171, Loss: 176.79591369628906\n",
      "Epoch 62, Batch 3172, Loss: 178.97752380371094\n",
      "Epoch 62, Batch 3173, Loss: 161.24349975585938\n",
      "Epoch 62, Batch 3174, Loss: 174.1713104248047\n",
      "Epoch 62, Batch 3175, Loss: 180.49313354492188\n",
      "Epoch 62, Batch 3176, Loss: 181.37562561035156\n",
      "Epoch 62, Batch 3177, Loss: 172.1368408203125\n",
      "Epoch 62, Batch 3178, Loss: 190.1700439453125\n",
      "Epoch 62, Batch 3179, Loss: 174.2357635498047\n",
      "Epoch 62, Batch 3180, Loss: 178.4065399169922\n",
      "Epoch 62, Batch 3181, Loss: 161.88706970214844\n",
      "Epoch 62, Batch 3182, Loss: 151.24134826660156\n",
      "Epoch 62, Batch 3183, Loss: 157.51181030273438\n",
      "Epoch 62, Batch 3184, Loss: 183.6444854736328\n",
      "Epoch 62, Batch 3185, Loss: 186.1568145751953\n",
      "Epoch 62, Batch 3186, Loss: 157.88430786132812\n",
      "Epoch 62, Batch 3187, Loss: 168.47018432617188\n",
      "Epoch 62, Batch 3188, Loss: 160.82611083984375\n",
      "Epoch 62, Batch 3189, Loss: 181.13368225097656\n",
      "Epoch 62, Batch 3190, Loss: 169.82986450195312\n",
      "Epoch 62, Batch 3191, Loss: 182.252685546875\n",
      "Epoch 62, Batch 3192, Loss: 146.01963806152344\n",
      "Epoch 62, Batch 3193, Loss: 166.84877014160156\n",
      "Epoch 62, Batch 3194, Loss: 197.96185302734375\n",
      "Epoch 62, Batch 3195, Loss: 178.03538513183594\n",
      "Epoch 62, Batch 3196, Loss: 176.65834045410156\n",
      "Epoch 62, Batch 3197, Loss: 174.47821044921875\n",
      "Epoch 62, Batch 3198, Loss: 165.9942626953125\n",
      "Epoch 62, Batch 3199, Loss: 171.8258056640625\n",
      "Epoch 62, Batch 3200, Loss: 162.9803924560547\n",
      "Epoch 62, Batch 3201, Loss: 162.81243896484375\n",
      "Epoch 62, Batch 3202, Loss: 160.42434692382812\n",
      "Epoch 62, Batch 3203, Loss: 160.55992126464844\n",
      "Epoch 62, Batch 3204, Loss: 199.86196899414062\n",
      "Epoch 62, Batch 3205, Loss: 180.10641479492188\n",
      "Epoch 62, Batch 3206, Loss: 187.06753540039062\n",
      "Epoch 62, Batch 3207, Loss: 190.42799377441406\n",
      "Epoch 62, Batch 3208, Loss: 182.51744079589844\n",
      "Epoch 62, Batch 3209, Loss: 164.63763427734375\n",
      "Epoch 62, Batch 3210, Loss: 178.57339477539062\n",
      "Epoch 62, Batch 3211, Loss: 170.45394897460938\n",
      "Epoch 62, Batch 3212, Loss: 166.6766357421875\n",
      "Epoch 62, Batch 3213, Loss: 166.8653564453125\n",
      "Epoch 62, Batch 3214, Loss: 178.5492401123047\n",
      "Epoch 62, Batch 3215, Loss: 179.9569091796875\n",
      "Epoch 62, Batch 3216, Loss: 168.83917236328125\n",
      "Epoch 62, Batch 3217, Loss: 160.20079040527344\n",
      "Epoch 62, Batch 3218, Loss: 167.30409240722656\n",
      "Epoch 62, Batch 3219, Loss: 175.6470184326172\n",
      "Epoch 62, Batch 3220, Loss: 189.51803588867188\n",
      "Epoch 62, Batch 3221, Loss: 164.0794677734375\n",
      "Epoch 62, Batch 3222, Loss: 179.9171600341797\n",
      "Epoch 62, Batch 3223, Loss: 185.0326385498047\n",
      "Epoch 62, Batch 3224, Loss: 186.18467712402344\n",
      "Epoch 62, Batch 3225, Loss: 178.8182830810547\n",
      "Epoch 62, Batch 3226, Loss: 180.74542236328125\n",
      "Epoch 62, Batch 3227, Loss: 188.71986389160156\n",
      "Epoch 62, Batch 3228, Loss: 188.1162872314453\n",
      "Epoch 62, Batch 3229, Loss: 161.85299682617188\n",
      "Epoch 62, Batch 3230, Loss: 175.3686065673828\n",
      "Epoch 62, Batch 3231, Loss: 183.45327758789062\n",
      "Epoch 62, Batch 3232, Loss: 187.93821716308594\n",
      "Epoch 62, Batch 3233, Loss: 164.61634826660156\n",
      "Epoch 62, Batch 3234, Loss: 167.9584197998047\n",
      "Epoch 62, Batch 3235, Loss: 164.38258361816406\n",
      "Epoch 62, Batch 3236, Loss: 168.46263122558594\n",
      "Epoch 62, Batch 3237, Loss: 170.79258728027344\n",
      "Epoch 62, Batch 3238, Loss: 167.61265563964844\n",
      "Epoch 62, Batch 3239, Loss: 157.70509338378906\n",
      "Epoch 62, Batch 3240, Loss: 185.08396911621094\n",
      "Epoch 62, Batch 3241, Loss: 175.69537353515625\n",
      "Epoch 62, Batch 3242, Loss: 180.70664978027344\n",
      "Epoch 62, Batch 3243, Loss: 158.53843688964844\n",
      "Epoch 62, Batch 3244, Loss: 166.16078186035156\n",
      "Epoch 62, Batch 3245, Loss: 170.08309936523438\n",
      "Epoch 62, Batch 3246, Loss: 168.63427734375\n",
      "Epoch 62, Batch 3247, Loss: 166.3255615234375\n",
      "Epoch 62, Batch 3248, Loss: 191.76300048828125\n",
      "Epoch 62, Batch 3249, Loss: 160.2913055419922\n",
      "Epoch 62, Batch 3250, Loss: 191.171630859375\n",
      "Epoch 62, Batch 3251, Loss: 196.2716522216797\n",
      "Epoch 62, Batch 3252, Loss: 167.04498291015625\n",
      "Epoch 62, Batch 3253, Loss: 168.1732177734375\n",
      "Epoch 62, Batch 3254, Loss: 191.40283203125\n",
      "Epoch 62, Batch 3255, Loss: 183.89366149902344\n",
      "Epoch 62, Batch 3256, Loss: 158.8077850341797\n",
      "Epoch 62, Batch 3257, Loss: 174.0912628173828\n",
      "Epoch 62, Batch 3258, Loss: 170.13417053222656\n",
      "Epoch 62, Batch 3259, Loss: 175.82763671875\n",
      "Epoch 62, Batch 3260, Loss: 170.03892517089844\n",
      "Epoch 62, Batch 3261, Loss: 167.2894744873047\n",
      "Epoch 62, Batch 3262, Loss: 181.0564422607422\n",
      "Epoch 62, Batch 3263, Loss: 181.2321319580078\n",
      "Epoch 62, Batch 3264, Loss: 158.27420043945312\n",
      "Epoch 62, Batch 3265, Loss: 193.02633666992188\n",
      "Epoch 62, Batch 3266, Loss: 174.04904174804688\n",
      "Epoch 62, Batch 3267, Loss: 188.92828369140625\n",
      "Epoch 62, Batch 3268, Loss: 177.19815063476562\n",
      "Epoch 62, Batch 3269, Loss: 171.77110290527344\n",
      "Epoch 62, Batch 3270, Loss: 156.83145141601562\n",
      "Epoch 62, Batch 3271, Loss: 170.37322998046875\n",
      "Epoch 62, Batch 3272, Loss: 154.82247924804688\n",
      "Epoch 62, Batch 3273, Loss: 171.2644805908203\n",
      "Epoch 62, Batch 3274, Loss: 184.94142150878906\n",
      "Epoch 62, Batch 3275, Loss: 170.5713348388672\n",
      "Epoch 62, Batch 3276, Loss: 161.18504333496094\n",
      "Epoch 62, Batch 3277, Loss: 182.74322509765625\n",
      "Epoch 62, Batch 3278, Loss: 170.28433227539062\n",
      "Epoch 62, Batch 3279, Loss: 175.65647888183594\n",
      "Epoch 62, Batch 3280, Loss: 185.60858154296875\n",
      "Epoch 62, Batch 3281, Loss: 177.07115173339844\n",
      "Epoch 62, Batch 3282, Loss: 170.63809204101562\n",
      "Epoch 62, Batch 3283, Loss: 160.8928985595703\n",
      "Epoch 62, Batch 3284, Loss: 179.29098510742188\n",
      "Epoch 62, Batch 3285, Loss: 156.9174346923828\n",
      "Epoch 62, Batch 3286, Loss: 173.6717071533203\n",
      "Epoch 62, Batch 3287, Loss: 170.47735595703125\n",
      "Epoch 62, Batch 3288, Loss: 175.64031982421875\n",
      "Epoch 62, Batch 3289, Loss: 161.35025024414062\n",
      "Epoch 62, Batch 3290, Loss: 174.65431213378906\n",
      "Epoch 62, Batch 3291, Loss: 178.35816955566406\n",
      "Epoch 62, Batch 3292, Loss: 169.6138458251953\n",
      "Epoch 62, Batch 3293, Loss: 179.8350067138672\n",
      "Epoch 62, Batch 3294, Loss: 158.27574157714844\n",
      "Epoch 62, Batch 3295, Loss: 187.55133056640625\n",
      "Epoch 62, Batch 3296, Loss: 177.85513305664062\n",
      "Epoch 62, Batch 3297, Loss: 172.81236267089844\n",
      "Epoch 62, Batch 3298, Loss: 175.54525756835938\n",
      "Epoch 62, Batch 3299, Loss: 166.48072814941406\n",
      "Epoch 62, Batch 3300, Loss: 181.14732360839844\n",
      "Epoch 62, Batch 3301, Loss: 166.1149139404297\n",
      "Epoch 62, Batch 3302, Loss: 188.2208709716797\n",
      "Epoch 62, Batch 3303, Loss: 168.4866180419922\n",
      "Epoch 62, Batch 3304, Loss: 178.710693359375\n",
      "Epoch 62, Batch 3305, Loss: 175.8488311767578\n",
      "Epoch 62, Batch 3306, Loss: 171.62413024902344\n",
      "Epoch 62, Batch 3307, Loss: 176.04393005371094\n",
      "Epoch 62, Batch 3308, Loss: 180.11416625976562\n",
      "Epoch 62, Batch 3309, Loss: 165.1134796142578\n",
      "Epoch 62, Batch 3310, Loss: 198.65452575683594\n",
      "Epoch 62, Batch 3311, Loss: 159.89588928222656\n",
      "Epoch 62, Batch 3312, Loss: 170.74720764160156\n",
      "Epoch 62, Batch 3313, Loss: 178.01495361328125\n",
      "Epoch 62, Batch 3314, Loss: 174.93186950683594\n",
      "Epoch 62, Batch 3315, Loss: 170.854248046875\n",
      "Epoch 62, Batch 3316, Loss: 170.0851287841797\n",
      "Epoch 62, Batch 3317, Loss: 174.25360107421875\n",
      "Epoch 62, Batch 3318, Loss: 190.1811065673828\n",
      "Epoch 62, Batch 3319, Loss: 172.59829711914062\n",
      "Epoch 62, Batch 3320, Loss: 182.03562927246094\n",
      "Epoch 62, Batch 3321, Loss: 156.35203552246094\n",
      "Epoch 62, Batch 3322, Loss: 190.97557067871094\n",
      "Epoch 62, Batch 3323, Loss: 173.47772216796875\n",
      "Epoch 62, Batch 3324, Loss: 168.47740173339844\n",
      "Epoch 62, Batch 3325, Loss: 165.0545654296875\n",
      "Epoch 62, Batch 3326, Loss: 162.56044006347656\n",
      "Epoch 62, Batch 3327, Loss: 172.48110961914062\n",
      "Epoch 62, Batch 3328, Loss: 150.56045532226562\n",
      "Epoch 62, Batch 3329, Loss: 185.20265197753906\n",
      "Epoch 62, Batch 3330, Loss: 168.2759552001953\n",
      "Epoch 62, Batch 3331, Loss: 165.24205017089844\n",
      "Epoch 62, Batch 3332, Loss: 173.9093780517578\n",
      "Epoch 62, Batch 3333, Loss: 164.57398986816406\n",
      "Epoch 62, Batch 3334, Loss: 169.92413330078125\n",
      "Epoch 62, Batch 3335, Loss: 156.5371551513672\n",
      "Epoch 62, Batch 3336, Loss: 175.35255432128906\n",
      "Epoch 62, Batch 3337, Loss: 162.8587646484375\n",
      "Epoch 62, Batch 3338, Loss: 167.04371643066406\n",
      "Epoch 62, Batch 3339, Loss: 174.81309509277344\n",
      "Epoch 62, Batch 3340, Loss: 183.10879516601562\n",
      "Epoch 62, Batch 3341, Loss: 191.19577026367188\n",
      "Epoch 62, Batch 3342, Loss: 159.8724822998047\n",
      "Epoch 62, Batch 3343, Loss: 176.70486450195312\n",
      "Epoch 62, Batch 3344, Loss: 160.27410888671875\n",
      "Epoch 62, Batch 3345, Loss: 190.64332580566406\n",
      "Epoch 62, Batch 3346, Loss: 170.16746520996094\n",
      "Epoch 62, Batch 3347, Loss: 171.803955078125\n",
      "Epoch 62, Batch 3348, Loss: 188.0999755859375\n",
      "Epoch 62, Batch 3349, Loss: 167.19076538085938\n",
      "Epoch 62, Batch 3350, Loss: 199.03062438964844\n",
      "Epoch 62, Batch 3351, Loss: 158.61468505859375\n",
      "Epoch 62, Batch 3352, Loss: 185.1334686279297\n",
      "Epoch 62, Batch 3353, Loss: 170.83154296875\n",
      "Epoch 62, Batch 3354, Loss: 184.668701171875\n",
      "Epoch 62, Batch 3355, Loss: 164.4520263671875\n",
      "Epoch 62, Batch 3356, Loss: 161.7894287109375\n",
      "Epoch 62, Batch 3357, Loss: 174.41180419921875\n",
      "Epoch 62, Batch 3358, Loss: 170.8448944091797\n",
      "Epoch 62, Batch 3359, Loss: 172.968017578125\n",
      "Epoch 62, Batch 3360, Loss: 184.19276428222656\n",
      "Epoch 62, Batch 3361, Loss: 173.33311462402344\n",
      "Epoch 62, Batch 3362, Loss: 168.42430114746094\n",
      "Epoch 62, Batch 3363, Loss: 183.20191955566406\n",
      "Epoch 62, Batch 3364, Loss: 179.70884704589844\n",
      "Epoch 62, Batch 3365, Loss: 178.2851104736328\n",
      "Epoch 62, Batch 3366, Loss: 185.34487915039062\n",
      "Epoch 62, Batch 3367, Loss: 174.17373657226562\n",
      "Epoch 62, Batch 3368, Loss: 186.42343139648438\n",
      "Epoch 62, Batch 3369, Loss: 184.60433959960938\n",
      "Epoch 62, Batch 3370, Loss: 168.55902099609375\n",
      "Epoch 62, Batch 3371, Loss: 175.95150756835938\n",
      "Epoch 62, Batch 3372, Loss: 163.40203857421875\n",
      "Epoch 62, Batch 3373, Loss: 163.06649780273438\n",
      "Epoch 62, Batch 3374, Loss: 171.6766815185547\n",
      "Epoch 62, Batch 3375, Loss: 177.5641632080078\n",
      "Epoch 62, Batch 3376, Loss: 188.99363708496094\n",
      "Epoch 62, Batch 3377, Loss: 166.01385498046875\n",
      "Epoch 62, Batch 3378, Loss: 171.9353790283203\n",
      "Epoch 62, Batch 3379, Loss: 179.50733947753906\n",
      "Epoch 62, Batch 3380, Loss: 160.49533081054688\n",
      "Epoch 62, Batch 3381, Loss: 175.75587463378906\n",
      "Epoch 62, Batch 3382, Loss: 182.9640350341797\n",
      "Epoch 62, Batch 3383, Loss: 163.97731018066406\n",
      "Epoch 62, Batch 3384, Loss: 159.8203582763672\n",
      "Epoch 62, Batch 3385, Loss: 164.66725158691406\n",
      "Epoch 62, Batch 3386, Loss: 161.63031005859375\n",
      "Epoch 62, Batch 3387, Loss: 186.84664916992188\n",
      "Epoch 62, Batch 3388, Loss: 177.0845184326172\n",
      "Epoch 62, Batch 3389, Loss: 186.83358764648438\n",
      "Epoch 62, Batch 3390, Loss: 174.56634521484375\n",
      "Epoch 62, Batch 3391, Loss: 161.83323669433594\n",
      "Epoch 62, Batch 3392, Loss: 179.70240783691406\n",
      "Epoch 62, Batch 3393, Loss: 176.9667510986328\n",
      "Epoch 62, Batch 3394, Loss: 174.8622589111328\n",
      "Epoch 62, Batch 3395, Loss: 177.80810546875\n",
      "Epoch 62, Batch 3396, Loss: 190.8054962158203\n",
      "Epoch 62, Batch 3397, Loss: 192.6095733642578\n",
      "Epoch 62, Batch 3398, Loss: 159.3280487060547\n",
      "Epoch 62, Batch 3399, Loss: 166.04879760742188\n",
      "Epoch 62, Batch 3400, Loss: 173.1781768798828\n",
      "Epoch 62, Batch 3401, Loss: 191.29458618164062\n",
      "Epoch 62, Batch 3402, Loss: 172.1256561279297\n",
      "Epoch 62, Batch 3403, Loss: 165.08212280273438\n",
      "Epoch 62, Batch 3404, Loss: 178.7579803466797\n",
      "Epoch 62, Batch 3405, Loss: 167.31109619140625\n",
      "Epoch 62, Batch 3406, Loss: 171.7228546142578\n",
      "Epoch 62, Batch 3407, Loss: 175.7808074951172\n",
      "Epoch 62, Batch 3408, Loss: 164.7705078125\n",
      "Epoch 62, Batch 3409, Loss: 159.26092529296875\n",
      "Epoch 62, Batch 3410, Loss: 185.96006774902344\n",
      "Epoch 62, Batch 3411, Loss: 170.00244140625\n",
      "Epoch 62, Batch 3412, Loss: 177.70477294921875\n",
      "Epoch 62, Batch 3413, Loss: 169.67596435546875\n",
      "Epoch 62, Batch 3414, Loss: 181.5399627685547\n",
      "Epoch 62, Batch 3415, Loss: 177.7443389892578\n",
      "Epoch 62, Batch 3416, Loss: 184.59078979492188\n",
      "Epoch 62, Batch 3417, Loss: 156.7008819580078\n",
      "Epoch 62, Batch 3418, Loss: 176.2384490966797\n",
      "Epoch 62, Batch 3419, Loss: 178.8006591796875\n",
      "Epoch 62, Batch 3420, Loss: 183.74925231933594\n",
      "Epoch 62, Batch 3421, Loss: 165.80438232421875\n",
      "Epoch 62, Batch 3422, Loss: 178.9311065673828\n",
      "Epoch 62, Batch 3423, Loss: 173.2002410888672\n",
      "Epoch 62, Batch 3424, Loss: 181.87823486328125\n",
      "Epoch 62, Batch 3425, Loss: 162.07347106933594\n",
      "Epoch 62, Batch 3426, Loss: 183.55850219726562\n",
      "Epoch 62, Batch 3427, Loss: 157.644775390625\n",
      "Epoch 62, Batch 3428, Loss: 172.21221923828125\n",
      "Epoch 62, Batch 3429, Loss: 181.5535888671875\n",
      "Epoch 62, Batch 3430, Loss: 166.10487365722656\n",
      "Epoch 62, Batch 3431, Loss: 180.27256774902344\n",
      "Epoch 62, Batch 3432, Loss: 164.96018981933594\n",
      "Epoch 62, Batch 3433, Loss: 181.8701171875\n",
      "Epoch 62, Batch 3434, Loss: 173.80422973632812\n",
      "Epoch 62, Batch 3435, Loss: 176.3406982421875\n",
      "Epoch 62, Batch 3436, Loss: 181.5201416015625\n",
      "Epoch 62, Batch 3437, Loss: 164.12295532226562\n",
      "Epoch 62, Batch 3438, Loss: 167.40682983398438\n",
      "Epoch 62, Batch 3439, Loss: 158.0238800048828\n",
      "Epoch 62, Batch 3440, Loss: 167.87875366210938\n",
      "Epoch 62, Batch 3441, Loss: 167.8140869140625\n",
      "Epoch 62, Batch 3442, Loss: 179.19081115722656\n",
      "Epoch 62, Batch 3443, Loss: 177.23553466796875\n",
      "Epoch 62, Batch 3444, Loss: 172.6079864501953\n",
      "Epoch 62, Batch 3445, Loss: 190.56503295898438\n",
      "Epoch 62, Batch 3446, Loss: 174.07171630859375\n",
      "Epoch 62, Batch 3447, Loss: 178.7053680419922\n",
      "Epoch 62, Batch 3448, Loss: 164.66595458984375\n",
      "Epoch 62, Batch 3449, Loss: 182.9833526611328\n",
      "Epoch 62, Batch 3450, Loss: 180.37130737304688\n",
      "Epoch 62, Batch 3451, Loss: 171.5124053955078\n",
      "Epoch 62, Batch 3452, Loss: 177.369873046875\n",
      "Epoch 62, Batch 3453, Loss: 174.41920471191406\n",
      "Epoch 62, Batch 3454, Loss: 175.29733276367188\n",
      "Epoch 62, Batch 3455, Loss: 174.8992462158203\n",
      "Epoch 62, Batch 3456, Loss: 176.70700073242188\n",
      "Epoch 62, Batch 3457, Loss: 161.53189086914062\n",
      "Epoch 62, Batch 3458, Loss: 166.4498748779297\n",
      "Epoch 62, Batch 3459, Loss: 192.7554473876953\n",
      "Epoch 62, Batch 3460, Loss: 181.95828247070312\n",
      "Epoch 62, Batch 3461, Loss: 174.0766143798828\n",
      "Epoch 62, Batch 3462, Loss: 175.86380004882812\n",
      "Epoch 62, Batch 3463, Loss: 166.89817810058594\n",
      "Epoch 62, Batch 3464, Loss: 165.1207275390625\n",
      "Epoch 62, Batch 3465, Loss: 168.27053833007812\n",
      "Epoch 62, Batch 3466, Loss: 164.3795166015625\n",
      "Epoch 62, Batch 3467, Loss: 190.30335998535156\n",
      "Epoch 62, Batch 3468, Loss: 161.86868286132812\n",
      "Epoch 62, Batch 3469, Loss: 159.82302856445312\n",
      "Epoch 62, Batch 3470, Loss: 180.356201171875\n",
      "Epoch 62, Batch 3471, Loss: 171.37139892578125\n",
      "Epoch 62, Batch 3472, Loss: 179.09957885742188\n",
      "Epoch 62, Batch 3473, Loss: 167.92999267578125\n",
      "Epoch 62, Batch 3474, Loss: 168.1838836669922\n",
      "Epoch 62, Batch 3475, Loss: 179.88897705078125\n",
      "Epoch 62, Batch 3476, Loss: 186.8834686279297\n",
      "Epoch 62, Batch 3477, Loss: 175.2064971923828\n",
      "Epoch 62, Batch 3478, Loss: 168.849365234375\n",
      "Epoch 62, Batch 3479, Loss: 184.08999633789062\n",
      "Epoch 62, Batch 3480, Loss: 172.84535217285156\n",
      "Epoch 62, Batch 3481, Loss: 181.2125701904297\n",
      "Epoch 62, Batch 3482, Loss: 183.89523315429688\n",
      "Epoch 62, Batch 3483, Loss: 161.12106323242188\n",
      "Epoch 62, Batch 3484, Loss: 184.73828125\n",
      "Epoch 62, Batch 3485, Loss: 167.34393310546875\n",
      "Epoch 62, Batch 3486, Loss: 174.0607452392578\n",
      "Epoch 62, Batch 3487, Loss: 178.2659912109375\n",
      "Epoch 62, Batch 3488, Loss: 170.1924591064453\n",
      "Epoch 62, Batch 3489, Loss: 187.82400512695312\n",
      "Epoch 62, Batch 3490, Loss: 176.18612670898438\n",
      "Epoch 62, Batch 3491, Loss: 178.24307250976562\n",
      "Epoch 62, Batch 3492, Loss: 166.4204864501953\n",
      "Epoch 62, Batch 3493, Loss: 191.57919311523438\n",
      "Epoch 62, Batch 3494, Loss: 171.9268798828125\n",
      "Epoch 62, Batch 3495, Loss: 177.6993865966797\n",
      "Epoch 62, Batch 3496, Loss: 167.72718811035156\n",
      "Epoch 62, Batch 3497, Loss: 164.5391387939453\n",
      "Epoch 62, Batch 3498, Loss: 165.83424377441406\n",
      "Epoch 62, Batch 3499, Loss: 173.74842834472656\n",
      "Epoch 62, Batch 3500, Loss: 198.4088897705078\n",
      "Epoch 62, Batch 3501, Loss: 189.13116455078125\n",
      "Epoch 62, Batch 3502, Loss: 156.42161560058594\n",
      "Epoch 62, Batch 3503, Loss: 168.57632446289062\n",
      "Epoch 62, Batch 3504, Loss: 146.94036865234375\n",
      "Epoch 62, Batch 3505, Loss: 173.75233459472656\n",
      "Epoch 62, Batch 3506, Loss: 165.48829650878906\n",
      "Epoch 62, Batch 3507, Loss: 157.9147491455078\n",
      "Epoch 62, Batch 3508, Loss: 167.104248046875\n",
      "Epoch 62, Batch 3509, Loss: 181.05902099609375\n",
      "Epoch 62, Batch 3510, Loss: 174.16831970214844\n",
      "Epoch 62, Batch 3511, Loss: 169.28575134277344\n",
      "Epoch 62, Batch 3512, Loss: 172.06773376464844\n",
      "Epoch 62, Batch 3513, Loss: 164.60438537597656\n",
      "Epoch 62, Batch 3514, Loss: 176.72311401367188\n",
      "Epoch 62, Batch 3515, Loss: 186.3477783203125\n",
      "Epoch 62, Batch 3516, Loss: 171.26173400878906\n",
      "Epoch 62, Batch 3517, Loss: 177.86070251464844\n",
      "Epoch 62, Batch 3518, Loss: 167.10157775878906\n",
      "Epoch 62, Batch 3519, Loss: 147.3708038330078\n",
      "Epoch 62, Batch 3520, Loss: 154.8806915283203\n",
      "Epoch 62, Batch 3521, Loss: 190.9881591796875\n",
      "Epoch 62, Batch 3522, Loss: 167.4948272705078\n",
      "Epoch 62, Batch 3523, Loss: 176.3864288330078\n",
      "Epoch 62, Batch 3524, Loss: 182.0738525390625\n",
      "Epoch 62, Batch 3525, Loss: 179.02777099609375\n",
      "Epoch 62, Batch 3526, Loss: 164.40786743164062\n",
      "Epoch 62, Batch 3527, Loss: 184.66677856445312\n",
      "Epoch 62, Batch 3528, Loss: 181.88417053222656\n",
      "Epoch 62, Batch 3529, Loss: 176.79847717285156\n",
      "Epoch 62, Batch 3530, Loss: 158.81103515625\n",
      "Epoch 62, Batch 3531, Loss: 183.1637725830078\n",
      "Epoch 62, Batch 3532, Loss: 170.1959991455078\n",
      "Epoch 62, Batch 3533, Loss: 177.63279724121094\n",
      "Epoch 62, Batch 3534, Loss: 193.21221923828125\n",
      "Epoch 62, Batch 3535, Loss: 176.33221435546875\n",
      "Epoch 62, Batch 3536, Loss: 177.42605590820312\n",
      "Epoch 62, Batch 3537, Loss: 183.54966735839844\n",
      "Epoch 62, Batch 3538, Loss: 151.694091796875\n",
      "Epoch 62, Batch 3539, Loss: 191.51170349121094\n",
      "Epoch 62, Batch 3540, Loss: 159.84255981445312\n",
      "Epoch 62, Batch 3541, Loss: 168.89988708496094\n",
      "Epoch 62, Batch 3542, Loss: 180.5917510986328\n",
      "Epoch 62, Batch 3543, Loss: 178.49935913085938\n",
      "Epoch 62, Batch 3544, Loss: 186.19395446777344\n",
      "Epoch 62, Batch 3545, Loss: 182.2968292236328\n",
      "Epoch 62, Batch 3546, Loss: 169.90780639648438\n",
      "Epoch 62, Batch 3547, Loss: 174.0082550048828\n",
      "Epoch 62, Batch 3548, Loss: 176.7538299560547\n",
      "Epoch 62, Batch 3549, Loss: 165.8962860107422\n",
      "Epoch 62, Batch 3550, Loss: 173.8547821044922\n",
      "Epoch 62, Batch 3551, Loss: 182.6751251220703\n",
      "Epoch 62, Batch 3552, Loss: 190.01902770996094\n",
      "Epoch 62, Batch 3553, Loss: 158.6399688720703\n",
      "Epoch 62, Batch 3554, Loss: 170.3893280029297\n",
      "Epoch 62, Batch 3555, Loss: 175.8100128173828\n",
      "Epoch 62, Batch 3556, Loss: 164.03187561035156\n",
      "Epoch 62, Batch 3557, Loss: 171.1591796875\n",
      "Epoch 62, Batch 3558, Loss: 178.41671752929688\n",
      "Epoch 62, Batch 3559, Loss: 164.18153381347656\n",
      "Epoch 62, Batch 3560, Loss: 172.17881774902344\n",
      "Epoch 62, Batch 3561, Loss: 179.50369262695312\n",
      "Epoch 62, Batch 3562, Loss: 171.52328491210938\n",
      "Epoch 62, Batch 3563, Loss: 167.0962677001953\n",
      "Epoch 62, Batch 3564, Loss: 168.20252990722656\n",
      "Epoch 62, Batch 3565, Loss: 155.45924377441406\n",
      "Epoch 62, Batch 3566, Loss: 163.58090209960938\n",
      "Epoch 62, Batch 3567, Loss: 159.80213928222656\n",
      "Epoch 62, Batch 3568, Loss: 181.04925537109375\n",
      "Epoch 62, Batch 3569, Loss: 160.6561737060547\n",
      "Epoch 62, Batch 3570, Loss: 182.21438598632812\n",
      "Epoch 62, Batch 3571, Loss: 162.7375030517578\n",
      "Epoch 62, Batch 3572, Loss: 167.5967254638672\n",
      "Epoch 62, Batch 3573, Loss: 179.71580505371094\n",
      "Epoch 62, Batch 3574, Loss: 152.56918334960938\n",
      "Epoch 62, Batch 3575, Loss: 161.40841674804688\n",
      "Epoch 62, Batch 3576, Loss: 169.34230041503906\n",
      "Epoch 62, Batch 3577, Loss: 175.126708984375\n",
      "Epoch 62, Batch 3578, Loss: 178.85049438476562\n",
      "Epoch 62, Batch 3579, Loss: 178.63316345214844\n",
      "Epoch 62, Batch 3580, Loss: 175.2609100341797\n",
      "Epoch 62, Batch 3581, Loss: 177.89923095703125\n",
      "Epoch 62, Batch 3582, Loss: 186.0255889892578\n",
      "Epoch 62, Batch 3583, Loss: 180.42799377441406\n",
      "Epoch 62, Batch 3584, Loss: 165.15359497070312\n",
      "Epoch 62, Batch 3585, Loss: 168.32955932617188\n",
      "Epoch 62, Batch 3586, Loss: 176.8573455810547\n",
      "Epoch 62, Batch 3587, Loss: 167.1588134765625\n",
      "Epoch 62, Batch 3588, Loss: 187.158447265625\n",
      "Epoch 62, Batch 3589, Loss: 167.085693359375\n",
      "Epoch 62, Batch 3590, Loss: 170.96881103515625\n",
      "Epoch 62, Batch 3591, Loss: 169.03599548339844\n",
      "Epoch 62, Batch 3592, Loss: 184.3898468017578\n",
      "Epoch 62, Batch 3593, Loss: 170.50906372070312\n",
      "Epoch 62, Batch 3594, Loss: 184.80575561523438\n",
      "Epoch 62, Batch 3595, Loss: 170.0943145751953\n",
      "Epoch 62, Batch 3596, Loss: 146.76126098632812\n",
      "Epoch 62, Batch 3597, Loss: 167.3539276123047\n",
      "Epoch 62, Batch 3598, Loss: 186.7876739501953\n",
      "Epoch 62, Batch 3599, Loss: 175.54168701171875\n",
      "Epoch 62, Batch 3600, Loss: 161.61207580566406\n",
      "Epoch 62, Batch 3601, Loss: 164.25135803222656\n",
      "Epoch 62, Batch 3602, Loss: 158.62860107421875\n",
      "Epoch 62, Batch 3603, Loss: 158.9577178955078\n",
      "Epoch 62, Batch 3604, Loss: 176.08935546875\n",
      "Epoch 62, Batch 3605, Loss: 173.9124298095703\n",
      "Epoch 62, Batch 3606, Loss: 162.2216796875\n",
      "Epoch 62, Batch 3607, Loss: 159.28607177734375\n",
      "Epoch 62, Batch 3608, Loss: 160.32469177246094\n",
      "Epoch 62, Batch 3609, Loss: 187.15603637695312\n",
      "Epoch 62, Batch 3610, Loss: 175.03887939453125\n",
      "Epoch 62, Batch 3611, Loss: 168.37368774414062\n",
      "Epoch 62, Batch 3612, Loss: 181.717041015625\n",
      "Epoch 62, Batch 3613, Loss: 152.2060546875\n",
      "Epoch 62, Batch 3614, Loss: 158.4630584716797\n",
      "Epoch 62, Batch 3615, Loss: 166.19520568847656\n",
      "Epoch 62, Batch 3616, Loss: 139.99197387695312\n",
      "Epoch 62, Batch 3617, Loss: 163.02626037597656\n",
      "Epoch 62, Batch 3618, Loss: 176.62857055664062\n",
      "Epoch 62, Batch 3619, Loss: 164.14352416992188\n",
      "Epoch 62, Batch 3620, Loss: 167.81256103515625\n",
      "Epoch 62, Batch 3621, Loss: 192.30880737304688\n",
      "Epoch 62, Batch 3622, Loss: 161.38731384277344\n",
      "Epoch 62, Batch 3623, Loss: 172.06797790527344\n",
      "Epoch 62, Batch 3624, Loss: 154.93707275390625\n",
      "Epoch 62, Batch 3625, Loss: 172.6817626953125\n",
      "Epoch 62, Batch 3626, Loss: 159.51611328125\n",
      "Epoch 62, Batch 3627, Loss: 169.7947998046875\n",
      "Epoch 62, Batch 3628, Loss: 185.0740203857422\n",
      "Epoch 62, Batch 3629, Loss: 164.2165069580078\n",
      "Epoch 62, Batch 3630, Loss: 158.84925842285156\n",
      "Epoch 62, Batch 3631, Loss: 159.5311279296875\n",
      "Epoch 62, Batch 3632, Loss: 171.14920043945312\n",
      "Epoch 62, Batch 3633, Loss: 167.95108032226562\n",
      "Epoch 62, Batch 3634, Loss: 165.7023468017578\n",
      "Epoch 62, Batch 3635, Loss: 170.19627380371094\n",
      "Epoch 62, Batch 3636, Loss: 158.43148803710938\n",
      "Epoch 62, Batch 3637, Loss: 192.55548095703125\n",
      "Epoch 62, Batch 3638, Loss: 170.39117431640625\n",
      "Epoch 62, Batch 3639, Loss: 189.3667755126953\n",
      "Epoch 62, Batch 3640, Loss: 169.87838745117188\n",
      "Epoch 62, Batch 3641, Loss: 138.82557678222656\n",
      "Epoch 62, Batch 3642, Loss: 166.21730041503906\n",
      "Epoch 62, Batch 3643, Loss: 187.24862670898438\n",
      "Epoch 62, Batch 3644, Loss: 162.4755401611328\n",
      "Epoch 62, Batch 3645, Loss: 177.81785583496094\n",
      "Epoch 62, Batch 3646, Loss: 193.45608520507812\n",
      "Epoch 62, Batch 3647, Loss: 173.93016052246094\n",
      "Epoch 62, Batch 3648, Loss: 183.01727294921875\n",
      "Epoch 62, Batch 3649, Loss: 186.67709350585938\n",
      "Epoch 62, Batch 3650, Loss: 168.99481201171875\n",
      "Epoch 62, Batch 3651, Loss: 169.32936096191406\n",
      "Epoch 62, Batch 3652, Loss: 161.05882263183594\n",
      "Epoch 62, Batch 3653, Loss: 165.55157470703125\n",
      "Epoch 62, Batch 3654, Loss: 170.0469207763672\n",
      "Epoch 62, Batch 3655, Loss: 178.91490173339844\n",
      "Epoch 62, Batch 3656, Loss: 192.8834686279297\n",
      "Epoch 62, Batch 3657, Loss: 162.25770568847656\n",
      "Epoch 62, Batch 3658, Loss: 158.1013641357422\n",
      "Epoch 62, Batch 3659, Loss: 178.5352325439453\n",
      "Epoch 62, Batch 3660, Loss: 172.2910614013672\n",
      "Epoch 62, Batch 3661, Loss: 178.81869506835938\n",
      "Epoch 62, Batch 3662, Loss: 161.81622314453125\n",
      "Epoch 62, Batch 3663, Loss: 163.36878967285156\n",
      "Epoch 62, Batch 3664, Loss: 173.0769805908203\n",
      "Epoch 62, Batch 3665, Loss: 192.417724609375\n",
      "Epoch 62, Batch 3666, Loss: 167.25892639160156\n",
      "Epoch 62, Batch 3667, Loss: 191.17555236816406\n",
      "Epoch 62, Batch 3668, Loss: 177.16001892089844\n",
      "Epoch 62, Batch 3669, Loss: 195.32894897460938\n",
      "Epoch 62, Batch 3670, Loss: 181.2135009765625\n",
      "Epoch 62, Batch 3671, Loss: 175.18016052246094\n",
      "Epoch 62, Batch 3672, Loss: 177.44720458984375\n",
      "Epoch 62, Batch 3673, Loss: 171.0178680419922\n",
      "Epoch 62, Batch 3674, Loss: 166.60775756835938\n",
      "Epoch 62, Batch 3675, Loss: 175.5144805908203\n",
      "Epoch 62, Batch 3676, Loss: 175.56912231445312\n",
      "Epoch 62, Batch 3677, Loss: 171.5753173828125\n",
      "Epoch 62, Batch 3678, Loss: 169.19113159179688\n",
      "Epoch 62, Batch 3679, Loss: 180.90591430664062\n",
      "Epoch 62, Batch 3680, Loss: 165.34991455078125\n",
      "Epoch 62, Batch 3681, Loss: 164.09201049804688\n",
      "Epoch 62, Batch 3682, Loss: 167.24920654296875\n",
      "Epoch 62, Batch 3683, Loss: 164.70010375976562\n",
      "Epoch 62, Batch 3684, Loss: 190.59800720214844\n",
      "Epoch 62, Batch 3685, Loss: 176.17454528808594\n",
      "Epoch 62, Batch 3686, Loss: 177.7662811279297\n",
      "Epoch 62, Batch 3687, Loss: 161.52359008789062\n",
      "Epoch 62, Batch 3688, Loss: 170.39956665039062\n",
      "Epoch 62, Batch 3689, Loss: 169.72793579101562\n",
      "Epoch 62, Batch 3690, Loss: 175.5746307373047\n",
      "Epoch 62, Batch 3691, Loss: 168.0986328125\n",
      "Epoch 62, Batch 3692, Loss: 186.85357666015625\n",
      "Epoch 62, Batch 3693, Loss: 177.97657775878906\n",
      "Epoch 62, Batch 3694, Loss: 174.7268524169922\n",
      "Epoch 62, Batch 3695, Loss: 178.1175994873047\n",
      "Epoch 62, Batch 3696, Loss: 162.96002197265625\n",
      "Epoch 62, Batch 3697, Loss: 188.74839782714844\n",
      "Epoch 62, Batch 3698, Loss: 174.60986328125\n",
      "Epoch 62, Batch 3699, Loss: 183.82131958007812\n",
      "Epoch 62, Batch 3700, Loss: 172.36595153808594\n",
      "Epoch 62, Batch 3701, Loss: 169.22210693359375\n",
      "Epoch 62, Batch 3702, Loss: 174.26962280273438\n",
      "Epoch 62, Batch 3703, Loss: 167.9973907470703\n",
      "Epoch 62, Batch 3704, Loss: 185.0078582763672\n",
      "Epoch 62, Batch 3705, Loss: 182.83251953125\n",
      "Epoch 62, Batch 3706, Loss: 176.8552703857422\n",
      "Epoch 62, Batch 3707, Loss: 159.70703125\n",
      "Epoch 62, Batch 3708, Loss: 177.59022521972656\n",
      "Epoch 62, Batch 3709, Loss: 173.0243377685547\n",
      "Epoch 62, Batch 3710, Loss: 162.1642608642578\n",
      "Epoch 62, Batch 3711, Loss: 171.7329559326172\n",
      "Epoch 62, Batch 3712, Loss: 155.69387817382812\n",
      "Epoch 62, Batch 3713, Loss: 180.37718200683594\n",
      "Epoch 62, Batch 3714, Loss: 178.19850158691406\n",
      "Epoch 62, Batch 3715, Loss: 166.27017211914062\n",
      "Epoch 62, Batch 3716, Loss: 170.5432891845703\n",
      "Epoch 62, Batch 3717, Loss: 158.4619140625\n",
      "Epoch 62, Batch 3718, Loss: 194.3499755859375\n",
      "Epoch 62, Batch 3719, Loss: 171.1654052734375\n",
      "Epoch 62, Batch 3720, Loss: 182.22386169433594\n",
      "Epoch 62, Batch 3721, Loss: 180.1923828125\n",
      "Epoch 62, Batch 3722, Loss: 177.19314575195312\n",
      "Epoch 62, Batch 3723, Loss: 186.14437866210938\n",
      "Epoch 62, Batch 3724, Loss: 173.57498168945312\n",
      "Epoch 62, Batch 3725, Loss: 153.69338989257812\n",
      "Epoch 62, Batch 3726, Loss: 181.06915283203125\n",
      "Epoch 62, Batch 3727, Loss: 164.38575744628906\n",
      "Epoch 62, Batch 3728, Loss: 176.34249877929688\n",
      "Epoch 62, Batch 3729, Loss: 183.2651824951172\n",
      "Epoch 62, Batch 3730, Loss: 179.4073944091797\n",
      "Epoch 62, Batch 3731, Loss: 167.14215087890625\n",
      "Epoch 62, Batch 3732, Loss: 159.68359375\n",
      "Epoch 62, Batch 3733, Loss: 187.36749267578125\n",
      "Epoch 62, Batch 3734, Loss: 173.72164916992188\n",
      "Epoch 62, Batch 3735, Loss: 174.22793579101562\n",
      "Epoch 62, Batch 3736, Loss: 189.75926208496094\n",
      "Epoch 62, Batch 3737, Loss: 178.37574768066406\n",
      "Epoch 62, Batch 3738, Loss: 170.26699829101562\n",
      "Epoch 62, Batch 3739, Loss: 173.71444702148438\n",
      "Epoch 62, Batch 3740, Loss: 177.44766235351562\n",
      "Epoch 62, Batch 3741, Loss: 182.9851837158203\n",
      "Epoch 62, Batch 3742, Loss: 181.249267578125\n",
      "Epoch 62, Batch 3743, Loss: 150.45803833007812\n",
      "Epoch 62, Batch 3744, Loss: 168.93267822265625\n",
      "Epoch 62, Batch 3745, Loss: 182.25636291503906\n",
      "Epoch 62, Batch 3746, Loss: 164.15255737304688\n",
      "Epoch 62, Batch 3747, Loss: 152.10232543945312\n",
      "Epoch 62, Batch 3748, Loss: 162.552001953125\n",
      "Epoch 62, Batch 3749, Loss: 178.33740234375\n",
      "Epoch 62, Batch 3750, Loss: 173.91436767578125\n",
      "Epoch 62, Batch 3751, Loss: 173.03614807128906\n",
      "Epoch 62, Batch 3752, Loss: 176.69442749023438\n",
      "Epoch 62, Batch 3753, Loss: 168.8700714111328\n",
      "Epoch 62, Batch 3754, Loss: 174.94854736328125\n",
      "Epoch 62, Batch 3755, Loss: 188.68313598632812\n",
      "Epoch 62, Batch 3756, Loss: 186.52154541015625\n",
      "Epoch 62, Batch 3757, Loss: 192.87550354003906\n",
      "Epoch 62, Batch 3758, Loss: 164.65733337402344\n",
      "Epoch 62, Batch 3759, Loss: 160.16958618164062\n",
      "Epoch 62, Batch 3760, Loss: 168.14051818847656\n",
      "Epoch 62, Batch 3761, Loss: 194.3863067626953\n",
      "Epoch 62, Batch 3762, Loss: 173.7332763671875\n",
      "Epoch 62, Batch 3763, Loss: 180.42672729492188\n",
      "Epoch 62, Batch 3764, Loss: 159.7872314453125\n",
      "Epoch 62, Batch 3765, Loss: 169.28245544433594\n",
      "Epoch 62, Batch 3766, Loss: 171.94288635253906\n",
      "Epoch 62, Batch 3767, Loss: 171.26853942871094\n",
      "Epoch 62, Batch 3768, Loss: 169.70718383789062\n",
      "Epoch 62, Batch 3769, Loss: 165.58836364746094\n",
      "Epoch 62, Batch 3770, Loss: 162.78057861328125\n",
      "Epoch 62, Batch 3771, Loss: 173.1782684326172\n",
      "Epoch 62, Batch 3772, Loss: 169.52017211914062\n",
      "Epoch 62, Batch 3773, Loss: 166.49502563476562\n",
      "Epoch 62, Batch 3774, Loss: 175.054931640625\n",
      "Epoch 62, Batch 3775, Loss: 176.21530151367188\n",
      "Epoch 62, Batch 3776, Loss: 176.91976928710938\n",
      "Epoch 62, Batch 3777, Loss: 186.4345245361328\n",
      "Epoch 62, Batch 3778, Loss: 160.68856811523438\n",
      "Epoch 62, Batch 3779, Loss: 172.54014587402344\n",
      "Epoch 62, Batch 3780, Loss: 156.630615234375\n",
      "Epoch 62, Batch 3781, Loss: 178.912841796875\n",
      "Epoch 62, Batch 3782, Loss: 193.0732879638672\n",
      "Epoch 62, Batch 3783, Loss: 159.56190490722656\n",
      "Epoch 62, Batch 3784, Loss: 163.80441284179688\n",
      "Epoch 62, Batch 3785, Loss: 180.60971069335938\n",
      "Epoch 62, Batch 3786, Loss: 162.76980590820312\n",
      "Epoch 62, Batch 3787, Loss: 156.1907958984375\n",
      "Epoch 62, Batch 3788, Loss: 184.58651733398438\n",
      "Epoch 62, Batch 3789, Loss: 167.37225341796875\n",
      "Epoch 62, Batch 3790, Loss: 175.70440673828125\n",
      "Epoch 62, Batch 3791, Loss: 181.7860565185547\n",
      "Epoch 62, Batch 3792, Loss: 163.59426879882812\n",
      "Epoch 62, Batch 3793, Loss: 162.40440368652344\n",
      "Epoch 62, Batch 3794, Loss: 170.49606323242188\n",
      "Epoch 62, Batch 3795, Loss: 206.9691162109375\n",
      "Epoch 62, Batch 3796, Loss: 186.71163940429688\n",
      "Epoch 62, Batch 3797, Loss: 165.30564880371094\n",
      "Epoch 62, Batch 3798, Loss: 176.06129455566406\n",
      "Epoch 62, Batch 3799, Loss: 182.69789123535156\n",
      "Epoch 62, Batch 3800, Loss: 195.55633544921875\n",
      "Epoch 62, Batch 3801, Loss: 170.0623321533203\n",
      "Epoch 62, Batch 3802, Loss: 174.239013671875\n",
      "Epoch 62, Batch 3803, Loss: 169.82106018066406\n",
      "Epoch 62, Batch 3804, Loss: 176.41201782226562\n",
      "Epoch 62, Batch 3805, Loss: 181.8625030517578\n",
      "Epoch 62, Batch 3806, Loss: 159.8175811767578\n",
      "Epoch 62, Batch 3807, Loss: 173.55633544921875\n",
      "Epoch 62, Batch 3808, Loss: 161.09963989257812\n",
      "Epoch 62, Batch 3809, Loss: 180.82423400878906\n",
      "Epoch 62, Batch 3810, Loss: 182.34719848632812\n",
      "Epoch 62, Batch 3811, Loss: 169.6294708251953\n",
      "Epoch 62, Batch 3812, Loss: 181.69017028808594\n",
      "Epoch 62, Batch 3813, Loss: 179.19287109375\n",
      "Epoch 62, Batch 3814, Loss: 167.45346069335938\n",
      "Epoch 62, Batch 3815, Loss: 166.35560607910156\n",
      "Epoch 62, Batch 3816, Loss: 161.4252471923828\n",
      "Epoch 62, Batch 3817, Loss: 186.1417236328125\n",
      "Epoch 62, Batch 3818, Loss: 175.53533935546875\n",
      "Epoch 62, Batch 3819, Loss: 162.0668487548828\n",
      "Epoch 62, Batch 3820, Loss: 168.0881805419922\n",
      "Epoch 62, Batch 3821, Loss: 162.64602661132812\n",
      "Epoch 62, Batch 3822, Loss: 185.48049926757812\n",
      "Epoch 62, Batch 3823, Loss: 165.0122528076172\n",
      "Epoch 62, Batch 3824, Loss: 183.84629821777344\n",
      "Epoch 62, Batch 3825, Loss: 187.922119140625\n",
      "Epoch 62, Batch 3826, Loss: 170.0160369873047\n",
      "Epoch 62, Batch 3827, Loss: 184.98184204101562\n",
      "Epoch 62, Batch 3828, Loss: 177.79000854492188\n",
      "Epoch 62, Batch 3829, Loss: 164.07537841796875\n",
      "Epoch 62, Batch 3830, Loss: 167.04139709472656\n",
      "Epoch 62, Batch 3831, Loss: 166.236572265625\n",
      "Epoch 62, Batch 3832, Loss: 171.73036193847656\n",
      "Epoch 62, Batch 3833, Loss: 185.352783203125\n",
      "Epoch 62, Batch 3834, Loss: 166.5524139404297\n",
      "Epoch 62, Batch 3835, Loss: 185.27822875976562\n",
      "Epoch 62, Batch 3836, Loss: 181.89759826660156\n",
      "Epoch 62, Batch 3837, Loss: 177.77552795410156\n",
      "Epoch 62, Batch 3838, Loss: 160.26515197753906\n",
      "Epoch 62, Batch 3839, Loss: 180.0921173095703\n",
      "Epoch 62, Batch 3840, Loss: 173.1338348388672\n",
      "Epoch 62, Batch 3841, Loss: 168.14353942871094\n",
      "Epoch 62, Batch 3842, Loss: 175.3629608154297\n",
      "Epoch 62, Batch 3843, Loss: 180.5767364501953\n",
      "Epoch 62, Batch 3844, Loss: 182.51361083984375\n",
      "Epoch 62, Batch 3845, Loss: 171.9161376953125\n",
      "Epoch 62, Batch 3846, Loss: 158.6710968017578\n",
      "Epoch 62, Batch 3847, Loss: 171.1093292236328\n",
      "Epoch 62, Batch 3848, Loss: 169.4872589111328\n",
      "Epoch 62, Batch 3849, Loss: 164.69801330566406\n",
      "Epoch 62, Batch 3850, Loss: 164.25750732421875\n",
      "Epoch 62, Batch 3851, Loss: 160.7403106689453\n",
      "Epoch 62, Batch 3852, Loss: 164.82852172851562\n",
      "Epoch 62, Batch 3853, Loss: 183.81663513183594\n",
      "Epoch 62, Batch 3854, Loss: 185.2421875\n",
      "Epoch 62, Batch 3855, Loss: 165.0852813720703\n",
      "Epoch 62, Batch 3856, Loss: 171.80624389648438\n",
      "Epoch 62, Batch 3857, Loss: 177.561767578125\n",
      "Epoch 62, Batch 3858, Loss: 162.36375427246094\n",
      "Epoch 62, Batch 3859, Loss: 162.9680633544922\n",
      "Epoch 62, Batch 3860, Loss: 192.781005859375\n",
      "Epoch 62, Batch 3861, Loss: 173.72860717773438\n",
      "Epoch 62, Batch 3862, Loss: 176.25686645507812\n",
      "Epoch 62, Batch 3863, Loss: 149.7705078125\n",
      "Epoch 62, Batch 3864, Loss: 175.12362670898438\n",
      "Epoch 62, Batch 3865, Loss: 171.21585083007812\n",
      "Epoch 62, Batch 3866, Loss: 186.1133270263672\n",
      "Epoch 62, Batch 3867, Loss: 171.69769287109375\n",
      "Epoch 62, Batch 3868, Loss: 180.16204833984375\n",
      "Epoch 62, Batch 3869, Loss: 166.75946044921875\n",
      "Epoch 62, Batch 3870, Loss: 165.47476196289062\n",
      "Epoch 62, Batch 3871, Loss: 165.808837890625\n",
      "Epoch 62, Batch 3872, Loss: 175.72401428222656\n",
      "Epoch 62, Batch 3873, Loss: 165.00550842285156\n",
      "Epoch 62, Batch 3874, Loss: 165.9561004638672\n",
      "Epoch 62, Batch 3875, Loss: 157.2215576171875\n",
      "Epoch 62, Batch 3876, Loss: 168.6702423095703\n",
      "Epoch 62, Batch 3877, Loss: 162.1906280517578\n",
      "Epoch 62, Batch 3878, Loss: 176.43997192382812\n",
      "Epoch 62, Batch 3879, Loss: 169.84939575195312\n",
      "Epoch 62, Batch 3880, Loss: 177.58584594726562\n",
      "Epoch 62, Batch 3881, Loss: 164.95513916015625\n",
      "Epoch 62, Batch 3882, Loss: 153.66363525390625\n",
      "Epoch 62, Batch 3883, Loss: 169.89794921875\n",
      "Epoch 62, Batch 3884, Loss: 159.23519897460938\n",
      "Epoch 62, Batch 3885, Loss: 159.2191619873047\n",
      "Epoch 62, Batch 3886, Loss: 177.4427490234375\n",
      "Epoch 62, Batch 3887, Loss: 180.14337158203125\n",
      "Epoch 62, Batch 3888, Loss: 168.8656463623047\n",
      "Epoch 62, Batch 3889, Loss: 165.865234375\n",
      "Epoch 62, Batch 3890, Loss: 156.36524963378906\n",
      "Epoch 62, Batch 3891, Loss: 168.431640625\n",
      "Epoch 62, Batch 3892, Loss: 168.87620544433594\n",
      "Epoch 62, Batch 3893, Loss: 182.01602172851562\n",
      "Epoch 62, Batch 3894, Loss: 165.11729431152344\n",
      "Epoch 62, Batch 3895, Loss: 174.26771545410156\n",
      "Epoch 62, Batch 3896, Loss: 176.6011505126953\n",
      "Epoch 62, Batch 3897, Loss: 171.3249053955078\n",
      "Epoch 62, Batch 3898, Loss: 175.3094940185547\n",
      "Epoch 62, Batch 3899, Loss: 176.78565979003906\n",
      "Epoch 62, Batch 3900, Loss: 183.36154174804688\n",
      "Epoch 62, Batch 3901, Loss: 175.41448974609375\n",
      "Epoch 62, Batch 3902, Loss: 169.6085968017578\n",
      "Epoch 62, Batch 3903, Loss: 166.61123657226562\n",
      "Epoch 62, Batch 3904, Loss: 183.53810119628906\n",
      "Epoch 62, Batch 3905, Loss: 161.08505249023438\n",
      "Epoch 62, Batch 3906, Loss: 177.3069305419922\n",
      "Epoch 62, Batch 3907, Loss: 175.16329956054688\n",
      "Epoch 62, Batch 3908, Loss: 174.8557891845703\n",
      "Epoch 62, Batch 3909, Loss: 173.2157745361328\n",
      "Epoch 62, Batch 3910, Loss: 184.5497283935547\n",
      "Epoch 62, Batch 3911, Loss: 166.00071716308594\n",
      "Epoch 62, Batch 3912, Loss: 157.93814086914062\n",
      "Epoch 62, Batch 3913, Loss: 171.20799255371094\n",
      "Epoch 62, Batch 3914, Loss: 174.4910125732422\n",
      "Epoch 62, Batch 3915, Loss: 155.10960388183594\n",
      "Epoch 62, Batch 3916, Loss: 163.52149963378906\n",
      "Epoch 62, Batch 3917, Loss: 184.0157012939453\n",
      "Epoch 62, Batch 3918, Loss: 164.90647888183594\n",
      "Epoch 62, Batch 3919, Loss: 169.6614532470703\n",
      "Epoch 62, Batch 3920, Loss: 169.97897338867188\n",
      "Epoch 62, Batch 3921, Loss: 149.2781524658203\n",
      "Epoch 62, Batch 3922, Loss: 180.8513946533203\n",
      "Epoch 62, Batch 3923, Loss: 175.30685424804688\n",
      "Epoch 62, Batch 3924, Loss: 186.5628662109375\n",
      "Epoch 62, Batch 3925, Loss: 166.3406524658203\n",
      "Epoch 62, Batch 3926, Loss: 173.74803161621094\n",
      "Epoch 62, Batch 3927, Loss: 195.3826446533203\n",
      "Epoch 62, Batch 3928, Loss: 172.28114318847656\n",
      "Epoch 62, Batch 3929, Loss: 179.10595703125\n",
      "Epoch 62, Batch 3930, Loss: 191.84510803222656\n",
      "Epoch 62, Batch 3931, Loss: 183.2910614013672\n",
      "Epoch 62, Batch 3932, Loss: 186.5347137451172\n",
      "Epoch 62, Batch 3933, Loss: 180.30181884765625\n",
      "Epoch 62, Batch 3934, Loss: 154.64085388183594\n",
      "Epoch 62, Batch 3935, Loss: 164.17819213867188\n",
      "Epoch 62, Batch 3936, Loss: 161.2775115966797\n",
      "Epoch 62, Batch 3937, Loss: 154.61837768554688\n",
      "Epoch 62, Batch 3938, Loss: 173.98931884765625\n",
      "Epoch 62, Batch 3939, Loss: 170.71096801757812\n",
      "Epoch 62, Batch 3940, Loss: 157.38372802734375\n",
      "Epoch 62, Batch 3941, Loss: 160.14447021484375\n",
      "Epoch 62, Batch 3942, Loss: 175.1677703857422\n",
      "Epoch 62, Batch 3943, Loss: 161.92189025878906\n",
      "Epoch 62, Batch 3944, Loss: 168.6825714111328\n",
      "Epoch 62, Batch 3945, Loss: 166.4455108642578\n",
      "Epoch 62, Batch 3946, Loss: 173.44793701171875\n",
      "Epoch 62, Batch 3947, Loss: 164.35012817382812\n",
      "Epoch 62, Batch 3948, Loss: 179.22433471679688\n",
      "Epoch 62, Batch 3949, Loss: 165.0846405029297\n",
      "Epoch 62, Batch 3950, Loss: 180.11334228515625\n",
      "Epoch 62, Batch 3951, Loss: 154.33584594726562\n",
      "Epoch 62, Batch 3952, Loss: 171.83853149414062\n",
      "Epoch 62, Batch 3953, Loss: 165.61607360839844\n",
      "Epoch 62, Batch 3954, Loss: 183.60182189941406\n",
      "Epoch 62, Batch 3955, Loss: 177.0493621826172\n",
      "Epoch 62, Batch 3956, Loss: 174.15579223632812\n",
      "Epoch 62, Batch 3957, Loss: 174.33419799804688\n",
      "Epoch 62, Batch 3958, Loss: 167.02694702148438\n",
      "Epoch 62, Batch 3959, Loss: 168.52284240722656\n",
      "Epoch 62, Batch 3960, Loss: 169.35260009765625\n",
      "Epoch 62, Batch 3961, Loss: 181.4107208251953\n",
      "Epoch 62, Batch 3962, Loss: 168.83541870117188\n",
      "Epoch 62, Batch 3963, Loss: 174.86465454101562\n",
      "Epoch 62, Batch 3964, Loss: 161.3150634765625\n",
      "Epoch 62, Batch 3965, Loss: 190.65895080566406\n",
      "Epoch 62, Batch 3966, Loss: 183.33279418945312\n",
      "Epoch 62, Batch 3967, Loss: 173.8255615234375\n",
      "Epoch 62, Batch 3968, Loss: 186.32571411132812\n",
      "Epoch 62, Batch 3969, Loss: 165.33531188964844\n",
      "Epoch 62, Batch 3970, Loss: 152.73873901367188\n",
      "Epoch 62, Batch 3971, Loss: 181.71981811523438\n",
      "Epoch 62, Batch 3972, Loss: 166.32177734375\n",
      "Epoch 62, Batch 3973, Loss: 183.67291259765625\n",
      "Epoch 62, Batch 3974, Loss: 180.98292541503906\n",
      "Epoch 62, Batch 3975, Loss: 159.9493865966797\n",
      "Epoch 62, Batch 3976, Loss: 165.18338012695312\n",
      "Epoch 62, Batch 3977, Loss: 182.60682678222656\n",
      "Epoch 62, Batch 3978, Loss: 165.04136657714844\n",
      "Epoch 62, Batch 3979, Loss: 171.97909545898438\n",
      "Epoch 62, Batch 3980, Loss: 169.77703857421875\n",
      "Epoch 62, Batch 3981, Loss: 177.18521118164062\n",
      "Epoch 62, Batch 3982, Loss: 189.62716674804688\n",
      "Epoch 62, Batch 3983, Loss: 184.91845703125\n",
      "Epoch 62, Batch 3984, Loss: 181.02438354492188\n",
      "Epoch 62, Batch 3985, Loss: 183.5076904296875\n",
      "Epoch 62, Batch 3986, Loss: 184.6066436767578\n",
      "Epoch 62, Batch 3987, Loss: 186.21188354492188\n",
      "Epoch 62, Batch 3988, Loss: 169.0247039794922\n",
      "Epoch 62, Batch 3989, Loss: 172.82888793945312\n",
      "Epoch 62, Batch 3990, Loss: 171.4936065673828\n",
      "Epoch 62, Batch 3991, Loss: 182.8300018310547\n",
      "Epoch 62, Batch 3992, Loss: 179.55677795410156\n",
      "Epoch 62, Batch 3993, Loss: 177.9047393798828\n",
      "Epoch 62, Batch 3994, Loss: 183.8640594482422\n",
      "Epoch 62, Batch 3995, Loss: 172.22666931152344\n",
      "Epoch 62, Batch 3996, Loss: 169.5878143310547\n",
      "Epoch 62, Batch 3997, Loss: 181.3325653076172\n",
      "Epoch 62, Batch 3998, Loss: 173.35647583007812\n",
      "Epoch 62, Batch 3999, Loss: 182.46389770507812\n",
      "Epoch 62, Batch 4000, Loss: 177.3890380859375\n",
      "Epoch 62, Batch 4001, Loss: 164.61236572265625\n",
      "Epoch 62, Batch 4002, Loss: 157.4103240966797\n",
      "Epoch 62, Batch 4003, Loss: 176.13571166992188\n",
      "Epoch 62, Batch 4004, Loss: 177.9587860107422\n",
      "Epoch 62, Batch 4005, Loss: 183.8825225830078\n",
      "Epoch 62, Batch 4006, Loss: 191.75184631347656\n",
      "Epoch 62, Batch 4007, Loss: 173.28204345703125\n",
      "Epoch 62, Batch 4008, Loss: 171.3059539794922\n",
      "Epoch 62, Batch 4009, Loss: 170.79653930664062\n",
      "Epoch 62, Batch 4010, Loss: 174.45846557617188\n",
      "Epoch 62, Batch 4011, Loss: 188.17410278320312\n",
      "Epoch 62, Batch 4012, Loss: 179.41522216796875\n",
      "Epoch 62, Batch 4013, Loss: 177.61280822753906\n",
      "Epoch 62, Batch 4014, Loss: 164.8664093017578\n",
      "Epoch 62, Batch 4015, Loss: 168.31031799316406\n",
      "Epoch 62, Batch 4016, Loss: 186.68492126464844\n",
      "Epoch 62, Batch 4017, Loss: 164.75527954101562\n",
      "Epoch 62, Batch 4018, Loss: 154.68368530273438\n",
      "Epoch 62, Batch 4019, Loss: 176.08953857421875\n",
      "Epoch 62, Batch 4020, Loss: 185.94996643066406\n",
      "Epoch 62, Batch 4021, Loss: 187.27780151367188\n",
      "Epoch 62, Batch 4022, Loss: 173.77716064453125\n",
      "Epoch 62, Batch 4023, Loss: 179.93170166015625\n",
      "Epoch 62, Batch 4024, Loss: 179.4832000732422\n",
      "Epoch 62, Batch 4025, Loss: 182.8545379638672\n",
      "Epoch 62, Batch 4026, Loss: 183.98123168945312\n",
      "Epoch 62, Batch 4027, Loss: 172.36825561523438\n",
      "Epoch 62, Batch 4028, Loss: 184.30233764648438\n",
      "Epoch 62, Batch 4029, Loss: 166.99911499023438\n",
      "Epoch 62, Batch 4030, Loss: 181.19337463378906\n",
      "Epoch 62, Batch 4031, Loss: 179.24232482910156\n",
      "Epoch 62, Batch 4032, Loss: 165.7484588623047\n",
      "Epoch 62, Batch 4033, Loss: 163.88182067871094\n",
      "Epoch 62, Batch 4034, Loss: 169.11131286621094\n",
      "Epoch 62, Batch 4035, Loss: 163.80712890625\n",
      "Epoch 62, Batch 4036, Loss: 158.2721710205078\n",
      "Epoch 62, Batch 4037, Loss: 164.46414184570312\n",
      "Epoch 62, Batch 4038, Loss: 159.86048889160156\n",
      "Epoch 62, Batch 4039, Loss: 200.2491912841797\n",
      "Epoch 62, Batch 4040, Loss: 174.33273315429688\n",
      "Epoch 62, Batch 4041, Loss: 173.49452209472656\n",
      "Epoch 62, Batch 4042, Loss: 154.58651733398438\n",
      "Epoch 62, Batch 4043, Loss: 180.78265380859375\n",
      "Epoch 62, Batch 4044, Loss: 156.01611328125\n",
      "Epoch 62, Batch 4045, Loss: 175.8357391357422\n",
      "Epoch 62, Batch 4046, Loss: 170.61776733398438\n",
      "Epoch 62, Batch 4047, Loss: 164.98886108398438\n",
      "Epoch 62, Batch 4048, Loss: 180.2427978515625\n",
      "Epoch 62, Batch 4049, Loss: 175.88331604003906\n",
      "Epoch 62, Batch 4050, Loss: 167.5267791748047\n",
      "Epoch 62, Batch 4051, Loss: 165.2771453857422\n",
      "Epoch 62, Batch 4052, Loss: 164.9754180908203\n",
      "Epoch 62, Batch 4053, Loss: 192.49412536621094\n",
      "Epoch 62, Batch 4054, Loss: 166.03927612304688\n",
      "Epoch 62, Batch 4055, Loss: 160.2263641357422\n",
      "Epoch 62, Batch 4056, Loss: 168.8828887939453\n",
      "Epoch 62, Batch 4057, Loss: 161.20249938964844\n",
      "Epoch 62, Batch 4058, Loss: 168.14828491210938\n",
      "Epoch 62, Batch 4059, Loss: 172.6746063232422\n",
      "Epoch 62, Batch 4060, Loss: 171.83213806152344\n",
      "Epoch 62, Batch 4061, Loss: 167.75753784179688\n",
      "Epoch 62, Batch 4062, Loss: 147.2237091064453\n",
      "Epoch 62, Batch 4063, Loss: 179.3622283935547\n",
      "Epoch 62, Batch 4064, Loss: 175.56642150878906\n",
      "Epoch 62, Batch 4065, Loss: 170.1658935546875\n",
      "Epoch 62, Batch 4066, Loss: 160.97714233398438\n",
      "Epoch 62, Batch 4067, Loss: 173.73153686523438\n",
      "Epoch 62, Batch 4068, Loss: 186.49505615234375\n",
      "Epoch 62, Batch 4069, Loss: 181.70005798339844\n",
      "Epoch 62, Batch 4070, Loss: 178.12831115722656\n",
      "Epoch 62, Batch 4071, Loss: 175.5978240966797\n",
      "Epoch 62, Batch 4072, Loss: 176.49911499023438\n",
      "Epoch 62, Batch 4073, Loss: 180.4195556640625\n",
      "Epoch 62, Batch 4074, Loss: 173.1017608642578\n",
      "Epoch 62, Batch 4075, Loss: 170.3876190185547\n",
      "Epoch 62, Batch 4076, Loss: 177.0255889892578\n",
      "Epoch 62, Batch 4077, Loss: 179.12330627441406\n",
      "Epoch 62, Batch 4078, Loss: 172.7753143310547\n",
      "Epoch 62, Batch 4079, Loss: 174.04112243652344\n",
      "Epoch 62, Batch 4080, Loss: 173.2895965576172\n",
      "Epoch 62, Batch 4081, Loss: 185.0108642578125\n",
      "Epoch 62, Batch 4082, Loss: 174.34600830078125\n",
      "Epoch 62, Batch 4083, Loss: 159.27857971191406\n",
      "Epoch 62, Batch 4084, Loss: 194.26438903808594\n",
      "Epoch 62, Batch 4085, Loss: 169.99325561523438\n",
      "Epoch 62, Batch 4086, Loss: 164.13629150390625\n",
      "Epoch 62, Batch 4087, Loss: 179.84483337402344\n",
      "Epoch 62, Batch 4088, Loss: 175.5979766845703\n",
      "Epoch 62, Batch 4089, Loss: 169.76119995117188\n",
      "Epoch 62, Batch 4090, Loss: 166.1580047607422\n",
      "Epoch 62, Batch 4091, Loss: 176.05099487304688\n",
      "Epoch 62, Batch 4092, Loss: 177.70924377441406\n",
      "Epoch 62, Batch 4093, Loss: 177.66796875\n",
      "Epoch 62, Batch 4094, Loss: 175.50979614257812\n",
      "Epoch 62, Batch 4095, Loss: 176.483642578125\n",
      "Epoch 62, Batch 4096, Loss: 172.49169921875\n",
      "Epoch 62, Batch 4097, Loss: 161.20822143554688\n",
      "Epoch 62, Batch 4098, Loss: 171.75408935546875\n",
      "Epoch 62, Batch 4099, Loss: 165.52491760253906\n",
      "Epoch 62, Batch 4100, Loss: 183.9105224609375\n",
      "Epoch 62, Batch 4101, Loss: 182.46786499023438\n",
      "Epoch 62, Batch 4102, Loss: 169.994140625\n",
      "Epoch 62, Batch 4103, Loss: 164.18812561035156\n",
      "Epoch 62, Batch 4104, Loss: 173.6319580078125\n",
      "Epoch 62, Batch 4105, Loss: 176.0697784423828\n",
      "Epoch 62, Batch 4106, Loss: 177.7154998779297\n",
      "Epoch 62, Batch 4107, Loss: 188.97364807128906\n",
      "Epoch 62, Batch 4108, Loss: 167.26364135742188\n",
      "Epoch 62, Batch 4109, Loss: 156.89102172851562\n",
      "Epoch 62, Batch 4110, Loss: 171.66749572753906\n",
      "Epoch 62, Batch 4111, Loss: 182.08070373535156\n",
      "Epoch 62, Batch 4112, Loss: 168.35154724121094\n",
      "Epoch 62, Batch 4113, Loss: 182.32225036621094\n",
      "Epoch 62, Batch 4114, Loss: 150.79649353027344\n",
      "Epoch 62, Batch 4115, Loss: 173.95452880859375\n",
      "Epoch 62, Batch 4116, Loss: 181.26904296875\n",
      "Epoch 62, Batch 4117, Loss: 162.60418701171875\n",
      "Epoch 62, Batch 4118, Loss: 172.2941131591797\n",
      "Epoch 62, Batch 4119, Loss: 175.89939880371094\n",
      "Epoch 62, Batch 4120, Loss: 186.99908447265625\n",
      "Epoch 62, Batch 4121, Loss: 167.35317993164062\n",
      "Epoch 62, Batch 4122, Loss: 149.01370239257812\n",
      "Epoch 62, Batch 4123, Loss: 160.2759552001953\n",
      "Epoch 62, Batch 4124, Loss: 161.6671600341797\n",
      "Epoch 62, Batch 4125, Loss: 180.29116821289062\n",
      "Epoch 62, Batch 4126, Loss: 156.1676788330078\n",
      "Epoch 62, Batch 4127, Loss: 163.49501037597656\n",
      "Epoch 62, Batch 4128, Loss: 178.11111450195312\n",
      "Epoch 62, Batch 4129, Loss: 168.00973510742188\n",
      "Epoch 62, Batch 4130, Loss: 169.57614135742188\n",
      "Epoch 62, Batch 4131, Loss: 171.45166015625\n",
      "Epoch 62, Batch 4132, Loss: 182.04945373535156\n",
      "Epoch 62, Batch 4133, Loss: 161.25767517089844\n",
      "Epoch 62, Batch 4134, Loss: 163.26092529296875\n",
      "Epoch 62, Batch 4135, Loss: 182.15528869628906\n",
      "Epoch 62, Batch 4136, Loss: 188.15504455566406\n",
      "Epoch 62, Batch 4137, Loss: 174.53726196289062\n",
      "Epoch 62, Batch 4138, Loss: 163.9437255859375\n",
      "Epoch 62, Batch 4139, Loss: 171.63587951660156\n",
      "Epoch 62, Batch 4140, Loss: 168.755859375\n",
      "Epoch 62, Batch 4141, Loss: 176.4320526123047\n",
      "Epoch 62, Batch 4142, Loss: 164.905029296875\n",
      "Epoch 62, Batch 4143, Loss: 169.7616424560547\n",
      "Epoch 62, Batch 4144, Loss: 154.27967834472656\n",
      "Epoch 62, Batch 4145, Loss: 175.94068908691406\n",
      "Epoch 62, Batch 4146, Loss: 165.51206970214844\n",
      "Epoch 62, Batch 4147, Loss: 155.4946746826172\n",
      "Epoch 62, Batch 4148, Loss: 170.0605926513672\n",
      "Epoch 62, Batch 4149, Loss: 167.71185302734375\n",
      "Epoch 62, Batch 4150, Loss: 188.03713989257812\n",
      "Epoch 62, Batch 4151, Loss: 179.71237182617188\n",
      "Epoch 62, Batch 4152, Loss: 181.04551696777344\n",
      "Epoch 62, Batch 4153, Loss: 174.148681640625\n",
      "Epoch 62, Batch 4154, Loss: 180.1066436767578\n",
      "Epoch 62, Batch 4155, Loss: 168.8618927001953\n",
      "Epoch 62, Batch 4156, Loss: 153.48480224609375\n",
      "Epoch 62, Batch 4157, Loss: 164.8258056640625\n",
      "Epoch 62, Batch 4158, Loss: 151.62826538085938\n",
      "Epoch 62, Batch 4159, Loss: 196.3166046142578\n",
      "Epoch 62, Batch 4160, Loss: 165.57606506347656\n",
      "Epoch 62, Batch 4161, Loss: 174.7948455810547\n",
      "Epoch 62, Batch 4162, Loss: 160.00282287597656\n",
      "Epoch 62, Batch 4163, Loss: 166.87435913085938\n",
      "Epoch 62, Batch 4164, Loss: 184.71820068359375\n",
      "Epoch 62, Batch 4165, Loss: 177.38368225097656\n",
      "Epoch 62, Batch 4166, Loss: 190.31808471679688\n",
      "Epoch 62, Batch 4167, Loss: 172.15621948242188\n",
      "Epoch 62, Batch 4168, Loss: 194.41244506835938\n",
      "Epoch 62, Batch 4169, Loss: 181.63790893554688\n",
      "Epoch 62, Batch 4170, Loss: 173.89378356933594\n",
      "Epoch 62, Batch 4171, Loss: 165.8135528564453\n",
      "Epoch 62, Batch 4172, Loss: 178.13104248046875\n",
      "Epoch 62, Batch 4173, Loss: 165.6007080078125\n",
      "Epoch 62, Batch 4174, Loss: 155.83822631835938\n",
      "Epoch 62, Batch 4175, Loss: 183.902587890625\n",
      "Epoch 62, Batch 4176, Loss: 163.3068389892578\n",
      "Epoch 62, Batch 4177, Loss: 175.81118774414062\n",
      "Epoch 62, Batch 4178, Loss: 178.03587341308594\n",
      "Epoch 62, Batch 4179, Loss: 178.67013549804688\n",
      "Epoch 62, Batch 4180, Loss: 179.86679077148438\n",
      "Epoch 62, Batch 4181, Loss: 179.6995086669922\n",
      "Epoch 62, Batch 4182, Loss: 160.9852752685547\n",
      "Epoch 62, Batch 4183, Loss: 170.73098754882812\n",
      "Epoch 62, Batch 4184, Loss: 169.6475372314453\n",
      "Epoch 62, Batch 4185, Loss: 171.55072021484375\n",
      "Epoch 62, Batch 4186, Loss: 165.0148468017578\n",
      "Epoch 62, Batch 4187, Loss: 179.19921875\n",
      "Epoch 62, Batch 4188, Loss: 182.7039337158203\n",
      "Epoch 62, Batch 4189, Loss: 170.44911193847656\n",
      "Epoch 62, Batch 4190, Loss: 182.96644592285156\n",
      "Epoch 62, Batch 4191, Loss: 170.23544311523438\n",
      "Epoch 62, Batch 4192, Loss: 178.81776428222656\n",
      "Epoch 62, Batch 4193, Loss: 168.8024444580078\n",
      "Epoch 62, Batch 4194, Loss: 174.1991424560547\n",
      "Epoch 62, Batch 4195, Loss: 187.9033660888672\n",
      "Epoch 62, Batch 4196, Loss: 171.37428283691406\n",
      "Epoch 62, Batch 4197, Loss: 165.43185424804688\n",
      "Epoch 62, Batch 4198, Loss: 162.43603515625\n",
      "Epoch 62, Batch 4199, Loss: 180.92918395996094\n",
      "Epoch 62, Batch 4200, Loss: 165.7684783935547\n",
      "Epoch 62, Batch 4201, Loss: 160.1139678955078\n",
      "Epoch 62, Batch 4202, Loss: 174.42990112304688\n",
      "Epoch 62, Batch 4203, Loss: 163.18063354492188\n",
      "Epoch 62, Batch 4204, Loss: 175.36038208007812\n",
      "Epoch 62, Batch 4205, Loss: 170.86863708496094\n",
      "Epoch 62, Batch 4206, Loss: 168.3628387451172\n",
      "Epoch 62, Batch 4207, Loss: 162.750244140625\n",
      "Epoch 62, Batch 4208, Loss: 172.78826904296875\n",
      "Epoch 62, Batch 4209, Loss: 163.52955627441406\n",
      "Epoch 62, Batch 4210, Loss: 170.9189910888672\n",
      "Epoch 62, Batch 4211, Loss: 165.0654296875\n",
      "Epoch 62, Batch 4212, Loss: 176.6092071533203\n",
      "Epoch 62, Batch 4213, Loss: 172.70533752441406\n",
      "Epoch 62, Batch 4214, Loss: 191.301025390625\n",
      "Epoch 62, Batch 4215, Loss: 184.28091430664062\n",
      "Epoch 62, Batch 4216, Loss: 167.5839080810547\n",
      "Epoch 62, Batch 4217, Loss: 181.61192321777344\n",
      "Epoch 62, Batch 4218, Loss: 180.84767150878906\n",
      "Epoch 62, Batch 4219, Loss: 187.30194091796875\n",
      "Epoch 62, Batch 4220, Loss: 174.20252990722656\n",
      "Epoch 62, Batch 4221, Loss: 186.80282592773438\n",
      "Epoch 62, Batch 4222, Loss: 174.80722045898438\n",
      "Epoch 62, Batch 4223, Loss: 167.9648895263672\n",
      "Epoch 62, Batch 4224, Loss: 175.6141357421875\n",
      "Epoch 62, Batch 4225, Loss: 180.48341369628906\n",
      "Epoch 62, Batch 4226, Loss: 180.5820770263672\n",
      "Epoch 62, Batch 4227, Loss: 184.0547637939453\n",
      "Epoch 62, Batch 4228, Loss: 178.18707275390625\n",
      "Epoch 62, Batch 4229, Loss: 170.19215393066406\n",
      "Epoch 62, Batch 4230, Loss: 169.7552490234375\n",
      "Epoch 62, Batch 4231, Loss: 149.443115234375\n",
      "Epoch 62, Batch 4232, Loss: 157.99630737304688\n",
      "Epoch 62, Batch 4233, Loss: 177.75912475585938\n",
      "Epoch 62, Batch 4234, Loss: 164.71620178222656\n",
      "Epoch 62, Batch 4235, Loss: 168.21315002441406\n",
      "Epoch 62, Batch 4236, Loss: 167.56932067871094\n",
      "Epoch 62, Batch 4237, Loss: 173.17929077148438\n",
      "Epoch 62, Batch 4238, Loss: 191.7893524169922\n",
      "Epoch 62, Batch 4239, Loss: 166.38018798828125\n",
      "Epoch 62, Batch 4240, Loss: 185.87498474121094\n",
      "Epoch 62, Batch 4241, Loss: 190.78981018066406\n",
      "Epoch 62, Batch 4242, Loss: 178.6682586669922\n",
      "Epoch 62, Batch 4243, Loss: 171.22512817382812\n",
      "Epoch 62, Batch 4244, Loss: 171.95237731933594\n",
      "Epoch 62, Batch 4245, Loss: 168.3928985595703\n",
      "Epoch 62, Batch 4246, Loss: 157.55926513671875\n",
      "Epoch 62, Batch 4247, Loss: 167.52993774414062\n",
      "Epoch 62, Batch 4248, Loss: 161.5873260498047\n",
      "Epoch 62, Batch 4249, Loss: 173.6627197265625\n",
      "Epoch 62, Batch 4250, Loss: 182.51878356933594\n",
      "Epoch 62, Batch 4251, Loss: 158.66624450683594\n",
      "Epoch 62, Batch 4252, Loss: 170.62844848632812\n",
      "Epoch 62, Batch 4253, Loss: 175.5980987548828\n",
      "Epoch 62, Batch 4254, Loss: 175.4078826904297\n",
      "Epoch 62, Batch 4255, Loss: 183.074462890625\n",
      "Epoch 62, Batch 4256, Loss: 173.61279296875\n",
      "Epoch 62, Batch 4257, Loss: 160.26638793945312\n",
      "Epoch 62, Batch 4258, Loss: 166.09332275390625\n",
      "Epoch 62, Batch 4259, Loss: 156.87698364257812\n",
      "Epoch 62, Batch 4260, Loss: 177.6155242919922\n",
      "Epoch 62, Batch 4261, Loss: 153.65530395507812\n",
      "Epoch 62, Batch 4262, Loss: 165.39366149902344\n",
      "Epoch 62, Batch 4263, Loss: 158.47373962402344\n",
      "Epoch 62, Batch 4264, Loss: 198.5549774169922\n",
      "Epoch 62, Batch 4265, Loss: 160.36077880859375\n",
      "Epoch 62, Batch 4266, Loss: 169.62518310546875\n",
      "Epoch 62, Batch 4267, Loss: 168.59864807128906\n",
      "Epoch 62, Batch 4268, Loss: 173.2584228515625\n",
      "Epoch 62, Batch 4269, Loss: 176.6322784423828\n",
      "Epoch 62, Batch 4270, Loss: 163.753173828125\n",
      "Epoch 62, Batch 4271, Loss: 179.36422729492188\n",
      "Epoch 62, Batch 4272, Loss: 188.04847717285156\n",
      "Epoch 62, Batch 4273, Loss: 186.4099884033203\n",
      "Epoch 62, Batch 4274, Loss: 171.29168701171875\n",
      "Epoch 62, Batch 4275, Loss: 167.1674041748047\n",
      "Epoch 62, Batch 4276, Loss: 157.03094482421875\n",
      "Epoch 62, Batch 4277, Loss: 164.66555786132812\n",
      "Epoch 62, Batch 4278, Loss: 181.37701416015625\n",
      "Epoch 62, Batch 4279, Loss: 168.39073181152344\n",
      "Epoch 62, Batch 4280, Loss: 158.2939453125\n",
      "Epoch 62, Batch 4281, Loss: 186.49594116210938\n",
      "Epoch 62, Batch 4282, Loss: 181.4095916748047\n",
      "Epoch 62, Batch 4283, Loss: 165.47872924804688\n",
      "Epoch 62, Batch 4284, Loss: 159.4067840576172\n",
      "Epoch 62, Batch 4285, Loss: 166.6898956298828\n",
      "Epoch 62, Batch 4286, Loss: 163.17379760742188\n",
      "Epoch 62, Batch 4287, Loss: 176.0377655029297\n",
      "Epoch 62, Batch 4288, Loss: 172.7324676513672\n",
      "Epoch 62, Batch 4289, Loss: 163.44093322753906\n",
      "Epoch 62, Batch 4290, Loss: 177.3541259765625\n",
      "Epoch 62, Batch 4291, Loss: 181.42198181152344\n",
      "Epoch 62, Batch 4292, Loss: 188.51077270507812\n",
      "Epoch 62, Batch 4293, Loss: 176.5845184326172\n",
      "Epoch 62, Batch 4294, Loss: 178.17977905273438\n",
      "Epoch 62, Batch 4295, Loss: 171.32269287109375\n",
      "Epoch 62, Batch 4296, Loss: 168.27798461914062\n",
      "Epoch 62, Batch 4297, Loss: 166.28131103515625\n",
      "Epoch 62, Batch 4298, Loss: 172.48619079589844\n",
      "Epoch 62, Batch 4299, Loss: 185.87948608398438\n",
      "Epoch 62, Batch 4300, Loss: 168.39541625976562\n",
      "Epoch 62, Batch 4301, Loss: 172.59547424316406\n",
      "Epoch 62, Batch 4302, Loss: 156.33285522460938\n",
      "Epoch 62, Batch 4303, Loss: 184.7462615966797\n",
      "Epoch 62, Batch 4304, Loss: 164.52044677734375\n",
      "Epoch 62, Batch 4305, Loss: 180.67552185058594\n",
      "Epoch 62, Batch 4306, Loss: 166.09190368652344\n",
      "Epoch 62, Batch 4307, Loss: 180.15225219726562\n",
      "Epoch 62, Batch 4308, Loss: 185.85289001464844\n",
      "Epoch 62, Batch 4309, Loss: 173.81190490722656\n",
      "Epoch 62, Batch 4310, Loss: 180.04054260253906\n",
      "Epoch 62, Batch 4311, Loss: 171.87258911132812\n",
      "Epoch 62, Batch 4312, Loss: 182.2724151611328\n",
      "Epoch 62, Batch 4313, Loss: 169.2111053466797\n",
      "Epoch 62, Batch 4314, Loss: 174.89190673828125\n",
      "Epoch 62, Batch 4315, Loss: 181.6844940185547\n",
      "Epoch 62, Batch 4316, Loss: 153.92825317382812\n",
      "Epoch 62, Batch 4317, Loss: 166.67160034179688\n",
      "Epoch 62, Batch 4318, Loss: 167.58387756347656\n",
      "Epoch 62, Batch 4319, Loss: 174.6121063232422\n",
      "Epoch 62, Batch 4320, Loss: 168.9742889404297\n",
      "Epoch 62, Batch 4321, Loss: 176.94851684570312\n",
      "Epoch 62, Batch 4322, Loss: 187.2515106201172\n",
      "Epoch 62, Batch 4323, Loss: 162.63356018066406\n",
      "Epoch 62, Batch 4324, Loss: 159.46127319335938\n",
      "Epoch 62, Batch 4325, Loss: 164.66189575195312\n",
      "Epoch 62, Batch 4326, Loss: 172.5127716064453\n",
      "Epoch 62, Batch 4327, Loss: 174.33456420898438\n",
      "Epoch 62, Batch 4328, Loss: 183.3550262451172\n",
      "Epoch 62, Batch 4329, Loss: 172.11936950683594\n",
      "Epoch 62, Batch 4330, Loss: 166.1361846923828\n",
      "Epoch 62, Batch 4331, Loss: 179.78521728515625\n",
      "Epoch 62, Batch 4332, Loss: 166.6086883544922\n",
      "Epoch 62, Batch 4333, Loss: 179.63990783691406\n",
      "Epoch 62, Batch 4334, Loss: 188.96311950683594\n",
      "Epoch 62, Batch 4335, Loss: 181.1334228515625\n",
      "Epoch 62, Batch 4336, Loss: 169.63272094726562\n",
      "Epoch 62, Batch 4337, Loss: 184.1814422607422\n",
      "Epoch 62, Batch 4338, Loss: 190.55010986328125\n",
      "Epoch 62, Batch 4339, Loss: 158.1185760498047\n",
      "Epoch 62, Batch 4340, Loss: 184.96995544433594\n",
      "Epoch 62, Batch 4341, Loss: 184.9094696044922\n",
      "Epoch 62, Batch 4342, Loss: 165.5810089111328\n",
      "Epoch 62, Batch 4343, Loss: 171.21255493164062\n",
      "Epoch 62, Batch 4344, Loss: 166.67234802246094\n",
      "Epoch 62, Batch 4345, Loss: 181.9456024169922\n",
      "Epoch 62, Batch 4346, Loss: 163.11761474609375\n",
      "Epoch 62, Batch 4347, Loss: 186.37998962402344\n",
      "Epoch 62, Batch 4348, Loss: 174.0359344482422\n",
      "Epoch 62, Batch 4349, Loss: 178.96829223632812\n",
      "Epoch 62, Batch 4350, Loss: 185.40061950683594\n",
      "Epoch 62, Batch 4351, Loss: 186.84103393554688\n",
      "Epoch 62, Batch 4352, Loss: 186.25950622558594\n",
      "Epoch 62, Batch 4353, Loss: 183.83529663085938\n",
      "Epoch 62, Batch 4354, Loss: 166.53590393066406\n",
      "Epoch 62, Batch 4355, Loss: 185.6488037109375\n",
      "Epoch 62, Batch 4356, Loss: 169.3446044921875\n",
      "Epoch 62, Batch 4357, Loss: 186.9385986328125\n",
      "Epoch 62, Batch 4358, Loss: 188.1967315673828\n",
      "Epoch 62, Batch 4359, Loss: 168.78175354003906\n",
      "Epoch 62, Batch 4360, Loss: 183.98818969726562\n",
      "Epoch 62, Batch 4361, Loss: 177.86044311523438\n",
      "Epoch 62, Batch 4362, Loss: 197.65927124023438\n",
      "Epoch 62, Batch 4363, Loss: 158.3726348876953\n",
      "Epoch 62, Batch 4364, Loss: 186.7712860107422\n",
      "Epoch 62, Batch 4365, Loss: 190.8165740966797\n",
      "Epoch 62, Batch 4366, Loss: 167.4175567626953\n",
      "Epoch 62, Batch 4367, Loss: 175.00277709960938\n",
      "Epoch 62, Batch 4368, Loss: 162.7289581298828\n",
      "Epoch 62, Batch 4369, Loss: 156.6359100341797\n",
      "Epoch 62, Batch 4370, Loss: 168.6326141357422\n",
      "Epoch 62, Batch 4371, Loss: 174.61358642578125\n",
      "Epoch 62, Batch 4372, Loss: 166.63314819335938\n",
      "Epoch 62, Batch 4373, Loss: 167.64471435546875\n",
      "Epoch 62, Batch 4374, Loss: 148.73919677734375\n",
      "Epoch 62, Batch 4375, Loss: 170.072265625\n",
      "Epoch 62, Batch 4376, Loss: 178.09469604492188\n",
      "Epoch 62, Batch 4377, Loss: 190.04635620117188\n",
      "Epoch 62, Batch 4378, Loss: 173.0958709716797\n",
      "Epoch 62, Batch 4379, Loss: 170.18284606933594\n",
      "Epoch 62, Batch 4380, Loss: 165.7870330810547\n",
      "Epoch 62, Batch 4381, Loss: 168.812255859375\n",
      "Epoch 62, Batch 4382, Loss: 185.16065979003906\n",
      "Epoch 62, Batch 4383, Loss: 179.21307373046875\n",
      "Epoch 62, Batch 4384, Loss: 180.11305236816406\n",
      "Epoch 62, Batch 4385, Loss: 173.01263427734375\n",
      "Epoch 62, Batch 4386, Loss: 182.94187927246094\n",
      "Epoch 62, Batch 4387, Loss: 178.77346801757812\n",
      "Epoch 62, Batch 4388, Loss: 156.9266357421875\n",
      "Epoch 62, Batch 4389, Loss: 172.33929443359375\n",
      "Epoch 62, Batch 4390, Loss: 182.6405487060547\n",
      "Epoch 62, Batch 4391, Loss: 155.0491180419922\n",
      "Epoch 62, Batch 4392, Loss: 160.33267211914062\n",
      "Epoch 62, Batch 4393, Loss: 180.5545654296875\n",
      "Epoch 62, Batch 4394, Loss: 169.0886993408203\n",
      "Epoch 62, Batch 4395, Loss: 164.38510131835938\n",
      "Epoch 62, Batch 4396, Loss: 164.1121063232422\n",
      "Epoch 62, Batch 4397, Loss: 161.80999755859375\n",
      "Epoch 62, Batch 4398, Loss: 166.99237060546875\n",
      "Epoch 62, Batch 4399, Loss: 155.35581970214844\n",
      "Epoch 62, Batch 4400, Loss: 191.2962188720703\n",
      "Epoch 62, Batch 4401, Loss: 158.9091796875\n",
      "Epoch 62, Batch 4402, Loss: 196.51315307617188\n",
      "Epoch 62, Batch 4403, Loss: 169.59945678710938\n",
      "Epoch 62, Batch 4404, Loss: 178.17593383789062\n",
      "Epoch 62, Batch 4405, Loss: 168.749267578125\n",
      "Epoch 62, Batch 4406, Loss: 161.79904174804688\n",
      "Epoch 62, Batch 4407, Loss: 180.34957885742188\n",
      "Epoch 62, Batch 4408, Loss: 186.6798858642578\n",
      "Epoch 62, Batch 4409, Loss: 164.28221130371094\n",
      "Epoch 62, Batch 4410, Loss: 165.08132934570312\n",
      "Epoch 62, Batch 4411, Loss: 157.8518524169922\n",
      "Epoch 62, Batch 4412, Loss: 174.82937622070312\n",
      "Epoch 62, Batch 4413, Loss: 178.6728515625\n",
      "Epoch 62, Batch 4414, Loss: 182.9801483154297\n",
      "Epoch 62, Batch 4415, Loss: 176.59396362304688\n",
      "Epoch 62, Batch 4416, Loss: 159.66334533691406\n",
      "Epoch 62, Batch 4417, Loss: 177.3793487548828\n",
      "Epoch 62, Batch 4418, Loss: 171.62600708007812\n",
      "Epoch 62, Batch 4419, Loss: 176.18743896484375\n",
      "Epoch 62, Batch 4420, Loss: 160.8992462158203\n",
      "Epoch 62, Batch 4421, Loss: 160.6604766845703\n",
      "Epoch 62, Batch 4422, Loss: 170.22019958496094\n",
      "Epoch 62, Batch 4423, Loss: 168.12466430664062\n",
      "Epoch 62, Batch 4424, Loss: 166.89283752441406\n",
      "Epoch 62, Batch 4425, Loss: 146.34519958496094\n",
      "Epoch 62, Batch 4426, Loss: 165.7817840576172\n",
      "Epoch 62, Batch 4427, Loss: 157.3642578125\n",
      "Epoch 62, Batch 4428, Loss: 170.60113525390625\n",
      "Epoch 62, Batch 4429, Loss: 177.57479858398438\n",
      "Epoch 62, Batch 4430, Loss: 191.94223022460938\n",
      "Epoch 62, Batch 4431, Loss: 169.12098693847656\n",
      "Epoch 62, Batch 4432, Loss: 188.4710693359375\n",
      "Epoch 62, Batch 4433, Loss: 182.67030334472656\n",
      "Epoch 62, Batch 4434, Loss: 174.50271606445312\n",
      "Epoch 62, Batch 4435, Loss: 174.11358642578125\n",
      "Epoch 62, Batch 4436, Loss: 175.14813232421875\n",
      "Epoch 62, Batch 4437, Loss: 174.19056701660156\n",
      "Epoch 62, Batch 4438, Loss: 170.1654510498047\n",
      "Epoch 62, Batch 4439, Loss: 157.8245086669922\n",
      "Epoch 62, Batch 4440, Loss: 173.6370086669922\n",
      "Epoch 62, Batch 4441, Loss: 186.26715087890625\n",
      "Epoch 62, Batch 4442, Loss: 158.466064453125\n",
      "Epoch 62, Batch 4443, Loss: 186.218505859375\n",
      "Epoch 62, Batch 4444, Loss: 195.57876586914062\n",
      "Epoch 62, Batch 4445, Loss: 169.99935913085938\n",
      "Epoch 62, Batch 4446, Loss: 171.33602905273438\n",
      "Epoch 62, Batch 4447, Loss: 187.60696411132812\n",
      "Epoch 62, Batch 4448, Loss: 195.49757385253906\n",
      "Epoch 62, Batch 4449, Loss: 193.81600952148438\n",
      "Epoch 62, Batch 4450, Loss: 171.58682250976562\n",
      "Epoch 62, Batch 4451, Loss: 172.98605346679688\n",
      "Epoch 62, Batch 4452, Loss: 176.89581298828125\n",
      "Epoch 62, Batch 4453, Loss: 176.13893127441406\n",
      "Epoch 62, Batch 4454, Loss: 177.18667602539062\n",
      "Epoch 62, Batch 4455, Loss: 171.69775390625\n",
      "Epoch 62, Batch 4456, Loss: 179.46939086914062\n",
      "Epoch 62, Batch 4457, Loss: 170.49607849121094\n",
      "Epoch 62, Batch 4458, Loss: 172.00526428222656\n",
      "Epoch 62, Batch 4459, Loss: 170.18704223632812\n",
      "Epoch 62, Batch 4460, Loss: 189.26707458496094\n",
      "Epoch 62, Batch 4461, Loss: 170.62257385253906\n",
      "Epoch 62, Batch 4462, Loss: 189.17291259765625\n",
      "Epoch 62, Batch 4463, Loss: 177.70718383789062\n",
      "Epoch 62, Batch 4464, Loss: 159.48385620117188\n",
      "Epoch 62, Batch 4465, Loss: 162.69593811035156\n",
      "Epoch 62, Batch 4466, Loss: 182.06793212890625\n",
      "Epoch 62, Batch 4467, Loss: 163.5177001953125\n",
      "Epoch 62, Batch 4468, Loss: 186.0425262451172\n",
      "Epoch 62, Batch 4469, Loss: 188.6241455078125\n",
      "Epoch 62, Batch 4470, Loss: 194.58578491210938\n",
      "Epoch 62, Batch 4471, Loss: 174.6101837158203\n",
      "Epoch 62, Batch 4472, Loss: 181.10719299316406\n",
      "Epoch 62, Batch 4473, Loss: 178.0182647705078\n",
      "Epoch 62, Batch 4474, Loss: 181.16131591796875\n",
      "Epoch 62, Batch 4475, Loss: 169.80857849121094\n",
      "Epoch 62, Batch 4476, Loss: 180.88088989257812\n",
      "Epoch 62, Batch 4477, Loss: 171.15573120117188\n",
      "Epoch 62, Batch 4478, Loss: 162.49237060546875\n",
      "Epoch 62, Batch 4479, Loss: 167.81011962890625\n",
      "Epoch 62, Batch 4480, Loss: 198.20358276367188\n",
      "Epoch 62, Batch 4481, Loss: 178.0788116455078\n",
      "Epoch 62, Batch 4482, Loss: 174.79156494140625\n",
      "Epoch 62, Batch 4483, Loss: 160.30332946777344\n",
      "Epoch 62, Batch 4484, Loss: 169.328857421875\n",
      "Epoch 62, Batch 4485, Loss: 194.97337341308594\n",
      "Epoch 62, Batch 4486, Loss: 156.48275756835938\n",
      "Epoch 62, Batch 4487, Loss: 170.52996826171875\n",
      "Epoch 62, Batch 4488, Loss: 159.13352966308594\n",
      "Epoch 62, Batch 4489, Loss: 160.07296752929688\n",
      "Epoch 62, Batch 4490, Loss: 173.34791564941406\n",
      "Epoch 62, Batch 4491, Loss: 173.68576049804688\n",
      "Epoch 62, Batch 4492, Loss: 176.79757690429688\n",
      "Epoch 62, Batch 4493, Loss: 162.82540893554688\n",
      "Epoch 62, Batch 4494, Loss: 182.53199768066406\n",
      "Epoch 62, Batch 4495, Loss: 177.623046875\n",
      "Epoch 62, Batch 4496, Loss: 177.22108459472656\n",
      "Epoch 62, Batch 4497, Loss: 158.86512756347656\n",
      "Epoch 62, Batch 4498, Loss: 186.24063110351562\n",
      "Epoch 62, Batch 4499, Loss: 184.10240173339844\n",
      "Epoch 62, Batch 4500, Loss: 175.98513793945312\n",
      "Epoch 62, Batch 4501, Loss: 184.45208740234375\n",
      "Epoch 62, Batch 4502, Loss: 177.33558654785156\n",
      "Epoch 62, Batch 4503, Loss: 165.81581115722656\n",
      "Epoch 62, Batch 4504, Loss: 160.11505126953125\n",
      "Epoch 62, Batch 4505, Loss: 187.6041259765625\n",
      "Epoch 62, Batch 4506, Loss: 172.04612731933594\n",
      "Epoch 62, Batch 4507, Loss: 186.4714813232422\n",
      "Epoch 62, Batch 4508, Loss: 196.7578582763672\n",
      "Epoch 62, Batch 4509, Loss: 164.662353515625\n",
      "Epoch 62, Batch 4510, Loss: 157.92633056640625\n",
      "Epoch 62, Batch 4511, Loss: 167.56573486328125\n",
      "Epoch 62, Batch 4512, Loss: 165.25631713867188\n",
      "Epoch 62, Batch 4513, Loss: 177.77554321289062\n",
      "Epoch 62, Batch 4514, Loss: 169.16867065429688\n",
      "Epoch 62, Batch 4515, Loss: 183.710693359375\n",
      "Epoch 62, Batch 4516, Loss: 180.4490203857422\n",
      "Epoch 62, Batch 4517, Loss: 176.3108367919922\n",
      "Epoch 62, Batch 4518, Loss: 171.7130889892578\n",
      "Epoch 62, Batch 4519, Loss: 152.19277954101562\n",
      "Epoch 62, Batch 4520, Loss: 168.1866455078125\n",
      "Epoch 62, Batch 4521, Loss: 190.55233764648438\n",
      "Epoch 62, Batch 4522, Loss: 162.51792907714844\n",
      "Epoch 62, Batch 4523, Loss: 177.03953552246094\n",
      "Epoch 62, Batch 4524, Loss: 158.61776733398438\n",
      "Epoch 62, Batch 4525, Loss: 174.50335693359375\n",
      "Epoch 62, Batch 4526, Loss: 170.6680450439453\n",
      "Epoch 62, Batch 4527, Loss: 179.40863037109375\n",
      "Epoch 62, Batch 4528, Loss: 160.32174682617188\n",
      "Epoch 62, Batch 4529, Loss: 163.8351593017578\n",
      "Epoch 62, Batch 4530, Loss: 173.85772705078125\n",
      "Epoch 62, Batch 4531, Loss: 163.1628875732422\n",
      "Epoch 62, Batch 4532, Loss: 168.90097045898438\n",
      "Epoch 62, Batch 4533, Loss: 161.3586883544922\n",
      "Epoch 62, Batch 4534, Loss: 170.0984344482422\n",
      "Epoch 62, Batch 4535, Loss: 175.84353637695312\n",
      "Epoch 62, Batch 4536, Loss: 165.1174774169922\n",
      "Epoch 62, Batch 4537, Loss: 166.4325714111328\n",
      "Epoch 62, Batch 4538, Loss: 161.33984375\n",
      "Epoch 62, Batch 4539, Loss: 180.48313903808594\n",
      "Epoch 62, Batch 4540, Loss: 167.14369201660156\n",
      "Epoch 62, Batch 4541, Loss: 173.8330535888672\n",
      "Epoch 62, Batch 4542, Loss: 162.8399658203125\n",
      "Epoch 62, Batch 4543, Loss: 160.7378387451172\n",
      "Epoch 62, Batch 4544, Loss: 153.52294921875\n",
      "Epoch 62, Batch 4545, Loss: 170.61595153808594\n",
      "Epoch 62, Batch 4546, Loss: 170.76759338378906\n",
      "Epoch 62, Batch 4547, Loss: 165.80435180664062\n",
      "Epoch 62, Batch 4548, Loss: 169.78460693359375\n",
      "Epoch 62, Batch 4549, Loss: 168.61505126953125\n",
      "Epoch 62, Batch 4550, Loss: 162.82325744628906\n",
      "Epoch 62, Batch 4551, Loss: 186.88232421875\n",
      "Epoch 62, Batch 4552, Loss: 192.58766174316406\n",
      "Epoch 62, Batch 4553, Loss: 164.8660430908203\n",
      "Epoch 62, Batch 4554, Loss: 180.95225524902344\n",
      "Epoch 62, Batch 4555, Loss: 187.93231201171875\n",
      "Epoch 62, Batch 4556, Loss: 180.97142028808594\n",
      "Epoch 62, Batch 4557, Loss: 168.7570037841797\n",
      "Epoch 62, Batch 4558, Loss: 175.2344512939453\n",
      "Epoch 62, Batch 4559, Loss: 186.14517211914062\n",
      "Epoch 62, Batch 4560, Loss: 156.17987060546875\n",
      "Epoch 62, Batch 4561, Loss: 160.4213409423828\n",
      "Epoch 62, Batch 4562, Loss: 179.54759216308594\n",
      "Epoch 62, Batch 4563, Loss: 149.03475952148438\n",
      "Epoch 62, Batch 4564, Loss: 190.64427185058594\n",
      "Epoch 62, Batch 4565, Loss: 165.55209350585938\n",
      "Epoch 62, Batch 4566, Loss: 169.29737854003906\n",
      "Epoch 62, Batch 4567, Loss: 158.8839874267578\n",
      "Epoch 62, Batch 4568, Loss: 182.09095764160156\n",
      "Epoch 62, Batch 4569, Loss: 176.65927124023438\n",
      "Epoch 62, Batch 4570, Loss: 193.2619171142578\n",
      "Epoch 62, Batch 4571, Loss: 172.73870849609375\n",
      "Epoch 62, Batch 4572, Loss: 176.5032958984375\n",
      "Epoch 62, Batch 4573, Loss: 172.51918029785156\n",
      "Epoch 62, Batch 4574, Loss: 174.95848083496094\n",
      "Epoch 62, Batch 4575, Loss: 179.9171600341797\n",
      "Epoch 62, Batch 4576, Loss: 170.23309326171875\n",
      "Epoch 62, Batch 4577, Loss: 152.38723754882812\n",
      "Epoch 62, Batch 4578, Loss: 159.2406768798828\n",
      "Epoch 62, Batch 4579, Loss: 172.07931518554688\n",
      "Epoch 62, Batch 4580, Loss: 160.09226989746094\n",
      "Epoch 62, Batch 4581, Loss: 166.81594848632812\n",
      "Epoch 62, Batch 4582, Loss: 175.76112365722656\n",
      "Epoch 62, Batch 4583, Loss: 177.8189697265625\n",
      "Epoch 62, Batch 4584, Loss: 172.56227111816406\n",
      "Epoch 62, Batch 4585, Loss: 173.45907592773438\n",
      "Epoch 62, Batch 4586, Loss: 179.06004333496094\n",
      "Epoch 62, Batch 4587, Loss: 165.13214111328125\n",
      "Epoch 62, Batch 4588, Loss: 175.55323791503906\n",
      "Epoch 62, Batch 4589, Loss: 175.6857452392578\n",
      "Epoch 62, Batch 4590, Loss: 162.99453735351562\n",
      "Epoch 62, Batch 4591, Loss: 175.5500946044922\n",
      "Epoch 62, Batch 4592, Loss: 174.4754638671875\n",
      "Epoch 62, Batch 4593, Loss: 168.4359893798828\n",
      "Epoch 62, Batch 4594, Loss: 159.02723693847656\n",
      "Epoch 62, Batch 4595, Loss: 170.9735565185547\n",
      "Epoch 62, Batch 4596, Loss: 172.5419921875\n",
      "Epoch 62, Batch 4597, Loss: 169.12063598632812\n",
      "Epoch 62, Batch 4598, Loss: 194.8392333984375\n",
      "Epoch 62, Batch 4599, Loss: 175.43165588378906\n",
      "Epoch 62, Batch 4600, Loss: 184.9654083251953\n",
      "Epoch 62, Batch 4601, Loss: 178.27285766601562\n",
      "Epoch 62, Batch 4602, Loss: 160.0330352783203\n",
      "Epoch 62, Batch 4603, Loss: 176.444580078125\n",
      "Epoch 62, Batch 4604, Loss: 179.12660217285156\n",
      "Epoch 62, Batch 4605, Loss: 174.4665985107422\n",
      "Epoch 62, Batch 4606, Loss: 186.36004638671875\n",
      "Epoch 62, Batch 4607, Loss: 178.92999267578125\n",
      "Epoch 62, Batch 4608, Loss: 180.1385955810547\n",
      "Epoch 62, Batch 4609, Loss: 179.32968139648438\n",
      "Epoch 62, Batch 4610, Loss: 176.3408660888672\n",
      "Epoch 62, Batch 4611, Loss: 163.3777618408203\n",
      "Epoch 62, Batch 4612, Loss: 150.1497039794922\n",
      "Epoch 62, Batch 4613, Loss: 175.17678833007812\n",
      "Epoch 62, Batch 4614, Loss: 182.36953735351562\n",
      "Epoch 62, Batch 4615, Loss: 167.6961212158203\n",
      "Epoch 62, Batch 4616, Loss: 182.88172912597656\n",
      "Epoch 62, Batch 4617, Loss: 174.40487670898438\n",
      "Epoch 62, Batch 4618, Loss: 172.3895263671875\n",
      "Epoch 62, Batch 4619, Loss: 174.4327850341797\n",
      "Epoch 62, Batch 4620, Loss: 180.3546142578125\n",
      "Epoch 62, Batch 4621, Loss: 186.1073760986328\n",
      "Epoch 62, Batch 4622, Loss: 178.5001220703125\n",
      "Epoch 62, Batch 4623, Loss: 188.8867950439453\n",
      "Epoch 62, Batch 4624, Loss: 167.19281005859375\n",
      "Epoch 62, Batch 4625, Loss: 159.13909912109375\n",
      "Epoch 62, Batch 4626, Loss: 176.88063049316406\n",
      "Epoch 62, Batch 4627, Loss: 165.86927795410156\n",
      "Epoch 62, Batch 4628, Loss: 175.79844665527344\n",
      "Epoch 62, Batch 4629, Loss: 180.2642822265625\n",
      "Epoch 62, Batch 4630, Loss: 163.64100646972656\n",
      "Epoch 62, Batch 4631, Loss: 160.65914916992188\n",
      "Epoch 62, Batch 4632, Loss: 175.89602661132812\n",
      "Epoch 62, Batch 4633, Loss: 178.2958221435547\n",
      "Epoch 62, Batch 4634, Loss: 182.22332763671875\n",
      "Epoch 62, Batch 4635, Loss: 173.27365112304688\n",
      "Epoch 62, Batch 4636, Loss: 185.24246215820312\n",
      "Epoch 62, Batch 4637, Loss: 180.0025177001953\n",
      "Epoch 62, Batch 4638, Loss: 165.29501342773438\n",
      "Epoch 62, Batch 4639, Loss: 167.0107879638672\n",
      "Epoch 62, Batch 4640, Loss: 177.07363891601562\n",
      "Epoch 62, Batch 4641, Loss: 181.62905883789062\n",
      "Epoch 62, Batch 4642, Loss: 184.8848876953125\n",
      "Epoch 62, Batch 4643, Loss: 178.26109313964844\n",
      "Epoch 62, Batch 4644, Loss: 184.9646759033203\n",
      "Epoch 62, Batch 4645, Loss: 161.9945526123047\n",
      "Epoch 62, Batch 4646, Loss: 167.7762908935547\n",
      "Epoch 62, Batch 4647, Loss: 184.9775848388672\n",
      "Epoch 62, Batch 4648, Loss: 170.3135528564453\n",
      "Epoch 62, Batch 4649, Loss: 151.98097229003906\n",
      "Epoch 62, Batch 4650, Loss: 146.32823181152344\n",
      "Epoch 62, Batch 4651, Loss: 167.62405395507812\n",
      "Epoch 62, Batch 4652, Loss: 158.0614013671875\n",
      "Epoch 62, Batch 4653, Loss: 178.89991760253906\n",
      "Epoch 62, Batch 4654, Loss: 175.27430725097656\n",
      "Epoch 62, Batch 4655, Loss: 203.18634033203125\n",
      "Epoch 62, Batch 4656, Loss: 180.7358856201172\n",
      "Epoch 62, Batch 4657, Loss: 171.689697265625\n",
      "Epoch 62, Batch 4658, Loss: 164.0630340576172\n",
      "Epoch 62, Batch 4659, Loss: 177.6126251220703\n",
      "Epoch 62, Batch 4660, Loss: 172.2923583984375\n",
      "Epoch 62, Batch 4661, Loss: 176.1356964111328\n",
      "Epoch 62, Batch 4662, Loss: 154.69374084472656\n",
      "Epoch 62, Batch 4663, Loss: 175.5190887451172\n",
      "Epoch 62, Batch 4664, Loss: 166.53927612304688\n",
      "Epoch 62, Batch 4665, Loss: 172.11326599121094\n",
      "Epoch 62, Batch 4666, Loss: 175.89039611816406\n",
      "Epoch 62, Batch 4667, Loss: 163.44541931152344\n",
      "Epoch 62, Batch 4668, Loss: 166.15171813964844\n",
      "Epoch 62, Batch 4669, Loss: 166.79443359375\n",
      "Epoch 62, Batch 4670, Loss: 183.36370849609375\n",
      "Epoch 62, Batch 4671, Loss: 168.07139587402344\n",
      "Epoch 62, Batch 4672, Loss: 177.842529296875\n",
      "Epoch 62, Batch 4673, Loss: 176.00320434570312\n",
      "Epoch 62, Batch 4674, Loss: 186.948974609375\n",
      "Epoch 62, Batch 4675, Loss: 165.86477661132812\n",
      "Epoch 62, Batch 4676, Loss: 171.2576141357422\n",
      "Epoch 62, Batch 4677, Loss: 155.15675354003906\n",
      "Epoch 62, Batch 4678, Loss: 150.15078735351562\n",
      "Epoch 62, Batch 4679, Loss: 183.12799072265625\n",
      "Epoch 62, Batch 4680, Loss: 189.27767944335938\n",
      "Epoch 62, Batch 4681, Loss: 166.27572631835938\n",
      "Epoch 62, Batch 4682, Loss: 175.8527374267578\n",
      "Epoch 62, Batch 4683, Loss: 153.62741088867188\n",
      "Epoch 62, Batch 4684, Loss: 166.48977661132812\n",
      "Epoch 62, Batch 4685, Loss: 166.01893615722656\n",
      "Epoch 62, Batch 4686, Loss: 173.9200439453125\n",
      "Epoch 62, Batch 4687, Loss: 167.48855590820312\n",
      "Epoch 62, Batch 4688, Loss: 173.88624572753906\n",
      "Epoch 62, Batch 4689, Loss: 180.02029418945312\n",
      "Epoch 62, Batch 4690, Loss: 178.5989532470703\n",
      "Epoch 62, Batch 4691, Loss: 159.53248596191406\n",
      "Epoch 62, Batch 4692, Loss: 167.39669799804688\n",
      "Epoch 62, Batch 4693, Loss: 177.84815979003906\n",
      "Epoch 62, Batch 4694, Loss: 174.759765625\n",
      "Epoch 62, Batch 4695, Loss: 177.19960021972656\n",
      "Epoch 62, Batch 4696, Loss: 190.2440948486328\n",
      "Epoch 62, Batch 4697, Loss: 170.5003204345703\n",
      "Epoch 62, Batch 4698, Loss: 171.58560180664062\n",
      "Epoch 62, Batch 4699, Loss: 178.48556518554688\n",
      "Epoch 62, Batch 4700, Loss: 169.45872497558594\n",
      "Epoch 62, Batch 4701, Loss: 165.81480407714844\n",
      "Epoch 62, Batch 4702, Loss: 180.54554748535156\n",
      "Epoch 62, Batch 4703, Loss: 172.84410095214844\n",
      "Epoch 62, Batch 4704, Loss: 175.37863159179688\n",
      "Epoch 62, Batch 4705, Loss: 174.3179168701172\n",
      "Epoch 62, Batch 4706, Loss: 170.01321411132812\n",
      "Epoch 62, Batch 4707, Loss: 170.64207458496094\n",
      "Epoch 62, Batch 4708, Loss: 186.5793914794922\n",
      "Epoch 62, Batch 4709, Loss: 168.09339904785156\n",
      "Epoch 62, Batch 4710, Loss: 173.4037322998047\n",
      "Epoch 62, Batch 4711, Loss: 160.55123901367188\n",
      "Epoch 62, Batch 4712, Loss: 176.42625427246094\n",
      "Epoch 62, Batch 4713, Loss: 165.65878295898438\n",
      "Epoch 62, Batch 4714, Loss: 186.54730224609375\n",
      "Epoch 62, Batch 4715, Loss: 173.62547302246094\n",
      "Epoch 62, Batch 4716, Loss: 182.21429443359375\n",
      "Epoch 62, Batch 4717, Loss: 167.99461364746094\n",
      "Epoch 62, Batch 4718, Loss: 176.31033325195312\n",
      "Epoch 62, Batch 4719, Loss: 177.7380828857422\n",
      "Epoch 62, Batch 4720, Loss: 174.81101989746094\n",
      "Epoch 62, Batch 4721, Loss: 164.80886840820312\n",
      "Epoch 62, Batch 4722, Loss: 165.60093688964844\n",
      "Epoch 62, Batch 4723, Loss: 177.3928680419922\n",
      "Epoch 62, Batch 4724, Loss: 175.0642547607422\n",
      "Epoch 62, Batch 4725, Loss: 175.1961669921875\n",
      "Epoch 62, Batch 4726, Loss: 169.88063049316406\n",
      "Epoch 62, Batch 4727, Loss: 203.0309295654297\n",
      "Epoch 62, Batch 4728, Loss: 187.61595153808594\n",
      "Epoch 62, Batch 4729, Loss: 168.61903381347656\n",
      "Epoch 62, Batch 4730, Loss: 184.83441162109375\n",
      "Epoch 62, Batch 4731, Loss: 177.38726806640625\n",
      "Epoch 62, Batch 4732, Loss: 171.11611938476562\n",
      "Epoch 62, Batch 4733, Loss: 184.8583221435547\n",
      "Epoch 62, Batch 4734, Loss: 178.38230895996094\n",
      "Epoch 62, Batch 4735, Loss: 160.65769958496094\n",
      "Epoch 62, Batch 4736, Loss: 166.9442138671875\n",
      "Epoch 62, Batch 4737, Loss: 175.8523712158203\n",
      "Epoch 62, Batch 4738, Loss: 171.60421752929688\n",
      "Epoch 62, Batch 4739, Loss: 174.78759765625\n",
      "Epoch 62, Batch 4740, Loss: 169.6279296875\n",
      "Epoch 62, Batch 4741, Loss: 172.49517822265625\n",
      "Epoch 62, Batch 4742, Loss: 173.40499877929688\n",
      "Epoch 62, Batch 4743, Loss: 175.01499938964844\n",
      "Epoch 62, Batch 4744, Loss: 194.03656005859375\n",
      "Epoch 62, Batch 4745, Loss: 179.39602661132812\n",
      "Epoch 62, Batch 4746, Loss: 177.38491821289062\n",
      "Epoch 62, Batch 4747, Loss: 165.72933959960938\n",
      "Epoch 62, Batch 4748, Loss: 170.1322479248047\n",
      "Epoch 62, Batch 4749, Loss: 162.5630645751953\n",
      "Epoch 62, Batch 4750, Loss: 165.15013122558594\n",
      "Epoch 62, Batch 4751, Loss: 159.98324584960938\n",
      "Epoch 62, Batch 4752, Loss: 168.4693145751953\n",
      "Epoch 62, Batch 4753, Loss: 188.1678009033203\n",
      "Epoch 62, Batch 4754, Loss: 166.9769287109375\n",
      "Epoch 62, Batch 4755, Loss: 182.19058227539062\n",
      "Epoch 62, Batch 4756, Loss: 179.39865112304688\n",
      "Epoch 62, Batch 4757, Loss: 181.9143829345703\n",
      "Epoch 62, Batch 4758, Loss: 161.09376525878906\n",
      "Epoch 62, Batch 4759, Loss: 179.9652557373047\n",
      "Epoch 62, Batch 4760, Loss: 171.924560546875\n",
      "Epoch 62, Batch 4761, Loss: 172.93759155273438\n",
      "Epoch 62, Batch 4762, Loss: 169.76214599609375\n",
      "Epoch 62, Batch 4763, Loss: 171.74993896484375\n",
      "Epoch 62, Batch 4764, Loss: 182.9075164794922\n",
      "Epoch 62, Batch 4765, Loss: 155.16278076171875\n",
      "Epoch 62, Batch 4766, Loss: 180.8411865234375\n",
      "Epoch 62, Batch 4767, Loss: 182.1382293701172\n",
      "Epoch 62, Batch 4768, Loss: 181.9879150390625\n",
      "Epoch 62, Batch 4769, Loss: 185.23167419433594\n",
      "Epoch 62, Batch 4770, Loss: 184.57765197753906\n",
      "Epoch 62, Batch 4771, Loss: 171.78318786621094\n",
      "Epoch 62, Batch 4772, Loss: 163.5299072265625\n",
      "Epoch 62, Batch 4773, Loss: 163.01004028320312\n",
      "Epoch 62, Batch 4774, Loss: 156.4097900390625\n",
      "Epoch 62, Batch 4775, Loss: 189.7496337890625\n",
      "Epoch 62, Batch 4776, Loss: 166.6390380859375\n",
      "Epoch 62, Batch 4777, Loss: 171.7819061279297\n",
      "Epoch 62, Batch 4778, Loss: 191.6672821044922\n",
      "Epoch 62, Batch 4779, Loss: 157.14830017089844\n",
      "Epoch 62, Batch 4780, Loss: 184.87696838378906\n",
      "Epoch 62, Batch 4781, Loss: 174.65843200683594\n",
      "Epoch 62, Batch 4782, Loss: 158.70689392089844\n",
      "Epoch 62, Batch 4783, Loss: 167.92733764648438\n",
      "Epoch 62, Batch 4784, Loss: 185.32879638671875\n",
      "Epoch 62, Batch 4785, Loss: 168.84628295898438\n",
      "Epoch 62, Batch 4786, Loss: 163.15176391601562\n",
      "Epoch 62, Batch 4787, Loss: 188.71324157714844\n",
      "Epoch 62, Batch 4788, Loss: 165.13352966308594\n",
      "Epoch 62, Batch 4789, Loss: 173.289794921875\n",
      "Epoch 62, Batch 4790, Loss: 181.84005737304688\n",
      "Epoch 62, Batch 4791, Loss: 191.427001953125\n",
      "Epoch 62, Batch 4792, Loss: 163.92921447753906\n",
      "Epoch 62, Batch 4793, Loss: 159.43881225585938\n",
      "Epoch 62, Batch 4794, Loss: 165.18299865722656\n",
      "Epoch 62, Batch 4795, Loss: 179.45339965820312\n",
      "Epoch 62, Batch 4796, Loss: 165.98159790039062\n",
      "Epoch 62, Batch 4797, Loss: 159.5118865966797\n",
      "Epoch 62, Batch 4798, Loss: 180.4771270751953\n",
      "Epoch 62, Batch 4799, Loss: 174.04649353027344\n",
      "Epoch 62, Batch 4800, Loss: 194.6289520263672\n",
      "Epoch 62, Batch 4801, Loss: 187.03506469726562\n",
      "Epoch 62, Batch 4802, Loss: 165.08409118652344\n",
      "Epoch 62, Batch 4803, Loss: 189.37481689453125\n",
      "Epoch 62, Batch 4804, Loss: 160.0934295654297\n",
      "Epoch 62, Batch 4805, Loss: 179.19444274902344\n",
      "Epoch 62, Batch 4806, Loss: 176.52037048339844\n",
      "Epoch 62, Batch 4807, Loss: 178.37234497070312\n",
      "Epoch 62, Batch 4808, Loss: 172.75006103515625\n",
      "Epoch 62, Batch 4809, Loss: 161.2833251953125\n",
      "Epoch 62, Batch 4810, Loss: 185.25294494628906\n",
      "Epoch 62, Batch 4811, Loss: 190.60862731933594\n",
      "Epoch 62, Batch 4812, Loss: 174.0213623046875\n",
      "Epoch 62, Batch 4813, Loss: 191.67861938476562\n",
      "Epoch 62, Batch 4814, Loss: 165.1298065185547\n",
      "Epoch 62, Batch 4815, Loss: 173.59458923339844\n",
      "Epoch 62, Batch 4816, Loss: 170.47320556640625\n",
      "Epoch 62, Batch 4817, Loss: 192.51751708984375\n",
      "Epoch 62, Batch 4818, Loss: 169.542724609375\n",
      "Epoch 62, Batch 4819, Loss: 161.61842346191406\n",
      "Epoch 62, Batch 4820, Loss: 176.4697723388672\n",
      "Epoch 62, Batch 4821, Loss: 185.78204345703125\n",
      "Epoch 62, Batch 4822, Loss: 169.88980102539062\n",
      "Epoch 62, Batch 4823, Loss: 179.54566955566406\n",
      "Epoch 62, Batch 4824, Loss: 170.13230895996094\n",
      "Epoch 62, Batch 4825, Loss: 167.547607421875\n",
      "Epoch 62, Batch 4826, Loss: 165.3914794921875\n",
      "Epoch 62, Batch 4827, Loss: 168.40548706054688\n",
      "Epoch 62, Batch 4828, Loss: 180.3999481201172\n",
      "Epoch 62, Batch 4829, Loss: 161.53469848632812\n",
      "Epoch 62, Batch 4830, Loss: 172.05517578125\n",
      "Epoch 62, Batch 4831, Loss: 166.2958984375\n",
      "Epoch 62, Batch 4832, Loss: 180.2749786376953\n",
      "Epoch 62, Batch 4833, Loss: 169.10072326660156\n",
      "Epoch 62, Batch 4834, Loss: 176.64735412597656\n",
      "Epoch 62, Batch 4835, Loss: 173.33267211914062\n",
      "Epoch 62, Batch 4836, Loss: 186.70169067382812\n",
      "Epoch 62, Batch 4837, Loss: 181.0908966064453\n",
      "Epoch 62, Batch 4838, Loss: 170.86744689941406\n",
      "Epoch 62, Batch 4839, Loss: 177.2420654296875\n",
      "Epoch 62, Batch 4840, Loss: 160.73680114746094\n",
      "Epoch 62, Batch 4841, Loss: 178.32275390625\n",
      "Epoch 62, Batch 4842, Loss: 168.1682586669922\n",
      "Epoch 62, Batch 4843, Loss: 169.64791870117188\n",
      "Epoch 62, Batch 4844, Loss: 185.6894073486328\n",
      "Epoch 62, Batch 4845, Loss: 173.64418029785156\n",
      "Epoch 62, Batch 4846, Loss: 157.31614685058594\n",
      "Epoch 62, Batch 4847, Loss: 176.26133728027344\n",
      "Epoch 62, Batch 4848, Loss: 157.42608642578125\n",
      "Epoch 62, Batch 4849, Loss: 178.75401306152344\n",
      "Epoch 62, Batch 4850, Loss: 174.15733337402344\n",
      "Epoch 62, Batch 4851, Loss: 174.02645874023438\n",
      "Epoch 62, Batch 4852, Loss: 175.79513549804688\n",
      "Epoch 62, Batch 4853, Loss: 179.3721923828125\n",
      "Epoch 62, Batch 4854, Loss: 168.82705688476562\n",
      "Epoch 62, Batch 4855, Loss: 175.44418334960938\n",
      "Epoch 62, Batch 4856, Loss: 176.39720153808594\n",
      "Epoch 62, Batch 4857, Loss: 183.11898803710938\n",
      "Epoch 62, Batch 4858, Loss: 172.43728637695312\n",
      "Epoch 62, Batch 4859, Loss: 170.43577575683594\n",
      "Epoch 62, Batch 4860, Loss: 174.1243896484375\n",
      "Epoch 62, Batch 4861, Loss: 166.66326904296875\n",
      "Epoch 62, Batch 4862, Loss: 179.2557830810547\n",
      "Epoch 62, Batch 4863, Loss: 162.07864379882812\n",
      "Epoch 62, Batch 4864, Loss: 171.5684356689453\n",
      "Epoch 62, Batch 4865, Loss: 165.2678680419922\n",
      "Epoch 62, Batch 4866, Loss: 170.4009552001953\n",
      "Epoch 62, Batch 4867, Loss: 166.62901306152344\n",
      "Epoch 62, Batch 4868, Loss: 166.88587951660156\n",
      "Epoch 62, Batch 4869, Loss: 179.70379638671875\n",
      "Epoch 62, Batch 4870, Loss: 198.0811004638672\n",
      "Epoch 62, Batch 4871, Loss: 164.77078247070312\n",
      "Epoch 62, Batch 4872, Loss: 170.96958923339844\n",
      "Epoch 62, Batch 4873, Loss: 160.6806640625\n",
      "Epoch 62, Batch 4874, Loss: 161.968994140625\n",
      "Epoch 62, Batch 4875, Loss: 170.2913055419922\n",
      "Epoch 62, Batch 4876, Loss: 169.16473388671875\n",
      "Epoch 62, Batch 4877, Loss: 175.04776000976562\n",
      "Epoch 62, Batch 4878, Loss: 188.7400360107422\n",
      "Epoch 62, Batch 4879, Loss: 187.08421325683594\n",
      "Epoch 62, Batch 4880, Loss: 162.63087463378906\n",
      "Epoch 62, Batch 4881, Loss: 184.53756713867188\n",
      "Epoch 62, Batch 4882, Loss: 174.75633239746094\n",
      "Epoch 62, Batch 4883, Loss: 170.48452758789062\n",
      "Epoch 62, Batch 4884, Loss: 169.2049560546875\n",
      "Epoch 62, Batch 4885, Loss: 173.57591247558594\n",
      "Epoch 62, Batch 4886, Loss: 179.91566467285156\n",
      "Epoch 62, Batch 4887, Loss: 167.3098907470703\n",
      "Epoch 62, Batch 4888, Loss: 172.30117797851562\n",
      "Epoch 62, Batch 4889, Loss: 166.17529296875\n",
      "Epoch 62, Batch 4890, Loss: 186.8970489501953\n",
      "Epoch 62, Batch 4891, Loss: 212.9838104248047\n",
      "Epoch 62, Batch 4892, Loss: 175.12449645996094\n",
      "Epoch 62, Batch 4893, Loss: 185.71522521972656\n",
      "Epoch 62, Batch 4894, Loss: 180.5825653076172\n",
      "Epoch 62, Batch 4895, Loss: 172.06878662109375\n",
      "Epoch 62, Batch 4896, Loss: 181.90223693847656\n",
      "Epoch 62, Batch 4897, Loss: 166.21401977539062\n",
      "Epoch 62, Batch 4898, Loss: 170.42759704589844\n",
      "Epoch 62, Batch 4899, Loss: 192.30416870117188\n",
      "Epoch 62, Batch 4900, Loss: 168.5148468017578\n",
      "Epoch 62, Batch 4901, Loss: 177.1151580810547\n",
      "Epoch 62, Batch 4902, Loss: 169.67916870117188\n",
      "Epoch 62, Batch 4903, Loss: 170.95896911621094\n",
      "Epoch 62, Batch 4904, Loss: 187.20718383789062\n",
      "Epoch 62, Batch 4905, Loss: 181.60496520996094\n",
      "Epoch 62, Batch 4906, Loss: 154.47232055664062\n",
      "Epoch 62, Batch 4907, Loss: 174.72164916992188\n",
      "Epoch 62, Batch 4908, Loss: 171.64625549316406\n",
      "Epoch 62, Batch 4909, Loss: 168.0165252685547\n",
      "Epoch 62, Batch 4910, Loss: 176.6087188720703\n",
      "Epoch 62, Batch 4911, Loss: 169.1785125732422\n",
      "Epoch 62, Batch 4912, Loss: 166.56503295898438\n",
      "Epoch 62, Batch 4913, Loss: 171.42575073242188\n",
      "Epoch 62, Batch 4914, Loss: 160.6569366455078\n",
      "Epoch 62, Batch 4915, Loss: 171.29141235351562\n",
      "Epoch 62, Batch 4916, Loss: 165.206298828125\n",
      "Epoch 62, Batch 4917, Loss: 161.14259338378906\n",
      "Epoch 62, Batch 4918, Loss: 161.509521484375\n",
      "Epoch 62, Batch 4919, Loss: 177.01290893554688\n",
      "Epoch 62, Batch 4920, Loss: 169.20042419433594\n",
      "Epoch 62, Batch 4921, Loss: 171.0619659423828\n",
      "Epoch 62, Batch 4922, Loss: 197.43252563476562\n",
      "Epoch 62, Batch 4923, Loss: 174.9795684814453\n",
      "Epoch 62, Batch 4924, Loss: 167.32904052734375\n",
      "Epoch 62, Batch 4925, Loss: 172.07656860351562\n",
      "Epoch 62, Batch 4926, Loss: 178.03143310546875\n",
      "Epoch 62, Batch 4927, Loss: 174.3609161376953\n",
      "Epoch 62, Batch 4928, Loss: 156.05909729003906\n",
      "Epoch 62, Batch 4929, Loss: 176.75404357910156\n",
      "Epoch 62, Batch 4930, Loss: 163.60923767089844\n",
      "Epoch 62, Batch 4931, Loss: 150.4654541015625\n",
      "Epoch 62, Batch 4932, Loss: 187.71591186523438\n",
      "Epoch 62, Batch 4933, Loss: 162.4706268310547\n",
      "Epoch 62, Batch 4934, Loss: 174.7543487548828\n",
      "Epoch 62, Batch 4935, Loss: 193.6222686767578\n",
      "Epoch 62, Batch 4936, Loss: 171.9154052734375\n",
      "Epoch 62, Batch 4937, Loss: 175.08551025390625\n",
      "Epoch 62, Batch 4938, Loss: 185.57386779785156\n",
      "Epoch 62, Batch 4939, Loss: 168.48916625976562\n",
      "Epoch 62, Batch 4940, Loss: 178.3384552001953\n",
      "Epoch 62, Batch 4941, Loss: 169.84312438964844\n",
      "Epoch 62, Batch 4942, Loss: 164.44898986816406\n",
      "Epoch 62, Batch 4943, Loss: 204.09689331054688\n",
      "Epoch 62, Batch 4944, Loss: 169.35926818847656\n",
      "Epoch 62, Batch 4945, Loss: 172.25686645507812\n",
      "Epoch 62, Batch 4946, Loss: 173.1012420654297\n",
      "Epoch 62, Batch 4947, Loss: 143.9784698486328\n",
      "Epoch 62, Batch 4948, Loss: 160.0103759765625\n",
      "Epoch 62, Batch 4949, Loss: 164.97816467285156\n",
      "Epoch 62, Batch 4950, Loss: 191.34263610839844\n",
      "Epoch 62, Batch 4951, Loss: 175.1393585205078\n",
      "Epoch 62, Batch 4952, Loss: 177.99501037597656\n",
      "Epoch 62, Batch 4953, Loss: 168.78900146484375\n",
      "Epoch 62, Batch 4954, Loss: 160.54661560058594\n",
      "Epoch 62, Batch 4955, Loss: 170.60348510742188\n",
      "Epoch 62, Batch 4956, Loss: 179.382568359375\n",
      "Epoch 62, Batch 4957, Loss: 174.5992431640625\n",
      "Epoch 62, Batch 4958, Loss: 185.5897216796875\n",
      "Epoch 62, Batch 4959, Loss: 179.2771759033203\n",
      "Epoch 62, Batch 4960, Loss: 168.53814697265625\n",
      "Epoch 62, Batch 4961, Loss: 173.11180114746094\n",
      "Epoch 62, Batch 4962, Loss: 180.96714782714844\n",
      "Epoch 62, Batch 4963, Loss: 165.69229125976562\n",
      "Epoch 62, Batch 4964, Loss: 176.20188903808594\n",
      "Epoch 62, Batch 4965, Loss: 185.0944366455078\n",
      "Epoch 62, Batch 4966, Loss: 176.05490112304688\n",
      "Epoch 62, Batch 4967, Loss: 169.41934204101562\n",
      "Epoch 62, Batch 4968, Loss: 170.3402862548828\n",
      "Epoch 62, Batch 4969, Loss: 178.16555786132812\n",
      "Epoch 62, Batch 4970, Loss: 163.96493530273438\n",
      "Epoch 62, Batch 4971, Loss: 186.1703643798828\n",
      "Epoch 62, Batch 4972, Loss: 172.30816650390625\n",
      "Epoch 62, Batch 4973, Loss: 177.3142852783203\n",
      "Epoch 62, Batch 4974, Loss: 180.16493225097656\n",
      "Epoch 62, Batch 4975, Loss: 175.25985717773438\n",
      "Epoch 62, Batch 4976, Loss: 169.1522216796875\n",
      "Epoch 62, Batch 4977, Loss: 181.32420349121094\n",
      "Epoch 62, Batch 4978, Loss: 177.610107421875\n",
      "Epoch 62, Batch 4979, Loss: 174.1019744873047\n",
      "Epoch 62, Batch 4980, Loss: 176.71054077148438\n",
      "Epoch 62, Batch 4981, Loss: 154.6216278076172\n",
      "Epoch 62, Batch 4982, Loss: 169.08448791503906\n",
      "Epoch 62, Batch 4983, Loss: 176.1847381591797\n",
      "Epoch 62, Batch 4984, Loss: 158.72328186035156\n",
      "Epoch 62, Batch 4985, Loss: 181.2838134765625\n",
      "Epoch 62, Batch 4986, Loss: 176.3138885498047\n",
      "Epoch 62, Batch 4987, Loss: 171.933349609375\n",
      "Epoch 62, Batch 4988, Loss: 183.30032348632812\n",
      "Epoch 62, Batch 4989, Loss: 188.43540954589844\n",
      "Epoch 62, Batch 4990, Loss: 190.41421508789062\n",
      "Epoch 62, Batch 4991, Loss: 174.23182678222656\n",
      "Epoch 62, Batch 4992, Loss: 195.15960693359375\n",
      "Epoch 62, Batch 4993, Loss: 170.87889099121094\n",
      "Epoch 62, Batch 4994, Loss: 165.3929901123047\n",
      "Epoch 62, Batch 4995, Loss: 178.94808959960938\n",
      "Epoch 62, Batch 4996, Loss: 187.9902801513672\n",
      "Epoch 62, Batch 4997, Loss: 165.40037536621094\n",
      "Epoch 62, Batch 4998, Loss: 179.1937713623047\n",
      "Epoch 62, Batch 4999, Loss: 157.93374633789062\n",
      "Epoch 62, Batch 5000, Loss: 158.17747497558594\n",
      "Epoch 62, Batch 5001, Loss: 207.4876251220703\n",
      "Epoch 62, Batch 5002, Loss: 180.12728881835938\n",
      "Epoch 62, Batch 5003, Loss: 186.27516174316406\n",
      "Epoch 62, Batch 5004, Loss: 158.0459442138672\n",
      "Epoch 62, Batch 5005, Loss: 161.46693420410156\n",
      "Epoch 62, Batch 5006, Loss: 178.03363037109375\n",
      "Epoch 62, Batch 5007, Loss: 169.61062622070312\n",
      "Epoch 62, Batch 5008, Loss: 191.120849609375\n",
      "Epoch 62, Batch 5009, Loss: 170.10879516601562\n",
      "Epoch 62, Batch 5010, Loss: 187.27565002441406\n",
      "Epoch 62, Batch 5011, Loss: 170.14877319335938\n",
      "Epoch 62, Batch 5012, Loss: 168.85943603515625\n",
      "Epoch 62, Batch 5013, Loss: 176.27108764648438\n",
      "Epoch 62, Batch 5014, Loss: 191.58956909179688\n",
      "Epoch 62, Batch 5015, Loss: 169.31446838378906\n",
      "Epoch 62, Batch 5016, Loss: 178.31838989257812\n",
      "Epoch 62, Batch 5017, Loss: 173.20936584472656\n",
      "Epoch 62, Batch 5018, Loss: 171.41644287109375\n",
      "Epoch 62, Batch 5019, Loss: 178.52777099609375\n",
      "Epoch 62, Batch 5020, Loss: 175.81549072265625\n",
      "Epoch 62, Batch 5021, Loss: 192.26016235351562\n",
      "Epoch 62, Batch 5022, Loss: 166.55308532714844\n",
      "Epoch 62, Batch 5023, Loss: 180.92124938964844\n",
      "Epoch 62, Batch 5024, Loss: 179.71426391601562\n",
      "Epoch 62, Batch 5025, Loss: 167.51634216308594\n",
      "Epoch 62, Batch 5026, Loss: 160.89276123046875\n",
      "Epoch 62, Batch 5027, Loss: 175.28414916992188\n",
      "Epoch 62, Batch 5028, Loss: 173.79629516601562\n",
      "Epoch 62, Batch 5029, Loss: 175.4103240966797\n",
      "Epoch 62, Batch 5030, Loss: 161.40716552734375\n",
      "Epoch 62, Batch 5031, Loss: 182.88845825195312\n",
      "Epoch 62, Batch 5032, Loss: 175.7001953125\n",
      "Epoch 62, Batch 5033, Loss: 177.33197021484375\n",
      "Epoch 62, Batch 5034, Loss: 170.1471405029297\n",
      "Epoch 62, Batch 5035, Loss: 170.5906219482422\n",
      "Epoch 62, Batch 5036, Loss: 147.05438232421875\n",
      "Epoch 62, Batch 5037, Loss: 170.86190795898438\n",
      "Epoch 62, Batch 5038, Loss: 166.22470092773438\n",
      "Epoch 62, Batch 5039, Loss: 165.24954223632812\n",
      "Epoch 62, Batch 5040, Loss: 185.65951538085938\n",
      "Epoch 62, Batch 5041, Loss: 154.77359008789062\n",
      "Epoch 62, Batch 5042, Loss: 181.49880981445312\n",
      "Epoch 62, Batch 5043, Loss: 170.41632080078125\n",
      "Epoch 62, Batch 5044, Loss: 166.4862518310547\n",
      "Epoch 62, Batch 5045, Loss: 185.35690307617188\n",
      "Epoch 62, Batch 5046, Loss: 180.95315551757812\n",
      "Epoch 62, Batch 5047, Loss: 173.64251708984375\n",
      "Epoch 62, Batch 5048, Loss: 161.73898315429688\n",
      "Epoch 62, Batch 5049, Loss: 180.35951232910156\n",
      "Epoch 62, Batch 5050, Loss: 169.6775360107422\n",
      "Epoch 62, Batch 5051, Loss: 185.6855010986328\n",
      "Epoch 62, Batch 5052, Loss: 173.23304748535156\n",
      "Epoch 62, Batch 5053, Loss: 170.99578857421875\n",
      "Epoch 62, Batch 5054, Loss: 162.388916015625\n",
      "Epoch 62, Batch 5055, Loss: 169.16786193847656\n",
      "Epoch 62, Batch 5056, Loss: 180.6499481201172\n",
      "Epoch 62, Batch 5057, Loss: 161.197021484375\n",
      "Epoch 62, Batch 5058, Loss: 171.91415405273438\n",
      "Epoch 62, Batch 5059, Loss: 155.0829315185547\n",
      "Epoch 62, Batch 5060, Loss: 173.09942626953125\n",
      "Epoch 62, Batch 5061, Loss: 180.8929901123047\n",
      "Epoch 62, Batch 5062, Loss: 172.95465087890625\n",
      "Epoch 62, Batch 5063, Loss: 179.11875915527344\n",
      "Epoch 62, Batch 5064, Loss: 151.17784118652344\n",
      "Epoch 62, Batch 5065, Loss: 172.3545379638672\n",
      "Epoch 62, Batch 5066, Loss: 155.2324676513672\n",
      "Epoch 62, Batch 5067, Loss: 174.46885681152344\n",
      "Epoch 62, Batch 5068, Loss: 168.80218505859375\n",
      "Epoch 62, Batch 5069, Loss: 177.0859375\n",
      "Epoch 62, Batch 5070, Loss: 178.15902709960938\n",
      "Epoch 62, Batch 5071, Loss: 169.61927795410156\n",
      "Epoch 62, Batch 5072, Loss: 175.1807861328125\n",
      "Epoch 62, Batch 5073, Loss: 180.72186279296875\n",
      "Epoch 62, Batch 5074, Loss: 160.40692138671875\n",
      "Epoch 62, Batch 5075, Loss: 163.23565673828125\n",
      "Epoch 62, Batch 5076, Loss: 164.4888916015625\n",
      "Epoch 62, Batch 5077, Loss: 171.21372985839844\n",
      "Epoch 62, Batch 5078, Loss: 162.4503173828125\n",
      "Epoch 62, Batch 5079, Loss: 156.2095184326172\n",
      "Epoch 62, Batch 5080, Loss: 180.48414611816406\n",
      "Epoch 62, Batch 5081, Loss: 171.78359985351562\n",
      "Epoch 62, Batch 5082, Loss: 171.95106506347656\n",
      "Epoch 62, Batch 5083, Loss: 180.15809631347656\n",
      "Epoch 62, Batch 5084, Loss: 163.2791748046875\n",
      "Epoch 62, Batch 5085, Loss: 174.8689422607422\n",
      "Epoch 62, Batch 5086, Loss: 184.43698120117188\n",
      "Epoch 62, Batch 5087, Loss: 178.4159698486328\n",
      "Epoch 62, Batch 5088, Loss: 168.87490844726562\n",
      "Epoch 62, Batch 5089, Loss: 177.1583709716797\n",
      "Epoch 62, Batch 5090, Loss: 165.60618591308594\n",
      "Epoch 62, Batch 5091, Loss: 190.2987823486328\n",
      "Epoch 62, Batch 5092, Loss: 167.28431701660156\n",
      "Epoch 62, Batch 5093, Loss: 159.8000946044922\n",
      "Epoch 62, Batch 5094, Loss: 155.97325134277344\n",
      "Epoch 62, Batch 5095, Loss: 180.63052368164062\n",
      "Epoch 62, Batch 5096, Loss: 174.01715087890625\n",
      "Epoch 62, Batch 5097, Loss: 174.3426513671875\n",
      "Epoch 62, Batch 5098, Loss: 182.09535217285156\n",
      "Epoch 62, Batch 5099, Loss: 160.31253051757812\n",
      "Epoch 62, Batch 5100, Loss: 162.10073852539062\n",
      "Epoch 62, Batch 5101, Loss: 181.88458251953125\n",
      "Epoch 62, Batch 5102, Loss: 182.82247924804688\n",
      "Epoch 62, Batch 5103, Loss: 187.65628051757812\n",
      "Epoch 62, Batch 5104, Loss: 178.7851104736328\n",
      "Epoch 62, Batch 5105, Loss: 196.3064727783203\n",
      "Epoch 62, Batch 5106, Loss: 172.2775421142578\n",
      "Epoch 62, Batch 5107, Loss: 188.7823028564453\n",
      "Epoch 62, Batch 5108, Loss: 173.98626708984375\n",
      "Epoch 62, Batch 5109, Loss: 166.61785888671875\n",
      "Epoch 62, Batch 5110, Loss: 178.36878967285156\n",
      "Epoch 62, Batch 5111, Loss: 197.4623260498047\n",
      "Epoch 62, Batch 5112, Loss: 156.8131561279297\n",
      "Epoch 62, Batch 5113, Loss: 172.05845642089844\n",
      "Epoch 62, Batch 5114, Loss: 175.26429748535156\n",
      "Epoch 62, Batch 5115, Loss: 184.2042236328125\n",
      "Epoch 62, Batch 5116, Loss: 182.28176879882812\n",
      "Epoch 62, Batch 5117, Loss: 155.6396026611328\n",
      "Epoch 62, Batch 5118, Loss: 179.82150268554688\n",
      "Epoch 62, Batch 5119, Loss: 163.61764526367188\n",
      "Epoch 62, Batch 5120, Loss: 175.14096069335938\n",
      "Epoch 62, Batch 5121, Loss: 178.68482971191406\n",
      "Epoch 62, Batch 5122, Loss: 174.5361785888672\n",
      "Epoch 62, Batch 5123, Loss: 174.51649475097656\n",
      "Epoch 62, Batch 5124, Loss: 170.2832489013672\n",
      "Epoch 62, Batch 5125, Loss: 160.02322387695312\n",
      "Epoch 62, Batch 5126, Loss: 174.0039520263672\n",
      "Epoch 62, Batch 5127, Loss: 186.3373260498047\n",
      "Epoch 62, Batch 5128, Loss: 173.8105010986328\n",
      "Epoch 62, Batch 5129, Loss: 185.53268432617188\n",
      "Epoch 62, Batch 5130, Loss: 168.36521911621094\n",
      "Epoch 62, Batch 5131, Loss: 167.54110717773438\n",
      "Epoch 62, Batch 5132, Loss: 168.8856964111328\n",
      "Epoch 62, Batch 5133, Loss: 162.41574096679688\n",
      "Epoch 62, Batch 5134, Loss: 178.9276580810547\n",
      "Epoch 62, Batch 5135, Loss: 160.03683471679688\n",
      "Epoch 62, Batch 5136, Loss: 174.12075805664062\n",
      "Epoch 62, Batch 5137, Loss: 179.29400634765625\n",
      "Epoch 62, Batch 5138, Loss: 170.63267517089844\n",
      "Epoch 62, Batch 5139, Loss: 183.35400390625\n",
      "Epoch 62, Batch 5140, Loss: 176.4381866455078\n",
      "Epoch 62, Batch 5141, Loss: 163.8250732421875\n",
      "Epoch 62, Batch 5142, Loss: 166.1837615966797\n",
      "Epoch 62, Batch 5143, Loss: 161.4124755859375\n",
      "Epoch 62, Batch 5144, Loss: 181.1898651123047\n",
      "Epoch 62, Batch 5145, Loss: 167.3531494140625\n",
      "Epoch 62, Batch 5146, Loss: 162.18959045410156\n",
      "Epoch 62, Batch 5147, Loss: 171.1617889404297\n",
      "Epoch 62, Batch 5148, Loss: 181.64303588867188\n",
      "Epoch 62, Batch 5149, Loss: 171.24575805664062\n",
      "Epoch 62, Batch 5150, Loss: 148.33367919921875\n",
      "Epoch 62, Batch 5151, Loss: 175.69627380371094\n",
      "Epoch 62, Batch 5152, Loss: 170.05909729003906\n",
      "Epoch 62, Batch 5153, Loss: 179.68785095214844\n",
      "Epoch 62, Batch 5154, Loss: 187.03042602539062\n",
      "Epoch 62, Batch 5155, Loss: 172.0355682373047\n",
      "Epoch 62, Batch 5156, Loss: 169.53712463378906\n",
      "Epoch 62, Batch 5157, Loss: 188.97161865234375\n",
      "Epoch 62, Batch 5158, Loss: 163.9635467529297\n",
      "Epoch 62, Batch 5159, Loss: 193.32449340820312\n",
      "Epoch 62, Batch 5160, Loss: 188.63800048828125\n",
      "Epoch 62, Batch 5161, Loss: 176.07449340820312\n",
      "Epoch 62, Batch 5162, Loss: 173.39202880859375\n",
      "Epoch 62, Batch 5163, Loss: 183.7576446533203\n",
      "Epoch 62, Batch 5164, Loss: 170.9772186279297\n",
      "Epoch 62, Batch 5165, Loss: 180.31883239746094\n",
      "Epoch 62, Batch 5166, Loss: 163.12860107421875\n",
      "Epoch 62, Batch 5167, Loss: 184.61827087402344\n",
      "Epoch 62, Batch 5168, Loss: 180.23255920410156\n",
      "Epoch 62, Batch 5169, Loss: 165.2790069580078\n",
      "Epoch 62, Batch 5170, Loss: 167.63916015625\n",
      "Epoch 62, Batch 5171, Loss: 157.80880737304688\n",
      "Epoch 62, Batch 5172, Loss: 176.11936950683594\n",
      "Epoch 62, Batch 5173, Loss: 191.48768615722656\n",
      "Epoch 62, Batch 5174, Loss: 176.27291870117188\n",
      "Epoch 62, Batch 5175, Loss: 168.96453857421875\n",
      "Epoch 62, Batch 5176, Loss: 176.58236694335938\n",
      "Epoch 62, Batch 5177, Loss: 170.44717407226562\n",
      "Epoch 62, Batch 5178, Loss: 181.40089416503906\n",
      "Epoch 62, Batch 5179, Loss: 166.76654052734375\n",
      "Epoch 62, Batch 5180, Loss: 180.22152709960938\n",
      "Epoch 62, Batch 5181, Loss: 176.57412719726562\n",
      "Epoch 62, Batch 5182, Loss: 170.21774291992188\n",
      "Epoch 62, Batch 5183, Loss: 162.3179473876953\n",
      "Epoch 62, Batch 5184, Loss: 177.57225036621094\n",
      "Epoch 62, Batch 5185, Loss: 167.09359741210938\n",
      "Epoch 62, Batch 5186, Loss: 185.36416625976562\n",
      "Epoch 62, Batch 5187, Loss: 172.7787322998047\n",
      "Epoch 62, Batch 5188, Loss: 148.57725524902344\n",
      "Epoch 62, Batch 5189, Loss: 179.2559814453125\n",
      "Epoch 62, Batch 5190, Loss: 170.36984252929688\n",
      "Epoch 62, Batch 5191, Loss: 175.15493774414062\n",
      "Epoch 62, Batch 5192, Loss: 165.40931701660156\n",
      "Epoch 62, Batch 5193, Loss: 167.620849609375\n",
      "Epoch 62, Batch 5194, Loss: 176.46737670898438\n",
      "Epoch 62, Batch 5195, Loss: 166.71890258789062\n",
      "Epoch 62, Batch 5196, Loss: 191.85508728027344\n",
      "Epoch 62, Batch 5197, Loss: 178.48974609375\n",
      "Epoch 62, Batch 5198, Loss: 182.9420623779297\n",
      "Epoch 62, Batch 5199, Loss: 154.2815704345703\n",
      "Epoch 62, Batch 5200, Loss: 183.6177520751953\n",
      "Epoch 62, Batch 5201, Loss: 177.798828125\n",
      "Epoch 62, Batch 5202, Loss: 172.95376586914062\n",
      "Epoch 62, Batch 5203, Loss: 159.5039520263672\n",
      "Epoch 62, Batch 5204, Loss: 188.2367706298828\n",
      "Epoch 62, Batch 5205, Loss: 168.5711212158203\n",
      "Epoch 62, Batch 5206, Loss: 163.64349365234375\n",
      "Epoch 62, Batch 5207, Loss: 167.97198486328125\n",
      "Epoch 62, Batch 5208, Loss: 176.01780700683594\n",
      "Epoch 62, Batch 5209, Loss: 155.52821350097656\n",
      "Epoch 62, Batch 5210, Loss: 180.7154998779297\n",
      "Epoch 62, Batch 5211, Loss: 165.67027282714844\n",
      "Epoch 62, Batch 5212, Loss: 164.98165893554688\n",
      "Epoch 62, Batch 5213, Loss: 154.1007537841797\n",
      "Epoch 62, Batch 5214, Loss: 177.01547241210938\n",
      "Epoch 62, Batch 5215, Loss: 181.0631866455078\n",
      "Epoch 62, Batch 5216, Loss: 172.37411499023438\n",
      "Epoch 62, Batch 5217, Loss: 187.6620635986328\n",
      "Epoch 62, Batch 5218, Loss: 169.41433715820312\n",
      "Epoch 62, Batch 5219, Loss: 161.1704864501953\n",
      "Epoch 62, Batch 5220, Loss: 166.74932861328125\n",
      "Epoch 62, Batch 5221, Loss: 186.00289916992188\n",
      "Epoch 62, Batch 5222, Loss: 175.92962646484375\n",
      "Epoch 62, Batch 5223, Loss: 182.31607055664062\n",
      "Epoch 62, Batch 5224, Loss: 155.3552703857422\n",
      "Epoch 62, Batch 5225, Loss: 175.15713500976562\n",
      "Epoch 62, Batch 5226, Loss: 179.17494201660156\n",
      "Epoch 62, Batch 5227, Loss: 173.17379760742188\n",
      "Epoch 62, Batch 5228, Loss: 171.90606689453125\n",
      "Epoch 62, Batch 5229, Loss: 167.121337890625\n",
      "Epoch 62, Batch 5230, Loss: 172.85055541992188\n",
      "Epoch 62, Batch 5231, Loss: 161.86044311523438\n",
      "Epoch 62, Batch 5232, Loss: 152.7068328857422\n",
      "Epoch 62, Batch 5233, Loss: 178.49212646484375\n",
      "Epoch 62, Batch 5234, Loss: 160.33743286132812\n",
      "Epoch 62, Batch 5235, Loss: 179.7752685546875\n",
      "Epoch 62, Batch 5236, Loss: 182.9183807373047\n",
      "Epoch 62, Batch 5237, Loss: 178.87696838378906\n",
      "Epoch 62, Batch 5238, Loss: 185.37315368652344\n",
      "Epoch 62, Batch 5239, Loss: 168.10321044921875\n",
      "Epoch 62, Batch 5240, Loss: 186.40940856933594\n",
      "Epoch 62, Batch 5241, Loss: 173.98741149902344\n",
      "Epoch 62, Batch 5242, Loss: 175.74905395507812\n",
      "Epoch 62, Batch 5243, Loss: 182.0406036376953\n",
      "Epoch 62, Batch 5244, Loss: 161.8839874267578\n",
      "Epoch 62, Batch 5245, Loss: 161.95040893554688\n",
      "Epoch 62, Batch 5246, Loss: 181.73028564453125\n",
      "Epoch 62, Batch 5247, Loss: 175.30006408691406\n",
      "Epoch 62, Batch 5248, Loss: 175.1804656982422\n",
      "Epoch 62, Batch 5249, Loss: 187.3677978515625\n",
      "Epoch 62, Batch 5250, Loss: 161.34890747070312\n",
      "Epoch 62, Batch 5251, Loss: 161.58729553222656\n",
      "Epoch 62, Batch 5252, Loss: 174.85739135742188\n",
      "Epoch 62, Batch 5253, Loss: 166.30294799804688\n",
      "Epoch 62, Batch 5254, Loss: 178.625244140625\n",
      "Epoch 62, Batch 5255, Loss: 167.26702880859375\n",
      "Epoch 62, Batch 5256, Loss: 154.78121948242188\n",
      "Epoch 62, Batch 5257, Loss: 172.58697509765625\n",
      "Epoch 62, Batch 5258, Loss: 177.1382598876953\n",
      "Epoch 62, Batch 5259, Loss: 179.026611328125\n",
      "Epoch 62, Batch 5260, Loss: 171.77670288085938\n",
      "Epoch 62, Batch 5261, Loss: 181.22909545898438\n",
      "Epoch 62, Batch 5262, Loss: 176.3643798828125\n",
      "Epoch 62, Batch 5263, Loss: 156.69390869140625\n",
      "Epoch 62, Batch 5264, Loss: 179.55105590820312\n",
      "Epoch 62, Batch 5265, Loss: 161.59292602539062\n",
      "Epoch 62, Batch 5266, Loss: 178.354736328125\n",
      "Epoch 62, Batch 5267, Loss: 179.83392333984375\n",
      "Epoch 62, Batch 5268, Loss: 158.82888793945312\n",
      "Epoch 62, Batch 5269, Loss: 186.15663146972656\n",
      "Epoch 62, Batch 5270, Loss: 170.89639282226562\n",
      "Epoch 62, Batch 5271, Loss: 165.78823852539062\n",
      "Epoch 62, Batch 5272, Loss: 167.55584716796875\n",
      "Epoch 62, Batch 5273, Loss: 160.48463439941406\n",
      "Epoch 62, Batch 5274, Loss: 178.71559143066406\n",
      "Epoch 62, Batch 5275, Loss: 180.4853973388672\n",
      "Epoch 62, Batch 5276, Loss: 164.04074096679688\n",
      "Epoch 62, Batch 5277, Loss: 182.13348388671875\n",
      "Epoch 62, Batch 5278, Loss: 167.02952575683594\n",
      "Epoch 62, Batch 5279, Loss: 172.24473571777344\n",
      "Epoch 62, Batch 5280, Loss: 165.3443145751953\n",
      "Epoch 62, Batch 5281, Loss: 150.75038146972656\n",
      "Epoch 62, Batch 5282, Loss: 170.68624877929688\n",
      "Epoch 62, Batch 5283, Loss: 178.68138122558594\n",
      "Epoch 62, Batch 5284, Loss: 152.82411193847656\n",
      "Epoch 62, Batch 5285, Loss: 157.69464111328125\n",
      "Epoch 62, Batch 5286, Loss: 171.47799682617188\n",
      "Epoch 62, Batch 5287, Loss: 159.20736694335938\n",
      "Epoch 62, Batch 5288, Loss: 171.3993377685547\n",
      "Epoch 62, Batch 5289, Loss: 160.2705078125\n",
      "Epoch 62, Batch 5290, Loss: 176.71505737304688\n",
      "Epoch 62, Batch 5291, Loss: 166.78802490234375\n",
      "Epoch 62, Batch 5292, Loss: 165.6422576904297\n",
      "Epoch 62, Batch 5293, Loss: 171.59751892089844\n",
      "Epoch 62, Batch 5294, Loss: 176.09080505371094\n",
      "Epoch 62, Batch 5295, Loss: 180.44711303710938\n",
      "Epoch 62, Batch 5296, Loss: 169.06536865234375\n",
      "Epoch 62, Batch 5297, Loss: 175.89451599121094\n",
      "Epoch 62, Batch 5298, Loss: 164.25917053222656\n",
      "Epoch 62, Batch 5299, Loss: 171.93292236328125\n",
      "Epoch 62, Batch 5300, Loss: 173.8975372314453\n",
      "Epoch 62, Batch 5301, Loss: 173.12872314453125\n",
      "Epoch 62, Batch 5302, Loss: 166.6535186767578\n",
      "Epoch 62, Batch 5303, Loss: 164.9597930908203\n",
      "Epoch 62, Batch 5304, Loss: 163.3422393798828\n",
      "Epoch 62, Batch 5305, Loss: 175.70571899414062\n",
      "Epoch 62, Batch 5306, Loss: 169.89633178710938\n",
      "Epoch 62, Batch 5307, Loss: 172.475830078125\n",
      "Epoch 62, Batch 5308, Loss: 179.11019897460938\n",
      "Epoch 62, Batch 5309, Loss: 161.4149169921875\n",
      "Epoch 62, Batch 5310, Loss: 168.34555053710938\n",
      "Epoch 62, Batch 5311, Loss: 190.54135131835938\n",
      "Epoch 62, Batch 5312, Loss: 160.1599578857422\n",
      "Epoch 62, Batch 5313, Loss: 166.9392547607422\n",
      "Epoch 62, Batch 5314, Loss: 170.9003448486328\n",
      "Epoch 62, Batch 5315, Loss: 168.83929443359375\n",
      "Epoch 62, Batch 5316, Loss: 173.38214111328125\n",
      "Epoch 62, Batch 5317, Loss: 161.61465454101562\n",
      "Epoch 62, Batch 5318, Loss: 178.2645721435547\n",
      "Epoch 62, Batch 5319, Loss: 186.35357666015625\n",
      "Epoch 62, Batch 5320, Loss: 176.5874481201172\n",
      "Epoch 62, Batch 5321, Loss: 175.8132781982422\n",
      "Epoch 62, Batch 5322, Loss: 164.3225860595703\n",
      "Epoch 62, Batch 5323, Loss: 162.56182861328125\n",
      "Epoch 62, Batch 5324, Loss: 185.95758056640625\n",
      "Epoch 62, Batch 5325, Loss: 175.3135223388672\n",
      "Epoch 62, Batch 5326, Loss: 173.25450134277344\n",
      "Epoch 62, Batch 5327, Loss: 173.24375915527344\n",
      "Epoch 62, Batch 5328, Loss: 178.88951110839844\n",
      "Epoch 62, Batch 5329, Loss: 189.88787841796875\n",
      "Epoch 62, Batch 5330, Loss: 168.29754638671875\n",
      "Epoch 62, Batch 5331, Loss: 168.96493530273438\n",
      "Epoch 62, Batch 5332, Loss: 155.6162872314453\n",
      "Epoch 62, Batch 5333, Loss: 165.06126403808594\n",
      "Epoch 62, Batch 5334, Loss: 163.84878540039062\n",
      "Epoch 62, Batch 5335, Loss: 175.56768798828125\n",
      "Epoch 62, Batch 5336, Loss: 163.88796997070312\n",
      "Epoch 62, Batch 5337, Loss: 174.9561004638672\n",
      "Epoch 62, Batch 5338, Loss: 157.6946563720703\n",
      "Epoch 62, Batch 5339, Loss: 174.58718872070312\n",
      "Epoch 62, Batch 5340, Loss: 169.06687927246094\n",
      "Epoch 62, Batch 5341, Loss: 184.81109619140625\n",
      "Epoch 62, Batch 5342, Loss: 173.8771514892578\n",
      "Epoch 62, Batch 5343, Loss: 177.2682342529297\n",
      "Epoch 62, Batch 5344, Loss: 171.56150817871094\n",
      "Epoch 62, Batch 5345, Loss: 162.95843505859375\n",
      "Epoch 62, Batch 5346, Loss: 177.44969177246094\n",
      "Epoch 62, Batch 5347, Loss: 180.24176025390625\n",
      "Epoch 62, Batch 5348, Loss: 182.02865600585938\n",
      "Epoch 62, Batch 5349, Loss: 200.9267578125\n",
      "Epoch 62, Batch 5350, Loss: 165.7255859375\n",
      "Epoch 62, Batch 5351, Loss: 178.4632110595703\n",
      "Epoch 62, Batch 5352, Loss: 179.916015625\n",
      "Epoch 62, Batch 5353, Loss: 169.68051147460938\n",
      "Epoch 62, Batch 5354, Loss: 168.92840576171875\n",
      "Epoch 62, Batch 5355, Loss: 179.82810974121094\n",
      "Epoch 62, Batch 5356, Loss: 181.79185485839844\n",
      "Epoch 62, Batch 5357, Loss: 181.11917114257812\n",
      "Epoch 62, Batch 5358, Loss: 161.00201416015625\n",
      "Epoch 62, Batch 5359, Loss: 171.0355987548828\n",
      "Epoch 62, Batch 5360, Loss: 187.03944396972656\n",
      "Epoch 62, Batch 5361, Loss: 169.50071716308594\n",
      "Epoch 62, Batch 5362, Loss: 178.71697998046875\n",
      "Epoch 62, Batch 5363, Loss: 181.9588623046875\n",
      "Epoch 62, Batch 5364, Loss: 170.7012481689453\n",
      "Epoch 62, Batch 5365, Loss: 171.42694091796875\n",
      "Epoch 62, Batch 5366, Loss: 186.39645385742188\n",
      "Epoch 62, Batch 5367, Loss: 176.94146728515625\n",
      "Epoch 62, Batch 5368, Loss: 173.85848999023438\n",
      "Epoch 62, Batch 5369, Loss: 181.6581573486328\n",
      "Epoch 62, Batch 5370, Loss: 173.3530731201172\n",
      "Epoch 62, Batch 5371, Loss: 176.00733947753906\n",
      "Epoch 62, Batch 5372, Loss: 188.35006713867188\n",
      "Epoch 62, Batch 5373, Loss: 155.9307861328125\n",
      "Epoch 62, Batch 5374, Loss: 189.88729858398438\n",
      "Epoch 62, Batch 5375, Loss: 172.69464111328125\n",
      "Epoch 62, Batch 5376, Loss: 189.55931091308594\n",
      "Epoch 62, Batch 5377, Loss: 171.9563446044922\n",
      "Epoch 62, Batch 5378, Loss: 170.9130096435547\n",
      "Epoch 62, Batch 5379, Loss: 164.29673767089844\n",
      "Epoch 62, Batch 5380, Loss: 184.26284790039062\n",
      "Epoch 62, Batch 5381, Loss: 163.0930633544922\n",
      "Epoch 62, Batch 5382, Loss: 160.37017822265625\n",
      "Epoch 62, Batch 5383, Loss: 170.56614685058594\n",
      "Epoch 62, Batch 5384, Loss: 165.68045043945312\n",
      "Epoch 62, Batch 5385, Loss: 152.76174926757812\n",
      "Epoch 62, Batch 5386, Loss: 156.34957885742188\n",
      "Epoch 62, Batch 5387, Loss: 173.01902770996094\n",
      "Epoch 62, Batch 5388, Loss: 158.9151611328125\n",
      "Epoch 62, Batch 5389, Loss: 169.5089111328125\n",
      "Epoch 62, Batch 5390, Loss: 177.03855895996094\n",
      "Epoch 62, Batch 5391, Loss: 174.69105529785156\n",
      "Epoch 62, Batch 5392, Loss: 170.73435974121094\n",
      "Epoch 62, Batch 5393, Loss: 164.26380920410156\n",
      "Epoch 62, Batch 5394, Loss: 181.07373046875\n",
      "Epoch 62, Batch 5395, Loss: 176.87493896484375\n",
      "Epoch 62, Batch 5396, Loss: 160.6891632080078\n",
      "Epoch 62, Batch 5397, Loss: 165.59519958496094\n",
      "Epoch 62, Batch 5398, Loss: 173.95155334472656\n",
      "Epoch 62, Batch 5399, Loss: 159.6858367919922\n",
      "Epoch 62, Batch 5400, Loss: 162.39151000976562\n",
      "Epoch 62, Batch 5401, Loss: 163.55625915527344\n",
      "Epoch 62, Batch 5402, Loss: 190.7400360107422\n",
      "Epoch 62, Batch 5403, Loss: 184.59181213378906\n",
      "Epoch 62, Batch 5404, Loss: 162.47100830078125\n",
      "Epoch 62, Batch 5405, Loss: 169.71414184570312\n",
      "Epoch 62, Batch 5406, Loss: 170.00375366210938\n",
      "Epoch 62, Batch 5407, Loss: 187.98138427734375\n",
      "Epoch 62, Batch 5408, Loss: 162.7831268310547\n",
      "Epoch 62, Batch 5409, Loss: 163.11976623535156\n",
      "Epoch 62, Batch 5410, Loss: 177.05722045898438\n",
      "Epoch 62, Batch 5411, Loss: 164.83433532714844\n",
      "Epoch 62, Batch 5412, Loss: 182.47796630859375\n",
      "Epoch 62, Batch 5413, Loss: 183.04171752929688\n",
      "Epoch 62, Batch 5414, Loss: 193.79600524902344\n",
      "Epoch 62, Batch 5415, Loss: 172.4408721923828\n",
      "Epoch 62, Batch 5416, Loss: 201.6186065673828\n",
      "Epoch 62, Batch 5417, Loss: 165.5428924560547\n",
      "Epoch 62, Batch 5418, Loss: 179.73858642578125\n",
      "Epoch 62, Batch 5419, Loss: 175.44662475585938\n",
      "Epoch 62, Batch 5420, Loss: 169.9355926513672\n",
      "Epoch 62, Batch 5421, Loss: 194.4316864013672\n",
      "Epoch 62, Batch 5422, Loss: 182.55239868164062\n",
      "Epoch 62, Batch 5423, Loss: 178.0067138671875\n",
      "Epoch 62, Batch 5424, Loss: 166.665283203125\n",
      "Epoch 62, Batch 5425, Loss: 193.94119262695312\n",
      "Epoch 62, Batch 5426, Loss: 193.11700439453125\n",
      "Epoch 62, Batch 5427, Loss: 176.41900634765625\n",
      "Epoch 62, Batch 5428, Loss: 189.88818359375\n",
      "Epoch 62, Batch 5429, Loss: 167.7282257080078\n",
      "Epoch 62, Batch 5430, Loss: 177.88336181640625\n",
      "Epoch 62, Batch 5431, Loss: 184.61619567871094\n",
      "Epoch 62, Batch 5432, Loss: 157.98635864257812\n",
      "Epoch 62, Batch 5433, Loss: 164.4461212158203\n",
      "Epoch 62, Batch 5434, Loss: 181.49176025390625\n",
      "Epoch 62, Batch 5435, Loss: 186.6605987548828\n",
      "Epoch 62, Batch 5436, Loss: 169.60723876953125\n",
      "Epoch 62, Batch 5437, Loss: 200.5718231201172\n",
      "Epoch 62, Batch 5438, Loss: 164.7830047607422\n",
      "Epoch 62, Batch 5439, Loss: 169.02993774414062\n",
      "Epoch 62, Batch 5440, Loss: 161.24917602539062\n",
      "Epoch 62, Batch 5441, Loss: 171.01318359375\n",
      "Epoch 62, Batch 5442, Loss: 172.0415496826172\n",
      "Epoch 62, Batch 5443, Loss: 160.304931640625\n",
      "Epoch 62, Batch 5444, Loss: 173.02525329589844\n",
      "Epoch 62, Batch 5445, Loss: 169.016845703125\n",
      "Epoch 62, Batch 5446, Loss: 185.5166778564453\n",
      "Epoch 62, Batch 5447, Loss: 170.57415771484375\n",
      "Epoch 62, Batch 5448, Loss: 194.55979919433594\n",
      "Epoch 62, Batch 5449, Loss: 180.54859924316406\n",
      "Epoch 62, Batch 5450, Loss: 170.39076232910156\n",
      "Epoch 62, Batch 5451, Loss: 179.30767822265625\n",
      "Epoch 62, Batch 5452, Loss: 160.719970703125\n",
      "Epoch 62, Batch 5453, Loss: 189.40676879882812\n",
      "Epoch 62, Batch 5454, Loss: 169.96568298339844\n",
      "Epoch 62, Batch 5455, Loss: 178.58738708496094\n",
      "Epoch 62, Batch 5456, Loss: 189.6552734375\n",
      "Epoch 62, Batch 5457, Loss: 170.6477813720703\n",
      "Epoch 62, Batch 5458, Loss: 188.7264862060547\n",
      "Epoch 62, Batch 5459, Loss: 163.3938446044922\n",
      "Epoch 62, Batch 5460, Loss: 167.42364501953125\n",
      "Epoch 62, Batch 5461, Loss: 170.37789916992188\n",
      "Epoch 62, Batch 5462, Loss: 179.3124237060547\n",
      "Epoch 62, Batch 5463, Loss: 165.7721405029297\n",
      "Epoch 62, Batch 5464, Loss: 185.87986755371094\n",
      "Epoch 62, Batch 5465, Loss: 173.11277770996094\n",
      "Epoch 62, Batch 5466, Loss: 183.4696807861328\n",
      "Epoch 62, Batch 5467, Loss: 180.56326293945312\n",
      "Epoch 62, Batch 5468, Loss: 164.3815460205078\n",
      "Epoch 62, Batch 5469, Loss: 159.11566162109375\n",
      "Epoch 62, Batch 5470, Loss: 174.15403747558594\n",
      "Epoch 62, Batch 5471, Loss: 178.23519897460938\n",
      "Epoch 62, Batch 5472, Loss: 186.6558837890625\n",
      "Epoch 62, Batch 5473, Loss: 166.57534790039062\n",
      "Epoch 62, Batch 5474, Loss: 176.6728973388672\n",
      "Epoch 62, Batch 5475, Loss: 161.76419067382812\n",
      "Epoch 62, Batch 5476, Loss: 174.61666870117188\n",
      "Epoch 62, Batch 5477, Loss: 176.8137664794922\n",
      "Epoch 62, Batch 5478, Loss: 171.48678588867188\n",
      "Epoch 62, Batch 5479, Loss: 173.6278076171875\n",
      "Epoch 62, Batch 5480, Loss: 172.64479064941406\n",
      "Epoch 62, Batch 5481, Loss: 149.00120544433594\n",
      "Epoch 62, Batch 5482, Loss: 184.50668334960938\n",
      "Epoch 62, Batch 5483, Loss: 177.4636688232422\n",
      "Epoch 62, Batch 5484, Loss: 176.6549072265625\n",
      "Epoch 62, Batch 5485, Loss: 174.21568298339844\n",
      "Epoch 62, Batch 5486, Loss: 172.1475372314453\n",
      "Epoch 62, Batch 5487, Loss: 162.1088409423828\n",
      "Epoch 62, Batch 5488, Loss: 151.33236694335938\n",
      "Epoch 62, Batch 5489, Loss: 162.12596130371094\n",
      "Epoch 62, Batch 5490, Loss: 172.52734375\n",
      "Epoch 62, Batch 5491, Loss: 165.59295654296875\n",
      "Epoch 62, Batch 5492, Loss: 171.84625244140625\n",
      "Epoch 62, Batch 5493, Loss: 155.82530212402344\n",
      "Epoch 62, Batch 5494, Loss: 158.225341796875\n",
      "Epoch 62, Batch 5495, Loss: 173.1375274658203\n",
      "Epoch 62, Batch 5496, Loss: 171.06903076171875\n",
      "Epoch 62, Batch 5497, Loss: 153.06918334960938\n",
      "Epoch 62, Batch 5498, Loss: 162.64544677734375\n",
      "Epoch 62, Batch 5499, Loss: 176.81600952148438\n",
      "Epoch 62, Batch 5500, Loss: 158.13540649414062\n",
      "Epoch 62, Batch 5501, Loss: 177.3694305419922\n",
      "Epoch 62, Batch 5502, Loss: 169.33563232421875\n",
      "Epoch 62, Batch 5503, Loss: 171.9133758544922\n",
      "Epoch 62, Batch 5504, Loss: 168.8744354248047\n",
      "Epoch 62, Batch 5505, Loss: 156.84710693359375\n",
      "Epoch 62, Batch 5506, Loss: 162.6012420654297\n",
      "Epoch 62, Batch 5507, Loss: 178.34518432617188\n",
      "Epoch 62, Batch 5508, Loss: 168.46458435058594\n",
      "Epoch 62, Batch 5509, Loss: 170.16627502441406\n",
      "Epoch 62, Batch 5510, Loss: 179.94113159179688\n",
      "Epoch 62, Batch 5511, Loss: 185.89248657226562\n",
      "Epoch 62, Batch 5512, Loss: 175.7957000732422\n",
      "Epoch 62, Batch 5513, Loss: 186.0966796875\n",
      "Epoch 62, Batch 5514, Loss: 195.01242065429688\n",
      "Epoch 62, Batch 5515, Loss: 156.31980895996094\n",
      "Epoch 62, Batch 5516, Loss: 162.26805114746094\n",
      "Epoch 62, Batch 5517, Loss: 157.509765625\n",
      "Epoch 62, Batch 5518, Loss: 182.04685974121094\n",
      "Epoch 62, Batch 5519, Loss: 167.79098510742188\n",
      "Epoch 62, Batch 5520, Loss: 183.44163513183594\n",
      "Epoch 62, Batch 5521, Loss: 173.37432861328125\n",
      "Epoch 62, Batch 5522, Loss: 171.30690002441406\n",
      "Epoch 62, Batch 5523, Loss: 172.89529418945312\n",
      "Epoch 62, Batch 5524, Loss: 190.8445281982422\n",
      "Epoch 62, Batch 5525, Loss: 158.42413330078125\n",
      "Epoch 62, Batch 5526, Loss: 189.06631469726562\n",
      "Epoch 62, Batch 5527, Loss: 177.37164306640625\n",
      "Epoch 62, Batch 5528, Loss: 183.89491271972656\n",
      "Epoch 62, Batch 5529, Loss: 168.86546325683594\n",
      "Epoch 62, Batch 5530, Loss: 178.78677368164062\n",
      "Epoch 62, Batch 5531, Loss: 166.51710510253906\n",
      "Epoch 62, Batch 5532, Loss: 182.41891479492188\n",
      "Epoch 62, Batch 5533, Loss: 170.5835723876953\n",
      "Epoch 62, Batch 5534, Loss: 160.1616973876953\n",
      "Epoch 62, Batch 5535, Loss: 181.21481323242188\n",
      "Epoch 62, Batch 5536, Loss: 162.65835571289062\n",
      "Epoch 62, Batch 5537, Loss: 171.03172302246094\n",
      "Epoch 62, Batch 5538, Loss: 167.92623901367188\n",
      "Epoch 62, Batch 5539, Loss: 163.5803985595703\n",
      "Epoch 62, Batch 5540, Loss: 171.49960327148438\n",
      "Epoch 62, Batch 5541, Loss: 172.52145385742188\n",
      "Epoch 62, Batch 5542, Loss: 167.41038513183594\n",
      "Epoch 62, Batch 5543, Loss: 180.18618774414062\n",
      "Epoch 62, Batch 5544, Loss: 166.7686767578125\n",
      "Epoch 62, Batch 5545, Loss: 161.67210388183594\n",
      "Epoch 62, Batch 5546, Loss: 171.32362365722656\n",
      "Epoch 62, Batch 5547, Loss: 179.25250244140625\n",
      "Epoch 62, Batch 5548, Loss: 179.30416870117188\n",
      "Epoch 62, Batch 5549, Loss: 169.7062225341797\n",
      "Epoch 62, Batch 5550, Loss: 169.4163818359375\n",
      "Epoch 62, Batch 5551, Loss: 193.4449005126953\n",
      "Epoch 62, Batch 5552, Loss: 183.12176513671875\n",
      "Epoch 62, Batch 5553, Loss: 172.39096069335938\n",
      "Epoch 62, Batch 5554, Loss: 173.9578857421875\n",
      "Epoch 62, Batch 5555, Loss: 171.9148406982422\n",
      "Epoch 62, Batch 5556, Loss: 169.87184143066406\n",
      "Epoch 62, Batch 5557, Loss: 157.91552734375\n",
      "Epoch 62, Batch 5558, Loss: 160.0857696533203\n",
      "Epoch 62, Batch 5559, Loss: 186.9937286376953\n",
      "Epoch 62, Batch 5560, Loss: 165.1198272705078\n",
      "Epoch 62, Batch 5561, Loss: 163.31776428222656\n",
      "Epoch 62, Batch 5562, Loss: 176.36294555664062\n",
      "Epoch 62, Batch 5563, Loss: 172.42306518554688\n",
      "Epoch 62, Batch 5564, Loss: 166.72190856933594\n",
      "Epoch 62, Batch 5565, Loss: 182.09027099609375\n",
      "Epoch 62, Batch 5566, Loss: 173.34471130371094\n",
      "Epoch 62, Batch 5567, Loss: 185.10464477539062\n",
      "Epoch 62, Batch 5568, Loss: 158.39146423339844\n",
      "Epoch 62, Batch 5569, Loss: 194.72596740722656\n",
      "Epoch 62, Batch 5570, Loss: 171.07696533203125\n",
      "Epoch 62, Batch 5571, Loss: 178.53363037109375\n",
      "Epoch 62, Batch 5572, Loss: 183.3760986328125\n",
      "Epoch 62, Batch 5573, Loss: 152.93789672851562\n",
      "Epoch 62, Batch 5574, Loss: 173.5993194580078\n",
      "Epoch 62, Batch 5575, Loss: 165.76251220703125\n",
      "Epoch 62, Batch 5576, Loss: 177.10287475585938\n",
      "Epoch 62, Batch 5577, Loss: 178.52362060546875\n",
      "Epoch 62, Batch 5578, Loss: 147.21263122558594\n",
      "Epoch 62, Batch 5579, Loss: 175.34461975097656\n",
      "Epoch 62, Batch 5580, Loss: 192.4701690673828\n",
      "Epoch 62, Batch 5581, Loss: 166.8834228515625\n",
      "Epoch 62, Batch 5582, Loss: 193.22352600097656\n",
      "Epoch 62, Batch 5583, Loss: 190.52792358398438\n",
      "Epoch 62, Batch 5584, Loss: 171.5087890625\n",
      "Epoch 62, Batch 5585, Loss: 172.30447387695312\n",
      "Epoch 62, Batch 5586, Loss: 168.06793212890625\n",
      "Epoch 62, Batch 5587, Loss: 186.20680236816406\n",
      "Epoch 62, Batch 5588, Loss: 168.40769958496094\n",
      "Epoch 62, Batch 5589, Loss: 168.00755310058594\n",
      "Epoch 62, Batch 5590, Loss: 175.41822814941406\n",
      "Epoch 62, Batch 5591, Loss: 171.0050811767578\n",
      "Epoch 62, Batch 5592, Loss: 170.7499542236328\n",
      "Epoch 62, Batch 5593, Loss: 184.92919921875\n",
      "Epoch 62, Batch 5594, Loss: 160.548583984375\n",
      "Epoch 62, Batch 5595, Loss: 171.10951232910156\n",
      "Epoch 62, Batch 5596, Loss: 195.22630310058594\n",
      "Epoch 62, Batch 5597, Loss: 192.03858947753906\n",
      "Epoch 62, Batch 5598, Loss: 185.7843475341797\n",
      "Epoch 62, Batch 5599, Loss: 167.6620635986328\n",
      "Epoch 62, Batch 5600, Loss: 162.00852966308594\n",
      "Epoch 62, Batch 5601, Loss: 174.8359375\n",
      "Epoch 62, Batch 5602, Loss: 170.3813934326172\n",
      "Epoch 62, Batch 5603, Loss: 173.89987182617188\n",
      "Epoch 62, Batch 5604, Loss: 175.65206909179688\n",
      "Epoch 62, Batch 5605, Loss: 176.7312774658203\n",
      "Epoch 62, Batch 5606, Loss: 180.2283172607422\n",
      "Epoch 62, Batch 5607, Loss: 170.2711639404297\n",
      "Epoch 62, Batch 5608, Loss: 158.8851318359375\n",
      "Epoch 62, Batch 5609, Loss: 166.86782836914062\n",
      "Epoch 62, Batch 5610, Loss: 159.25746154785156\n",
      "Epoch 62, Batch 5611, Loss: 172.90687561035156\n",
      "Epoch 62, Batch 5612, Loss: 167.34788513183594\n",
      "Epoch 62, Batch 5613, Loss: 164.6580810546875\n",
      "Epoch 62, Batch 5614, Loss: 166.52481079101562\n",
      "Epoch 62, Batch 5615, Loss: 170.1262664794922\n",
      "Epoch 62, Batch 5616, Loss: 174.33306884765625\n",
      "Epoch 62, Batch 5617, Loss: 173.67408752441406\n",
      "Epoch 62, Batch 5618, Loss: 174.2507781982422\n",
      "Epoch 62, Batch 5619, Loss: 186.93124389648438\n",
      "Epoch 62, Batch 5620, Loss: 168.15162658691406\n",
      "Epoch 62, Batch 5621, Loss: 164.17706298828125\n",
      "Epoch 62, Batch 5622, Loss: 180.09422302246094\n",
      "Epoch 62, Batch 5623, Loss: 164.1923370361328\n",
      "Epoch 62, Batch 5624, Loss: 187.39051818847656\n",
      "Epoch 62, Batch 5625, Loss: 196.79983520507812\n",
      "Epoch 62, Batch 5626, Loss: 191.3487091064453\n",
      "Epoch 62, Batch 5627, Loss: 171.74229431152344\n",
      "Epoch 62, Batch 5628, Loss: 186.76271057128906\n",
      "Epoch 62, Batch 5629, Loss: 167.9443359375\n",
      "Epoch 62, Batch 5630, Loss: 183.17884826660156\n",
      "Epoch 62, Batch 5631, Loss: 175.66114807128906\n",
      "Epoch 62, Batch 5632, Loss: 182.61244201660156\n",
      "Epoch 62, Batch 5633, Loss: 167.23614501953125\n",
      "Epoch 62, Batch 5634, Loss: 175.35302734375\n",
      "Epoch 62, Batch 5635, Loss: 181.18679809570312\n",
      "Epoch 62, Batch 5636, Loss: 167.52078247070312\n",
      "Epoch 62, Batch 5637, Loss: 165.7643280029297\n",
      "Epoch 62, Batch 5638, Loss: 180.8022003173828\n",
      "Epoch 62, Batch 5639, Loss: 177.79229736328125\n",
      "Epoch 62, Batch 5640, Loss: 173.26084899902344\n",
      "Epoch 62, Batch 5641, Loss: 178.70362854003906\n",
      "Epoch 62, Batch 5642, Loss: 191.28160095214844\n",
      "Epoch 62, Batch 5643, Loss: 178.3906707763672\n",
      "Epoch 62, Batch 5644, Loss: 165.95433044433594\n",
      "Epoch 62, Batch 5645, Loss: 166.3980712890625\n",
      "Epoch 62, Batch 5646, Loss: 159.15045166015625\n",
      "Epoch 62, Batch 5647, Loss: 182.0270233154297\n",
      "Epoch 62, Batch 5648, Loss: 166.08273315429688\n",
      "Epoch 62, Batch 5649, Loss: 170.00579833984375\n",
      "Epoch 62, Batch 5650, Loss: 168.6414794921875\n",
      "Epoch 62, Batch 5651, Loss: 170.32949829101562\n",
      "Epoch 62, Batch 5652, Loss: 164.95237731933594\n",
      "Epoch 62, Batch 5653, Loss: 173.79074096679688\n",
      "Epoch 62, Batch 5654, Loss: 178.0283660888672\n",
      "Epoch 62, Batch 5655, Loss: 154.0980682373047\n",
      "Epoch 62, Batch 5656, Loss: 157.5778350830078\n",
      "Epoch 62, Batch 5657, Loss: 188.18080139160156\n",
      "Epoch 62, Batch 5658, Loss: 152.29994201660156\n",
      "Epoch 62, Batch 5659, Loss: 164.93336486816406\n",
      "Epoch 62, Batch 5660, Loss: 161.2911376953125\n",
      "Epoch 62, Batch 5661, Loss: 172.3520965576172\n",
      "Epoch 62, Batch 5662, Loss: 179.82203674316406\n",
      "Epoch 62, Batch 5663, Loss: 174.14205932617188\n",
      "Epoch 62, Batch 5664, Loss: 170.4888153076172\n",
      "Epoch 62, Batch 5665, Loss: 167.44960021972656\n",
      "Epoch 62, Batch 5666, Loss: 160.117431640625\n",
      "Epoch 62, Batch 5667, Loss: 161.4755859375\n",
      "Epoch 62, Batch 5668, Loss: 173.5133056640625\n",
      "Epoch 62, Batch 5669, Loss: 172.21238708496094\n",
      "Epoch 62, Batch 5670, Loss: 180.4368896484375\n",
      "Epoch 62, Batch 5671, Loss: 186.25765991210938\n",
      "Epoch 62, Batch 5672, Loss: 158.3963623046875\n",
      "Epoch 62, Batch 5673, Loss: 170.35011291503906\n",
      "Epoch 62, Batch 5674, Loss: 156.71910095214844\n",
      "Epoch 62, Batch 5675, Loss: 159.73074340820312\n",
      "Epoch 62, Batch 5676, Loss: 184.32980346679688\n",
      "Epoch 62, Batch 5677, Loss: 173.52438354492188\n",
      "Epoch 62, Batch 5678, Loss: 186.8707275390625\n",
      "Epoch 62, Batch 5679, Loss: 173.074951171875\n",
      "Epoch 62, Batch 5680, Loss: 188.34329223632812\n",
      "Epoch 62, Batch 5681, Loss: 166.1588134765625\n",
      "Epoch 62, Batch 5682, Loss: 188.09176635742188\n",
      "Epoch 62, Batch 5683, Loss: 187.9879913330078\n",
      "Epoch 62, Batch 5684, Loss: 177.70314025878906\n",
      "Epoch 62, Batch 5685, Loss: 163.3866424560547\n",
      "Epoch 62, Batch 5686, Loss: 166.8204803466797\n",
      "Epoch 62, Batch 5687, Loss: 164.9090576171875\n",
      "Epoch 62, Batch 5688, Loss: 169.68966674804688\n",
      "Epoch 62, Batch 5689, Loss: 185.3120574951172\n",
      "Epoch 62, Batch 5690, Loss: 167.9063262939453\n",
      "Epoch 62, Batch 5691, Loss: 171.70245361328125\n",
      "Epoch 62, Batch 5692, Loss: 173.69259643554688\n",
      "Epoch 62, Batch 5693, Loss: 180.2860107421875\n",
      "Epoch 62, Batch 5694, Loss: 184.6999969482422\n",
      "Epoch 62, Batch 5695, Loss: 165.0667724609375\n",
      "Epoch 62, Batch 5696, Loss: 185.04376220703125\n",
      "Epoch 62, Batch 5697, Loss: 180.65142822265625\n",
      "Epoch 62, Batch 5698, Loss: 162.5083465576172\n",
      "Epoch 62, Batch 5699, Loss: 173.36477661132812\n",
      "Epoch 62, Batch 5700, Loss: 173.38763427734375\n",
      "Epoch 62, Batch 5701, Loss: 191.47000122070312\n",
      "Epoch 62, Batch 5702, Loss: 167.97279357910156\n",
      "Epoch 62, Batch 5703, Loss: 156.6460418701172\n",
      "Epoch 62, Batch 5704, Loss: 171.82452392578125\n",
      "Epoch 62, Batch 5705, Loss: 162.7090301513672\n",
      "Epoch 62, Batch 5706, Loss: 159.64532470703125\n",
      "Epoch 62, Batch 5707, Loss: 172.9197998046875\n",
      "Epoch 62, Batch 5708, Loss: 168.5787353515625\n",
      "Epoch 62, Batch 5709, Loss: 155.9579315185547\n",
      "Epoch 62, Batch 5710, Loss: 171.18264770507812\n",
      "Epoch 62, Batch 5711, Loss: 167.0950164794922\n",
      "Epoch 62, Batch 5712, Loss: 175.53480529785156\n",
      "Epoch 62, Batch 5713, Loss: 175.64495849609375\n",
      "Epoch 62, Batch 5714, Loss: 168.1096649169922\n",
      "Epoch 62, Batch 5715, Loss: 173.24061584472656\n",
      "Epoch 62, Batch 5716, Loss: 178.14161682128906\n",
      "Epoch 62, Batch 5717, Loss: 177.232421875\n",
      "Epoch 62, Batch 5718, Loss: 166.08303833007812\n",
      "Epoch 62, Batch 5719, Loss: 184.3389434814453\n",
      "Epoch 62, Batch 5720, Loss: 174.2122039794922\n",
      "Epoch 62, Batch 5721, Loss: 182.48001098632812\n",
      "Epoch 62, Batch 5722, Loss: 188.6586151123047\n",
      "Epoch 62, Batch 5723, Loss: 176.40098571777344\n",
      "Epoch 62, Batch 5724, Loss: 172.52882385253906\n",
      "Epoch 62, Batch 5725, Loss: 175.63827514648438\n",
      "Epoch 62, Batch 5726, Loss: 164.03726196289062\n",
      "Epoch 62, Batch 5727, Loss: 186.81588745117188\n",
      "Epoch 62, Batch 5728, Loss: 189.24449157714844\n",
      "Epoch 62, Batch 5729, Loss: 178.17845153808594\n",
      "Epoch 62, Batch 5730, Loss: 189.46217346191406\n",
      "Epoch 62, Batch 5731, Loss: 178.1370086669922\n",
      "Epoch 62, Batch 5732, Loss: 172.08038330078125\n",
      "Epoch 62, Batch 5733, Loss: 169.41526794433594\n",
      "Epoch 62, Batch 5734, Loss: 151.89767456054688\n",
      "Epoch 62, Batch 5735, Loss: 171.77139282226562\n",
      "Epoch 62, Batch 5736, Loss: 156.10421752929688\n",
      "Epoch 62, Batch 5737, Loss: 179.0320587158203\n",
      "Epoch 62, Batch 5738, Loss: 176.04354858398438\n",
      "Epoch 62, Batch 5739, Loss: 190.68356323242188\n",
      "Epoch 62, Batch 5740, Loss: 180.4495086669922\n",
      "Epoch 62, Batch 5741, Loss: 182.57713317871094\n",
      "Epoch 62, Batch 5742, Loss: 178.5713653564453\n",
      "Epoch 62, Batch 5743, Loss: 159.2937774658203\n",
      "Epoch 62, Batch 5744, Loss: 178.56126403808594\n",
      "Epoch 62, Batch 5745, Loss: 178.1110076904297\n",
      "Epoch 62, Batch 5746, Loss: 177.9168701171875\n",
      "Epoch 62, Batch 5747, Loss: 178.94662475585938\n",
      "Epoch 62, Batch 5748, Loss: 186.85134887695312\n",
      "Epoch 62, Batch 5749, Loss: 170.4996337890625\n",
      "Epoch 62, Batch 5750, Loss: 164.6372833251953\n",
      "Epoch 62, Batch 5751, Loss: 169.11721801757812\n",
      "Epoch 62, Batch 5752, Loss: 173.8319091796875\n",
      "Epoch 62, Batch 5753, Loss: 175.70306396484375\n",
      "Epoch 62, Batch 5754, Loss: 171.70310974121094\n",
      "Epoch 62, Batch 5755, Loss: 176.45703125\n",
      "Epoch 62, Batch 5756, Loss: 168.341064453125\n",
      "Epoch 62, Batch 5757, Loss: 182.07888793945312\n",
      "Epoch 62, Batch 5758, Loss: 167.76158142089844\n",
      "Epoch 62, Batch 5759, Loss: 167.89852905273438\n",
      "Epoch 62, Batch 5760, Loss: 152.85372924804688\n",
      "Epoch 62, Batch 5761, Loss: 170.8286590576172\n",
      "Epoch 62, Batch 5762, Loss: 181.69203186035156\n",
      "Epoch 62, Batch 5763, Loss: 176.83297729492188\n",
      "Epoch 62, Batch 5764, Loss: 173.1985321044922\n",
      "Epoch 62, Batch 5765, Loss: 181.47352600097656\n",
      "Epoch 62, Batch 5766, Loss: 169.95321655273438\n",
      "Epoch 62, Batch 5767, Loss: 168.04039001464844\n",
      "Epoch 62, Batch 5768, Loss: 172.19151306152344\n",
      "Epoch 62, Batch 5769, Loss: 171.45114135742188\n",
      "Epoch 62, Batch 5770, Loss: 195.3317108154297\n",
      "Epoch 62, Batch 5771, Loss: 159.07716369628906\n",
      "Epoch 62, Batch 5772, Loss: 180.05889892578125\n",
      "Epoch 62, Batch 5773, Loss: 172.71795654296875\n",
      "Epoch 62, Batch 5774, Loss: 162.84671020507812\n",
      "Epoch 62, Batch 5775, Loss: 196.4952392578125\n",
      "Epoch 62, Batch 5776, Loss: 174.2892608642578\n",
      "Epoch 62, Batch 5777, Loss: 165.55084228515625\n",
      "Epoch 62, Batch 5778, Loss: 181.11746215820312\n",
      "Epoch 62, Batch 5779, Loss: 179.87867736816406\n",
      "Epoch 62, Batch 5780, Loss: 182.9556427001953\n",
      "Epoch 62, Batch 5781, Loss: 162.2735595703125\n",
      "Epoch 62, Batch 5782, Loss: 175.3280487060547\n",
      "Epoch 62, Batch 5783, Loss: 184.56265258789062\n",
      "Epoch 62, Batch 5784, Loss: 177.3753204345703\n",
      "Epoch 62, Batch 5785, Loss: 159.760498046875\n",
      "Epoch 62, Batch 5786, Loss: 163.62362670898438\n",
      "Epoch 62, Batch 5787, Loss: 156.78587341308594\n",
      "Epoch 62, Batch 5788, Loss: 158.16455078125\n",
      "Epoch 62, Batch 5789, Loss: 173.2528076171875\n",
      "Epoch 62, Batch 5790, Loss: 175.57568359375\n",
      "Epoch 62, Batch 5791, Loss: 173.57235717773438\n",
      "Epoch 62, Batch 5792, Loss: 185.9784698486328\n",
      "Epoch 62, Batch 5793, Loss: 188.49700927734375\n",
      "Epoch 62, Batch 5794, Loss: 170.31784057617188\n",
      "Epoch 62, Batch 5795, Loss: 174.78955078125\n",
      "Epoch 62, Batch 5796, Loss: 183.9951171875\n",
      "Epoch 62, Batch 5797, Loss: 183.6102294921875\n",
      "Epoch 62, Batch 5798, Loss: 166.2646942138672\n",
      "Epoch 62, Batch 5799, Loss: 176.43072509765625\n",
      "Epoch 62, Batch 5800, Loss: 167.46595764160156\n",
      "Epoch 62, Batch 5801, Loss: 186.65402221679688\n",
      "Epoch 62, Batch 5802, Loss: 167.6254425048828\n",
      "Epoch 62, Batch 5803, Loss: 187.47543334960938\n",
      "Epoch 62, Batch 5804, Loss: 178.7666015625\n",
      "Epoch 62, Batch 5805, Loss: 154.39186096191406\n",
      "Epoch 62, Batch 5806, Loss: 186.7084197998047\n",
      "Epoch 62, Batch 5807, Loss: 178.42019653320312\n",
      "Epoch 62, Batch 5808, Loss: 181.22694396972656\n",
      "Epoch 62, Batch 5809, Loss: 172.17587280273438\n",
      "Epoch 62, Batch 5810, Loss: 172.4681396484375\n",
      "Epoch 62, Batch 5811, Loss: 174.9376678466797\n",
      "Epoch 62, Batch 5812, Loss: 174.04551696777344\n",
      "Epoch 62, Batch 5813, Loss: 170.51841735839844\n",
      "Epoch 62, Batch 5814, Loss: 175.8611297607422\n",
      "Epoch 62, Batch 5815, Loss: 163.8714599609375\n",
      "Epoch 62, Batch 5816, Loss: 154.22703552246094\n",
      "Epoch 62, Batch 5817, Loss: 169.2191619873047\n",
      "Epoch 62, Batch 5818, Loss: 152.7662811279297\n",
      "Epoch 62, Batch 5819, Loss: 174.0032958984375\n",
      "Epoch 62, Batch 5820, Loss: 175.16481018066406\n",
      "Epoch 62, Batch 5821, Loss: 172.62844848632812\n",
      "Epoch 62, Batch 5822, Loss: 161.30001831054688\n",
      "Epoch 62, Batch 5823, Loss: 156.03408813476562\n",
      "Epoch 62, Batch 5824, Loss: 186.5900115966797\n",
      "Epoch 62, Batch 5825, Loss: 163.564697265625\n",
      "Epoch 62, Batch 5826, Loss: 169.37989807128906\n",
      "Epoch 62, Batch 5827, Loss: 189.57669067382812\n",
      "Epoch 62, Batch 5828, Loss: 164.85794067382812\n",
      "Epoch 62, Batch 5829, Loss: 151.9595947265625\n",
      "Epoch 62, Batch 5830, Loss: 171.308349609375\n",
      "Epoch 62, Batch 5831, Loss: 179.12269592285156\n",
      "Epoch 62, Batch 5832, Loss: 174.4300537109375\n",
      "Epoch 62, Batch 5833, Loss: 186.759033203125\n",
      "Epoch 62, Batch 5834, Loss: 166.46009826660156\n",
      "Epoch 62, Batch 5835, Loss: 168.5629425048828\n",
      "Epoch 62, Batch 5836, Loss: 169.24636840820312\n",
      "Epoch 62, Batch 5837, Loss: 175.12147521972656\n",
      "Epoch 62, Batch 5838, Loss: 172.7257080078125\n",
      "Epoch 62, Batch 5839, Loss: 171.44070434570312\n",
      "Epoch 62, Batch 5840, Loss: 172.24288940429688\n",
      "Epoch 62, Batch 5841, Loss: 186.7093505859375\n",
      "Epoch 62, Batch 5842, Loss: 190.91290283203125\n",
      "Epoch 62, Batch 5843, Loss: 178.76016235351562\n",
      "Epoch 62, Batch 5844, Loss: 174.65574645996094\n",
      "Epoch 62, Batch 5845, Loss: 192.28855895996094\n",
      "Epoch 62, Batch 5846, Loss: 177.72537231445312\n",
      "Epoch 62, Batch 5847, Loss: 168.85977172851562\n",
      "Epoch 62, Batch 5848, Loss: 166.08514404296875\n",
      "Epoch 62, Batch 5849, Loss: 174.9241180419922\n",
      "Epoch 62, Batch 5850, Loss: 169.95712280273438\n",
      "Epoch 62, Batch 5851, Loss: 161.42608642578125\n",
      "Epoch 62, Batch 5852, Loss: 170.4636688232422\n",
      "Epoch 62, Batch 5853, Loss: 190.5500946044922\n",
      "Epoch 62, Batch 5854, Loss: 173.77114868164062\n",
      "Epoch 62, Batch 5855, Loss: 186.05471801757812\n",
      "Epoch 62, Batch 5856, Loss: 171.5868682861328\n",
      "Epoch 62, Batch 5857, Loss: 182.64645385742188\n",
      "Epoch 62, Batch 5858, Loss: 167.7115478515625\n",
      "Epoch 62, Batch 5859, Loss: 156.77928161621094\n",
      "Epoch 62, Batch 5860, Loss: 179.67762756347656\n",
      "Epoch 62, Batch 5861, Loss: 177.06964111328125\n",
      "Epoch 62, Batch 5862, Loss: 158.30015563964844\n",
      "Epoch 62, Batch 5863, Loss: 185.76979064941406\n",
      "Epoch 62, Batch 5864, Loss: 178.6999969482422\n",
      "Epoch 62, Batch 5865, Loss: 164.5015869140625\n",
      "Epoch 62, Batch 5866, Loss: 178.3622589111328\n",
      "Epoch 62, Batch 5867, Loss: 176.5486602783203\n",
      "Epoch 62, Batch 5868, Loss: 165.74073791503906\n",
      "Epoch 62, Batch 5869, Loss: 176.7339324951172\n",
      "Epoch 62, Batch 5870, Loss: 190.09242248535156\n",
      "Epoch 62, Batch 5871, Loss: 166.6815185546875\n",
      "Epoch 62, Batch 5872, Loss: 161.73568725585938\n",
      "Epoch 62, Batch 5873, Loss: 164.58935546875\n",
      "Epoch 62, Batch 5874, Loss: 179.94302368164062\n",
      "Epoch 62, Batch 5875, Loss: 172.3353271484375\n",
      "Epoch 62, Batch 5876, Loss: 167.5994415283203\n",
      "Epoch 62, Batch 5877, Loss: 184.88262939453125\n",
      "Epoch 62, Batch 5878, Loss: 170.76556396484375\n",
      "Epoch 62, Batch 5879, Loss: 161.36929321289062\n",
      "Epoch 62, Batch 5880, Loss: 175.36087036132812\n",
      "Epoch 62, Batch 5881, Loss: 170.34925842285156\n",
      "Epoch 62, Batch 5882, Loss: 166.18270874023438\n",
      "Epoch 62, Batch 5883, Loss: 166.71890258789062\n",
      "Epoch 62, Batch 5884, Loss: 175.19046020507812\n",
      "Epoch 62, Batch 5885, Loss: 175.4913330078125\n",
      "Epoch 62, Batch 5886, Loss: 175.6026153564453\n",
      "Epoch 62, Batch 5887, Loss: 168.36582946777344\n",
      "Epoch 62, Batch 5888, Loss: 189.74172973632812\n",
      "Epoch 62, Batch 5889, Loss: 167.43797302246094\n",
      "Epoch 62, Batch 5890, Loss: 180.97787475585938\n",
      "Epoch 62, Batch 5891, Loss: 186.41378784179688\n",
      "Epoch 62, Batch 5892, Loss: 175.33572387695312\n",
      "Epoch 62, Batch 5893, Loss: 162.00640869140625\n",
      "Epoch 62, Batch 5894, Loss: 177.55807495117188\n",
      "Epoch 62, Batch 5895, Loss: 168.25770568847656\n",
      "Epoch 62, Batch 5896, Loss: 174.9879913330078\n",
      "Epoch 62, Batch 5897, Loss: 174.2614288330078\n",
      "Epoch 62, Batch 5898, Loss: 165.74234008789062\n",
      "Epoch 62, Batch 5899, Loss: 149.03726196289062\n",
      "Epoch 62, Batch 5900, Loss: 175.97341918945312\n",
      "Epoch 62, Batch 5901, Loss: 175.30206298828125\n",
      "Epoch 62, Batch 5902, Loss: 150.68148803710938\n",
      "Epoch 62, Batch 5903, Loss: 169.67898559570312\n",
      "Epoch 62, Batch 5904, Loss: 162.0133056640625\n",
      "Epoch 62, Batch 5905, Loss: 160.99855041503906\n",
      "Epoch 62, Batch 5906, Loss: 161.13482666015625\n",
      "Epoch 62, Batch 5907, Loss: 167.99354553222656\n",
      "Epoch 62, Batch 5908, Loss: 168.85202026367188\n",
      "Epoch 62, Batch 5909, Loss: 168.79446411132812\n",
      "Epoch 62, Batch 5910, Loss: 168.580078125\n",
      "Epoch 62, Batch 5911, Loss: 172.09849548339844\n",
      "Epoch 62, Batch 5912, Loss: 176.63328552246094\n",
      "Epoch 62, Batch 5913, Loss: 177.6648712158203\n",
      "Epoch 62, Batch 5914, Loss: 172.00758361816406\n",
      "Epoch 62, Batch 5915, Loss: 180.9685821533203\n",
      "Epoch 62, Batch 5916, Loss: 163.8665313720703\n",
      "Epoch 62, Batch 5917, Loss: 173.48817443847656\n",
      "Epoch 62, Batch 5918, Loss: 164.23023986816406\n",
      "Epoch 62, Batch 5919, Loss: 164.3336639404297\n",
      "Epoch 62, Batch 5920, Loss: 178.70626831054688\n",
      "Epoch 62, Batch 5921, Loss: 174.63873291015625\n",
      "Epoch 62, Batch 5922, Loss: 183.70201110839844\n",
      "Epoch 62, Batch 5923, Loss: 166.68850708007812\n",
      "Epoch 62, Batch 5924, Loss: 166.41688537597656\n",
      "Epoch 62, Batch 5925, Loss: 176.5492706298828\n",
      "Epoch 62, Batch 5926, Loss: 160.87094116210938\n",
      "Epoch 62, Batch 5927, Loss: 161.1478729248047\n",
      "Epoch 62, Batch 5928, Loss: 186.3303680419922\n",
      "Epoch 62, Batch 5929, Loss: 166.15762329101562\n",
      "Epoch 62, Batch 5930, Loss: 153.25160217285156\n",
      "Epoch 62, Batch 5931, Loss: 160.050537109375\n",
      "Epoch 62, Batch 5932, Loss: 174.86465454101562\n",
      "Epoch 62, Batch 5933, Loss: 173.41500854492188\n",
      "Epoch 62, Batch 5934, Loss: 189.3302764892578\n",
      "Epoch 62, Batch 5935, Loss: 181.0299530029297\n",
      "Epoch 62, Batch 5936, Loss: 183.85946655273438\n",
      "Epoch 62, Batch 5937, Loss: 156.153076171875\n",
      "Epoch 62, Batch 5938, Loss: 187.822998046875\n",
      "Epoch 62, Batch 5939, Loss: 181.98011779785156\n",
      "Epoch 62, Batch 5940, Loss: 176.80673217773438\n",
      "Epoch 62, Batch 5941, Loss: 164.94615173339844\n",
      "Epoch 62, Batch 5942, Loss: 165.53790283203125\n",
      "Epoch 62, Batch 5943, Loss: 165.61241149902344\n",
      "Epoch 62, Batch 5944, Loss: 166.43556213378906\n",
      "Epoch 62, Batch 5945, Loss: 170.48008728027344\n",
      "Epoch 62, Batch 5946, Loss: 176.9958953857422\n",
      "Epoch 62, Batch 5947, Loss: 183.10841369628906\n",
      "Epoch 62, Batch 5948, Loss: 154.50567626953125\n",
      "Epoch 62, Batch 5949, Loss: 153.0885772705078\n",
      "Epoch 62, Batch 5950, Loss: 167.3475799560547\n",
      "Epoch 62, Batch 5951, Loss: 156.28897094726562\n",
      "Epoch 62, Batch 5952, Loss: 177.6254425048828\n",
      "Epoch 62, Batch 5953, Loss: 151.6123504638672\n",
      "Epoch 62, Batch 5954, Loss: 173.4193572998047\n",
      "Epoch 62, Batch 5955, Loss: 181.88671875\n",
      "Epoch 62, Batch 5956, Loss: 197.53143310546875\n",
      "Epoch 62, Batch 5957, Loss: 157.02792358398438\n",
      "Epoch 62, Batch 5958, Loss: 176.53904724121094\n",
      "Epoch 62, Batch 5959, Loss: 182.47842407226562\n",
      "Epoch 62, Batch 5960, Loss: 196.84591674804688\n",
      "Epoch 62, Batch 5961, Loss: 172.68045043945312\n",
      "Epoch 62, Batch 5962, Loss: 176.8279571533203\n",
      "Epoch 62, Batch 5963, Loss: 170.548095703125\n",
      "Epoch 62, Batch 5964, Loss: 164.7064971923828\n",
      "Epoch 62, Batch 5965, Loss: 153.12826538085938\n",
      "Epoch 62, Batch 5966, Loss: 145.78334045410156\n",
      "Epoch 62, Batch 5967, Loss: 166.04005432128906\n",
      "Epoch 62, Batch 5968, Loss: 166.6393585205078\n",
      "Epoch 62, Batch 5969, Loss: 166.21458435058594\n",
      "Epoch 62, Batch 5970, Loss: 176.23236083984375\n",
      "Epoch 62, Batch 5971, Loss: 185.72203063964844\n",
      "Epoch 62, Batch 5972, Loss: 178.60850524902344\n",
      "Epoch 62, Batch 5973, Loss: 158.80812072753906\n",
      "Epoch 62, Batch 5974, Loss: 188.99282836914062\n",
      "Epoch 62, Batch 5975, Loss: 182.27149963378906\n",
      "Epoch 62, Batch 5976, Loss: 157.1402587890625\n",
      "Epoch 62, Batch 5977, Loss: 171.00840759277344\n",
      "Epoch 62, Batch 5978, Loss: 185.35000610351562\n",
      "Epoch 62, Batch 5979, Loss: 169.90924072265625\n",
      "Epoch 62, Batch 5980, Loss: 174.8668212890625\n",
      "Epoch 62, Batch 5981, Loss: 190.2378692626953\n",
      "Epoch 62, Batch 5982, Loss: 179.28945922851562\n",
      "Epoch 62, Batch 5983, Loss: 183.6686248779297\n",
      "Epoch 62, Batch 5984, Loss: 165.0192413330078\n",
      "Epoch 62, Batch 5985, Loss: 184.6636199951172\n",
      "Epoch 62, Batch 5986, Loss: 171.01593017578125\n",
      "Epoch 62, Batch 5987, Loss: 168.53836059570312\n",
      "Epoch 62, Batch 5988, Loss: 174.17648315429688\n",
      "Epoch 62, Batch 5989, Loss: 176.31361389160156\n",
      "Epoch 62, Batch 5990, Loss: 177.31820678710938\n",
      "Epoch 62, Batch 5991, Loss: 176.97482299804688\n",
      "Epoch 62, Batch 5992, Loss: 194.46778869628906\n",
      "Epoch 62, Batch 5993, Loss: 154.93455505371094\n",
      "Epoch 62, Batch 5994, Loss: 171.127685546875\n",
      "Epoch 62, Batch 5995, Loss: 185.87405395507812\n",
      "Epoch 62, Batch 5996, Loss: 166.32516479492188\n",
      "Epoch 62, Batch 5997, Loss: 191.14810180664062\n",
      "Epoch 62, Batch 5998, Loss: 170.64479064941406\n",
      "Epoch 62, Batch 5999, Loss: 160.3450927734375\n",
      "Epoch 62, Batch 6000, Loss: 177.01937866210938\n",
      "Epoch 62, Batch 6001, Loss: 176.50535583496094\n",
      "Epoch 62, Batch 6002, Loss: 168.52490234375\n",
      "Epoch 62, Batch 6003, Loss: 181.55320739746094\n",
      "Epoch 62, Batch 6004, Loss: 175.03460693359375\n",
      "Epoch 62, Batch 6005, Loss: 163.19517517089844\n",
      "Epoch 62, Batch 6006, Loss: 170.9969024658203\n",
      "Epoch 62, Batch 6007, Loss: 185.23202514648438\n",
      "Epoch 62, Batch 6008, Loss: 176.4385528564453\n",
      "Epoch 62, Batch 6009, Loss: 179.20692443847656\n",
      "Epoch 62, Batch 6010, Loss: 181.26980590820312\n",
      "Epoch 62, Batch 6011, Loss: 194.79525756835938\n",
      "Epoch 62, Batch 6012, Loss: 172.05519104003906\n",
      "Epoch 62, Batch 6013, Loss: 177.39341735839844\n",
      "Epoch 62, Batch 6014, Loss: 173.12599182128906\n",
      "Epoch 62, Batch 6015, Loss: 159.06845092773438\n",
      "Epoch 62, Batch 6016, Loss: 146.92494201660156\n",
      "Epoch 62, Batch 6017, Loss: 168.8109130859375\n",
      "Epoch 62, Batch 6018, Loss: 167.4838409423828\n",
      "Epoch 62, Batch 6019, Loss: 165.1544647216797\n",
      "Epoch 62, Batch 6020, Loss: 168.9638214111328\n",
      "Epoch 62, Batch 6021, Loss: 195.2238006591797\n",
      "Epoch 62, Batch 6022, Loss: 170.82211303710938\n",
      "Epoch 62, Batch 6023, Loss: 145.70440673828125\n",
      "Epoch 62, Batch 6024, Loss: 181.4503173828125\n",
      "Epoch 62, Batch 6025, Loss: 171.57887268066406\n",
      "Epoch 62, Batch 6026, Loss: 189.5601043701172\n",
      "Epoch 62, Batch 6027, Loss: 186.50592041015625\n",
      "Epoch 62, Batch 6028, Loss: 177.34951782226562\n",
      "Epoch 62, Batch 6029, Loss: 177.1017303466797\n",
      "Epoch 62, Batch 6030, Loss: 166.03692626953125\n",
      "Epoch 62, Batch 6031, Loss: 169.64849853515625\n",
      "Epoch 62, Batch 6032, Loss: 187.36740112304688\n",
      "Epoch 62, Batch 6033, Loss: 174.48202514648438\n",
      "Epoch 62, Batch 6034, Loss: 170.91351318359375\n",
      "Epoch 62, Batch 6035, Loss: 164.9402618408203\n",
      "Epoch 62, Batch 6036, Loss: 176.86807250976562\n",
      "Epoch 62, Batch 6037, Loss: 175.3411102294922\n",
      "Epoch 62, Batch 6038, Loss: 161.1453857421875\n",
      "Epoch 62, Batch 6039, Loss: 192.36102294921875\n",
      "Epoch 62, Batch 6040, Loss: 153.5181121826172\n",
      "Epoch 62, Batch 6041, Loss: 165.40013122558594\n",
      "Epoch 62, Batch 6042, Loss: 169.61830139160156\n",
      "Epoch 62, Batch 6043, Loss: 161.332763671875\n",
      "Epoch 62, Batch 6044, Loss: 195.79209899902344\n",
      "Epoch 62, Batch 6045, Loss: 173.03768920898438\n",
      "Epoch 62, Batch 6046, Loss: 164.0355987548828\n",
      "Epoch 62, Batch 6047, Loss: 175.79953002929688\n",
      "Epoch 62, Batch 6048, Loss: 180.76699829101562\n",
      "Epoch 62, Batch 6049, Loss: 188.3212127685547\n",
      "Epoch 62, Batch 6050, Loss: 174.10646057128906\n",
      "Epoch 62, Batch 6051, Loss: 168.30068969726562\n",
      "Epoch 62, Batch 6052, Loss: 160.91229248046875\n",
      "Epoch 62, Batch 6053, Loss: 173.16282653808594\n",
      "Epoch 62, Batch 6054, Loss: 165.79156494140625\n",
      "Epoch 62, Batch 6055, Loss: 174.3267822265625\n",
      "Epoch 62, Batch 6056, Loss: 163.87411499023438\n",
      "Epoch 62, Batch 6057, Loss: 172.15252685546875\n",
      "Epoch 62, Batch 6058, Loss: 169.78895568847656\n",
      "Epoch 62, Batch 6059, Loss: 180.8905792236328\n",
      "Epoch 62, Batch 6060, Loss: 185.3133087158203\n",
      "Epoch 62, Batch 6061, Loss: 168.47088623046875\n",
      "Epoch 62, Batch 6062, Loss: 188.26727294921875\n",
      "Epoch 62, Batch 6063, Loss: 162.72088623046875\n",
      "Epoch 62, Batch 6064, Loss: 178.6647186279297\n",
      "Epoch 62, Batch 6065, Loss: 160.145751953125\n",
      "Epoch 62, Batch 6066, Loss: 185.096435546875\n",
      "Epoch 62, Batch 6067, Loss: 166.06182861328125\n",
      "Epoch 62, Batch 6068, Loss: 174.9774932861328\n",
      "Epoch 62, Batch 6069, Loss: 180.95738220214844\n",
      "Epoch 62, Batch 6070, Loss: 164.677001953125\n",
      "Epoch 62, Batch 6071, Loss: 169.05764770507812\n",
      "Epoch 62, Batch 6072, Loss: 170.92587280273438\n",
      "Epoch 62, Batch 6073, Loss: 196.51463317871094\n",
      "Epoch 62, Batch 6074, Loss: 189.51792907714844\n",
      "Epoch 62, Batch 6075, Loss: 179.68011474609375\n",
      "Epoch 62, Batch 6076, Loss: 173.71591186523438\n",
      "Epoch 62, Batch 6077, Loss: 168.1469268798828\n",
      "Epoch 62, Batch 6078, Loss: 188.64437866210938\n",
      "Epoch 62, Batch 6079, Loss: 166.5035400390625\n",
      "Epoch 62, Batch 6080, Loss: 172.0398406982422\n",
      "Epoch 62, Batch 6081, Loss: 172.5817108154297\n",
      "Epoch 62, Batch 6082, Loss: 175.1622314453125\n",
      "Epoch 62, Batch 6083, Loss: 179.13429260253906\n",
      "Epoch 62, Batch 6084, Loss: 175.84686279296875\n",
      "Epoch 62, Batch 6085, Loss: 172.6475372314453\n",
      "Epoch 62, Batch 6086, Loss: 181.03042602539062\n",
      "Epoch 62, Batch 6087, Loss: 169.12342834472656\n",
      "Epoch 62, Batch 6088, Loss: 187.54234313964844\n",
      "Epoch 62, Batch 6089, Loss: 174.0423126220703\n",
      "Epoch 62, Batch 6090, Loss: 162.66571044921875\n",
      "Epoch 62, Batch 6091, Loss: 187.58346557617188\n",
      "Epoch 62, Batch 6092, Loss: 184.69683837890625\n",
      "Epoch 62, Batch 6093, Loss: 185.7274169921875\n",
      "Epoch 62, Batch 6094, Loss: 177.52708435058594\n",
      "Epoch 62, Batch 6095, Loss: 167.31820678710938\n",
      "Epoch 62, Batch 6096, Loss: 178.55166625976562\n",
      "Epoch 62, Batch 6097, Loss: 175.7956085205078\n",
      "Epoch 62, Batch 6098, Loss: 162.0247344970703\n",
      "Epoch 62, Batch 6099, Loss: 190.35040283203125\n",
      "Epoch 62, Batch 6100, Loss: 184.36627197265625\n",
      "Epoch 62, Batch 6101, Loss: 184.6631622314453\n",
      "Epoch 62, Batch 6102, Loss: 169.673583984375\n",
      "Epoch 62, Batch 6103, Loss: 170.06414794921875\n",
      "Epoch 62, Batch 6104, Loss: 173.83306884765625\n",
      "Epoch 62, Batch 6105, Loss: 173.86228942871094\n",
      "Epoch 62, Batch 6106, Loss: 180.99319458007812\n",
      "Epoch 62, Batch 6107, Loss: 171.25250244140625\n",
      "Epoch 62, Batch 6108, Loss: 196.88584899902344\n",
      "Epoch 62, Batch 6109, Loss: 176.22525024414062\n",
      "Epoch 62, Batch 6110, Loss: 169.36009216308594\n",
      "Epoch 62, Batch 6111, Loss: 186.4967041015625\n",
      "Epoch 62, Batch 6112, Loss: 158.14698791503906\n",
      "Epoch 62, Batch 6113, Loss: 183.9317626953125\n",
      "Epoch 62, Batch 6114, Loss: 167.22718811035156\n",
      "Epoch 62, Batch 6115, Loss: 161.392578125\n",
      "Epoch 62, Batch 6116, Loss: 178.66737365722656\n",
      "Epoch 62, Batch 6117, Loss: 168.7564697265625\n",
      "Epoch 62, Batch 6118, Loss: 168.9936981201172\n",
      "Epoch 62, Batch 6119, Loss: 174.82937622070312\n",
      "Epoch 62, Batch 6120, Loss: 174.039794921875\n",
      "Epoch 62, Batch 6121, Loss: 176.88088989257812\n",
      "Epoch 62, Batch 6122, Loss: 165.4523468017578\n",
      "Epoch 62, Batch 6123, Loss: 174.56390380859375\n",
      "Epoch 62, Batch 6124, Loss: 174.80593872070312\n",
      "Epoch 62, Batch 6125, Loss: 170.92892456054688\n",
      "Epoch 62, Batch 6126, Loss: 185.2489013671875\n",
      "Epoch 62, Batch 6127, Loss: 174.52467346191406\n",
      "Epoch 62, Batch 6128, Loss: 165.7747344970703\n",
      "Epoch 62, Batch 6129, Loss: 173.67066955566406\n",
      "Epoch 62, Batch 6130, Loss: 182.31214904785156\n",
      "Epoch 62, Batch 6131, Loss: 163.17050170898438\n",
      "Epoch 62, Batch 6132, Loss: 174.5569305419922\n",
      "Epoch 62, Batch 6133, Loss: 181.1929931640625\n",
      "Epoch 62, Batch 6134, Loss: 177.91183471679688\n",
      "Epoch 62, Batch 6135, Loss: 182.2893829345703\n",
      "Epoch 62, Batch 6136, Loss: 173.9188690185547\n",
      "Epoch 62, Batch 6137, Loss: 184.52517700195312\n",
      "Epoch 62, Batch 6138, Loss: 176.48077392578125\n",
      "Epoch 62, Batch 6139, Loss: 180.69606018066406\n",
      "Epoch 62, Batch 6140, Loss: 187.57322692871094\n",
      "Epoch 62, Batch 6141, Loss: 166.5458984375\n",
      "Epoch 62, Batch 6142, Loss: 169.8365020751953\n",
      "Epoch 62, Batch 6143, Loss: 174.06443786621094\n",
      "Epoch 62, Batch 6144, Loss: 189.8463134765625\n",
      "Epoch 62, Batch 6145, Loss: 172.7740936279297\n",
      "Epoch 62, Batch 6146, Loss: 169.80767822265625\n",
      "Epoch 62, Batch 6147, Loss: 169.62623596191406\n",
      "Epoch 62, Batch 6148, Loss: 181.69012451171875\n",
      "Epoch 62, Batch 6149, Loss: 160.38156127929688\n",
      "Epoch 62, Batch 6150, Loss: 171.98251342773438\n",
      "Epoch 62, Batch 6151, Loss: 158.6387481689453\n",
      "Epoch 62, Batch 6152, Loss: 182.9791717529297\n",
      "Epoch 62, Batch 6153, Loss: 168.49497985839844\n",
      "Epoch 62, Batch 6154, Loss: 178.96092224121094\n",
      "Epoch 62, Batch 6155, Loss: 185.17994689941406\n",
      "Epoch 62, Batch 6156, Loss: 182.26260375976562\n",
      "Epoch 62, Batch 6157, Loss: 180.736083984375\n",
      "Epoch 62, Batch 6158, Loss: 157.3245391845703\n",
      "Epoch 62, Batch 6159, Loss: 176.9181365966797\n",
      "Epoch 62, Batch 6160, Loss: 167.77670288085938\n",
      "Epoch 62, Batch 6161, Loss: 171.85911560058594\n",
      "Epoch 62, Batch 6162, Loss: 161.76272583007812\n",
      "Epoch 62, Batch 6163, Loss: 191.4322967529297\n",
      "Epoch 62, Batch 6164, Loss: 148.0258026123047\n",
      "Epoch 62, Batch 6165, Loss: 162.7061309814453\n",
      "Epoch 62, Batch 6166, Loss: 173.76002502441406\n",
      "Epoch 62, Batch 6167, Loss: 172.04908752441406\n",
      "Epoch 62, Batch 6168, Loss: 161.49081420898438\n",
      "Epoch 62, Batch 6169, Loss: 180.7598419189453\n",
      "Epoch 62, Batch 6170, Loss: 178.25344848632812\n",
      "Epoch 62, Batch 6171, Loss: 175.45811462402344\n",
      "Epoch 62, Batch 6172, Loss: 180.0651397705078\n",
      "Epoch 62, Batch 6173, Loss: 181.69512939453125\n",
      "Epoch 62, Batch 6174, Loss: 164.6827392578125\n",
      "Epoch 62, Batch 6175, Loss: 171.25450134277344\n",
      "Epoch 62, Batch 6176, Loss: 158.507568359375\n",
      "Epoch 62, Batch 6177, Loss: 169.8665313720703\n",
      "Epoch 62, Batch 6178, Loss: 165.02259826660156\n",
      "Epoch 62, Batch 6179, Loss: 173.00970458984375\n",
      "Epoch 62, Batch 6180, Loss: 180.54080200195312\n",
      "Epoch 62, Batch 6181, Loss: 189.22088623046875\n",
      "Epoch 62, Batch 6182, Loss: 178.5777130126953\n",
      "Epoch 62, Batch 6183, Loss: 189.79464721679688\n",
      "Epoch 62, Batch 6184, Loss: 171.49696350097656\n",
      "Epoch 62, Batch 6185, Loss: 173.71102905273438\n",
      "Epoch 62, Batch 6186, Loss: 160.28843688964844\n",
      "Epoch 62, Batch 6187, Loss: 159.21763610839844\n",
      "Epoch 62, Batch 6188, Loss: 161.58200073242188\n",
      "Epoch 62, Batch 6189, Loss: 167.02984619140625\n",
      "Epoch 62, Batch 6190, Loss: 165.59861755371094\n",
      "Epoch 62, Batch 6191, Loss: 169.40687561035156\n",
      "Epoch 62, Batch 6192, Loss: 182.35586547851562\n",
      "Epoch 62, Batch 6193, Loss: 172.05697631835938\n",
      "Epoch 62, Batch 6194, Loss: 170.5605010986328\n",
      "Epoch 62, Batch 6195, Loss: 156.4414520263672\n",
      "Epoch 62, Batch 6196, Loss: 185.07725524902344\n",
      "Epoch 62, Batch 6197, Loss: 162.94102478027344\n",
      "Epoch 62, Batch 6198, Loss: 183.60928344726562\n",
      "Epoch 62, Batch 6199, Loss: 157.7416229248047\n",
      "Epoch 62, Batch 6200, Loss: 161.58590698242188\n",
      "Epoch 62, Batch 6201, Loss: 162.44003295898438\n",
      "Epoch 62, Batch 6202, Loss: 169.51866149902344\n",
      "Epoch 62, Batch 6203, Loss: 175.40359497070312\n",
      "Epoch 62, Batch 6204, Loss: 179.8616485595703\n",
      "Epoch 62, Batch 6205, Loss: 166.3508758544922\n",
      "Epoch 62, Batch 6206, Loss: 166.15969848632812\n",
      "Epoch 62, Batch 6207, Loss: 165.57005310058594\n",
      "Epoch 62, Batch 6208, Loss: 164.30641174316406\n",
      "Epoch 62, Batch 6209, Loss: 164.90805053710938\n",
      "Epoch 62, Batch 6210, Loss: 179.56320190429688\n",
      "Epoch 62, Batch 6211, Loss: 175.84326171875\n",
      "Epoch 62, Batch 6212, Loss: 161.158935546875\n",
      "Epoch 62, Batch 6213, Loss: 159.77334594726562\n",
      "Epoch 62, Batch 6214, Loss: 183.9105224609375\n",
      "Epoch 62, Batch 6215, Loss: 174.232421875\n",
      "Epoch 62, Batch 6216, Loss: 177.40634155273438\n",
      "Epoch 62, Batch 6217, Loss: 179.3521270751953\n",
      "Epoch 62, Batch 6218, Loss: 176.81234741210938\n",
      "Epoch 62, Batch 6219, Loss: 161.67178344726562\n",
      "Epoch 62, Batch 6220, Loss: 158.49757385253906\n",
      "Epoch 62, Batch 6221, Loss: 174.70721435546875\n",
      "Epoch 62, Batch 6222, Loss: 180.87393188476562\n",
      "Epoch 62, Batch 6223, Loss: 205.01333618164062\n",
      "Epoch 62, Batch 6224, Loss: 163.37257385253906\n",
      "Epoch 62, Batch 6225, Loss: 167.82623291015625\n",
      "Epoch 62, Batch 6226, Loss: 170.45599365234375\n",
      "Epoch 62, Batch 6227, Loss: 173.57537841796875\n",
      "Epoch 62, Batch 6228, Loss: 167.67132568359375\n",
      "Epoch 62, Batch 6229, Loss: 176.2438201904297\n",
      "Epoch 62, Batch 6230, Loss: 163.2073974609375\n",
      "Epoch 62, Batch 6231, Loss: 173.30670166015625\n",
      "Epoch 62, Batch 6232, Loss: 176.8388671875\n",
      "Epoch 62, Batch 6233, Loss: 163.28033447265625\n",
      "Epoch 62, Batch 6234, Loss: 164.53041076660156\n",
      "Epoch 62, Batch 6235, Loss: 167.1728973388672\n",
      "Epoch 62, Batch 6236, Loss: 164.34730529785156\n",
      "Epoch 62, Batch 6237, Loss: 176.9348907470703\n",
      "Epoch 62, Batch 6238, Loss: 181.87673950195312\n",
      "Epoch 62, Batch 6239, Loss: 159.84275817871094\n",
      "Epoch 62, Batch 6240, Loss: 163.40672302246094\n",
      "Epoch 62, Batch 6241, Loss: 164.1365966796875\n",
      "Epoch 62, Batch 6242, Loss: 164.0802459716797\n",
      "Epoch 62, Batch 6243, Loss: 184.6107635498047\n",
      "Epoch 62, Batch 6244, Loss: 170.40760803222656\n",
      "Epoch 62, Batch 6245, Loss: 176.30459594726562\n",
      "Epoch 62, Batch 6246, Loss: 169.8933868408203\n",
      "Epoch 62, Batch 6247, Loss: 178.98831176757812\n",
      "Epoch 62, Batch 6248, Loss: 161.0897979736328\n",
      "Epoch 62, Batch 6249, Loss: 159.32176208496094\n",
      "Epoch 62, Batch 6250, Loss: 161.55780029296875\n",
      "Epoch 62, Batch 6251, Loss: 168.4322967529297\n",
      "Epoch 62, Batch 6252, Loss: 168.69554138183594\n",
      "Epoch 62, Batch 6253, Loss: 175.16647338867188\n",
      "Epoch 62, Batch 6254, Loss: 174.4980926513672\n",
      "Epoch 62, Batch 6255, Loss: 188.03257751464844\n",
      "Epoch 62, Batch 6256, Loss: 186.41925048828125\n",
      "Epoch 62, Batch 6257, Loss: 152.9809112548828\n",
      "Epoch 62, Batch 6258, Loss: 174.41653442382812\n",
      "Epoch 62, Batch 6259, Loss: 181.3876495361328\n",
      "Epoch 62, Batch 6260, Loss: 169.3420867919922\n",
      "Epoch 62, Batch 6261, Loss: 178.87269592285156\n",
      "Epoch 62, Batch 6262, Loss: 180.32183837890625\n",
      "Epoch 62, Batch 6263, Loss: 186.68157958984375\n",
      "Epoch 62, Batch 6264, Loss: 169.4775848388672\n",
      "Epoch 62, Batch 6265, Loss: 170.7841033935547\n",
      "Epoch 62, Batch 6266, Loss: 172.97679138183594\n",
      "Epoch 62, Batch 6267, Loss: 188.3733673095703\n",
      "Epoch 62, Batch 6268, Loss: 181.9747772216797\n",
      "Epoch 62, Batch 6269, Loss: 165.9480438232422\n",
      "Epoch 62, Batch 6270, Loss: 155.89085388183594\n",
      "Epoch 62, Batch 6271, Loss: 185.91424560546875\n",
      "Epoch 62, Batch 6272, Loss: 173.0114288330078\n",
      "Epoch 62, Batch 6273, Loss: 185.0339813232422\n",
      "Epoch 62, Batch 6274, Loss: 189.97816467285156\n",
      "Epoch 62, Batch 6275, Loss: 173.3652801513672\n",
      "Epoch 62, Batch 6276, Loss: 173.53082275390625\n",
      "Epoch 62, Batch 6277, Loss: 187.39804077148438\n",
      "Epoch 62, Batch 6278, Loss: 171.35772705078125\n",
      "Epoch 62, Batch 6279, Loss: 180.7713165283203\n",
      "Epoch 62, Batch 6280, Loss: 174.89093017578125\n",
      "Epoch 62, Batch 6281, Loss: 184.9833221435547\n",
      "Epoch 62, Batch 6282, Loss: 160.90611267089844\n",
      "Epoch 62, Batch 6283, Loss: 173.37103271484375\n",
      "Epoch 62, Batch 6284, Loss: 177.65277099609375\n",
      "Epoch 62, Batch 6285, Loss: 163.41014099121094\n",
      "Epoch 62, Batch 6286, Loss: 165.72764587402344\n",
      "Epoch 62, Batch 6287, Loss: 166.66366577148438\n",
      "Epoch 62, Batch 6288, Loss: 171.30197143554688\n",
      "Epoch 62, Batch 6289, Loss: 154.36581420898438\n",
      "Epoch 62, Batch 6290, Loss: 185.7138671875\n",
      "Epoch 62, Batch 6291, Loss: 168.55172729492188\n",
      "Epoch 62, Batch 6292, Loss: 175.82647705078125\n",
      "Epoch 62, Batch 6293, Loss: 164.7311248779297\n",
      "Epoch 62, Batch 6294, Loss: 173.42977905273438\n",
      "Epoch 62, Batch 6295, Loss: 167.07933044433594\n",
      "Epoch 62, Batch 6296, Loss: 170.39549255371094\n",
      "Epoch 62, Batch 6297, Loss: 173.6664276123047\n",
      "Epoch 62, Batch 6298, Loss: 192.84249877929688\n",
      "Epoch 62, Batch 6299, Loss: 177.1435546875\n",
      "Epoch 62, Batch 6300, Loss: 166.7043914794922\n",
      "Epoch 62, Batch 6301, Loss: 176.13319396972656\n",
      "Epoch 62, Batch 6302, Loss: 174.01266479492188\n",
      "Epoch 62, Batch 6303, Loss: 174.9273681640625\n",
      "Epoch 62, Batch 6304, Loss: 166.3164520263672\n",
      "Epoch 62, Batch 6305, Loss: 172.27273559570312\n",
      "Epoch 62, Batch 6306, Loss: 180.70603942871094\n",
      "Epoch 62, Batch 6307, Loss: 167.57469177246094\n",
      "Epoch 62, Batch 6308, Loss: 177.81460571289062\n",
      "Epoch 62, Batch 6309, Loss: 179.4034881591797\n",
      "Epoch 62, Batch 6310, Loss: 189.76324462890625\n",
      "Epoch 62, Batch 6311, Loss: 180.10633850097656\n",
      "Epoch 62, Batch 6312, Loss: 173.62417602539062\n",
      "Epoch 62, Batch 6313, Loss: 178.56893920898438\n",
      "Epoch 62, Batch 6314, Loss: 173.10189819335938\n",
      "Epoch 62, Batch 6315, Loss: 176.02935791015625\n",
      "Epoch 62, Batch 6316, Loss: 167.00059509277344\n",
      "Epoch 62, Batch 6317, Loss: 177.7620391845703\n",
      "Epoch 62, Batch 6318, Loss: 186.10076904296875\n",
      "Epoch 62, Batch 6319, Loss: 173.6547088623047\n",
      "Epoch 62, Batch 6320, Loss: 175.45428466796875\n",
      "Epoch 62, Batch 6321, Loss: 171.8649444580078\n",
      "Epoch 62, Batch 6322, Loss: 154.58328247070312\n",
      "Epoch 62, Batch 6323, Loss: 160.83807373046875\n",
      "Epoch 62, Batch 6324, Loss: 164.68357849121094\n",
      "Epoch 62, Batch 6325, Loss: 181.0645294189453\n",
      "Epoch 62, Batch 6326, Loss: 162.18577575683594\n",
      "Epoch 62, Batch 6327, Loss: 168.6083221435547\n",
      "Epoch 62, Batch 6328, Loss: 179.98989868164062\n",
      "Epoch 62, Batch 6329, Loss: 170.82598876953125\n",
      "Epoch 62, Batch 6330, Loss: 170.00498962402344\n",
      "Epoch 62, Batch 6331, Loss: 190.069580078125\n",
      "Epoch 62, Batch 6332, Loss: 180.33836364746094\n",
      "Epoch 62, Batch 6333, Loss: 179.2253875732422\n",
      "Epoch 62, Batch 6334, Loss: 180.0352325439453\n",
      "Epoch 62, Batch 6335, Loss: 182.64901733398438\n",
      "Epoch 62, Batch 6336, Loss: 162.5614776611328\n",
      "Epoch 62, Batch 6337, Loss: 203.2423858642578\n",
      "Epoch 62, Batch 6338, Loss: 149.9111785888672\n",
      "Epoch 62, Batch 6339, Loss: 182.80587768554688\n",
      "Epoch 62, Batch 6340, Loss: 164.55517578125\n",
      "Epoch 62, Batch 6341, Loss: 173.45314025878906\n",
      "Epoch 62, Batch 6342, Loss: 184.81044006347656\n",
      "Epoch 62, Batch 6343, Loss: 184.7703857421875\n",
      "Epoch 62, Batch 6344, Loss: 196.0351104736328\n",
      "Epoch 62, Batch 6345, Loss: 176.3866424560547\n",
      "Epoch 62, Batch 6346, Loss: 164.246826171875\n",
      "Epoch 62, Batch 6347, Loss: 170.5796661376953\n",
      "Epoch 62, Batch 6348, Loss: 156.01031494140625\n",
      "Epoch 62, Batch 6349, Loss: 160.7133331298828\n",
      "Epoch 62, Batch 6350, Loss: 191.6432342529297\n",
      "Epoch 62, Batch 6351, Loss: 186.44874572753906\n",
      "Epoch 62, Batch 6352, Loss: 188.0399932861328\n",
      "Epoch 62, Batch 6353, Loss: 179.9069061279297\n",
      "Epoch 62, Batch 6354, Loss: 171.75299072265625\n",
      "Epoch 62, Batch 6355, Loss: 183.2085723876953\n",
      "Epoch 62, Batch 6356, Loss: 159.57525634765625\n",
      "Epoch 62, Batch 6357, Loss: 168.00274658203125\n",
      "Epoch 62, Batch 6358, Loss: 176.0162353515625\n",
      "Epoch 62, Batch 6359, Loss: 186.03465270996094\n",
      "Epoch 62, Batch 6360, Loss: 177.92889404296875\n",
      "Epoch 62, Batch 6361, Loss: 163.05746459960938\n",
      "Epoch 62, Batch 6362, Loss: 174.98492431640625\n",
      "Epoch 62, Batch 6363, Loss: 175.3456573486328\n",
      "Epoch 62, Batch 6364, Loss: 167.4355926513672\n",
      "Epoch 62, Batch 6365, Loss: 178.73944091796875\n",
      "Epoch 62, Batch 6366, Loss: 160.4611358642578\n",
      "Epoch 62, Batch 6367, Loss: 161.44825744628906\n",
      "Epoch 62, Batch 6368, Loss: 180.84889221191406\n",
      "Epoch 62, Batch 6369, Loss: 170.5338134765625\n",
      "Epoch 62, Batch 6370, Loss: 165.65489196777344\n",
      "Epoch 62, Batch 6371, Loss: 140.65585327148438\n",
      "Epoch 62, Batch 6372, Loss: 192.61431884765625\n",
      "Epoch 62, Batch 6373, Loss: 180.5111541748047\n",
      "Epoch 62, Batch 6374, Loss: 173.84619140625\n",
      "Epoch 62, Batch 6375, Loss: 162.54481506347656\n",
      "Epoch 62, Batch 6376, Loss: 159.73680114746094\n",
      "Epoch 62, Batch 6377, Loss: 158.58636474609375\n",
      "Epoch 62, Batch 6378, Loss: 189.14186096191406\n",
      "Epoch 62, Batch 6379, Loss: 174.1125030517578\n",
      "Epoch 62, Batch 6380, Loss: 173.353759765625\n",
      "Epoch 62, Batch 6381, Loss: 171.5401153564453\n",
      "Epoch 62, Batch 6382, Loss: 170.69610595703125\n",
      "Epoch 62, Batch 6383, Loss: 168.72476196289062\n",
      "Epoch 62, Batch 6384, Loss: 182.05557250976562\n",
      "Epoch 62, Batch 6385, Loss: 170.9429473876953\n",
      "Epoch 62, Batch 6386, Loss: 164.67921447753906\n",
      "Epoch 62, Batch 6387, Loss: 181.7462158203125\n",
      "Epoch 62, Batch 6388, Loss: 159.05055236816406\n",
      "Epoch 62, Batch 6389, Loss: 177.07611083984375\n",
      "Epoch 62, Batch 6390, Loss: 182.41030883789062\n",
      "Epoch 62, Batch 6391, Loss: 172.76016235351562\n",
      "Epoch 62, Batch 6392, Loss: 176.93487548828125\n",
      "Epoch 62, Batch 6393, Loss: 189.50502014160156\n",
      "Epoch 62, Batch 6394, Loss: 172.9947967529297\n",
      "Epoch 62, Batch 6395, Loss: 172.25425720214844\n",
      "Epoch 62, Batch 6396, Loss: 176.6685028076172\n",
      "Epoch 62, Batch 6397, Loss: 159.068359375\n",
      "Epoch 62, Batch 6398, Loss: 160.32986450195312\n",
      "Epoch 62, Batch 6399, Loss: 185.12750244140625\n",
      "Epoch 62, Batch 6400, Loss: 164.95993041992188\n",
      "Epoch 62, Batch 6401, Loss: 169.94227600097656\n",
      "Epoch 62, Batch 6402, Loss: 172.86375427246094\n",
      "Epoch 62, Batch 6403, Loss: 170.55259704589844\n",
      "Epoch 62, Batch 6404, Loss: 167.2290802001953\n",
      "Epoch 62, Batch 6405, Loss: 185.44058227539062\n",
      "Epoch 62, Batch 6406, Loss: 171.70750427246094\n",
      "Epoch 62, Batch 6407, Loss: 183.90158081054688\n",
      "Epoch 62, Batch 6408, Loss: 160.16676330566406\n",
      "Epoch 62, Batch 6409, Loss: 183.63282775878906\n",
      "Epoch 62, Batch 6410, Loss: 175.4875946044922\n",
      "Epoch 62, Batch 6411, Loss: 176.01229858398438\n",
      "Epoch 62, Batch 6412, Loss: 176.50421142578125\n",
      "Epoch 62, Batch 6413, Loss: 172.79071044921875\n",
      "Epoch 62, Batch 6414, Loss: 165.74908447265625\n",
      "Epoch 62, Batch 6415, Loss: 172.74119567871094\n",
      "Epoch 62, Batch 6416, Loss: 174.9768524169922\n",
      "Epoch 62, Batch 6417, Loss: 176.86561584472656\n",
      "Epoch 62, Batch 6418, Loss: 164.14370727539062\n",
      "Epoch 62, Batch 6419, Loss: 169.29209899902344\n",
      "Epoch 62, Batch 6420, Loss: 188.13717651367188\n",
      "Epoch 62, Batch 6421, Loss: 171.32464599609375\n",
      "Epoch 62, Batch 6422, Loss: 156.60787963867188\n",
      "Epoch 62, Batch 6423, Loss: 171.23329162597656\n",
      "Epoch 62, Batch 6424, Loss: 169.5854034423828\n",
      "Epoch 62, Batch 6425, Loss: 164.07142639160156\n",
      "Epoch 62, Batch 6426, Loss: 182.26490783691406\n",
      "Epoch 62, Batch 6427, Loss: 174.0699005126953\n",
      "Epoch 62, Batch 6428, Loss: 163.50167846679688\n",
      "Epoch 62, Batch 6429, Loss: 164.77911376953125\n",
      "Epoch 62, Batch 6430, Loss: 183.3442840576172\n",
      "Epoch 62, Batch 6431, Loss: 163.91624450683594\n",
      "Epoch 62, Batch 6432, Loss: 164.09341430664062\n",
      "Epoch 62, Batch 6433, Loss: 198.0664825439453\n",
      "Epoch 62, Batch 6434, Loss: 172.9888916015625\n",
      "Epoch 62, Batch 6435, Loss: 195.0803985595703\n",
      "Epoch 62, Batch 6436, Loss: 182.0015411376953\n",
      "Epoch 62, Batch 6437, Loss: 171.46066284179688\n",
      "Epoch 62, Batch 6438, Loss: 167.19822692871094\n",
      "Epoch 62, Batch 6439, Loss: 169.99879455566406\n",
      "Epoch 62, Batch 6440, Loss: 158.17279052734375\n",
      "Epoch 62, Batch 6441, Loss: 169.2432403564453\n",
      "Epoch 62, Batch 6442, Loss: 192.55252075195312\n",
      "Epoch 62, Batch 6443, Loss: 168.70132446289062\n",
      "Epoch 62, Batch 6444, Loss: 175.98568725585938\n",
      "Epoch 62, Batch 6445, Loss: 167.9205780029297\n",
      "Epoch 62, Batch 6446, Loss: 183.23562622070312\n",
      "Epoch 62, Batch 6447, Loss: 165.4118194580078\n",
      "Epoch 62, Batch 6448, Loss: 172.53468322753906\n",
      "Epoch 62, Batch 6449, Loss: 190.9554443359375\n",
      "Epoch 62, Batch 6450, Loss: 179.283203125\n",
      "Epoch 62, Batch 6451, Loss: 176.7021484375\n",
      "Epoch 62, Batch 6452, Loss: 182.03073120117188\n",
      "Epoch 62, Batch 6453, Loss: 172.62753295898438\n",
      "Epoch 62, Batch 6454, Loss: 169.0805206298828\n",
      "Epoch 62, Batch 6455, Loss: 177.4596405029297\n",
      "Epoch 62, Batch 6456, Loss: 189.51644897460938\n",
      "Epoch 62, Batch 6457, Loss: 167.73602294921875\n",
      "Epoch 62, Batch 6458, Loss: 164.40023803710938\n",
      "Epoch 62, Batch 6459, Loss: 156.77761840820312\n",
      "Epoch 62, Batch 6460, Loss: 156.65188598632812\n",
      "Epoch 62, Batch 6461, Loss: 176.56443786621094\n",
      "Epoch 62, Batch 6462, Loss: 167.77362060546875\n",
      "Epoch 62, Batch 6463, Loss: 177.4112548828125\n",
      "Epoch 62, Batch 6464, Loss: 164.29098510742188\n",
      "Epoch 62, Batch 6465, Loss: 161.6430206298828\n",
      "Epoch 62, Batch 6466, Loss: 180.166748046875\n",
      "Epoch 62, Batch 6467, Loss: 172.04409790039062\n",
      "Epoch 62, Batch 6468, Loss: 171.88565063476562\n",
      "Epoch 62, Batch 6469, Loss: 170.1603546142578\n",
      "Epoch 62, Batch 6470, Loss: 165.24990844726562\n",
      "Epoch 62, Batch 6471, Loss: 187.3148193359375\n",
      "Epoch 62, Batch 6472, Loss: 144.82826232910156\n",
      "Epoch 62, Batch 6473, Loss: 186.05274963378906\n",
      "Epoch 62, Batch 6474, Loss: 170.96109008789062\n",
      "Epoch 62, Batch 6475, Loss: 167.80409240722656\n",
      "Epoch 62, Batch 6476, Loss: 181.898681640625\n",
      "Epoch 62, Batch 6477, Loss: 152.12841796875\n",
      "Epoch 62, Batch 6478, Loss: 173.7954864501953\n",
      "Epoch 62, Batch 6479, Loss: 161.8067169189453\n",
      "Epoch 62, Batch 6480, Loss: 168.60499572753906\n",
      "Epoch 62, Batch 6481, Loss: 179.9765167236328\n",
      "Epoch 62, Batch 6482, Loss: 162.57093811035156\n",
      "Epoch 62, Batch 6483, Loss: 177.03585815429688\n",
      "Epoch 62, Batch 6484, Loss: 169.79847717285156\n",
      "Epoch 62, Batch 6485, Loss: 176.02450561523438\n",
      "Epoch 62, Batch 6486, Loss: 164.59463500976562\n",
      "Epoch 62, Batch 6487, Loss: 188.80934143066406\n",
      "Epoch 62, Batch 6488, Loss: 171.46107482910156\n",
      "Epoch 62, Batch 6489, Loss: 181.44847106933594\n",
      "Epoch 62, Batch 6490, Loss: 174.8761444091797\n",
      "Epoch 62, Batch 6491, Loss: 172.64764404296875\n",
      "Epoch 62, Batch 6492, Loss: 173.09326171875\n",
      "Epoch 62, Batch 6493, Loss: 166.48577880859375\n",
      "Epoch 62, Batch 6494, Loss: 170.32989501953125\n",
      "Epoch 62, Batch 6495, Loss: 176.2090301513672\n",
      "Epoch 62, Batch 6496, Loss: 176.34744262695312\n",
      "Epoch 62, Batch 6497, Loss: 181.80465698242188\n",
      "Epoch 62, Batch 6498, Loss: 162.0697021484375\n",
      "Epoch 62, Batch 6499, Loss: 186.2821807861328\n",
      "Epoch 62, Batch 6500, Loss: 189.56651306152344\n",
      "Epoch 62, Batch 6501, Loss: 174.80169677734375\n",
      "Epoch 62, Batch 6502, Loss: 184.1826934814453\n",
      "Epoch 62, Batch 6503, Loss: 177.84254455566406\n",
      "Epoch 62, Batch 6504, Loss: 152.8079071044922\n",
      "Epoch 62, Batch 6505, Loss: 177.1509552001953\n",
      "Epoch 62, Batch 6506, Loss: 185.87939453125\n",
      "Epoch 62, Batch 6507, Loss: 185.05313110351562\n",
      "Epoch 62, Batch 6508, Loss: 160.2196807861328\n",
      "Epoch 62, Batch 6509, Loss: 186.72927856445312\n",
      "Epoch 62, Batch 6510, Loss: 159.89102172851562\n",
      "Epoch 62, Batch 6511, Loss: 187.91310119628906\n",
      "Epoch 62, Batch 6512, Loss: 168.86990356445312\n",
      "Epoch 62, Batch 6513, Loss: 181.94717407226562\n",
      "Epoch 62, Batch 6514, Loss: 182.12965393066406\n",
      "Epoch 62, Batch 6515, Loss: 180.15634155273438\n",
      "Epoch 62, Batch 6516, Loss: 174.74449157714844\n",
      "Epoch 62, Batch 6517, Loss: 181.1116485595703\n",
      "Epoch 62, Batch 6518, Loss: 150.9781494140625\n",
      "Epoch 62, Batch 6519, Loss: 150.30638122558594\n",
      "Epoch 62, Batch 6520, Loss: 174.30348205566406\n",
      "Epoch 62, Batch 6521, Loss: 188.54498291015625\n",
      "Epoch 62, Batch 6522, Loss: 168.1532440185547\n",
      "Epoch 62, Batch 6523, Loss: 178.83193969726562\n",
      "Epoch 62, Batch 6524, Loss: 185.20361328125\n",
      "Epoch 62, Batch 6525, Loss: 158.96270751953125\n",
      "Epoch 62, Batch 6526, Loss: 179.00869750976562\n",
      "Epoch 62, Batch 6527, Loss: 190.889404296875\n",
      "Epoch 62, Batch 6528, Loss: 167.11280822753906\n",
      "Epoch 62, Batch 6529, Loss: 156.60569763183594\n",
      "Epoch 62, Batch 6530, Loss: 170.78005981445312\n",
      "Epoch 62, Batch 6531, Loss: 170.79129028320312\n",
      "Epoch 62, Batch 6532, Loss: 165.32810974121094\n",
      "Epoch 62, Batch 6533, Loss: 183.0255889892578\n",
      "Epoch 62, Batch 6534, Loss: 163.9143524169922\n",
      "Epoch 62, Batch 6535, Loss: 158.15721130371094\n",
      "Epoch 62, Batch 6536, Loss: 158.82081604003906\n",
      "Epoch 62, Batch 6537, Loss: 156.3560791015625\n",
      "Epoch 62, Batch 6538, Loss: 161.81312561035156\n",
      "Epoch 62, Batch 6539, Loss: 171.58082580566406\n",
      "Epoch 62, Batch 6540, Loss: 170.5636444091797\n",
      "Epoch 62, Batch 6541, Loss: 180.0545196533203\n",
      "Epoch 62, Batch 6542, Loss: 167.0418701171875\n",
      "Epoch 62, Batch 6543, Loss: 162.32528686523438\n",
      "Epoch 62, Batch 6544, Loss: 187.08551025390625\n",
      "Epoch 62, Batch 6545, Loss: 176.5845489501953\n",
      "Epoch 62, Batch 6546, Loss: 171.09791564941406\n",
      "Epoch 62, Batch 6547, Loss: 179.66807556152344\n",
      "Epoch 62, Batch 6548, Loss: 182.5876007080078\n",
      "Epoch 62, Batch 6549, Loss: 174.4478759765625\n",
      "Epoch 62, Batch 6550, Loss: 168.6515350341797\n",
      "Epoch 62, Batch 6551, Loss: 184.74932861328125\n",
      "Epoch 62, Batch 6552, Loss: 162.77565002441406\n",
      "Epoch 62, Batch 6553, Loss: 164.3673095703125\n",
      "Epoch 62, Batch 6554, Loss: 171.00238037109375\n",
      "Epoch 62, Batch 6555, Loss: 149.0188751220703\n",
      "Epoch 62, Batch 6556, Loss: 170.53060913085938\n",
      "Epoch 62, Batch 6557, Loss: 179.49208068847656\n",
      "Epoch 62, Batch 6558, Loss: 190.002685546875\n",
      "Epoch 62, Batch 6559, Loss: 159.82186889648438\n",
      "Epoch 62, Batch 6560, Loss: 168.95745849609375\n",
      "Epoch 62, Batch 6561, Loss: 183.0698699951172\n",
      "Epoch 62, Batch 6562, Loss: 174.01487731933594\n",
      "Epoch 62, Batch 6563, Loss: 171.70570373535156\n",
      "Epoch 62, Batch 6564, Loss: 178.57412719726562\n",
      "Epoch 62, Batch 6565, Loss: 186.4114990234375\n",
      "Epoch 62, Batch 6566, Loss: 185.08416748046875\n",
      "Epoch 62, Batch 6567, Loss: 177.70761108398438\n",
      "Epoch 62, Batch 6568, Loss: 151.52857971191406\n",
      "Epoch 62, Batch 6569, Loss: 168.97740173339844\n",
      "Epoch 62, Batch 6570, Loss: 166.83583068847656\n",
      "Epoch 62, Batch 6571, Loss: 166.93650817871094\n",
      "Epoch 62, Batch 6572, Loss: 167.6322784423828\n",
      "Epoch 62, Batch 6573, Loss: 156.0667266845703\n",
      "Epoch 62, Batch 6574, Loss: 177.32562255859375\n",
      "Epoch 62, Batch 6575, Loss: 170.0548858642578\n",
      "Epoch 62, Batch 6576, Loss: 159.12330627441406\n",
      "Epoch 62, Batch 6577, Loss: 175.56471252441406\n",
      "Epoch 62, Batch 6578, Loss: 171.41329956054688\n",
      "Epoch 62, Batch 6579, Loss: 171.9727020263672\n",
      "Epoch 62, Batch 6580, Loss: 179.82196044921875\n",
      "Epoch 62, Batch 6581, Loss: 172.26034545898438\n",
      "Epoch 62, Batch 6582, Loss: 169.2244873046875\n",
      "Epoch 62, Batch 6583, Loss: 172.6371612548828\n",
      "Epoch 62, Batch 6584, Loss: 172.5295867919922\n",
      "Epoch 62, Batch 6585, Loss: 179.54183959960938\n",
      "Epoch 62, Batch 6586, Loss: 160.47740173339844\n",
      "Epoch 62, Batch 6587, Loss: 169.96568298339844\n",
      "Epoch 62, Batch 6588, Loss: 180.5860595703125\n",
      "Epoch 62, Batch 6589, Loss: 164.0669403076172\n",
      "Epoch 62, Batch 6590, Loss: 177.20594787597656\n",
      "Epoch 62, Batch 6591, Loss: 179.4112548828125\n",
      "Epoch 62, Batch 6592, Loss: 190.25668334960938\n",
      "Epoch 62, Batch 6593, Loss: 176.84210205078125\n",
      "Epoch 62, Batch 6594, Loss: 174.41111755371094\n",
      "Epoch 62, Batch 6595, Loss: 154.82989501953125\n",
      "Epoch 62, Batch 6596, Loss: 155.84877014160156\n",
      "Epoch 62, Batch 6597, Loss: 177.9888458251953\n",
      "Epoch 62, Batch 6598, Loss: 156.32571411132812\n",
      "Epoch 62, Batch 6599, Loss: 173.13516235351562\n",
      "Epoch 62, Batch 6600, Loss: 163.49673461914062\n",
      "Epoch 62, Batch 6601, Loss: 159.6769561767578\n",
      "Epoch 62, Batch 6602, Loss: 188.5777130126953\n",
      "Epoch 62, Batch 6603, Loss: 180.83157348632812\n",
      "Epoch 62, Batch 6604, Loss: 184.00865173339844\n",
      "Epoch 62, Batch 6605, Loss: 163.4553680419922\n",
      "Epoch 62, Batch 6606, Loss: 169.6996612548828\n",
      "Epoch 62, Batch 6607, Loss: 174.33969116210938\n",
      "Epoch 62, Batch 6608, Loss: 183.91819763183594\n",
      "Epoch 62, Batch 6609, Loss: 167.72352600097656\n",
      "Epoch 62, Batch 6610, Loss: 184.62635803222656\n",
      "Epoch 62, Batch 6611, Loss: 159.08209228515625\n",
      "Epoch 62, Batch 6612, Loss: 177.454345703125\n",
      "Epoch 62, Batch 6613, Loss: 186.5465545654297\n",
      "Epoch 62, Batch 6614, Loss: 156.94009399414062\n",
      "Epoch 62, Batch 6615, Loss: 176.24513244628906\n",
      "Epoch 62, Batch 6616, Loss: 171.19235229492188\n",
      "Epoch 62, Batch 6617, Loss: 196.02476501464844\n",
      "Epoch 62, Batch 6618, Loss: 169.9512939453125\n",
      "Epoch 62, Batch 6619, Loss: 171.38888549804688\n",
      "Epoch 62, Batch 6620, Loss: 170.5851593017578\n",
      "Epoch 62, Batch 6621, Loss: 159.05723571777344\n",
      "Epoch 62, Batch 6622, Loss: 182.87986755371094\n",
      "Epoch 62, Batch 6623, Loss: 171.39215087890625\n",
      "Epoch 62, Batch 6624, Loss: 188.3564910888672\n",
      "Epoch 62, Batch 6625, Loss: 155.34022521972656\n",
      "Epoch 62, Batch 6626, Loss: 172.37571716308594\n",
      "Epoch 62, Batch 6627, Loss: 181.8829803466797\n",
      "Epoch 62, Batch 6628, Loss: 171.3575439453125\n",
      "Epoch 62, Batch 6629, Loss: 169.00372314453125\n",
      "Epoch 62, Batch 6630, Loss: 176.9373016357422\n",
      "Epoch 62, Batch 6631, Loss: 164.8980712890625\n",
      "Epoch 62, Batch 6632, Loss: 178.59190368652344\n",
      "Epoch 62, Batch 6633, Loss: 166.8699188232422\n",
      "Epoch 62, Batch 6634, Loss: 171.35923767089844\n",
      "Epoch 62, Batch 6635, Loss: 164.78323364257812\n",
      "Epoch 62, Batch 6636, Loss: 199.2554168701172\n",
      "Epoch 62, Batch 6637, Loss: 175.3270721435547\n",
      "Epoch 62, Batch 6638, Loss: 183.21681213378906\n",
      "Epoch 62, Batch 6639, Loss: 178.05502319335938\n",
      "Epoch 62, Batch 6640, Loss: 172.90240478515625\n",
      "Epoch 62, Batch 6641, Loss: 156.5790252685547\n",
      "Epoch 62, Batch 6642, Loss: 159.69544982910156\n",
      "Epoch 62, Batch 6643, Loss: 183.61163330078125\n",
      "Epoch 62, Batch 6644, Loss: 170.9259033203125\n",
      "Epoch 62, Batch 6645, Loss: 172.8602294921875\n",
      "Epoch 62, Batch 6646, Loss: 180.1175537109375\n",
      "Epoch 62, Batch 6647, Loss: 187.3666534423828\n",
      "Epoch 62, Batch 6648, Loss: 177.44784545898438\n",
      "Epoch 62, Batch 6649, Loss: 167.80287170410156\n",
      "Epoch 62, Batch 6650, Loss: 165.826904296875\n",
      "Epoch 62, Batch 6651, Loss: 171.13348388671875\n",
      "Epoch 62, Batch 6652, Loss: 162.07110595703125\n",
      "Epoch 62, Batch 6653, Loss: 176.54156494140625\n",
      "Epoch 62, Batch 6654, Loss: 198.97445678710938\n",
      "Epoch 62, Batch 6655, Loss: 191.12315368652344\n",
      "Epoch 62, Batch 6656, Loss: 180.20887756347656\n",
      "Epoch 62, Batch 6657, Loss: 180.95106506347656\n",
      "Epoch 62, Batch 6658, Loss: 148.1551513671875\n",
      "Epoch 62, Batch 6659, Loss: 155.73870849609375\n",
      "Epoch 62, Batch 6660, Loss: 148.0207061767578\n",
      "Epoch 62, Batch 6661, Loss: 177.09304809570312\n",
      "Epoch 62, Batch 6662, Loss: 159.25828552246094\n",
      "Epoch 62, Batch 6663, Loss: 178.0955047607422\n",
      "Epoch 62, Batch 6664, Loss: 177.98863220214844\n",
      "Epoch 62, Batch 6665, Loss: 179.41566467285156\n",
      "Epoch 62, Batch 6666, Loss: 158.07574462890625\n",
      "Epoch 62, Batch 6667, Loss: 186.9295654296875\n",
      "Epoch 62, Batch 6668, Loss: 170.9173583984375\n",
      "Epoch 62, Batch 6669, Loss: 169.26121520996094\n",
      "Epoch 62, Batch 6670, Loss: 173.8659210205078\n",
      "Epoch 62, Batch 6671, Loss: 170.06983947753906\n",
      "Epoch 62, Batch 6672, Loss: 180.49110412597656\n",
      "Epoch 62, Batch 6673, Loss: 183.63824462890625\n",
      "Epoch 62, Batch 6674, Loss: 183.34429931640625\n",
      "Epoch 62, Batch 6675, Loss: 179.7064971923828\n",
      "Epoch 62, Batch 6676, Loss: 171.93162536621094\n",
      "Epoch 62, Batch 6677, Loss: 181.25479125976562\n",
      "Epoch 62, Batch 6678, Loss: 167.38113403320312\n",
      "Epoch 62, Batch 6679, Loss: 177.2532958984375\n",
      "Epoch 62, Batch 6680, Loss: 156.42971801757812\n",
      "Epoch 62, Batch 6681, Loss: 165.64207458496094\n",
      "Epoch 62, Batch 6682, Loss: 158.2984619140625\n",
      "Epoch 62, Batch 6683, Loss: 176.0531463623047\n",
      "Epoch 62, Batch 6684, Loss: 169.9890594482422\n",
      "Epoch 62, Batch 6685, Loss: 166.86549377441406\n",
      "Epoch 62, Batch 6686, Loss: 172.50831604003906\n",
      "Epoch 62, Batch 6687, Loss: 174.28147888183594\n",
      "Epoch 62, Batch 6688, Loss: 169.00711059570312\n",
      "Epoch 62, Batch 6689, Loss: 165.23123168945312\n",
      "Epoch 62, Batch 6690, Loss: 200.42103576660156\n",
      "Epoch 62, Batch 6691, Loss: 186.82765197753906\n",
      "Epoch 62, Batch 6692, Loss: 180.4495391845703\n",
      "Epoch 62, Batch 6693, Loss: 186.599365234375\n",
      "Epoch 62, Batch 6694, Loss: 168.5108184814453\n",
      "Epoch 62, Batch 6695, Loss: 186.3170623779297\n",
      "Epoch 62, Batch 6696, Loss: 195.99884033203125\n",
      "Epoch 62, Batch 6697, Loss: 164.45994567871094\n",
      "Epoch 62, Batch 6698, Loss: 199.21820068359375\n",
      "Epoch 62, Batch 6699, Loss: 179.14596557617188\n",
      "Epoch 62, Batch 6700, Loss: 164.75247192382812\n",
      "Epoch 62, Batch 6701, Loss: 169.5167999267578\n",
      "Epoch 62, Batch 6702, Loss: 163.2933807373047\n",
      "Epoch 62, Batch 6703, Loss: 182.1546173095703\n",
      "Epoch 62, Batch 6704, Loss: 164.44456481933594\n",
      "Epoch 62, Batch 6705, Loss: 168.53163146972656\n",
      "Epoch 62, Batch 6706, Loss: 159.1301727294922\n",
      "Epoch 62, Batch 6707, Loss: 170.82591247558594\n",
      "Epoch 62, Batch 6708, Loss: 181.84823608398438\n",
      "Epoch 62, Batch 6709, Loss: 190.916259765625\n",
      "Epoch 62, Batch 6710, Loss: 171.9722137451172\n",
      "Epoch 62, Batch 6711, Loss: 151.34475708007812\n",
      "Epoch 62, Batch 6712, Loss: 158.93325805664062\n",
      "Epoch 62, Batch 6713, Loss: 182.704833984375\n",
      "Epoch 62, Batch 6714, Loss: 182.2967071533203\n",
      "Epoch 62, Batch 6715, Loss: 187.3722686767578\n",
      "Epoch 62, Batch 6716, Loss: 175.07041931152344\n",
      "Epoch 62, Batch 6717, Loss: 169.59030151367188\n",
      "Epoch 62, Batch 6718, Loss: 176.7916717529297\n",
      "Epoch 62, Batch 6719, Loss: 177.11236572265625\n",
      "Epoch 62, Batch 6720, Loss: 172.8651580810547\n",
      "Epoch 62, Batch 6721, Loss: 179.41668701171875\n",
      "Epoch 62, Batch 6722, Loss: 171.02383422851562\n",
      "Epoch 62, Batch 6723, Loss: 183.99827575683594\n",
      "Epoch 62, Batch 6724, Loss: 169.23516845703125\n",
      "Epoch 62, Batch 6725, Loss: 184.5215301513672\n",
      "Epoch 62, Batch 6726, Loss: 179.26046752929688\n",
      "Epoch 62, Batch 6727, Loss: 188.73190307617188\n",
      "Epoch 62, Batch 6728, Loss: 176.98812866210938\n",
      "Epoch 62, Batch 6729, Loss: 186.673828125\n",
      "Epoch 62, Batch 6730, Loss: 173.04730224609375\n",
      "Epoch 62, Batch 6731, Loss: 177.22288513183594\n",
      "Epoch 62, Batch 6732, Loss: 178.91537475585938\n",
      "Epoch 62, Batch 6733, Loss: 189.55650329589844\n",
      "Epoch 62, Batch 6734, Loss: 167.88827514648438\n",
      "Epoch 62, Batch 6735, Loss: 175.76609802246094\n",
      "Epoch 62, Batch 6736, Loss: 158.44415283203125\n",
      "Epoch 62, Batch 6737, Loss: 166.2438507080078\n",
      "Epoch 62, Batch 6738, Loss: 166.73831176757812\n",
      "Epoch 62, Batch 6739, Loss: 170.52586364746094\n",
      "Epoch 62, Batch 6740, Loss: 170.29417419433594\n",
      "Epoch 62, Batch 6741, Loss: 171.72152709960938\n",
      "Epoch 62, Batch 6742, Loss: 170.60302734375\n",
      "Epoch 62, Batch 6743, Loss: 179.2052001953125\n",
      "Epoch 62, Batch 6744, Loss: 187.32601928710938\n",
      "Epoch 62, Batch 6745, Loss: 191.50836181640625\n",
      "Epoch 62, Batch 6746, Loss: 168.47140502929688\n",
      "Epoch 62, Batch 6747, Loss: 176.21791076660156\n",
      "Epoch 62, Batch 6748, Loss: 195.99169921875\n",
      "Epoch 62, Batch 6749, Loss: 180.544677734375\n",
      "Epoch 62, Batch 6750, Loss: 162.09323120117188\n",
      "Epoch 62, Batch 6751, Loss: 164.82371520996094\n",
      "Epoch 62, Batch 6752, Loss: 182.96371459960938\n",
      "Epoch 62, Batch 6753, Loss: 188.45736694335938\n",
      "Epoch 62, Batch 6754, Loss: 158.53201293945312\n",
      "Epoch 62, Batch 6755, Loss: 186.52447509765625\n",
      "Epoch 62, Batch 6756, Loss: 156.9171905517578\n",
      "Epoch 62, Batch 6757, Loss: 193.54554748535156\n",
      "Epoch 62, Batch 6758, Loss: 166.24203491210938\n",
      "Epoch 62, Batch 6759, Loss: 170.74423217773438\n",
      "Epoch 62, Batch 6760, Loss: 179.0852508544922\n",
      "Epoch 62, Batch 6761, Loss: 180.1830596923828\n",
      "Epoch 62, Batch 6762, Loss: 175.4822998046875\n",
      "Epoch 62, Batch 6763, Loss: 180.9886474609375\n",
      "Epoch 62, Batch 6764, Loss: 177.32818603515625\n",
      "Epoch 62, Batch 6765, Loss: 184.00247192382812\n",
      "Epoch 62, Batch 6766, Loss: 173.98593139648438\n",
      "Epoch 62, Batch 6767, Loss: 181.25975036621094\n",
      "Epoch 62, Batch 6768, Loss: 161.15846252441406\n",
      "Epoch 62, Batch 6769, Loss: 178.66366577148438\n",
      "Epoch 62, Batch 6770, Loss: 172.4707489013672\n",
      "Epoch 62, Batch 6771, Loss: 177.3629608154297\n",
      "Epoch 62, Batch 6772, Loss: 170.97543334960938\n",
      "Epoch 62, Batch 6773, Loss: 161.6611328125\n",
      "Epoch 62, Batch 6774, Loss: 168.6544647216797\n",
      "Epoch 62, Batch 6775, Loss: 170.6658935546875\n",
      "Epoch 62, Batch 6776, Loss: 180.7661895751953\n",
      "Epoch 62, Batch 6777, Loss: 166.34815979003906\n",
      "Epoch 62, Batch 6778, Loss: 166.6994171142578\n",
      "Epoch 62, Batch 6779, Loss: 157.6373291015625\n",
      "Epoch 62, Batch 6780, Loss: 182.531005859375\n",
      "Epoch 62, Batch 6781, Loss: 188.60345458984375\n",
      "Epoch 62, Batch 6782, Loss: 161.47425842285156\n",
      "Epoch 62, Batch 6783, Loss: 180.13430786132812\n",
      "Epoch 62, Batch 6784, Loss: 168.5591583251953\n",
      "Epoch 62, Batch 6785, Loss: 171.55194091796875\n",
      "Epoch 62, Batch 6786, Loss: 179.9168243408203\n",
      "Epoch 62, Batch 6787, Loss: 163.6307830810547\n",
      "Epoch 62, Batch 6788, Loss: 172.86070251464844\n",
      "Epoch 62, Batch 6789, Loss: 168.47784423828125\n",
      "Epoch 62, Batch 6790, Loss: 170.62110900878906\n",
      "Epoch 62, Batch 6791, Loss: 176.84364318847656\n",
      "Epoch 62, Batch 6792, Loss: 166.7581787109375\n",
      "Epoch 62, Batch 6793, Loss: 168.33914184570312\n",
      "Epoch 62, Batch 6794, Loss: 184.23080444335938\n",
      "Epoch 62, Batch 6795, Loss: 198.79664611816406\n",
      "Epoch 62, Batch 6796, Loss: 170.151611328125\n",
      "Epoch 62, Batch 6797, Loss: 161.30120849609375\n",
      "Epoch 62, Batch 6798, Loss: 189.5968017578125\n",
      "Epoch 62, Batch 6799, Loss: 165.79922485351562\n",
      "Epoch 62, Batch 6800, Loss: 182.4938507080078\n",
      "Epoch 62, Batch 6801, Loss: 167.9059295654297\n",
      "Epoch 62, Batch 6802, Loss: 176.16383361816406\n",
      "Epoch 62, Batch 6803, Loss: 167.8035430908203\n",
      "Epoch 62, Batch 6804, Loss: 182.64073181152344\n",
      "Epoch 62, Batch 6805, Loss: 176.50376892089844\n",
      "Epoch 62, Batch 6806, Loss: 165.53294372558594\n",
      "Epoch 62, Batch 6807, Loss: 172.33282470703125\n",
      "Epoch 62, Batch 6808, Loss: 174.32528686523438\n",
      "Epoch 62, Batch 6809, Loss: 168.96466064453125\n",
      "Epoch 62, Batch 6810, Loss: 171.0557403564453\n",
      "Epoch 62, Batch 6811, Loss: 162.821533203125\n",
      "Epoch 62, Batch 6812, Loss: 186.88577270507812\n",
      "Epoch 62, Batch 6813, Loss: 188.81988525390625\n",
      "Epoch 62, Batch 6814, Loss: 161.15542602539062\n",
      "Epoch 62, Batch 6815, Loss: 174.9266357421875\n",
      "Epoch 62, Batch 6816, Loss: 185.6507568359375\n",
      "Epoch 62, Batch 6817, Loss: 170.96542358398438\n",
      "Epoch 62, Batch 6818, Loss: 194.0979766845703\n",
      "Epoch 62, Batch 6819, Loss: 153.79881286621094\n",
      "Epoch 62, Batch 6820, Loss: 182.31463623046875\n",
      "Epoch 62, Batch 6821, Loss: 164.90289306640625\n",
      "Epoch 62, Batch 6822, Loss: 161.02381896972656\n",
      "Epoch 62, Batch 6823, Loss: 161.815673828125\n",
      "Epoch 62, Batch 6824, Loss: 172.80148315429688\n",
      "Epoch 62, Batch 6825, Loss: 187.3455047607422\n",
      "Epoch 62, Batch 6826, Loss: 181.26539611816406\n",
      "Epoch 62, Batch 6827, Loss: 161.92042541503906\n",
      "Epoch 62, Batch 6828, Loss: 172.71633911132812\n",
      "Epoch 62, Batch 6829, Loss: 180.3740692138672\n",
      "Epoch 62, Batch 6830, Loss: 171.0847930908203\n",
      "Epoch 62, Batch 6831, Loss: 166.53407287597656\n",
      "Epoch 62, Batch 6832, Loss: 178.7586669921875\n",
      "Epoch 62, Batch 6833, Loss: 170.66757202148438\n",
      "Epoch 62, Batch 6834, Loss: 149.62478637695312\n",
      "Epoch 62, Batch 6835, Loss: 173.51010131835938\n",
      "Epoch 62, Batch 6836, Loss: 161.7225341796875\n",
      "Epoch 62, Batch 6837, Loss: 165.72364807128906\n",
      "Epoch 62, Batch 6838, Loss: 159.5358428955078\n",
      "Epoch 62, Batch 6839, Loss: 156.5218505859375\n",
      "Epoch 62, Batch 6840, Loss: 183.2437286376953\n",
      "Epoch 62, Batch 6841, Loss: 189.58230590820312\n",
      "Epoch 62, Batch 6842, Loss: 164.81124877929688\n",
      "Epoch 62, Batch 6843, Loss: 166.3280029296875\n",
      "Epoch 62, Batch 6844, Loss: 182.5180206298828\n",
      "Epoch 62, Batch 6845, Loss: 177.2449951171875\n",
      "Epoch 62, Batch 6846, Loss: 175.96487426757812\n",
      "Epoch 62, Batch 6847, Loss: 171.59530639648438\n",
      "Epoch 62, Batch 6848, Loss: 187.30685424804688\n",
      "Epoch 62, Batch 6849, Loss: 158.09075927734375\n",
      "Epoch 62, Batch 6850, Loss: 157.45223999023438\n",
      "Epoch 62, Batch 6851, Loss: 183.7796173095703\n",
      "Epoch 62, Batch 6852, Loss: 183.4825439453125\n",
      "Epoch 62, Batch 6853, Loss: 160.8473358154297\n",
      "Epoch 62, Batch 6854, Loss: 168.7099609375\n",
      "Epoch 62, Batch 6855, Loss: 172.24288940429688\n",
      "Epoch 62, Batch 6856, Loss: 157.31687927246094\n",
      "Epoch 62, Batch 6857, Loss: 190.665771484375\n",
      "Epoch 62, Batch 6858, Loss: 159.53085327148438\n",
      "Epoch 62, Batch 6859, Loss: 175.40037536621094\n",
      "Epoch 62, Batch 6860, Loss: 177.70970153808594\n",
      "Epoch 62, Batch 6861, Loss: 154.74578857421875\n",
      "Epoch 62, Batch 6862, Loss: 170.7743682861328\n",
      "Epoch 62, Batch 6863, Loss: 195.636962890625\n",
      "Epoch 62, Batch 6864, Loss: 188.53094482421875\n",
      "Epoch 62, Batch 6865, Loss: 183.39515686035156\n",
      "Epoch 62, Batch 6866, Loss: 158.84071350097656\n",
      "Epoch 62, Batch 6867, Loss: 177.3953857421875\n",
      "Epoch 62, Batch 6868, Loss: 188.75123596191406\n",
      "Epoch 62, Batch 6869, Loss: 167.23907470703125\n",
      "Epoch 62, Batch 6870, Loss: 188.74269104003906\n",
      "Epoch 62, Batch 6871, Loss: 176.3751678466797\n",
      "Epoch 62, Batch 6872, Loss: 191.02581787109375\n",
      "Epoch 62, Batch 6873, Loss: 158.23599243164062\n",
      "Epoch 62, Batch 6874, Loss: 160.07452392578125\n",
      "Epoch 62, Batch 6875, Loss: 190.46542358398438\n",
      "Epoch 62, Batch 6876, Loss: 182.82403564453125\n",
      "Epoch 62, Batch 6877, Loss: 195.81211853027344\n",
      "Epoch 62, Batch 6878, Loss: 182.2562255859375\n",
      "Epoch 62, Batch 6879, Loss: 167.68411254882812\n",
      "Epoch 62, Batch 6880, Loss: 170.9671173095703\n",
      "Epoch 62, Batch 6881, Loss: 189.7613983154297\n",
      "Epoch 62, Batch 6882, Loss: 154.12351989746094\n",
      "Epoch 62, Batch 6883, Loss: 167.62496948242188\n",
      "Epoch 62, Batch 6884, Loss: 176.07183837890625\n",
      "Epoch 62, Batch 6885, Loss: 182.4726104736328\n",
      "Epoch 62, Batch 6886, Loss: 151.64466857910156\n",
      "Epoch 62, Batch 6887, Loss: 182.8443603515625\n",
      "Epoch 62, Batch 6888, Loss: 167.6753692626953\n",
      "Epoch 62, Batch 6889, Loss: 170.63722229003906\n",
      "Epoch 62, Batch 6890, Loss: 181.85182189941406\n",
      "Epoch 62, Batch 6891, Loss: 159.31796264648438\n",
      "Epoch 62, Batch 6892, Loss: 176.8389892578125\n",
      "Epoch 62, Batch 6893, Loss: 168.36659240722656\n",
      "Epoch 62, Batch 6894, Loss: 171.22805786132812\n",
      "Epoch 62, Batch 6895, Loss: 179.35635375976562\n",
      "Epoch 62, Batch 6896, Loss: 176.32516479492188\n",
      "Epoch 62, Batch 6897, Loss: 166.73489379882812\n",
      "Epoch 62, Batch 6898, Loss: 167.94639587402344\n",
      "Epoch 62, Batch 6899, Loss: 172.8723907470703\n",
      "Epoch 62, Batch 6900, Loss: 168.19984436035156\n",
      "Epoch 62, Batch 6901, Loss: 184.57199096679688\n",
      "Epoch 62, Batch 6902, Loss: 165.21080017089844\n",
      "Epoch 62, Batch 6903, Loss: 189.42796325683594\n",
      "Epoch 62, Batch 6904, Loss: 170.12875366210938\n",
      "Epoch 62, Batch 6905, Loss: 169.7086944580078\n",
      "Epoch 62, Batch 6906, Loss: 189.142578125\n",
      "Epoch 62, Batch 6907, Loss: 179.09371948242188\n",
      "Epoch 62, Batch 6908, Loss: 182.91720581054688\n",
      "Epoch 62, Batch 6909, Loss: 169.39593505859375\n",
      "Epoch 62, Batch 6910, Loss: 160.63436889648438\n",
      "Epoch 62, Batch 6911, Loss: 171.07632446289062\n",
      "Epoch 62, Batch 6912, Loss: 176.25538635253906\n",
      "Epoch 62, Batch 6913, Loss: 164.4220733642578\n",
      "Epoch 62, Batch 6914, Loss: 171.23233032226562\n",
      "Epoch 62, Batch 6915, Loss: 160.52503967285156\n",
      "Epoch 62, Batch 6916, Loss: 166.66917419433594\n",
      "Epoch 62, Batch 6917, Loss: 182.66522216796875\n",
      "Epoch 62, Batch 6918, Loss: 175.45175170898438\n",
      "Epoch 62, Batch 6919, Loss: 165.5569305419922\n",
      "Epoch 62, Batch 6920, Loss: 174.4181671142578\n",
      "Epoch 62, Batch 6921, Loss: 176.03195190429688\n",
      "Epoch 62, Batch 6922, Loss: 179.387939453125\n",
      "Epoch 62, Batch 6923, Loss: 179.71066284179688\n",
      "Epoch 62, Batch 6924, Loss: 180.61251831054688\n",
      "Epoch 62, Batch 6925, Loss: 156.94381713867188\n",
      "Epoch 62, Batch 6926, Loss: 157.00286865234375\n",
      "Epoch 62, Batch 6927, Loss: 186.4063720703125\n",
      "Epoch 62, Batch 6928, Loss: 182.75730895996094\n",
      "Epoch 62, Batch 6929, Loss: 162.7667694091797\n",
      "Epoch 62, Batch 6930, Loss: 174.67103576660156\n",
      "Epoch 62, Batch 6931, Loss: 171.0933380126953\n",
      "Epoch 62, Batch 6932, Loss: 163.73834228515625\n",
      "Epoch 62, Batch 6933, Loss: 168.66217041015625\n",
      "Epoch 62, Batch 6934, Loss: 184.26483154296875\n",
      "Epoch 62, Batch 6935, Loss: 183.82290649414062\n",
      "Epoch 62, Batch 6936, Loss: 177.1187286376953\n",
      "Epoch 62, Batch 6937, Loss: 167.68356323242188\n",
      "Epoch 62, Batch 6938, Loss: 157.37847900390625\n",
      "Epoch 62, Batch 6939, Loss: 182.64088439941406\n",
      "Epoch 62, Batch 6940, Loss: 177.73611450195312\n",
      "Epoch 62, Batch 6941, Loss: 178.6470184326172\n",
      "Epoch 62, Batch 6942, Loss: 157.0768280029297\n",
      "Epoch 62, Batch 6943, Loss: 157.06466674804688\n",
      "Epoch 62, Batch 6944, Loss: 174.84552001953125\n",
      "Epoch 62, Batch 6945, Loss: 177.57847595214844\n",
      "Epoch 62, Batch 6946, Loss: 177.16265869140625\n",
      "Epoch 62, Batch 6947, Loss: 189.66921997070312\n",
      "Epoch 62, Batch 6948, Loss: 180.4719696044922\n",
      "Epoch 62, Batch 6949, Loss: 178.9734649658203\n",
      "Epoch 62, Batch 6950, Loss: 169.35598754882812\n",
      "Epoch 62, Batch 6951, Loss: 192.34971618652344\n",
      "Epoch 62, Batch 6952, Loss: 183.5868377685547\n",
      "Epoch 62, Batch 6953, Loss: 162.58470153808594\n",
      "Epoch 62, Batch 6954, Loss: 176.61618041992188\n",
      "Epoch 62, Batch 6955, Loss: 170.3488006591797\n",
      "Epoch 62, Batch 6956, Loss: 181.41775512695312\n",
      "Epoch 62, Batch 6957, Loss: 162.0955352783203\n",
      "Epoch 62, Batch 6958, Loss: 177.6843719482422\n",
      "Epoch 62, Batch 6959, Loss: 169.22007751464844\n",
      "Epoch 62, Batch 6960, Loss: 192.59854125976562\n",
      "Epoch 62, Batch 6961, Loss: 166.70355224609375\n",
      "Epoch 62, Batch 6962, Loss: 168.4933624267578\n",
      "Epoch 62, Batch 6963, Loss: 181.3904266357422\n",
      "Epoch 62, Batch 6964, Loss: 163.24375915527344\n",
      "Epoch 62, Batch 6965, Loss: 162.73704528808594\n",
      "Epoch 62, Batch 6966, Loss: 160.3754425048828\n",
      "Epoch 62, Batch 6967, Loss: 161.24444580078125\n",
      "Epoch 62, Batch 6968, Loss: 173.33297729492188\n",
      "Epoch 62, Batch 6969, Loss: 180.08087158203125\n",
      "Epoch 62, Batch 6970, Loss: 185.28762817382812\n",
      "Epoch 62, Batch 6971, Loss: 169.20718383789062\n",
      "Epoch 62, Batch 6972, Loss: 169.96046447753906\n",
      "Epoch 62, Batch 6973, Loss: 175.5906219482422\n",
      "Epoch 62, Batch 6974, Loss: 179.07736206054688\n",
      "Epoch 62, Batch 6975, Loss: 175.84619140625\n",
      "Epoch 62, Batch 6976, Loss: 175.59649658203125\n",
      "Epoch 62, Batch 6977, Loss: 180.85520935058594\n",
      "Epoch 62, Batch 6978, Loss: 161.12461853027344\n",
      "Epoch 62, Batch 6979, Loss: 176.5595245361328\n",
      "Epoch 62, Batch 6980, Loss: 173.30831909179688\n",
      "Epoch 62, Batch 6981, Loss: 173.45932006835938\n",
      "Epoch 62, Batch 6982, Loss: 173.63331604003906\n",
      "Epoch 62, Batch 6983, Loss: 172.88279724121094\n",
      "Epoch 62, Batch 6984, Loss: 190.90013122558594\n",
      "Epoch 62, Batch 6985, Loss: 163.5879364013672\n",
      "Epoch 62, Batch 6986, Loss: 185.43264770507812\n",
      "Epoch 62, Batch 6987, Loss: 169.73760986328125\n",
      "Epoch 62, Batch 6988, Loss: 179.49664306640625\n",
      "Epoch 62, Batch 6989, Loss: 165.99969482421875\n",
      "Epoch 62, Batch 6990, Loss: 189.1338653564453\n",
      "Epoch 62, Batch 6991, Loss: 166.98831176757812\n",
      "Epoch 62, Batch 6992, Loss: 175.1255340576172\n",
      "Epoch 62, Batch 6993, Loss: 193.41468811035156\n",
      "Epoch 62, Batch 6994, Loss: 165.43418884277344\n",
      "Epoch 62, Batch 6995, Loss: 186.01416015625\n",
      "Epoch 62, Batch 6996, Loss: 173.7876434326172\n",
      "Epoch 62, Batch 6997, Loss: 166.38145446777344\n",
      "Epoch 62, Batch 6998, Loss: 164.2986602783203\n",
      "Epoch 62, Batch 6999, Loss: 180.2068328857422\n",
      "Epoch 62, Batch 7000, Loss: 174.36407470703125\n",
      "Epoch 62, Batch 7001, Loss: 192.20074462890625\n",
      "Epoch 62, Batch 7002, Loss: 168.73085021972656\n",
      "Epoch 62, Batch 7003, Loss: 167.79673767089844\n",
      "Epoch 62, Batch 7004, Loss: 157.03155517578125\n",
      "Epoch 62, Batch 7005, Loss: 186.93910217285156\n",
      "Epoch 62, Batch 7006, Loss: 178.54412841796875\n",
      "Epoch 62, Batch 7007, Loss: 193.8422088623047\n",
      "Epoch 62, Batch 7008, Loss: 176.8407440185547\n",
      "Epoch 62, Batch 7009, Loss: 175.2795867919922\n",
      "Epoch 62, Batch 7010, Loss: 184.4700469970703\n",
      "Epoch 62, Batch 7011, Loss: 165.83177185058594\n",
      "Epoch 62, Batch 7012, Loss: 176.7484130859375\n",
      "Epoch 62, Batch 7013, Loss: 169.73361206054688\n",
      "Epoch 62, Batch 7014, Loss: 171.20095825195312\n",
      "Epoch 62, Batch 7015, Loss: 173.4207000732422\n",
      "Epoch 62, Batch 7016, Loss: 169.68470764160156\n",
      "Epoch 62, Batch 7017, Loss: 186.0315399169922\n",
      "Epoch 62, Batch 7018, Loss: 183.97122192382812\n",
      "Epoch 62, Batch 7019, Loss: 176.52020263671875\n",
      "Epoch 62, Batch 7020, Loss: 172.13568115234375\n",
      "Epoch 62, Batch 7021, Loss: 184.52462768554688\n",
      "Epoch 62, Batch 7022, Loss: 188.0214385986328\n",
      "Epoch 62, Batch 7023, Loss: 164.91123962402344\n",
      "Epoch 62, Batch 7024, Loss: 181.8549041748047\n",
      "Epoch 62, Batch 7025, Loss: 179.30038452148438\n",
      "Epoch 62, Batch 7026, Loss: 168.3318328857422\n",
      "Epoch 62, Batch 7027, Loss: 164.59324645996094\n",
      "Epoch 62, Batch 7028, Loss: 158.5551300048828\n",
      "Epoch 62, Batch 7029, Loss: 161.24932861328125\n",
      "Epoch 62, Batch 7030, Loss: 178.28646850585938\n",
      "Epoch 62, Batch 7031, Loss: 164.28781127929688\n",
      "Epoch 62, Batch 7032, Loss: 183.12355041503906\n",
      "Epoch 62, Batch 7033, Loss: 186.21469116210938\n",
      "Epoch 62, Batch 7034, Loss: 178.209228515625\n",
      "Epoch 62, Batch 7035, Loss: 171.8617706298828\n",
      "Epoch 62, Batch 7036, Loss: 180.20582580566406\n",
      "Epoch 62, Batch 7037, Loss: 162.3346405029297\n",
      "Epoch 62, Batch 7038, Loss: 178.67897033691406\n",
      "Epoch 62, Batch 7039, Loss: 182.40504455566406\n",
      "Epoch 62, Batch 7040, Loss: 181.57835388183594\n",
      "Epoch 62, Batch 7041, Loss: 172.0222625732422\n",
      "Epoch 62, Batch 7042, Loss: 168.75108337402344\n",
      "Epoch 62, Batch 7043, Loss: 160.3095703125\n",
      "Epoch 62, Batch 7044, Loss: 173.34255981445312\n",
      "Epoch 62, Batch 7045, Loss: 158.60951232910156\n",
      "Epoch 62, Batch 7046, Loss: 174.5082244873047\n",
      "Epoch 62, Batch 7047, Loss: 170.461669921875\n",
      "Epoch 62, Batch 7048, Loss: 176.97085571289062\n",
      "Epoch 62, Batch 7049, Loss: 181.1577911376953\n",
      "Epoch 62, Batch 7050, Loss: 169.93212890625\n",
      "Epoch 62, Batch 7051, Loss: 173.98629760742188\n",
      "Epoch 62, Batch 7052, Loss: 178.97804260253906\n",
      "Epoch 62, Batch 7053, Loss: 179.936279296875\n",
      "Epoch 62, Batch 7054, Loss: 168.78070068359375\n",
      "Epoch 62, Batch 7055, Loss: 173.69154357910156\n",
      "Epoch 62, Batch 7056, Loss: 165.64866638183594\n",
      "Epoch 62, Batch 7057, Loss: 173.26976013183594\n",
      "Epoch 62, Batch 7058, Loss: 184.39341735839844\n",
      "Epoch 62, Batch 7059, Loss: 168.62469482421875\n",
      "Epoch 62, Batch 7060, Loss: 166.232421875\n",
      "Epoch 62, Batch 7061, Loss: 180.16026306152344\n",
      "Epoch 62, Batch 7062, Loss: 161.66770935058594\n",
      "Epoch 62, Batch 7063, Loss: 179.5020294189453\n",
      "Epoch 62, Batch 7064, Loss: 177.3479461669922\n",
      "Epoch 62, Batch 7065, Loss: 208.8324432373047\n",
      "Epoch 62, Batch 7066, Loss: 182.0840606689453\n",
      "Epoch 62, Batch 7067, Loss: 157.75428771972656\n",
      "Epoch 62, Batch 7068, Loss: 167.77284240722656\n",
      "Epoch 62, Batch 7069, Loss: 187.8032684326172\n",
      "Epoch 62, Batch 7070, Loss: 180.8765106201172\n",
      "Epoch 62, Batch 7071, Loss: 174.37600708007812\n",
      "Epoch 62, Batch 7072, Loss: 172.05471801757812\n",
      "Epoch 62, Batch 7073, Loss: 170.97927856445312\n",
      "Epoch 62, Batch 7074, Loss: 158.14846801757812\n",
      "Epoch 62, Batch 7075, Loss: 165.2751922607422\n",
      "Epoch 62, Batch 7076, Loss: 182.4239044189453\n",
      "Epoch 62, Batch 7077, Loss: 163.88681030273438\n",
      "Epoch 62, Batch 7078, Loss: 157.32177734375\n",
      "Epoch 62, Batch 7079, Loss: 185.4042510986328\n",
      "Epoch 62, Batch 7080, Loss: 171.83876037597656\n",
      "Epoch 62, Batch 7081, Loss: 169.2590789794922\n",
      "Epoch 62, Batch 7082, Loss: 173.126708984375\n",
      "Epoch 62, Batch 7083, Loss: 164.81582641601562\n",
      "Epoch 62, Batch 7084, Loss: 187.88751220703125\n",
      "Epoch 62, Batch 7085, Loss: 187.14947509765625\n",
      "Epoch 62, Batch 7086, Loss: 166.8547821044922\n",
      "Epoch 62, Batch 7087, Loss: 169.6463165283203\n",
      "Epoch 62, Batch 7088, Loss: 169.60626220703125\n",
      "Epoch 62, Batch 7089, Loss: 157.6317901611328\n",
      "Epoch 62, Batch 7090, Loss: 169.40277099609375\n",
      "Epoch 62, Batch 7091, Loss: 151.71881103515625\n",
      "Epoch 62, Batch 7092, Loss: 164.96884155273438\n",
      "Epoch 62, Batch 7093, Loss: 193.8451385498047\n",
      "Epoch 62, Batch 7094, Loss: 167.94906616210938\n",
      "Epoch 62, Batch 7095, Loss: 174.20126342773438\n",
      "Epoch 62, Batch 7096, Loss: 198.20252990722656\n",
      "Epoch 62, Batch 7097, Loss: 183.0943145751953\n",
      "Epoch 62, Batch 7098, Loss: 164.34458923339844\n",
      "Epoch 62, Batch 7099, Loss: 172.74609375\n",
      "Epoch 62, Batch 7100, Loss: 170.8743896484375\n",
      "Epoch 62, Batch 7101, Loss: 172.33140563964844\n",
      "Epoch 62, Batch 7102, Loss: 183.35057067871094\n",
      "Epoch 62, Batch 7103, Loss: 184.7226104736328\n",
      "Epoch 62, Batch 7104, Loss: 158.1180419921875\n",
      "Epoch 62, Batch 7105, Loss: 176.94898986816406\n",
      "Epoch 62, Batch 7106, Loss: 172.9929656982422\n",
      "Epoch 62, Batch 7107, Loss: 168.86158752441406\n",
      "Epoch 62, Batch 7108, Loss: 190.280029296875\n",
      "Epoch 62, Batch 7109, Loss: 177.1423797607422\n",
      "Epoch 62, Batch 7110, Loss: 168.8422393798828\n",
      "Epoch 62, Batch 7111, Loss: 179.6986846923828\n",
      "Epoch 62, Batch 7112, Loss: 177.8965606689453\n",
      "Epoch 62, Batch 7113, Loss: 177.011962890625\n",
      "Epoch 62, Batch 7114, Loss: 156.43045043945312\n",
      "Epoch 62, Batch 7115, Loss: 163.0560760498047\n",
      "Epoch 62, Batch 7116, Loss: 167.46437072753906\n",
      "Epoch 62, Batch 7117, Loss: 156.02755737304688\n",
      "Epoch 62, Batch 7118, Loss: 182.31240844726562\n",
      "Epoch 62, Batch 7119, Loss: 188.259765625\n",
      "Epoch 62, Batch 7120, Loss: 180.26759338378906\n",
      "Epoch 62, Batch 7121, Loss: 171.34017944335938\n",
      "Epoch 62, Batch 7122, Loss: 165.79400634765625\n",
      "Epoch 62, Batch 7123, Loss: 162.8026885986328\n",
      "Epoch 62, Batch 7124, Loss: 165.98831176757812\n",
      "Epoch 62, Batch 7125, Loss: 192.59915161132812\n",
      "Epoch 62, Batch 7126, Loss: 163.08389282226562\n",
      "Epoch 62, Batch 7127, Loss: 161.16839599609375\n",
      "Epoch 62, Batch 7128, Loss: 163.03562927246094\n",
      "Epoch 62, Batch 7129, Loss: 160.88787841796875\n",
      "Epoch 62, Batch 7130, Loss: 178.75711059570312\n",
      "Epoch 62, Batch 7131, Loss: 178.22921752929688\n",
      "Epoch 62, Batch 7132, Loss: 163.72329711914062\n",
      "Epoch 62, Batch 7133, Loss: 161.20433044433594\n",
      "Epoch 62, Batch 7134, Loss: 177.52691650390625\n",
      "Epoch 62, Batch 7135, Loss: 174.559814453125\n",
      "Epoch 62, Batch 7136, Loss: 165.74429321289062\n",
      "Epoch 62, Batch 7137, Loss: 181.31927490234375\n",
      "Epoch 62, Batch 7138, Loss: 162.29403686523438\n",
      "Epoch 62, Batch 7139, Loss: 189.55313110351562\n",
      "Epoch 62, Batch 7140, Loss: 183.26235961914062\n",
      "Epoch 62, Batch 7141, Loss: 159.2089385986328\n",
      "Epoch 62, Batch 7142, Loss: 161.3907928466797\n",
      "Epoch 62, Batch 7143, Loss: 175.27362060546875\n",
      "Epoch 62, Batch 7144, Loss: 179.9229736328125\n",
      "Epoch 62, Batch 7145, Loss: 193.33116149902344\n",
      "Epoch 62, Batch 7146, Loss: 162.75241088867188\n",
      "Epoch 62, Batch 7147, Loss: 187.4623565673828\n",
      "Epoch 62, Batch 7148, Loss: 183.10047912597656\n",
      "Epoch 62, Batch 7149, Loss: 157.35772705078125\n",
      "Epoch 62, Batch 7150, Loss: 193.76878356933594\n",
      "Epoch 62, Batch 7151, Loss: 185.29376220703125\n",
      "Epoch 62, Batch 7152, Loss: 175.93470764160156\n",
      "Epoch 62, Batch 7153, Loss: 171.7637939453125\n",
      "Epoch 62, Batch 7154, Loss: 161.80703735351562\n",
      "Epoch 62, Batch 7155, Loss: 163.78648376464844\n",
      "Epoch 62, Batch 7156, Loss: 186.32696533203125\n",
      "Epoch 62, Batch 7157, Loss: 181.10958862304688\n",
      "Epoch 62, Batch 7158, Loss: 167.4768829345703\n",
      "Epoch 62, Batch 7159, Loss: 170.22109985351562\n",
      "Epoch 62, Batch 7160, Loss: 174.46505737304688\n",
      "Epoch 62, Batch 7161, Loss: 164.98948669433594\n",
      "Epoch 62, Batch 7162, Loss: 166.28359985351562\n",
      "Epoch 62, Batch 7163, Loss: 172.6080322265625\n",
      "Epoch 62, Batch 7164, Loss: 180.13223266601562\n",
      "Epoch 62, Batch 7165, Loss: 186.6424102783203\n",
      "Epoch 62, Batch 7166, Loss: 190.4658966064453\n",
      "Epoch 62, Batch 7167, Loss: 185.53997802734375\n",
      "Epoch 62, Batch 7168, Loss: 178.0232696533203\n",
      "Epoch 62, Batch 7169, Loss: 176.51983642578125\n",
      "Epoch 62, Batch 7170, Loss: 170.93313598632812\n",
      "Epoch 62, Batch 7171, Loss: 171.32110595703125\n",
      "Epoch 62, Batch 7172, Loss: 162.16368103027344\n",
      "Epoch 62, Batch 7173, Loss: 160.5855255126953\n",
      "Epoch 62, Batch 7174, Loss: 179.50613403320312\n",
      "Epoch 62, Batch 7175, Loss: 172.5109100341797\n",
      "Epoch 62, Batch 7176, Loss: 163.20997619628906\n",
      "Epoch 62, Batch 7177, Loss: 185.10301208496094\n",
      "Epoch 62, Batch 7178, Loss: 153.15733337402344\n",
      "Epoch 62, Batch 7179, Loss: 173.7268524169922\n",
      "Epoch 62, Batch 7180, Loss: 190.36280822753906\n",
      "Epoch 62, Batch 7181, Loss: 188.49642944335938\n",
      "Epoch 62, Batch 7182, Loss: 168.74600219726562\n",
      "Epoch 62, Batch 7183, Loss: 188.39834594726562\n",
      "Epoch 62, Batch 7184, Loss: 178.24191284179688\n",
      "Epoch 62, Batch 7185, Loss: 170.72238159179688\n",
      "Epoch 62, Batch 7186, Loss: 176.95863342285156\n",
      "Epoch 62, Batch 7187, Loss: 163.58340454101562\n",
      "Epoch 62, Batch 7188, Loss: 157.02171325683594\n",
      "Epoch 62, Batch 7189, Loss: 183.0478973388672\n",
      "Epoch 62, Batch 7190, Loss: 166.8190460205078\n",
      "Epoch 62, Batch 7191, Loss: 149.1177978515625\n",
      "Epoch 62, Batch 7192, Loss: 156.49896240234375\n",
      "Epoch 62, Batch 7193, Loss: 176.18544006347656\n",
      "Epoch 62, Batch 7194, Loss: 166.37574768066406\n",
      "Epoch 62, Batch 7195, Loss: 162.7001190185547\n",
      "Epoch 62, Batch 7196, Loss: 183.4077911376953\n",
      "Epoch 62, Batch 7197, Loss: 173.30908203125\n",
      "Epoch 62, Batch 7198, Loss: 164.07064819335938\n",
      "Epoch 62, Batch 7199, Loss: 162.47549438476562\n",
      "Epoch 62, Batch 7200, Loss: 173.16419982910156\n",
      "Epoch 62, Batch 7201, Loss: 172.57090759277344\n",
      "Epoch 62, Batch 7202, Loss: 177.4741668701172\n",
      "Epoch 62, Batch 7203, Loss: 164.148681640625\n",
      "Epoch 62, Batch 7204, Loss: 179.54058837890625\n",
      "Epoch 62, Batch 7205, Loss: 166.0611114501953\n",
      "Epoch 62, Batch 7206, Loss: 178.96029663085938\n",
      "Epoch 62, Batch 7207, Loss: 158.9081268310547\n",
      "Epoch 62, Batch 7208, Loss: 178.90550231933594\n",
      "Epoch 62, Batch 7209, Loss: 174.4009246826172\n",
      "Epoch 62, Batch 7210, Loss: 158.97195434570312\n",
      "Epoch 62, Batch 7211, Loss: 160.64344787597656\n",
      "Epoch 62, Batch 7212, Loss: 182.2831573486328\n",
      "Epoch 62, Batch 7213, Loss: 171.73861694335938\n",
      "Epoch 62, Batch 7214, Loss: 167.03575134277344\n",
      "Epoch 62, Batch 7215, Loss: 165.33102416992188\n",
      "Epoch 62, Batch 7216, Loss: 170.71063232421875\n",
      "Epoch 62, Batch 7217, Loss: 195.5133514404297\n",
      "Epoch 62, Batch 7218, Loss: 192.56431579589844\n",
      "Epoch 62, Batch 7219, Loss: 185.52272033691406\n",
      "Epoch 62, Batch 7220, Loss: 158.21975708007812\n",
      "Epoch 62, Batch 7221, Loss: 174.70220947265625\n",
      "Epoch 62, Batch 7222, Loss: 154.08578491210938\n",
      "Epoch 62, Batch 7223, Loss: 177.82131958007812\n",
      "Epoch 62, Batch 7224, Loss: 182.17333984375\n",
      "Epoch 62, Batch 7225, Loss: 180.70709228515625\n",
      "Epoch 62, Batch 7226, Loss: 177.42063903808594\n",
      "Epoch 62, Batch 7227, Loss: 169.25563049316406\n",
      "Epoch 62, Batch 7228, Loss: 180.19847106933594\n",
      "Epoch 62, Batch 7229, Loss: 176.8815460205078\n",
      "Epoch 62, Batch 7230, Loss: 170.3220672607422\n",
      "Epoch 62, Batch 7231, Loss: 191.55783081054688\n",
      "Epoch 62, Batch 7232, Loss: 179.843994140625\n",
      "Epoch 62, Batch 7233, Loss: 196.19532775878906\n",
      "Epoch 62, Batch 7234, Loss: 181.77386474609375\n",
      "Epoch 62, Batch 7235, Loss: 166.068359375\n",
      "Epoch 62, Batch 7236, Loss: 184.6854248046875\n",
      "Epoch 62, Batch 7237, Loss: 157.96693420410156\n",
      "Epoch 62, Batch 7238, Loss: 171.07009887695312\n",
      "Epoch 62, Batch 7239, Loss: 167.20419311523438\n",
      "Epoch 62, Batch 7240, Loss: 174.60804748535156\n",
      "Epoch 62, Batch 7241, Loss: 178.99221801757812\n",
      "Epoch 62, Batch 7242, Loss: 186.74383544921875\n",
      "Epoch 62, Batch 7243, Loss: 193.86764526367188\n",
      "Epoch 62, Batch 7244, Loss: 180.2576446533203\n",
      "Epoch 62, Batch 7245, Loss: 166.7825164794922\n",
      "Epoch 62, Batch 7246, Loss: 173.88938903808594\n",
      "Epoch 62, Batch 7247, Loss: 167.62405395507812\n",
      "Epoch 62, Batch 7248, Loss: 160.34005737304688\n",
      "Epoch 62, Batch 7249, Loss: 186.08995056152344\n",
      "Epoch 62, Batch 7250, Loss: 170.9326171875\n",
      "Epoch 62, Batch 7251, Loss: 170.58297729492188\n",
      "Epoch 62, Batch 7252, Loss: 169.0498809814453\n",
      "Epoch 62, Batch 7253, Loss: 177.5293426513672\n",
      "Epoch 62, Batch 7254, Loss: 188.80711364746094\n",
      "Epoch 62, Batch 7255, Loss: 165.0465545654297\n",
      "Epoch 62, Batch 7256, Loss: 177.8162078857422\n",
      "Epoch 62, Batch 7257, Loss: 185.0438232421875\n",
      "Epoch 62, Batch 7258, Loss: 174.7552947998047\n",
      "Epoch 62, Batch 7259, Loss: 164.48590087890625\n",
      "Epoch 62, Batch 7260, Loss: 182.4770050048828\n",
      "Epoch 62, Batch 7261, Loss: 154.8655548095703\n",
      "Epoch 62, Batch 7262, Loss: 172.55776977539062\n",
      "Epoch 62, Batch 7263, Loss: 180.16995239257812\n",
      "Epoch 62, Batch 7264, Loss: 170.12435913085938\n",
      "Epoch 62, Batch 7265, Loss: 171.50563049316406\n",
      "Epoch 62, Batch 7266, Loss: 160.6458740234375\n",
      "Epoch 62, Batch 7267, Loss: 174.9923095703125\n",
      "Epoch 62, Batch 7268, Loss: 168.02333068847656\n",
      "Epoch 62, Batch 7269, Loss: 187.36659240722656\n",
      "Epoch 62, Batch 7270, Loss: 168.93470764160156\n",
      "Epoch 62, Batch 7271, Loss: 174.02639770507812\n",
      "Epoch 62, Batch 7272, Loss: 160.0437774658203\n",
      "Epoch 62, Batch 7273, Loss: 192.12850952148438\n",
      "Epoch 62, Batch 7274, Loss: 169.81321716308594\n",
      "Epoch 62, Batch 7275, Loss: 159.1045684814453\n",
      "Epoch 62, Batch 7276, Loss: 167.93695068359375\n",
      "Epoch 62, Batch 7277, Loss: 186.20779418945312\n",
      "Epoch 62, Batch 7278, Loss: 165.84970092773438\n",
      "Epoch 62, Batch 7279, Loss: 166.1442413330078\n",
      "Epoch 62, Batch 7280, Loss: 163.61376953125\n",
      "Epoch 62, Batch 7281, Loss: 168.95883178710938\n",
      "Epoch 62, Batch 7282, Loss: 159.45298767089844\n",
      "Epoch 62, Batch 7283, Loss: 170.8715362548828\n",
      "Epoch 62, Batch 7284, Loss: 177.07403564453125\n",
      "Epoch 62, Batch 7285, Loss: 179.2719268798828\n",
      "Epoch 62, Batch 7286, Loss: 163.94412231445312\n",
      "Epoch 62, Batch 7287, Loss: 165.96238708496094\n",
      "Epoch 62, Batch 7288, Loss: 176.08447265625\n",
      "Epoch 62, Batch 7289, Loss: 167.23928833007812\n",
      "Epoch 62, Batch 7290, Loss: 182.1162567138672\n",
      "Epoch 62, Batch 7291, Loss: 163.63479614257812\n",
      "Epoch 62, Batch 7292, Loss: 186.75914001464844\n",
      "Epoch 62, Batch 7293, Loss: 176.61253356933594\n",
      "Epoch 62, Batch 7294, Loss: 154.48513793945312\n",
      "Epoch 62, Batch 7295, Loss: 183.4993896484375\n",
      "Epoch 62, Batch 7296, Loss: 174.7454376220703\n",
      "Epoch 62, Batch 7297, Loss: 182.20643615722656\n",
      "Epoch 62, Batch 7298, Loss: 155.37997436523438\n",
      "Epoch 62, Batch 7299, Loss: 172.45533752441406\n",
      "Epoch 62, Batch 7300, Loss: 173.7207489013672\n",
      "Epoch 62, Batch 7301, Loss: 161.27340698242188\n",
      "Epoch 62, Batch 7302, Loss: 190.49400329589844\n",
      "Epoch 62, Batch 7303, Loss: 155.35308837890625\n",
      "Epoch 62, Batch 7304, Loss: 171.62689208984375\n",
      "Epoch 62, Batch 7305, Loss: 163.32562255859375\n",
      "Epoch 62, Batch 7306, Loss: 173.7828369140625\n",
      "Epoch 62, Batch 7307, Loss: 168.25238037109375\n",
      "Epoch 62, Batch 7308, Loss: 169.47344970703125\n",
      "Epoch 62, Batch 7309, Loss: 180.09945678710938\n",
      "Epoch 62, Batch 7310, Loss: 178.2029571533203\n",
      "Epoch 62, Batch 7311, Loss: 190.0975341796875\n",
      "Epoch 62, Batch 7312, Loss: 175.42156982421875\n",
      "Epoch 62, Batch 7313, Loss: 165.28396606445312\n",
      "Epoch 62, Batch 7314, Loss: 164.8079833984375\n",
      "Epoch 62, Batch 7315, Loss: 165.13458251953125\n",
      "Epoch 62, Batch 7316, Loss: 159.03094482421875\n",
      "Epoch 62, Batch 7317, Loss: 159.32852172851562\n",
      "Epoch 62, Batch 7318, Loss: 168.57254028320312\n",
      "Epoch 62, Batch 7319, Loss: 171.9202880859375\n",
      "Epoch 62, Batch 7320, Loss: 158.2394561767578\n",
      "Epoch 62, Batch 7321, Loss: 181.09683227539062\n",
      "Epoch 62, Batch 7322, Loss: 181.49853515625\n",
      "Epoch 62, Batch 7323, Loss: 175.13685607910156\n",
      "Epoch 62, Batch 7324, Loss: 159.02845764160156\n",
      "Epoch 62, Batch 7325, Loss: 171.962158203125\n",
      "Epoch 62, Batch 7326, Loss: 192.2134246826172\n",
      "Epoch 62, Batch 7327, Loss: 144.83950805664062\n",
      "Epoch 62, Batch 7328, Loss: 177.77096557617188\n",
      "Epoch 62, Batch 7329, Loss: 182.6778106689453\n",
      "Epoch 62, Batch 7330, Loss: 179.94277954101562\n",
      "Epoch 62, Batch 7331, Loss: 170.3270721435547\n",
      "Epoch 62, Batch 7332, Loss: 173.31112670898438\n",
      "Epoch 62, Batch 7333, Loss: 167.35562133789062\n",
      "Epoch 62, Batch 7334, Loss: 155.43173217773438\n",
      "Epoch 62, Batch 7335, Loss: 177.46994018554688\n",
      "Epoch 62, Batch 7336, Loss: 182.21502685546875\n",
      "Epoch 62, Batch 7337, Loss: 177.69041442871094\n",
      "Epoch 62, Batch 7338, Loss: 170.45025634765625\n",
      "Epoch 62, Batch 7339, Loss: 174.12692260742188\n",
      "Epoch 62, Batch 7340, Loss: 184.50624084472656\n",
      "Epoch 62, Batch 7341, Loss: 184.93463134765625\n",
      "Epoch 62, Batch 7342, Loss: 163.48387145996094\n",
      "Epoch 62, Batch 7343, Loss: 163.3477783203125\n",
      "Epoch 62, Batch 7344, Loss: 172.84344482421875\n",
      "Epoch 62, Batch 7345, Loss: 179.238525390625\n",
      "Epoch 62, Batch 7346, Loss: 162.24229431152344\n",
      "Epoch 62, Batch 7347, Loss: 172.66323852539062\n",
      "Epoch 62, Batch 7348, Loss: 171.960693359375\n",
      "Epoch 62, Batch 7349, Loss: 169.35638427734375\n",
      "Epoch 62, Batch 7350, Loss: 187.27706909179688\n",
      "Epoch 62, Batch 7351, Loss: 167.98670959472656\n",
      "Epoch 62, Batch 7352, Loss: 159.03427124023438\n",
      "Epoch 62, Batch 7353, Loss: 164.88870239257812\n",
      "Epoch 62, Batch 7354, Loss: 162.4524688720703\n",
      "Epoch 62, Batch 7355, Loss: 166.01141357421875\n",
      "Epoch 62, Batch 7356, Loss: 159.5641326904297\n",
      "Epoch 62, Batch 7357, Loss: 184.87342834472656\n",
      "Epoch 62, Batch 7358, Loss: 174.5952606201172\n",
      "Epoch 62, Batch 7359, Loss: 179.27731323242188\n",
      "Epoch 62, Batch 7360, Loss: 183.79437255859375\n",
      "Epoch 62, Batch 7361, Loss: 159.04440307617188\n",
      "Epoch 62, Batch 7362, Loss: 151.02601623535156\n",
      "Epoch 62, Batch 7363, Loss: 172.42379760742188\n",
      "Epoch 62, Batch 7364, Loss: 163.1619110107422\n",
      "Epoch 62, Batch 7365, Loss: 173.8773956298828\n",
      "Epoch 62, Batch 7366, Loss: 182.55751037597656\n",
      "Epoch 62, Batch 7367, Loss: 164.54443359375\n",
      "Epoch 62, Batch 7368, Loss: 181.114501953125\n",
      "Epoch 62, Batch 7369, Loss: 167.1697235107422\n",
      "Epoch 62, Batch 7370, Loss: 175.2681884765625\n",
      "Epoch 62, Batch 7371, Loss: 159.2650604248047\n",
      "Epoch 62, Batch 7372, Loss: 172.6588134765625\n",
      "Epoch 62, Batch 7373, Loss: 172.38812255859375\n",
      "Epoch 62, Batch 7374, Loss: 164.385009765625\n",
      "Epoch 62, Batch 7375, Loss: 172.9775848388672\n",
      "Epoch 62, Batch 7376, Loss: 171.1299285888672\n",
      "Epoch 62, Batch 7377, Loss: 181.14756774902344\n",
      "Epoch 62, Batch 7378, Loss: 171.26792907714844\n",
      "Epoch 62, Batch 7379, Loss: 165.24778747558594\n",
      "Epoch 62, Batch 7380, Loss: 158.90184020996094\n",
      "Epoch 62, Batch 7381, Loss: 181.15316772460938\n",
      "Epoch 62, Batch 7382, Loss: 182.48849487304688\n",
      "Epoch 62, Batch 7383, Loss: 171.73179626464844\n",
      "Epoch 62, Batch 7384, Loss: 163.41311645507812\n",
      "Epoch 62, Batch 7385, Loss: 167.12677001953125\n",
      "Epoch 62, Batch 7386, Loss: 158.81088256835938\n",
      "Epoch 62, Batch 7387, Loss: 187.38885498046875\n",
      "Epoch 62, Batch 7388, Loss: 161.547119140625\n",
      "Epoch 62, Batch 7389, Loss: 161.37986755371094\n",
      "Epoch 62, Batch 7390, Loss: 182.830810546875\n",
      "Epoch 62, Batch 7391, Loss: 166.37454223632812\n",
      "Epoch 62, Batch 7392, Loss: 172.41961669921875\n",
      "Epoch 62, Batch 7393, Loss: 169.9947052001953\n",
      "Epoch 62, Batch 7394, Loss: 176.69483947753906\n",
      "Epoch 62, Batch 7395, Loss: 180.9110107421875\n",
      "Epoch 62, Batch 7396, Loss: 168.01010131835938\n",
      "Epoch 62, Batch 7397, Loss: 165.5364227294922\n",
      "Epoch 62, Batch 7398, Loss: 180.05667114257812\n",
      "Epoch 62, Batch 7399, Loss: 170.5913543701172\n",
      "Epoch 62, Batch 7400, Loss: 171.1371307373047\n",
      "Epoch 62, Batch 7401, Loss: 178.82127380371094\n",
      "Epoch 62, Batch 7402, Loss: 186.21986389160156\n",
      "Epoch 62, Batch 7403, Loss: 175.4346160888672\n",
      "Epoch 62, Batch 7404, Loss: 161.28421020507812\n",
      "Epoch 62, Batch 7405, Loss: 181.49862670898438\n",
      "Epoch 62, Batch 7406, Loss: 166.23460388183594\n",
      "Epoch 62, Batch 7407, Loss: 173.82168579101562\n",
      "Epoch 62, Batch 7408, Loss: 170.49400329589844\n",
      "Epoch 62, Batch 7409, Loss: 188.74452209472656\n",
      "Epoch 62, Batch 7410, Loss: 169.486328125\n",
      "Epoch 62, Batch 7411, Loss: 173.74961853027344\n",
      "Epoch 62, Batch 7412, Loss: 165.0065155029297\n",
      "Epoch 62, Batch 7413, Loss: 170.48175048828125\n",
      "Epoch 62, Batch 7414, Loss: 181.4749755859375\n",
      "Epoch 62, Batch 7415, Loss: 172.70689392089844\n",
      "Epoch 62, Batch 7416, Loss: 184.8602294921875\n",
      "Epoch 62, Batch 7417, Loss: 162.27499389648438\n",
      "Epoch 62, Batch 7418, Loss: 175.92835998535156\n",
      "Epoch 62, Batch 7419, Loss: 163.20594787597656\n",
      "Epoch 62, Batch 7420, Loss: 173.4564666748047\n",
      "Epoch 62, Batch 7421, Loss: 168.072021484375\n",
      "Epoch 62, Batch 7422, Loss: 174.72129821777344\n",
      "Epoch 62, Batch 7423, Loss: 171.18104553222656\n",
      "Epoch 62, Batch 7424, Loss: 165.29684448242188\n",
      "Epoch 62, Batch 7425, Loss: 180.81666564941406\n",
      "Epoch 62, Batch 7426, Loss: 167.3690948486328\n",
      "Epoch 62, Batch 7427, Loss: 165.18475341796875\n",
      "Epoch 62, Batch 7428, Loss: 162.21676635742188\n",
      "Epoch 62, Batch 7429, Loss: 168.94602966308594\n",
      "Epoch 62, Batch 7430, Loss: 180.1603240966797\n",
      "Epoch 62, Batch 7431, Loss: 166.31024169921875\n",
      "Epoch 62, Batch 7432, Loss: 163.64584350585938\n",
      "Epoch 62, Batch 7433, Loss: 173.3828125\n",
      "Epoch 62, Batch 7434, Loss: 168.5277862548828\n",
      "Epoch 62, Batch 7435, Loss: 172.03416442871094\n",
      "Epoch 62, Batch 7436, Loss: 183.9359588623047\n",
      "Epoch 62, Batch 7437, Loss: 183.2090301513672\n",
      "Epoch 62, Batch 7438, Loss: 189.52783203125\n",
      "Epoch 62, Batch 7439, Loss: 160.26992797851562\n",
      "Epoch 62, Batch 7440, Loss: 179.174560546875\n",
      "Epoch 62, Batch 7441, Loss: 174.29518127441406\n",
      "Epoch 62, Batch 7442, Loss: 183.4024658203125\n",
      "Epoch 62, Batch 7443, Loss: 186.11903381347656\n",
      "Epoch 62, Batch 7444, Loss: 156.75526428222656\n",
      "Epoch 62, Batch 7445, Loss: 170.5374298095703\n",
      "Epoch 62, Batch 7446, Loss: 157.28427124023438\n",
      "Epoch 62, Batch 7447, Loss: 167.5114288330078\n",
      "Epoch 62, Batch 7448, Loss: 164.4951934814453\n",
      "Epoch 62, Batch 7449, Loss: 177.3499755859375\n",
      "Epoch 62, Batch 7450, Loss: 166.22201538085938\n",
      "Epoch 62, Batch 7451, Loss: 173.59507751464844\n",
      "Epoch 62, Batch 7452, Loss: 173.14305114746094\n",
      "Epoch 62, Batch 7453, Loss: 163.4314422607422\n",
      "Epoch 62, Batch 7454, Loss: 178.6727294921875\n",
      "Epoch 62, Batch 7455, Loss: 154.82479858398438\n",
      "Epoch 62, Batch 7456, Loss: 173.66261291503906\n",
      "Epoch 62, Batch 7457, Loss: 165.00311279296875\n",
      "Epoch 62, Batch 7458, Loss: 169.3029327392578\n",
      "Epoch 62, Batch 7459, Loss: 169.9803009033203\n",
      "Epoch 62, Batch 7460, Loss: 183.6593017578125\n",
      "Epoch 62, Batch 7461, Loss: 159.24720764160156\n",
      "Epoch 62, Batch 7462, Loss: 161.0848846435547\n",
      "Epoch 62, Batch 7463, Loss: 180.84725952148438\n",
      "Epoch 62, Batch 7464, Loss: 189.29249572753906\n",
      "Epoch 62, Batch 7465, Loss: 154.4053192138672\n",
      "Epoch 62, Batch 7466, Loss: 177.1373748779297\n",
      "Epoch 62, Batch 7467, Loss: 168.38760375976562\n",
      "Epoch 62, Batch 7468, Loss: 176.12269592285156\n",
      "Epoch 62, Batch 7469, Loss: 189.84994506835938\n",
      "Epoch 62, Batch 7470, Loss: 164.4519805908203\n",
      "Epoch 62, Batch 7471, Loss: 176.33518981933594\n",
      "Epoch 62, Batch 7472, Loss: 177.84527587890625\n",
      "Epoch 62, Batch 7473, Loss: 177.2624053955078\n",
      "Epoch 62, Batch 7474, Loss: 165.6509246826172\n",
      "Epoch 62, Batch 7475, Loss: 169.89486694335938\n",
      "Epoch 62, Batch 7476, Loss: 192.99072265625\n",
      "Epoch 62, Batch 7477, Loss: 172.54823303222656\n",
      "Epoch 62, Batch 7478, Loss: 167.5433349609375\n",
      "Epoch 62, Batch 7479, Loss: 174.58270263671875\n",
      "Epoch 62, Batch 7480, Loss: 169.00704956054688\n",
      "Epoch 62, Batch 7481, Loss: 184.34967041015625\n",
      "Epoch 62, Batch 7482, Loss: 162.22235107421875\n",
      "Epoch 62, Batch 7483, Loss: 174.27996826171875\n",
      "Epoch 62, Batch 7484, Loss: 181.2888641357422\n",
      "Epoch 62, Batch 7485, Loss: 179.8167266845703\n",
      "Epoch 62, Batch 7486, Loss: 160.7572479248047\n",
      "Epoch 62, Batch 7487, Loss: 175.33212280273438\n",
      "Epoch 62, Batch 7488, Loss: 167.49200439453125\n",
      "Epoch 62, Batch 7489, Loss: 177.48004150390625\n",
      "Epoch 62, Batch 7490, Loss: 176.38209533691406\n",
      "Epoch 62, Batch 7491, Loss: 175.19036865234375\n",
      "Epoch 62, Batch 7492, Loss: 161.82269287109375\n",
      "Epoch 62, Batch 7493, Loss: 164.59095764160156\n",
      "Epoch 62, Batch 7494, Loss: 164.543212890625\n",
      "Epoch 62, Batch 7495, Loss: 180.14450073242188\n",
      "Epoch 62, Batch 7496, Loss: 177.27798461914062\n",
      "Epoch 62, Batch 7497, Loss: 163.6525115966797\n",
      "Epoch 62, Batch 7498, Loss: 155.3173065185547\n",
      "Epoch 62, Batch 7499, Loss: 190.80137634277344\n",
      "Epoch 62, Batch 7500, Loss: 170.3900604248047\n",
      "Epoch 62, Batch 7501, Loss: 168.4421844482422\n",
      "Epoch 62, Batch 7502, Loss: 177.9139862060547\n",
      "Epoch 62, Batch 7503, Loss: 189.07131958007812\n",
      "Epoch 62, Batch 7504, Loss: 167.40065002441406\n",
      "Epoch 62, Batch 7505, Loss: 173.34603881835938\n",
      "Epoch 62, Batch 7506, Loss: 155.68495178222656\n",
      "Epoch 62, Batch 7507, Loss: 182.9019012451172\n",
      "Epoch 62, Batch 7508, Loss: 176.22279357910156\n",
      "Epoch 62, Batch 7509, Loss: 184.12051391601562\n",
      "Epoch 62, Batch 7510, Loss: 171.5497283935547\n",
      "Epoch 62, Batch 7511, Loss: 166.33709716796875\n",
      "Epoch 62, Batch 7512, Loss: 182.48431396484375\n",
      "Epoch 62, Batch 7513, Loss: 181.533447265625\n",
      "Epoch 62, Batch 7514, Loss: 167.20095825195312\n",
      "Epoch 62, Batch 7515, Loss: 187.078369140625\n",
      "Epoch 62, Batch 7516, Loss: 161.95748901367188\n",
      "Epoch 62, Batch 7517, Loss: 181.86715698242188\n",
      "Epoch 62, Batch 7518, Loss: 157.42037963867188\n",
      "Epoch 62, Batch 7519, Loss: 169.41751098632812\n",
      "Epoch 62, Batch 7520, Loss: 157.54510498046875\n",
      "Epoch 62, Batch 7521, Loss: 163.22608947753906\n",
      "Epoch 62, Batch 7522, Loss: 187.4914093017578\n",
      "Epoch 62, Batch 7523, Loss: 177.7462158203125\n",
      "Epoch 62, Batch 7524, Loss: 177.86001586914062\n",
      "Epoch 62, Batch 7525, Loss: 177.56842041015625\n",
      "Epoch 62, Batch 7526, Loss: 171.25816345214844\n",
      "Epoch 62, Batch 7527, Loss: 162.4482421875\n",
      "Epoch 62, Batch 7528, Loss: 169.88064575195312\n",
      "Epoch 62, Batch 7529, Loss: 165.5173797607422\n",
      "Epoch 62, Batch 7530, Loss: 170.8394775390625\n",
      "Epoch 62, Batch 7531, Loss: 174.4713134765625\n",
      "Epoch 62, Batch 7532, Loss: 177.71958923339844\n",
      "Epoch 62, Batch 7533, Loss: 172.26632690429688\n",
      "Epoch 62, Batch 7534, Loss: 190.8478240966797\n",
      "Epoch 62, Batch 7535, Loss: 178.479248046875\n",
      "Epoch 62, Batch 7536, Loss: 179.49488830566406\n",
      "Epoch 62, Batch 7537, Loss: 174.2010498046875\n",
      "Epoch 62, Batch 7538, Loss: 177.530517578125\n",
      "Epoch 62, Batch 7539, Loss: 179.3231201171875\n",
      "Epoch 62, Batch 7540, Loss: 163.74258422851562\n",
      "Epoch 62, Batch 7541, Loss: 180.42385864257812\n",
      "Epoch 62, Batch 7542, Loss: 169.9055938720703\n",
      "Epoch 62, Batch 7543, Loss: 169.83010864257812\n",
      "Epoch 62, Batch 7544, Loss: 180.0665283203125\n",
      "Epoch 62, Batch 7545, Loss: 179.05686950683594\n",
      "Epoch 62, Batch 7546, Loss: 170.6046142578125\n",
      "Epoch 62, Batch 7547, Loss: 192.55831909179688\n",
      "Epoch 62, Batch 7548, Loss: 163.17584228515625\n",
      "Epoch 62, Batch 7549, Loss: 157.22222900390625\n",
      "Epoch 62, Batch 7550, Loss: 182.377685546875\n",
      "Epoch 62, Batch 7551, Loss: 169.19464111328125\n",
      "Epoch 62, Batch 7552, Loss: 185.7889862060547\n",
      "Epoch 62, Batch 7553, Loss: 175.08526611328125\n",
      "Epoch 62, Batch 7554, Loss: 176.24449157714844\n",
      "Epoch 62, Batch 7555, Loss: 174.62454223632812\n",
      "Epoch 62, Batch 7556, Loss: 182.12234497070312\n",
      "Epoch 62, Batch 7557, Loss: 177.36778259277344\n",
      "Epoch 62, Batch 7558, Loss: 166.93777465820312\n",
      "Epoch 62, Batch 7559, Loss: 154.92007446289062\n",
      "Epoch 62, Batch 7560, Loss: 163.31466674804688\n",
      "Epoch 62, Batch 7561, Loss: 174.66116333007812\n",
      "Epoch 62, Batch 7562, Loss: 172.81781005859375\n",
      "Epoch 62, Batch 7563, Loss: 173.94908142089844\n",
      "Epoch 62, Batch 7564, Loss: 178.5470428466797\n",
      "Epoch 62, Batch 7565, Loss: 183.24896240234375\n",
      "Epoch 62, Batch 7566, Loss: 165.6050262451172\n",
      "Epoch 62, Batch 7567, Loss: 178.26947021484375\n",
      "Epoch 62, Batch 7568, Loss: 191.44642639160156\n",
      "Epoch 62, Batch 7569, Loss: 171.40478515625\n",
      "Epoch 62, Batch 7570, Loss: 170.1529998779297\n",
      "Epoch 62, Batch 7571, Loss: 184.00120544433594\n",
      "Epoch 62, Batch 7572, Loss: 182.7982940673828\n",
      "Epoch 62, Batch 7573, Loss: 162.631591796875\n",
      "Epoch 62, Batch 7574, Loss: 169.33375549316406\n",
      "Epoch 62, Batch 7575, Loss: 186.10968017578125\n",
      "Epoch 62, Batch 7576, Loss: 173.09219360351562\n",
      "Epoch 62, Batch 7577, Loss: 176.9988555908203\n",
      "Epoch 62, Batch 7578, Loss: 166.70733642578125\n",
      "Epoch 62, Batch 7579, Loss: 176.44638061523438\n",
      "Epoch 62, Batch 7580, Loss: 182.22454833984375\n",
      "Epoch 62, Batch 7581, Loss: 164.1329803466797\n",
      "Epoch 62, Batch 7582, Loss: 172.18289184570312\n",
      "Epoch 62, Batch 7583, Loss: 161.533935546875\n",
      "Epoch 62, Batch 7584, Loss: 167.79180908203125\n",
      "Epoch 62, Batch 7585, Loss: 185.98170471191406\n",
      "Epoch 62, Batch 7586, Loss: 170.2989959716797\n",
      "Epoch 62, Batch 7587, Loss: 162.59640502929688\n",
      "Epoch 62, Batch 7588, Loss: 180.31434631347656\n",
      "Epoch 62, Batch 7589, Loss: 173.3141632080078\n",
      "Epoch 62, Batch 7590, Loss: 154.0113525390625\n",
      "Epoch 62, Batch 7591, Loss: 153.88973999023438\n",
      "Epoch 62, Batch 7592, Loss: 154.72291564941406\n",
      "Epoch 62, Batch 7593, Loss: 176.40464782714844\n",
      "Epoch 62, Batch 7594, Loss: 167.569091796875\n",
      "Epoch 62, Batch 7595, Loss: 188.02906799316406\n",
      "Epoch 62, Batch 7596, Loss: 174.3052978515625\n",
      "Epoch 62, Batch 7597, Loss: 188.6534423828125\n",
      "Epoch 62, Batch 7598, Loss: 160.34519958496094\n",
      "Epoch 62, Batch 7599, Loss: 165.51376342773438\n",
      "Epoch 62, Batch 7600, Loss: 161.3486785888672\n",
      "Epoch 62, Batch 7601, Loss: 173.52781677246094\n",
      "Epoch 62, Batch 7602, Loss: 177.50184631347656\n",
      "Epoch 62, Batch 7603, Loss: 201.9198760986328\n",
      "Epoch 62, Batch 7604, Loss: 179.66392517089844\n",
      "Epoch 62, Batch 7605, Loss: 166.35751342773438\n",
      "Epoch 62, Batch 7606, Loss: 169.43865966796875\n",
      "Epoch 62, Batch 7607, Loss: 168.96932983398438\n",
      "Epoch 62, Batch 7608, Loss: 184.33917236328125\n",
      "Epoch 62, Batch 7609, Loss: 163.1926727294922\n",
      "Epoch 62, Batch 7610, Loss: 150.55458068847656\n",
      "Epoch 62, Batch 7611, Loss: 177.9680938720703\n",
      "Epoch 62, Batch 7612, Loss: 166.5148468017578\n",
      "Epoch 62, Batch 7613, Loss: 178.6040496826172\n",
      "Epoch 62, Batch 7614, Loss: 174.66607666015625\n",
      "Epoch 62, Batch 7615, Loss: 191.15841674804688\n",
      "Epoch 62, Batch 7616, Loss: 164.9452667236328\n",
      "Epoch 62, Batch 7617, Loss: 154.20042419433594\n",
      "Epoch 62, Batch 7618, Loss: 187.48361206054688\n",
      "Epoch 62, Batch 7619, Loss: 155.28973388671875\n",
      "Epoch 62, Batch 7620, Loss: 155.4137725830078\n",
      "Epoch 62, Batch 7621, Loss: 173.51560974121094\n",
      "Epoch 62, Batch 7622, Loss: 171.84689331054688\n",
      "Epoch 62, Batch 7623, Loss: 182.82266235351562\n",
      "Epoch 62, Batch 7624, Loss: 159.21728515625\n",
      "Epoch 62, Batch 7625, Loss: 173.56582641601562\n",
      "Epoch 62, Batch 7626, Loss: 179.64405822753906\n",
      "Epoch 62, Batch 7627, Loss: 188.6466827392578\n",
      "Epoch 62, Batch 7628, Loss: 170.69932556152344\n",
      "Epoch 62, Batch 7629, Loss: 171.08299255371094\n",
      "Epoch 62, Batch 7630, Loss: 176.09446716308594\n",
      "Epoch 62, Batch 7631, Loss: 182.49951171875\n",
      "Epoch 62, Batch 7632, Loss: 183.20108032226562\n",
      "Epoch 62, Batch 7633, Loss: 165.8663787841797\n",
      "Epoch 62, Batch 7634, Loss: 178.292724609375\n",
      "Epoch 62, Batch 7635, Loss: 183.1343231201172\n",
      "Epoch 62, Batch 7636, Loss: 177.2026824951172\n",
      "Epoch 62, Batch 7637, Loss: 172.363037109375\n",
      "Epoch 62, Batch 7638, Loss: 181.8157958984375\n",
      "Epoch 62, Batch 7639, Loss: 153.47120666503906\n",
      "Epoch 62, Batch 7640, Loss: 169.57225036621094\n",
      "Epoch 62, Batch 7641, Loss: 170.36265563964844\n",
      "Epoch 62, Batch 7642, Loss: 171.0944366455078\n",
      "Epoch 62, Batch 7643, Loss: 173.7401580810547\n",
      "Epoch 62, Batch 7644, Loss: 180.67166137695312\n",
      "Epoch 62, Batch 7645, Loss: 178.76856994628906\n",
      "Epoch 62, Batch 7646, Loss: 168.16339111328125\n",
      "Epoch 62, Batch 7647, Loss: 165.48007202148438\n",
      "Epoch 62, Batch 7648, Loss: 188.62887573242188\n",
      "Epoch 62, Batch 7649, Loss: 174.87001037597656\n",
      "Epoch 62, Batch 7650, Loss: 168.06089782714844\n",
      "Epoch 62, Batch 7651, Loss: 160.79318237304688\n",
      "Epoch 62, Batch 7652, Loss: 178.8272247314453\n",
      "Epoch 62, Batch 7653, Loss: 174.528564453125\n",
      "Epoch 62, Batch 7654, Loss: 163.3408660888672\n",
      "Epoch 62, Batch 7655, Loss: 181.9625244140625\n",
      "Epoch 62, Batch 7656, Loss: 176.31076049804688\n",
      "Epoch 62, Batch 7657, Loss: 189.5755615234375\n",
      "Epoch 62, Batch 7658, Loss: 186.38621520996094\n",
      "Epoch 62, Batch 7659, Loss: 171.52659606933594\n",
      "Epoch 62, Batch 7660, Loss: 171.1470489501953\n",
      "Epoch 62, Batch 7661, Loss: 165.928955078125\n",
      "Epoch 62, Batch 7662, Loss: 172.68287658691406\n",
      "Epoch 62, Batch 7663, Loss: 165.29751586914062\n",
      "Epoch 62, Batch 7664, Loss: 182.36721801757812\n",
      "Epoch 62, Batch 7665, Loss: 192.91712951660156\n",
      "Epoch 62, Batch 7666, Loss: 194.24972534179688\n",
      "Epoch 62, Batch 7667, Loss: 155.88613891601562\n",
      "Epoch 62, Batch 7668, Loss: 166.50796508789062\n",
      "Epoch 62, Batch 7669, Loss: 185.09323120117188\n",
      "Epoch 62, Batch 7670, Loss: 172.30763244628906\n",
      "Epoch 62, Batch 7671, Loss: 151.4696502685547\n",
      "Epoch 62, Batch 7672, Loss: 180.2913055419922\n",
      "Epoch 62, Batch 7673, Loss: 173.5181427001953\n",
      "Epoch 62, Batch 7674, Loss: 167.8062286376953\n",
      "Epoch 62, Batch 7675, Loss: 163.36810302734375\n",
      "Epoch 62, Batch 7676, Loss: 171.8256072998047\n",
      "Epoch 62, Batch 7677, Loss: 173.967529296875\n",
      "Epoch 62, Batch 7678, Loss: 173.11282348632812\n",
      "Epoch 62, Batch 7679, Loss: 161.3325958251953\n",
      "Epoch 62, Batch 7680, Loss: 165.2498016357422\n",
      "Epoch 62, Batch 7681, Loss: 156.466796875\n",
      "Epoch 62, Batch 7682, Loss: 171.2936553955078\n",
      "Epoch 62, Batch 7683, Loss: 172.33316040039062\n",
      "Epoch 62, Batch 7684, Loss: 181.49093627929688\n",
      "Epoch 62, Batch 7685, Loss: 150.86026000976562\n",
      "Epoch 62, Batch 7686, Loss: 177.43077087402344\n",
      "Epoch 62, Batch 7687, Loss: 178.71054077148438\n",
      "Epoch 62, Batch 7688, Loss: 175.74964904785156\n",
      "Epoch 62, Batch 7689, Loss: 183.03350830078125\n",
      "Epoch 62, Batch 7690, Loss: 163.38743591308594\n",
      "Epoch 62, Batch 7691, Loss: 172.78866577148438\n",
      "Epoch 62, Batch 7692, Loss: 182.97128295898438\n",
      "Epoch 62, Batch 7693, Loss: 186.75401306152344\n",
      "Epoch 62, Batch 7694, Loss: 179.8896942138672\n",
      "Epoch 62, Batch 7695, Loss: 172.218017578125\n",
      "Epoch 62, Batch 7696, Loss: 173.68223571777344\n",
      "Epoch 62, Batch 7697, Loss: 178.5326690673828\n",
      "Epoch 62, Batch 7698, Loss: 158.11880493164062\n",
      "Epoch 62, Batch 7699, Loss: 164.38560485839844\n",
      "Epoch 62, Batch 7700, Loss: 170.60333251953125\n",
      "Epoch 62, Batch 7701, Loss: 162.2071075439453\n",
      "Epoch 62, Batch 7702, Loss: 170.24569702148438\n",
      "Epoch 62, Batch 7703, Loss: 160.84030151367188\n",
      "Epoch 62, Batch 7704, Loss: 179.26893615722656\n",
      "Epoch 62, Batch 7705, Loss: 180.32479858398438\n",
      "Epoch 62, Batch 7706, Loss: 185.95896911621094\n",
      "Epoch 62, Batch 7707, Loss: 189.5911102294922\n",
      "Epoch 62, Batch 7708, Loss: 183.00997924804688\n",
      "Epoch 62, Batch 7709, Loss: 173.2772674560547\n",
      "Epoch 62, Batch 7710, Loss: 176.5068817138672\n",
      "Epoch 62, Batch 7711, Loss: 172.76502990722656\n",
      "Epoch 62, Batch 7712, Loss: 183.24929809570312\n",
      "Epoch 62, Batch 7713, Loss: 182.64268493652344\n",
      "Epoch 62, Batch 7714, Loss: 162.40562438964844\n",
      "Epoch 62, Batch 7715, Loss: 182.88742065429688\n",
      "Epoch 62, Batch 7716, Loss: 184.05429077148438\n",
      "Epoch 62, Batch 7717, Loss: 177.8999481201172\n",
      "Epoch 62, Batch 7718, Loss: 174.7890625\n",
      "Epoch 62, Batch 7719, Loss: 176.08102416992188\n",
      "Epoch 62, Batch 7720, Loss: 171.5972900390625\n",
      "Epoch 62, Batch 7721, Loss: 178.99964904785156\n",
      "Epoch 62, Batch 7722, Loss: 168.19671630859375\n",
      "Epoch 62, Batch 7723, Loss: 186.172607421875\n",
      "Epoch 62, Batch 7724, Loss: 175.26707458496094\n",
      "Epoch 62, Batch 7725, Loss: 177.4814453125\n",
      "Epoch 62, Batch 7726, Loss: 184.40501403808594\n",
      "Epoch 62, Batch 7727, Loss: 164.3385467529297\n",
      "Epoch 62, Batch 7728, Loss: 173.2406463623047\n",
      "Epoch 62, Batch 7729, Loss: 172.44639587402344\n",
      "Epoch 62, Batch 7730, Loss: 150.27928161621094\n",
      "Epoch 62, Batch 7731, Loss: 162.97921752929688\n",
      "Epoch 62, Batch 7732, Loss: 168.61181640625\n",
      "Epoch 62, Batch 7733, Loss: 165.36367797851562\n",
      "Epoch 62, Batch 7734, Loss: 170.3218994140625\n",
      "Epoch 62, Batch 7735, Loss: 159.1267547607422\n",
      "Epoch 62, Batch 7736, Loss: 194.696044921875\n",
      "Epoch 62, Batch 7737, Loss: 183.60047912597656\n",
      "Epoch 62, Batch 7738, Loss: 159.24874877929688\n",
      "Epoch 62, Batch 7739, Loss: 161.71868896484375\n",
      "Epoch 62, Batch 7740, Loss: 170.109375\n",
      "Epoch 62, Batch 7741, Loss: 169.53146362304688\n",
      "Epoch 62, Batch 7742, Loss: 166.21096801757812\n",
      "Epoch 62, Batch 7743, Loss: 172.36004638671875\n",
      "Epoch 62, Batch 7744, Loss: 197.4022216796875\n",
      "Epoch 62, Batch 7745, Loss: 162.45582580566406\n",
      "Epoch 62, Batch 7746, Loss: 168.6229248046875\n",
      "Epoch 62, Batch 7747, Loss: 144.90936279296875\n",
      "Epoch 62, Batch 7748, Loss: 184.91567993164062\n",
      "Epoch 62, Batch 7749, Loss: 184.28172302246094\n",
      "Epoch 62, Batch 7750, Loss: 160.3966522216797\n",
      "Epoch 62, Batch 7751, Loss: 184.3417205810547\n",
      "Epoch 62, Batch 7752, Loss: 164.08436584472656\n",
      "Epoch 62, Batch 7753, Loss: 172.0867919921875\n",
      "Epoch 62, Batch 7754, Loss: 175.50169372558594\n",
      "Epoch 62, Batch 7755, Loss: 175.86842346191406\n",
      "Epoch 62, Batch 7756, Loss: 162.968994140625\n",
      "Epoch 62, Batch 7757, Loss: 167.6159210205078\n",
      "Epoch 62, Batch 7758, Loss: 172.7100067138672\n",
      "Epoch 62, Batch 7759, Loss: 181.0009002685547\n",
      "Epoch 62, Batch 7760, Loss: 160.43162536621094\n",
      "Epoch 62, Batch 7761, Loss: 162.52175903320312\n",
      "Epoch 62, Batch 7762, Loss: 171.33810424804688\n",
      "Epoch 62, Batch 7763, Loss: 180.53021240234375\n",
      "Epoch 62, Batch 7764, Loss: 167.0601043701172\n",
      "Epoch 62, Batch 7765, Loss: 179.5906219482422\n",
      "Epoch 62, Batch 7766, Loss: 167.05435180664062\n",
      "Epoch 62, Batch 7767, Loss: 183.24176025390625\n",
      "Epoch 62, Batch 7768, Loss: 177.12318420410156\n",
      "Epoch 62, Batch 7769, Loss: 186.13449096679688\n",
      "Epoch 62, Batch 7770, Loss: 173.8428192138672\n",
      "Epoch 62, Batch 7771, Loss: 172.37074279785156\n",
      "Epoch 62, Batch 7772, Loss: 168.3571319580078\n",
      "Epoch 62, Batch 7773, Loss: 185.23292541503906\n",
      "Epoch 62, Batch 7774, Loss: 174.54656982421875\n",
      "Epoch 62, Batch 7775, Loss: 166.38125610351562\n",
      "Epoch 62, Batch 7776, Loss: 167.3321533203125\n",
      "Epoch 62, Batch 7777, Loss: 173.27621459960938\n",
      "Epoch 62, Batch 7778, Loss: 166.47543334960938\n",
      "Epoch 62, Batch 7779, Loss: 166.4990234375\n",
      "Epoch 62, Batch 7780, Loss: 184.88067626953125\n",
      "Epoch 62, Batch 7781, Loss: 164.8267364501953\n",
      "Epoch 62, Batch 7782, Loss: 166.2825164794922\n",
      "Epoch 62, Batch 7783, Loss: 178.28897094726562\n",
      "Epoch 62, Batch 7784, Loss: 188.49948120117188\n",
      "Epoch 62, Batch 7785, Loss: 181.51524353027344\n",
      "Epoch 62, Batch 7786, Loss: 146.3608856201172\n",
      "Epoch 62, Batch 7787, Loss: 172.20449829101562\n",
      "Epoch 62, Batch 7788, Loss: 188.5113525390625\n",
      "Epoch 62, Batch 7789, Loss: 168.30801391601562\n",
      "Epoch 62, Batch 7790, Loss: 197.0663299560547\n",
      "Epoch 62, Batch 7791, Loss: 189.0294952392578\n",
      "Epoch 62, Batch 7792, Loss: 182.0756072998047\n",
      "Epoch 62, Batch 7793, Loss: 183.68943786621094\n",
      "Epoch 62, Batch 7794, Loss: 172.94964599609375\n",
      "Epoch 62, Batch 7795, Loss: 169.976318359375\n",
      "Epoch 62, Batch 7796, Loss: 172.15298461914062\n",
      "Epoch 62, Batch 7797, Loss: 169.3131561279297\n",
      "Epoch 62, Batch 7798, Loss: 177.2764129638672\n",
      "Epoch 62, Batch 7799, Loss: 179.1913299560547\n",
      "Epoch 62, Batch 7800, Loss: 181.2627716064453\n",
      "Epoch 62, Batch 7801, Loss: 192.86532592773438\n",
      "Epoch 62, Batch 7802, Loss: 157.74842834472656\n",
      "Epoch 62, Batch 7803, Loss: 164.1326141357422\n",
      "Epoch 62, Batch 7804, Loss: 163.6465606689453\n",
      "Epoch 62, Batch 7805, Loss: 175.6702117919922\n",
      "Epoch 62, Batch 7806, Loss: 168.35818481445312\n",
      "Epoch 62, Batch 7807, Loss: 156.60511779785156\n",
      "Epoch 62, Batch 7808, Loss: 185.66592407226562\n",
      "Epoch 62, Batch 7809, Loss: 162.85443115234375\n",
      "Epoch 62, Batch 7810, Loss: 169.52732849121094\n",
      "Epoch 62, Batch 7811, Loss: 157.24649047851562\n",
      "Epoch 62, Batch 7812, Loss: 163.0000457763672\n",
      "Epoch 62, Batch 7813, Loss: 181.1766357421875\n",
      "Epoch 62, Batch 7814, Loss: 183.76742553710938\n",
      "Epoch 62, Batch 7815, Loss: 151.68374633789062\n",
      "Epoch 62, Batch 7816, Loss: 166.05343627929688\n",
      "Epoch 62, Batch 7817, Loss: 158.25938415527344\n",
      "Epoch 62, Batch 7818, Loss: 165.37460327148438\n",
      "Epoch 62, Batch 7819, Loss: 182.3843231201172\n",
      "Epoch 62, Batch 7820, Loss: 163.6367950439453\n",
      "Epoch 62, Batch 7821, Loss: 182.4870147705078\n",
      "Epoch 62, Batch 7822, Loss: 160.855224609375\n",
      "Epoch 62, Batch 7823, Loss: 191.42678833007812\n",
      "Epoch 62, Batch 7824, Loss: 173.4671173095703\n",
      "Epoch 62, Batch 7825, Loss: 169.60411071777344\n",
      "Epoch 62, Batch 7826, Loss: 164.70321655273438\n",
      "Epoch 62, Batch 7827, Loss: 197.9824981689453\n",
      "Epoch 62, Batch 7828, Loss: 174.8559112548828\n",
      "Epoch 62, Batch 7829, Loss: 174.1054229736328\n",
      "Epoch 62, Batch 7830, Loss: 153.99996948242188\n",
      "Epoch 62, Batch 7831, Loss: 162.0652313232422\n",
      "Epoch 62, Batch 7832, Loss: 156.5293426513672\n",
      "Epoch 62, Batch 7833, Loss: 164.9674530029297\n",
      "Epoch 62, Batch 7834, Loss: 155.19264221191406\n",
      "Epoch 62, Batch 7835, Loss: 156.50279235839844\n",
      "Epoch 62, Batch 7836, Loss: 166.8976593017578\n",
      "Epoch 62, Batch 7837, Loss: 175.2373504638672\n",
      "Epoch 62, Batch 7838, Loss: 189.35606384277344\n",
      "Epoch 62, Batch 7839, Loss: 156.58839416503906\n",
      "Epoch 62, Batch 7840, Loss: 160.36581420898438\n",
      "Epoch 62, Batch 7841, Loss: 164.22801208496094\n",
      "Epoch 62, Batch 7842, Loss: 164.07626342773438\n",
      "Epoch 62, Batch 7843, Loss: 183.51937866210938\n",
      "Epoch 62, Batch 7844, Loss: 158.21929931640625\n",
      "Epoch 62, Batch 7845, Loss: 154.82232666015625\n",
      "Epoch 62, Batch 7846, Loss: 176.8817138671875\n",
      "Epoch 62, Batch 7847, Loss: 166.9499053955078\n",
      "Epoch 62, Batch 7848, Loss: 161.8249969482422\n",
      "Epoch 62, Batch 7849, Loss: 167.4060516357422\n",
      "Epoch 62, Batch 7850, Loss: 172.49203491210938\n",
      "Epoch 62, Batch 7851, Loss: 176.28802490234375\n",
      "Epoch 62, Batch 7852, Loss: 184.14366149902344\n",
      "Epoch 62, Batch 7853, Loss: 187.3444366455078\n",
      "Epoch 62, Batch 7854, Loss: 194.2969207763672\n",
      "Epoch 62, Batch 7855, Loss: 174.3235321044922\n",
      "Epoch 62, Batch 7856, Loss: 161.1371307373047\n",
      "Epoch 62, Batch 7857, Loss: 191.12704467773438\n",
      "Epoch 62, Batch 7858, Loss: 182.16189575195312\n",
      "Epoch 62, Batch 7859, Loss: 180.30397033691406\n",
      "Epoch 62, Batch 7860, Loss: 168.93551635742188\n",
      "Epoch 62, Batch 7861, Loss: 167.25628662109375\n",
      "Epoch 62, Batch 7862, Loss: 167.18865966796875\n",
      "Epoch 62, Batch 7863, Loss: 163.61004638671875\n",
      "Epoch 62, Batch 7864, Loss: 168.41744995117188\n",
      "Epoch 62, Batch 7865, Loss: 176.81321716308594\n",
      "Epoch 62, Batch 7866, Loss: 176.4672088623047\n",
      "Epoch 62, Batch 7867, Loss: 184.52011108398438\n",
      "Epoch 62, Batch 7868, Loss: 179.54580688476562\n",
      "Epoch 62, Batch 7869, Loss: 155.83944702148438\n",
      "Epoch 62, Batch 7870, Loss: 167.8501434326172\n",
      "Epoch 62, Batch 7871, Loss: 185.35781860351562\n",
      "Epoch 62, Batch 7872, Loss: 168.4392547607422\n",
      "Epoch 62, Batch 7873, Loss: 167.79995727539062\n",
      "Epoch 62, Batch 7874, Loss: 177.4127655029297\n",
      "Epoch 62, Batch 7875, Loss: 172.65060424804688\n",
      "Epoch 62, Batch 7876, Loss: 198.91552734375\n",
      "Epoch 62, Batch 7877, Loss: 167.04995727539062\n",
      "Epoch 62, Batch 7878, Loss: 170.5912322998047\n",
      "Epoch 62, Batch 7879, Loss: 175.91171264648438\n",
      "Epoch 62, Batch 7880, Loss: 179.56846618652344\n",
      "Epoch 62, Batch 7881, Loss: 159.25946044921875\n",
      "Epoch 62, Batch 7882, Loss: 184.44638061523438\n",
      "Epoch 62, Batch 7883, Loss: 155.2797393798828\n",
      "Epoch 62, Batch 7884, Loss: 189.25689697265625\n",
      "Epoch 62, Batch 7885, Loss: 161.00982666015625\n",
      "Epoch 62, Batch 7886, Loss: 176.0831298828125\n",
      "Epoch 62, Batch 7887, Loss: 171.37484741210938\n",
      "Epoch 62, Batch 7888, Loss: 175.58065795898438\n",
      "Epoch 62, Batch 7889, Loss: 173.1934814453125\n",
      "Epoch 62, Batch 7890, Loss: 178.8797149658203\n",
      "Epoch 62, Batch 7891, Loss: 165.61183166503906\n",
      "Epoch 62, Batch 7892, Loss: 160.90553283691406\n",
      "Epoch 62, Batch 7893, Loss: 169.1334686279297\n",
      "Epoch 62, Batch 7894, Loss: 177.93504333496094\n",
      "Epoch 62, Batch 7895, Loss: 159.883056640625\n",
      "Epoch 62, Batch 7896, Loss: 162.374267578125\n",
      "Epoch 62, Batch 7897, Loss: 174.99436950683594\n",
      "Epoch 62, Batch 7898, Loss: 174.6708984375\n",
      "Epoch 62, Batch 7899, Loss: 184.79290771484375\n",
      "Epoch 62, Batch 7900, Loss: 166.1322021484375\n",
      "Epoch 62, Batch 7901, Loss: 174.52023315429688\n",
      "Epoch 62, Batch 7902, Loss: 167.71536254882812\n",
      "Epoch 62, Batch 7903, Loss: 161.06204223632812\n",
      "Epoch 62, Batch 7904, Loss: 153.3509521484375\n",
      "Epoch 62, Batch 7905, Loss: 172.40151977539062\n",
      "Epoch 62, Batch 7906, Loss: 154.19168090820312\n",
      "Epoch 62, Batch 7907, Loss: 163.07061767578125\n",
      "Epoch 62, Batch 7908, Loss: 170.74673461914062\n",
      "Epoch 62, Batch 7909, Loss: 174.57562255859375\n",
      "Epoch 62, Batch 7910, Loss: 163.4397430419922\n",
      "Epoch 62, Batch 7911, Loss: 182.17578125\n",
      "Epoch 62, Batch 7912, Loss: 159.4285125732422\n",
      "Epoch 62, Batch 7913, Loss: 174.8719024658203\n",
      "Epoch 62, Batch 7914, Loss: 165.2749481201172\n",
      "Epoch 62, Batch 7915, Loss: 169.23146057128906\n",
      "Epoch 62, Batch 7916, Loss: 181.67051696777344\n",
      "Epoch 62, Batch 7917, Loss: 167.9788818359375\n",
      "Epoch 62, Batch 7918, Loss: 167.4426727294922\n",
      "Epoch 62, Batch 7919, Loss: 149.6629638671875\n",
      "Epoch 62, Batch 7920, Loss: 177.91079711914062\n",
      "Epoch 62, Batch 7921, Loss: 173.31983947753906\n",
      "Epoch 62, Batch 7922, Loss: 185.1013946533203\n",
      "Epoch 62, Batch 7923, Loss: 176.55471801757812\n",
      "Epoch 62, Batch 7924, Loss: 170.5625457763672\n",
      "Epoch 62, Batch 7925, Loss: 164.21847534179688\n",
      "Epoch 62, Batch 7926, Loss: 170.73666381835938\n",
      "Epoch 62, Batch 7927, Loss: 164.40640258789062\n",
      "Epoch 62, Batch 7928, Loss: 174.9349822998047\n",
      "Epoch 62, Batch 7929, Loss: 176.43446350097656\n",
      "Epoch 62, Batch 7930, Loss: 173.74249267578125\n",
      "Epoch 62, Batch 7931, Loss: 189.86724853515625\n",
      "Epoch 62, Batch 7932, Loss: 177.78660583496094\n",
      "Epoch 62, Batch 7933, Loss: 175.10415649414062\n",
      "Epoch 62, Batch 7934, Loss: 193.3414306640625\n",
      "Epoch 62, Batch 7935, Loss: 183.0303955078125\n",
      "Epoch 62, Batch 7936, Loss: 168.04306030273438\n",
      "Epoch 62, Batch 7937, Loss: 175.9044647216797\n",
      "Epoch 62, Batch 7938, Loss: 164.77073669433594\n",
      "Epoch 62, Batch 7939, Loss: 170.36203002929688\n",
      "Epoch 62, Batch 7940, Loss: 184.36575317382812\n",
      "Epoch 62, Batch 7941, Loss: 160.3777313232422\n",
      "Epoch 62, Batch 7942, Loss: 180.98541259765625\n",
      "Epoch 62, Batch 7943, Loss: 181.27479553222656\n",
      "Epoch 62, Batch 7944, Loss: 171.3601837158203\n",
      "Epoch 62, Batch 7945, Loss: 162.7526397705078\n",
      "Epoch 62, Batch 7946, Loss: 165.7753448486328\n",
      "Epoch 62, Batch 7947, Loss: 188.13185119628906\n",
      "Epoch 62, Batch 7948, Loss: 167.61741638183594\n",
      "Epoch 62, Batch 7949, Loss: 167.10311889648438\n",
      "Epoch 62, Batch 7950, Loss: 163.53302001953125\n",
      "Epoch 62, Batch 7951, Loss: 183.63072204589844\n",
      "Epoch 62, Batch 7952, Loss: 181.05958557128906\n",
      "Epoch 62, Batch 7953, Loss: 170.7423095703125\n",
      "Epoch 62, Batch 7954, Loss: 188.27731323242188\n",
      "Epoch 62, Batch 7955, Loss: 174.03883361816406\n",
      "Epoch 62, Batch 7956, Loss: 173.56109619140625\n",
      "Epoch 62, Batch 7957, Loss: 169.2968292236328\n",
      "Epoch 62, Batch 7958, Loss: 174.05284118652344\n",
      "Epoch 62, Batch 7959, Loss: 170.80810546875\n",
      "Epoch 62, Batch 7960, Loss: 180.54067993164062\n",
      "Epoch 62, Batch 7961, Loss: 186.12930297851562\n",
      "Epoch 62, Batch 7962, Loss: 151.88070678710938\n",
      "Epoch 62, Batch 7963, Loss: 162.62586975097656\n",
      "Epoch 62, Batch 7964, Loss: 165.2168731689453\n",
      "Epoch 62, Batch 7965, Loss: 168.74261474609375\n",
      "Epoch 62, Batch 7966, Loss: 173.6533203125\n",
      "Epoch 62, Batch 7967, Loss: 164.95758056640625\n",
      "Epoch 62, Batch 7968, Loss: 179.71444702148438\n",
      "Epoch 62, Batch 7969, Loss: 178.75100708007812\n",
      "Epoch 62, Batch 7970, Loss: 148.77328491210938\n",
      "Epoch 62, Batch 7971, Loss: 163.12835693359375\n",
      "Epoch 62, Batch 7972, Loss: 157.00897216796875\n",
      "Epoch 62, Batch 7973, Loss: 193.1852569580078\n",
      "Epoch 62, Batch 7974, Loss: 162.4695281982422\n",
      "Epoch 62, Batch 7975, Loss: 176.37625122070312\n",
      "Epoch 62, Batch 7976, Loss: 173.039306640625\n",
      "Epoch 62, Batch 7977, Loss: 190.0928192138672\n",
      "Epoch 62, Batch 7978, Loss: 182.93182373046875\n",
      "Epoch 62, Batch 7979, Loss: 196.68873596191406\n",
      "Epoch 62, Batch 7980, Loss: 168.44886779785156\n",
      "Epoch 62, Batch 7981, Loss: 188.24082946777344\n",
      "Epoch 62, Batch 7982, Loss: 165.38218688964844\n",
      "Epoch 62, Batch 7983, Loss: 166.73439025878906\n",
      "Epoch 62, Batch 7984, Loss: 168.78868103027344\n",
      "Epoch 62, Batch 7985, Loss: 178.71124267578125\n",
      "Epoch 62, Batch 7986, Loss: 170.5545654296875\n",
      "Epoch 62, Batch 7987, Loss: 175.26490783691406\n",
      "Epoch 62, Batch 7988, Loss: 172.5030059814453\n",
      "Epoch 62, Batch 7989, Loss: 180.2480010986328\n",
      "Epoch 62, Batch 7990, Loss: 163.3089141845703\n",
      "Epoch 62, Batch 7991, Loss: 174.06265258789062\n",
      "Epoch 62, Batch 7992, Loss: 163.372314453125\n",
      "Epoch 62, Batch 7993, Loss: 162.69757080078125\n",
      "Epoch 62, Batch 7994, Loss: 171.4134521484375\n",
      "Epoch 62, Batch 7995, Loss: 161.712646484375\n",
      "Epoch 62, Batch 7996, Loss: 163.78335571289062\n",
      "Epoch 62, Batch 7997, Loss: 180.2023162841797\n",
      "Epoch 62, Batch 7998, Loss: 174.3967742919922\n",
      "Epoch 62, Batch 7999, Loss: 178.88858032226562\n",
      "Epoch 62, Batch 8000, Loss: 177.7470703125\n",
      "Epoch 62, Batch 8001, Loss: 168.78582763671875\n",
      "Epoch 62, Batch 8002, Loss: 174.4093017578125\n",
      "Epoch 62, Batch 8003, Loss: 167.2329559326172\n",
      "Epoch 62, Batch 8004, Loss: 163.2014617919922\n",
      "Epoch 62, Batch 8005, Loss: 167.6827392578125\n",
      "Epoch 62, Batch 8006, Loss: 171.6846466064453\n",
      "Epoch 62, Batch 8007, Loss: 171.24664306640625\n",
      "Epoch 62, Batch 8008, Loss: 170.00636291503906\n",
      "Epoch 62, Batch 8009, Loss: 186.59432983398438\n",
      "Epoch 62, Batch 8010, Loss: 177.29554748535156\n",
      "Epoch 62, Batch 8011, Loss: 182.32521057128906\n",
      "Epoch 62, Batch 8012, Loss: 174.0556640625\n",
      "Epoch 62, Batch 8013, Loss: 183.94464111328125\n",
      "Epoch 62, Batch 8014, Loss: 167.1409149169922\n",
      "Epoch 62, Batch 8015, Loss: 179.3664093017578\n",
      "Epoch 62, Batch 8016, Loss: 170.61666870117188\n",
      "Epoch 62, Batch 8017, Loss: 190.6207275390625\n",
      "Epoch 62, Batch 8018, Loss: 180.84405517578125\n",
      "Epoch 62, Batch 8019, Loss: 174.34461975097656\n",
      "Epoch 62, Batch 8020, Loss: 174.3000946044922\n",
      "Epoch 62, Batch 8021, Loss: 173.16465759277344\n",
      "Epoch 62, Batch 8022, Loss: 160.3935089111328\n",
      "Epoch 62, Batch 8023, Loss: 161.41148376464844\n",
      "Epoch 62, Batch 8024, Loss: 170.85101318359375\n",
      "Epoch 62, Batch 8025, Loss: 178.69479370117188\n",
      "Epoch 62, Batch 8026, Loss: 188.5638427734375\n",
      "Epoch 62, Batch 8027, Loss: 176.9723358154297\n",
      "Epoch 62, Batch 8028, Loss: 159.96441650390625\n",
      "Epoch 62, Batch 8029, Loss: 176.14382934570312\n",
      "Epoch 62, Batch 8030, Loss: 168.12684631347656\n",
      "Epoch 62, Batch 8031, Loss: 184.1984100341797\n",
      "Epoch 62, Batch 8032, Loss: 180.14854431152344\n",
      "Epoch 62, Batch 8033, Loss: 177.58070373535156\n",
      "Epoch 62, Batch 8034, Loss: 165.73251342773438\n",
      "Epoch 62, Batch 8035, Loss: 181.09461975097656\n",
      "Epoch 62, Batch 8036, Loss: 166.88717651367188\n",
      "Epoch 62, Batch 8037, Loss: 166.88612365722656\n",
      "Epoch 62, Batch 8038, Loss: 179.14276123046875\n",
      "Epoch 62, Batch 8039, Loss: 184.76441955566406\n",
      "Epoch 62, Batch 8040, Loss: 167.42294311523438\n",
      "Epoch 62, Batch 8041, Loss: 167.04335021972656\n",
      "Epoch 62, Batch 8042, Loss: 164.3793487548828\n",
      "Epoch 62, Batch 8043, Loss: 167.7713165283203\n",
      "Epoch 62, Batch 8044, Loss: 177.88352966308594\n",
      "Epoch 62, Batch 8045, Loss: 162.09507751464844\n",
      "Epoch 62, Batch 8046, Loss: 153.87734985351562\n",
      "Epoch 62, Batch 8047, Loss: 170.32217407226562\n",
      "Epoch 62, Batch 8048, Loss: 179.97044372558594\n",
      "Epoch 62, Batch 8049, Loss: 160.58187866210938\n",
      "Epoch 62, Batch 8050, Loss: 181.6094207763672\n",
      "Epoch 62, Batch 8051, Loss: 173.53842163085938\n",
      "Epoch 62, Batch 8052, Loss: 183.36337280273438\n",
      "Epoch 62, Batch 8053, Loss: 163.2522735595703\n",
      "Epoch 62, Batch 8054, Loss: 185.45587158203125\n",
      "Epoch 62, Batch 8055, Loss: 174.541015625\n",
      "Epoch 62, Batch 8056, Loss: 166.9464111328125\n",
      "Epoch 62, Batch 8057, Loss: 168.93411254882812\n",
      "Epoch 62, Batch 8058, Loss: 170.69454956054688\n",
      "Epoch 62, Batch 8059, Loss: 184.034423828125\n",
      "Epoch 62, Batch 8060, Loss: 211.1194305419922\n",
      "Epoch 62, Batch 8061, Loss: 173.201171875\n",
      "Epoch 62, Batch 8062, Loss: 170.42962646484375\n",
      "Epoch 62, Batch 8063, Loss: 176.43785095214844\n",
      "Epoch 62, Batch 8064, Loss: 156.02096557617188\n",
      "Epoch 62, Batch 8065, Loss: 188.55538940429688\n",
      "Epoch 62, Batch 8066, Loss: 180.86302185058594\n",
      "Epoch 62, Batch 8067, Loss: 177.32733154296875\n",
      "Epoch 62, Batch 8068, Loss: 155.25205993652344\n",
      "Epoch 62, Batch 8069, Loss: 169.26588439941406\n",
      "Epoch 62, Batch 8070, Loss: 172.45880126953125\n",
      "Epoch 62, Batch 8071, Loss: 175.74794006347656\n",
      "Epoch 62, Batch 8072, Loss: 173.76890563964844\n",
      "Epoch 62, Batch 8073, Loss: 177.62294006347656\n",
      "Epoch 62, Batch 8074, Loss: 194.63949584960938\n",
      "Epoch 62, Batch 8075, Loss: 196.69247436523438\n",
      "Epoch 62, Batch 8076, Loss: 156.79783630371094\n",
      "Epoch 62, Batch 8077, Loss: 169.36830139160156\n",
      "Epoch 62, Batch 8078, Loss: 168.86737060546875\n",
      "Epoch 62, Batch 8079, Loss: 167.4327392578125\n",
      "Epoch 62, Batch 8080, Loss: 165.58737182617188\n",
      "Epoch 62, Batch 8081, Loss: 158.1468963623047\n",
      "Epoch 62, Batch 8082, Loss: 162.05958557128906\n",
      "Epoch 62, Batch 8083, Loss: 166.9664764404297\n",
      "Epoch 62, Batch 8084, Loss: 161.42674255371094\n",
      "Epoch 62, Batch 8085, Loss: 161.1255645751953\n",
      "Epoch 62, Batch 8086, Loss: 164.9008331298828\n",
      "Epoch 62, Batch 8087, Loss: 182.8480682373047\n",
      "Epoch 62, Batch 8088, Loss: 172.90768432617188\n",
      "Epoch 62, Batch 8089, Loss: 167.5848846435547\n",
      "Epoch 62, Batch 8090, Loss: 177.27870178222656\n",
      "Epoch 62, Batch 8091, Loss: 164.4691925048828\n",
      "Epoch 62, Batch 8092, Loss: 156.0345916748047\n",
      "Epoch 62, Batch 8093, Loss: 172.47315979003906\n",
      "Epoch 62, Batch 8094, Loss: 165.54930114746094\n",
      "Epoch 62, Batch 8095, Loss: 163.54978942871094\n",
      "Epoch 62, Batch 8096, Loss: 178.28448486328125\n",
      "Epoch 62, Batch 8097, Loss: 173.73045349121094\n",
      "Epoch 62, Batch 8098, Loss: 177.86705017089844\n",
      "Epoch 62, Batch 8099, Loss: 172.1537322998047\n",
      "Epoch 62, Batch 8100, Loss: 177.0149688720703\n",
      "Epoch 62, Batch 8101, Loss: 167.9305419921875\n",
      "Epoch 62, Batch 8102, Loss: 159.735107421875\n",
      "Epoch 62, Batch 8103, Loss: 162.1678009033203\n",
      "Epoch 62, Batch 8104, Loss: 193.2097930908203\n",
      "Epoch 62, Batch 8105, Loss: 187.21392822265625\n",
      "Epoch 62, Batch 8106, Loss: 173.87039184570312\n",
      "Epoch 62, Batch 8107, Loss: 183.92953491210938\n",
      "Epoch 62, Batch 8108, Loss: 203.38926696777344\n",
      "Epoch 62, Batch 8109, Loss: 171.8649444580078\n",
      "Epoch 62, Batch 8110, Loss: 172.68878173828125\n",
      "Epoch 62, Batch 8111, Loss: 177.00218200683594\n",
      "Epoch 62, Batch 8112, Loss: 179.9348602294922\n",
      "Epoch 62, Batch 8113, Loss: 162.81895446777344\n",
      "Epoch 62, Batch 8114, Loss: 175.77024841308594\n",
      "Epoch 62, Batch 8115, Loss: 178.4476318359375\n",
      "Epoch 62, Batch 8116, Loss: 171.26934814453125\n",
      "Epoch 62, Batch 8117, Loss: 185.20399475097656\n",
      "Epoch 62, Batch 8118, Loss: 168.61767578125\n",
      "Epoch 62, Batch 8119, Loss: 175.9065399169922\n",
      "Epoch 62, Batch 8120, Loss: 199.33892822265625\n",
      "Epoch 62, Batch 8121, Loss: 170.82196044921875\n",
      "Epoch 62, Batch 8122, Loss: 180.3323211669922\n",
      "Epoch 62, Batch 8123, Loss: 169.3818359375\n",
      "Epoch 62, Batch 8124, Loss: 197.52952575683594\n",
      "Epoch 62, Batch 8125, Loss: 164.61566162109375\n",
      "Epoch 62, Batch 8126, Loss: 184.01589965820312\n",
      "Epoch 62, Batch 8127, Loss: 182.18504333496094\n",
      "Epoch 62, Batch 8128, Loss: 181.48590087890625\n",
      "Epoch 62, Batch 8129, Loss: 179.7921142578125\n",
      "Epoch 62, Batch 8130, Loss: 186.54017639160156\n",
      "Epoch 62, Batch 8131, Loss: 176.0888214111328\n",
      "Epoch 62, Batch 8132, Loss: 164.46932983398438\n",
      "Epoch 62, Batch 8133, Loss: 173.67420959472656\n",
      "Epoch 62, Batch 8134, Loss: 170.51956176757812\n",
      "Epoch 62, Batch 8135, Loss: 170.83346557617188\n",
      "Epoch 62, Batch 8136, Loss: 176.1573028564453\n",
      "Epoch 62, Batch 8137, Loss: 191.06509399414062\n",
      "Epoch 62, Batch 8138, Loss: 186.56349182128906\n",
      "Epoch 62, Batch 8139, Loss: 176.85447692871094\n",
      "Epoch 62, Batch 8140, Loss: 171.88119506835938\n",
      "Epoch 62, Batch 8141, Loss: 172.6597900390625\n",
      "Epoch 62, Batch 8142, Loss: 160.68882751464844\n",
      "Epoch 62, Batch 8143, Loss: 184.40216064453125\n",
      "Epoch 62, Batch 8144, Loss: 168.2393798828125\n",
      "Epoch 62, Batch 8145, Loss: 180.61476135253906\n",
      "Epoch 62, Batch 8146, Loss: 170.91561889648438\n",
      "Epoch 62, Batch 8147, Loss: 165.07904052734375\n",
      "Epoch 62, Batch 8148, Loss: 184.48529052734375\n",
      "Epoch 62, Batch 8149, Loss: 170.17796325683594\n",
      "Epoch 62, Batch 8150, Loss: 175.25570678710938\n",
      "Epoch 62, Batch 8151, Loss: 170.11380004882812\n",
      "Epoch 62, Batch 8152, Loss: 176.4806671142578\n",
      "Epoch 62, Batch 8153, Loss: 183.3353271484375\n",
      "Epoch 62, Batch 8154, Loss: 174.43788146972656\n",
      "Epoch 62, Batch 8155, Loss: 169.16358947753906\n",
      "Epoch 62, Batch 8156, Loss: 169.0814971923828\n",
      "Epoch 62, Batch 8157, Loss: 190.67164611816406\n",
      "Epoch 62, Batch 8158, Loss: 173.19320678710938\n",
      "Epoch 62, Batch 8159, Loss: 180.5617218017578\n",
      "Epoch 62, Batch 8160, Loss: 188.6981658935547\n",
      "Epoch 62, Batch 8161, Loss: 178.22560119628906\n",
      "Epoch 62, Batch 8162, Loss: 165.55227661132812\n",
      "Epoch 62, Batch 8163, Loss: 170.57632446289062\n",
      "Epoch 62, Batch 8164, Loss: 193.28741455078125\n",
      "Epoch 62, Batch 8165, Loss: 168.2210693359375\n",
      "Epoch 62, Batch 8166, Loss: 168.7813262939453\n",
      "Epoch 62, Batch 8167, Loss: 180.4697265625\n",
      "Epoch 62, Batch 8168, Loss: 168.17037963867188\n",
      "Epoch 62, Batch 8169, Loss: 164.18426513671875\n",
      "Epoch 62, Batch 8170, Loss: 167.1663818359375\n",
      "Epoch 62, Batch 8171, Loss: 182.63784790039062\n",
      "Epoch 62, Batch 8172, Loss: 167.42164611816406\n",
      "Epoch 62, Batch 8173, Loss: 190.20755004882812\n",
      "Epoch 62, Batch 8174, Loss: 179.46583557128906\n",
      "Epoch 62, Batch 8175, Loss: 173.41961669921875\n",
      "Epoch 62, Batch 8176, Loss: 168.4368896484375\n",
      "Epoch 62, Batch 8177, Loss: 160.4839324951172\n",
      "Epoch 62, Batch 8178, Loss: 176.11595153808594\n",
      "Epoch 62, Batch 8179, Loss: 197.70074462890625\n",
      "Epoch 62, Batch 8180, Loss: 181.3027801513672\n",
      "Epoch 62, Batch 8181, Loss: 182.71316528320312\n",
      "Epoch 62, Batch 8182, Loss: 159.48733520507812\n",
      "Epoch 62, Batch 8183, Loss: 166.72354125976562\n",
      "Epoch 62, Batch 8184, Loss: 182.5601348876953\n",
      "Epoch 62, Batch 8185, Loss: 181.489501953125\n",
      "Epoch 62, Batch 8186, Loss: 184.1150360107422\n",
      "Epoch 62, Batch 8187, Loss: 166.0933380126953\n",
      "Epoch 62, Batch 8188, Loss: 157.95452880859375\n",
      "Epoch 62, Batch 8189, Loss: 183.02684020996094\n",
      "Epoch 62, Batch 8190, Loss: 173.63905334472656\n",
      "Epoch 62, Batch 8191, Loss: 160.2519989013672\n",
      "Epoch 62, Batch 8192, Loss: 176.22781372070312\n",
      "Epoch 62, Batch 8193, Loss: 184.20492553710938\n",
      "Epoch 62, Batch 8194, Loss: 180.20738220214844\n",
      "Epoch 62, Batch 8195, Loss: 173.03746032714844\n",
      "Epoch 62, Batch 8196, Loss: 151.43121337890625\n",
      "Epoch 62, Batch 8197, Loss: 179.39471435546875\n",
      "Epoch 62, Batch 8198, Loss: 170.1184844970703\n",
      "Epoch 62, Batch 8199, Loss: 180.28375244140625\n",
      "Epoch 62, Batch 8200, Loss: 161.61233520507812\n",
      "Epoch 62, Batch 8201, Loss: 177.35787963867188\n",
      "Epoch 62, Batch 8202, Loss: 177.59945678710938\n",
      "Epoch 62, Batch 8203, Loss: 167.7716064453125\n",
      "Epoch 62, Batch 8204, Loss: 160.7416534423828\n",
      "Epoch 62, Batch 8205, Loss: 180.08322143554688\n",
      "Epoch 62, Batch 8206, Loss: 165.03480529785156\n",
      "Epoch 62, Batch 8207, Loss: 175.75979614257812\n",
      "Epoch 62, Batch 8208, Loss: 188.46026611328125\n",
      "Epoch 62, Batch 8209, Loss: 159.90609741210938\n",
      "Epoch 62, Batch 8210, Loss: 172.4288330078125\n",
      "Epoch 62, Batch 8211, Loss: 162.98712158203125\n",
      "Epoch 62, Batch 8212, Loss: 170.27325439453125\n",
      "Epoch 62, Batch 8213, Loss: 194.87281799316406\n",
      "Epoch 62, Batch 8214, Loss: 166.9903106689453\n",
      "Epoch 62, Batch 8215, Loss: 163.95486450195312\n",
      "Epoch 62, Batch 8216, Loss: 168.6874542236328\n",
      "Epoch 62, Batch 8217, Loss: 168.8426513671875\n",
      "Epoch 62, Batch 8218, Loss: 179.6636962890625\n",
      "Epoch 62, Batch 8219, Loss: 176.4558563232422\n",
      "Epoch 62, Batch 8220, Loss: 164.56053161621094\n",
      "Epoch 62, Batch 8221, Loss: 183.20240783691406\n",
      "Epoch 62, Batch 8222, Loss: 170.3826141357422\n",
      "Epoch 62, Batch 8223, Loss: 170.08547973632812\n",
      "Epoch 62, Batch 8224, Loss: 179.28878784179688\n",
      "Epoch 62, Batch 8225, Loss: 178.52789306640625\n",
      "Epoch 62, Batch 8226, Loss: 174.6887969970703\n",
      "Epoch 62, Batch 8227, Loss: 176.25967407226562\n",
      "Epoch 62, Batch 8228, Loss: 167.8958740234375\n",
      "Epoch 62, Batch 8229, Loss: 169.5744171142578\n",
      "Epoch 62, Batch 8230, Loss: 171.2633056640625\n",
      "Epoch 62, Batch 8231, Loss: 174.01010131835938\n",
      "Epoch 62, Batch 8232, Loss: 175.0017547607422\n",
      "Epoch 62, Batch 8233, Loss: 172.8250732421875\n",
      "Epoch 62, Batch 8234, Loss: 171.62841796875\n",
      "Epoch 62, Batch 8235, Loss: 164.8874053955078\n",
      "Epoch 62, Batch 8236, Loss: 170.34674072265625\n",
      "Epoch 62, Batch 8237, Loss: 171.6395263671875\n",
      "Epoch 62, Batch 8238, Loss: 172.02316284179688\n",
      "Epoch 62, Batch 8239, Loss: 172.48350524902344\n",
      "Epoch 62, Batch 8240, Loss: 174.0234832763672\n",
      "Epoch 62, Batch 8241, Loss: 175.9696807861328\n",
      "Epoch 62, Batch 8242, Loss: 175.0862274169922\n",
      "Epoch 62, Batch 8243, Loss: 171.81179809570312\n",
      "Epoch 62, Batch 8244, Loss: 171.70040893554688\n",
      "Epoch 62, Batch 8245, Loss: 177.46287536621094\n",
      "Epoch 62, Batch 8246, Loss: 166.4534454345703\n",
      "Epoch 62, Batch 8247, Loss: 158.25338745117188\n",
      "Epoch 62, Batch 8248, Loss: 159.51394653320312\n",
      "Epoch 62, Batch 8249, Loss: 172.01051330566406\n",
      "Epoch 62, Batch 8250, Loss: 154.47940063476562\n",
      "Epoch 62, Batch 8251, Loss: 155.67633056640625\n",
      "Epoch 62, Batch 8252, Loss: 177.06153869628906\n",
      "Epoch 62, Batch 8253, Loss: 175.7429962158203\n",
      "Epoch 62, Batch 8254, Loss: 181.1934356689453\n",
      "Epoch 62, Batch 8255, Loss: 167.1545867919922\n",
      "Epoch 62, Batch 8256, Loss: 162.08790588378906\n",
      "Epoch 62, Batch 8257, Loss: 174.49325561523438\n",
      "Epoch 62, Batch 8258, Loss: 177.3165740966797\n",
      "Epoch 62, Batch 8259, Loss: 175.6392059326172\n",
      "Epoch 62, Batch 8260, Loss: 169.940185546875\n",
      "Epoch 62, Batch 8261, Loss: 179.4110565185547\n",
      "Epoch 62, Batch 8262, Loss: 166.95970153808594\n",
      "Epoch 62, Batch 8263, Loss: 189.87107849121094\n",
      "Epoch 62, Batch 8264, Loss: 169.28916931152344\n",
      "Epoch 62, Batch 8265, Loss: 183.228271484375\n",
      "Epoch 62, Batch 8266, Loss: 182.09072875976562\n",
      "Epoch 62, Batch 8267, Loss: 167.48675537109375\n",
      "Epoch 62, Batch 8268, Loss: 170.18115234375\n",
      "Epoch 62, Batch 8269, Loss: 175.87725830078125\n",
      "Epoch 62, Batch 8270, Loss: 166.10647583007812\n",
      "Epoch 62, Batch 8271, Loss: 158.73768615722656\n",
      "Epoch 62, Batch 8272, Loss: 162.88742065429688\n",
      "Epoch 62, Batch 8273, Loss: 180.2712860107422\n",
      "Epoch 62, Batch 8274, Loss: 156.3217315673828\n",
      "Epoch 62, Batch 8275, Loss: 180.84878540039062\n",
      "Epoch 62, Batch 8276, Loss: 176.7329559326172\n",
      "Epoch 62, Batch 8277, Loss: 163.66122436523438\n",
      "Epoch 62, Batch 8278, Loss: 167.8694610595703\n",
      "Epoch 62, Batch 8279, Loss: 186.56321716308594\n",
      "Epoch 62, Batch 8280, Loss: 176.01907348632812\n",
      "Epoch 62, Batch 8281, Loss: 167.88189697265625\n",
      "Epoch 62, Batch 8282, Loss: 175.78799438476562\n",
      "Epoch 62, Batch 8283, Loss: 172.60447692871094\n",
      "Epoch 62, Batch 8284, Loss: 168.48953247070312\n",
      "Epoch 62, Batch 8285, Loss: 182.4141387939453\n",
      "Epoch 62, Batch 8286, Loss: 182.16616821289062\n",
      "Epoch 62, Batch 8287, Loss: 151.12985229492188\n",
      "Epoch 62, Batch 8288, Loss: 182.98683166503906\n",
      "Epoch 62, Batch 8289, Loss: 162.6184844970703\n",
      "Epoch 62, Batch 8290, Loss: 177.57797241210938\n",
      "Epoch 62, Batch 8291, Loss: 169.0124969482422\n",
      "Epoch 62, Batch 8292, Loss: 172.3209686279297\n",
      "Epoch 62, Batch 8293, Loss: 177.08291625976562\n",
      "Epoch 62, Batch 8294, Loss: 176.93411254882812\n",
      "Epoch 62, Batch 8295, Loss: 181.42469787597656\n",
      "Epoch 62, Batch 8296, Loss: 167.19859313964844\n",
      "Epoch 62, Batch 8297, Loss: 169.14813232421875\n",
      "Epoch 62, Batch 8298, Loss: 171.0144500732422\n",
      "Epoch 62, Batch 8299, Loss: 191.0030059814453\n",
      "Epoch 62, Batch 8300, Loss: 169.80027770996094\n",
      "Epoch 62, Batch 8301, Loss: 176.48626708984375\n",
      "Epoch 62, Batch 8302, Loss: 161.13201904296875\n",
      "Epoch 62, Batch 8303, Loss: 176.73526000976562\n",
      "Epoch 62, Batch 8304, Loss: 179.99290466308594\n",
      "Epoch 62, Batch 8305, Loss: 192.06295776367188\n",
      "Epoch 62, Batch 8306, Loss: 169.0489959716797\n",
      "Epoch 62, Batch 8307, Loss: 172.97201538085938\n",
      "Epoch 62, Batch 8308, Loss: 186.99957275390625\n",
      "Epoch 62, Batch 8309, Loss: 163.13194274902344\n",
      "Epoch 62, Batch 8310, Loss: 178.1623992919922\n",
      "Epoch 62, Batch 8311, Loss: 177.85594177246094\n",
      "Epoch 62, Batch 8312, Loss: 168.97975158691406\n",
      "Epoch 62, Batch 8313, Loss: 161.44728088378906\n",
      "Epoch 62, Batch 8314, Loss: 164.734130859375\n",
      "Epoch 62, Batch 8315, Loss: 176.8477325439453\n",
      "Epoch 62, Batch 8316, Loss: 177.50259399414062\n",
      "Epoch 62, Batch 8317, Loss: 176.0552215576172\n",
      "Epoch 62, Batch 8318, Loss: 156.5427703857422\n",
      "Epoch 62, Batch 8319, Loss: 168.72747802734375\n",
      "Epoch 62, Batch 8320, Loss: 181.17926025390625\n",
      "Epoch 62, Batch 8321, Loss: 181.6639862060547\n",
      "Epoch 62, Batch 8322, Loss: 164.83233642578125\n",
      "Epoch 62, Batch 8323, Loss: 178.0675048828125\n",
      "Epoch 62, Batch 8324, Loss: 170.38412475585938\n",
      "Epoch 62, Batch 8325, Loss: 172.9095458984375\n",
      "Epoch 62, Batch 8326, Loss: 177.0335693359375\n",
      "Epoch 62, Batch 8327, Loss: 181.90609741210938\n",
      "Epoch 62, Batch 8328, Loss: 174.2515106201172\n",
      "Epoch 62, Batch 8329, Loss: 174.832275390625\n",
      "Epoch 62, Batch 8330, Loss: 179.093017578125\n",
      "Epoch 62, Batch 8331, Loss: 173.38772583007812\n",
      "Epoch 62, Batch 8332, Loss: 176.9355926513672\n",
      "Epoch 62, Batch 8333, Loss: 173.60462951660156\n",
      "Epoch 62, Batch 8334, Loss: 174.28335571289062\n",
      "Epoch 62, Batch 8335, Loss: 184.7394561767578\n",
      "Epoch 62, Batch 8336, Loss: 172.67315673828125\n",
      "Epoch 62, Batch 8337, Loss: 173.81005859375\n",
      "Epoch 62, Batch 8338, Loss: 174.17672729492188\n",
      "Epoch 62, Batch 8339, Loss: 180.61940002441406\n",
      "Epoch 62, Batch 8340, Loss: 166.5035858154297\n",
      "Epoch 62, Batch 8341, Loss: 165.6504364013672\n",
      "Epoch 62, Batch 8342, Loss: 200.158935546875\n",
      "Epoch 62, Batch 8343, Loss: 166.6308135986328\n",
      "Epoch 62, Batch 8344, Loss: 163.1141815185547\n",
      "Epoch 62, Batch 8345, Loss: 185.79449462890625\n",
      "Epoch 62, Batch 8346, Loss: 188.2609100341797\n",
      "Epoch 62, Batch 8347, Loss: 170.56358337402344\n",
      "Epoch 62, Batch 8348, Loss: 161.03060913085938\n",
      "Epoch 62, Batch 8349, Loss: 185.95611572265625\n",
      "Epoch 62, Batch 8350, Loss: 178.0111846923828\n",
      "Epoch 62, Batch 8351, Loss: 184.86154174804688\n",
      "Epoch 62, Batch 8352, Loss: 175.36973571777344\n",
      "Epoch 62, Batch 8353, Loss: 181.6131591796875\n",
      "Epoch 62, Batch 8354, Loss: 176.6941680908203\n",
      "Epoch 62, Batch 8355, Loss: 147.84906005859375\n",
      "Epoch 62, Batch 8356, Loss: 168.50692749023438\n",
      "Epoch 62, Batch 8357, Loss: 177.7417755126953\n",
      "Epoch 62, Batch 8358, Loss: 172.99497985839844\n",
      "Epoch 62, Batch 8359, Loss: 172.21456909179688\n",
      "Epoch 62, Batch 8360, Loss: 159.33270263671875\n",
      "Epoch 62, Batch 8361, Loss: 170.39569091796875\n",
      "Epoch 62, Batch 8362, Loss: 158.84857177734375\n",
      "Epoch 62, Batch 8363, Loss: 179.71554565429688\n",
      "Epoch 62, Batch 8364, Loss: 172.12001037597656\n",
      "Epoch 62, Batch 8365, Loss: 174.46823120117188\n",
      "Epoch 62, Batch 8366, Loss: 176.1175537109375\n",
      "Epoch 62, Batch 8367, Loss: 168.7576446533203\n",
      "Epoch 62, Batch 8368, Loss: 164.0115509033203\n",
      "Epoch 62, Batch 8369, Loss: 178.34934997558594\n",
      "Epoch 62, Batch 8370, Loss: 168.3651580810547\n",
      "Epoch 62, Batch 8371, Loss: 157.03176879882812\n",
      "Epoch 62, Batch 8372, Loss: 201.4847412109375\n",
      "Epoch 62, Batch 8373, Loss: 174.6082305908203\n",
      "Epoch 62, Batch 8374, Loss: 169.54443359375\n",
      "Epoch 62, Batch 8375, Loss: 168.3287353515625\n",
      "Epoch 62, Batch 8376, Loss: 163.94200134277344\n",
      "Epoch 62, Batch 8377, Loss: 184.01620483398438\n",
      "Epoch 62, Batch 8378, Loss: 178.06826782226562\n",
      "Epoch 62, Batch 8379, Loss: 171.99546813964844\n",
      "Epoch 62, Batch 8380, Loss: 176.40625\n",
      "Epoch 62, Batch 8381, Loss: 160.20083618164062\n",
      "Epoch 62, Batch 8382, Loss: 161.67784118652344\n",
      "Epoch 62, Batch 8383, Loss: 163.49295043945312\n",
      "Epoch 62, Batch 8384, Loss: 169.86669921875\n",
      "Epoch 62, Batch 8385, Loss: 176.6149444580078\n",
      "Epoch 62, Batch 8386, Loss: 175.88150024414062\n",
      "Epoch 62, Batch 8387, Loss: 168.92344665527344\n",
      "Epoch 62, Batch 8388, Loss: 169.5144805908203\n",
      "Epoch 62, Batch 8389, Loss: 166.73187255859375\n",
      "Epoch 62, Batch 8390, Loss: 164.03160095214844\n",
      "Epoch 62, Batch 8391, Loss: 160.14730834960938\n",
      "Epoch 62, Batch 8392, Loss: 164.5596923828125\n",
      "Epoch 62, Batch 8393, Loss: 172.49742126464844\n",
      "Epoch 62, Batch 8394, Loss: 161.25856018066406\n",
      "Epoch 62, Batch 8395, Loss: 158.0569610595703\n",
      "Epoch 62, Batch 8396, Loss: 162.22085571289062\n",
      "Epoch 62, Batch 8397, Loss: 169.61888122558594\n",
      "Epoch 62, Batch 8398, Loss: 154.14210510253906\n",
      "Epoch 62, Batch 8399, Loss: 180.47288513183594\n",
      "Epoch 62, Batch 8400, Loss: 182.44227600097656\n",
      "Epoch 62, Batch 8401, Loss: 168.39190673828125\n",
      "Epoch 62, Batch 8402, Loss: 178.02455139160156\n",
      "Epoch 62, Batch 8403, Loss: 181.9688720703125\n",
      "Epoch 62, Batch 8404, Loss: 169.84512329101562\n",
      "Epoch 62, Batch 8405, Loss: 173.3289337158203\n",
      "Epoch 62, Batch 8406, Loss: 166.62928771972656\n",
      "Epoch 62, Batch 8407, Loss: 183.83177185058594\n",
      "Epoch 62, Batch 8408, Loss: 160.10446166992188\n",
      "Epoch 62, Batch 8409, Loss: 176.3402099609375\n",
      "Epoch 62, Batch 8410, Loss: 171.08750915527344\n",
      "Epoch 62, Batch 8411, Loss: 152.8007354736328\n",
      "Epoch 62, Batch 8412, Loss: 175.98114013671875\n",
      "Epoch 62, Batch 8413, Loss: 183.9449005126953\n",
      "Epoch 62, Batch 8414, Loss: 182.54977416992188\n",
      "Epoch 62, Batch 8415, Loss: 164.22740173339844\n",
      "Epoch 62, Batch 8416, Loss: 168.89466857910156\n",
      "Epoch 62, Batch 8417, Loss: 181.15821838378906\n",
      "Epoch 62, Batch 8418, Loss: 174.06927490234375\n",
      "Epoch 62, Batch 8419, Loss: 182.90704345703125\n",
      "Epoch 62, Batch 8420, Loss: 164.568359375\n",
      "Epoch 62, Batch 8421, Loss: 168.7064666748047\n",
      "Epoch 62, Batch 8422, Loss: 176.13121032714844\n",
      "Epoch 62, Batch 8423, Loss: 182.0659637451172\n",
      "Epoch 62, Batch 8424, Loss: 175.79319763183594\n",
      "Epoch 62, Batch 8425, Loss: 194.22190856933594\n",
      "Epoch 62, Batch 8426, Loss: 166.1782989501953\n",
      "Epoch 62, Batch 8427, Loss: 175.47291564941406\n",
      "Epoch 62, Batch 8428, Loss: 175.4490203857422\n",
      "Epoch 62, Batch 8429, Loss: 150.94464111328125\n",
      "Epoch 62, Batch 8430, Loss: 168.79612731933594\n",
      "Epoch 62, Batch 8431, Loss: 182.43426513671875\n",
      "Epoch 62, Batch 8432, Loss: 169.78762817382812\n",
      "Epoch 62, Batch 8433, Loss: 167.23126220703125\n",
      "Epoch 62, Batch 8434, Loss: 173.52328491210938\n",
      "Epoch 62, Batch 8435, Loss: 187.2455291748047\n",
      "Epoch 62, Batch 8436, Loss: 167.00979614257812\n",
      "Epoch 62, Batch 8437, Loss: 168.47882080078125\n",
      "Epoch 62, Batch 8438, Loss: 181.10496520996094\n",
      "Epoch 62, Batch 8439, Loss: 171.16844177246094\n",
      "Epoch 62, Batch 8440, Loss: 175.1918487548828\n",
      "Epoch 62, Batch 8441, Loss: 178.29052734375\n",
      "Epoch 62, Batch 8442, Loss: 168.82992553710938\n",
      "Epoch 62, Batch 8443, Loss: 190.77552795410156\n",
      "Epoch 62, Batch 8444, Loss: 189.53475952148438\n",
      "Epoch 62, Batch 8445, Loss: 158.91041564941406\n",
      "Epoch 62, Batch 8446, Loss: 178.41598510742188\n",
      "Epoch 62, Batch 8447, Loss: 195.85108947753906\n",
      "Epoch 62, Batch 8448, Loss: 163.8623809814453\n",
      "Epoch 62, Batch 8449, Loss: 174.3502197265625\n",
      "Epoch 62, Batch 8450, Loss: 168.90475463867188\n",
      "Epoch 62, Batch 8451, Loss: 172.7160186767578\n",
      "Epoch 62, Batch 8452, Loss: 156.5295867919922\n",
      "Epoch 62, Batch 8453, Loss: 172.80181884765625\n",
      "Epoch 62, Batch 8454, Loss: 183.07249450683594\n",
      "Epoch 62, Batch 8455, Loss: 182.55538940429688\n",
      "Epoch 62, Batch 8456, Loss: 176.36912536621094\n",
      "Epoch 62, Batch 8457, Loss: 176.00308227539062\n",
      "Epoch 62, Batch 8458, Loss: 162.5464630126953\n",
      "Epoch 62, Batch 8459, Loss: 166.29489135742188\n",
      "Epoch 62, Batch 8460, Loss: 157.6748504638672\n",
      "Epoch 62, Batch 8461, Loss: 168.46463012695312\n",
      "Epoch 62, Batch 8462, Loss: 181.34201049804688\n",
      "Epoch 62, Batch 8463, Loss: 167.56639099121094\n",
      "Epoch 62, Batch 8464, Loss: 183.86981201171875\n",
      "Epoch 62, Batch 8465, Loss: 180.5390625\n",
      "Epoch 62, Batch 8466, Loss: 165.29029846191406\n",
      "Epoch 62, Batch 8467, Loss: 184.25393676757812\n",
      "Epoch 62, Batch 8468, Loss: 155.28981018066406\n",
      "Epoch 62, Batch 8469, Loss: 181.49270629882812\n",
      "Epoch 62, Batch 8470, Loss: 171.47006225585938\n",
      "Epoch 62, Batch 8471, Loss: 171.57858276367188\n",
      "Epoch 62, Batch 8472, Loss: 199.39244079589844\n",
      "Epoch 62, Batch 8473, Loss: 170.4746551513672\n",
      "Epoch 62, Batch 8474, Loss: 168.9873504638672\n",
      "Epoch 62, Batch 8475, Loss: 162.15859985351562\n",
      "Epoch 62, Batch 8476, Loss: 179.3549346923828\n",
      "Epoch 62, Batch 8477, Loss: 171.44822692871094\n",
      "Epoch 62, Batch 8478, Loss: 172.8056640625\n",
      "Epoch 62, Batch 8479, Loss: 184.79037475585938\n",
      "Epoch 62, Batch 8480, Loss: 180.0930938720703\n",
      "Epoch 62, Batch 8481, Loss: 190.16677856445312\n",
      "Epoch 62, Batch 8482, Loss: 164.47882080078125\n",
      "Epoch 62, Batch 8483, Loss: 176.25552368164062\n",
      "Epoch 62, Batch 8484, Loss: 187.11012268066406\n",
      "Epoch 62, Batch 8485, Loss: 163.96336364746094\n",
      "Epoch 62, Batch 8486, Loss: 175.25271606445312\n",
      "Epoch 62, Batch 8487, Loss: 167.9044647216797\n",
      "Epoch 62, Batch 8488, Loss: 171.17820739746094\n",
      "Epoch 62, Batch 8489, Loss: 160.55682373046875\n",
      "Epoch 62, Batch 8490, Loss: 164.23658752441406\n",
      "Epoch 62, Batch 8491, Loss: 176.34072875976562\n",
      "Epoch 62, Batch 8492, Loss: 170.7218017578125\n",
      "Epoch 62, Batch 8493, Loss: 172.99928283691406\n",
      "Epoch 62, Batch 8494, Loss: 183.28964233398438\n",
      "Epoch 62, Batch 8495, Loss: 194.0652618408203\n",
      "Epoch 62, Batch 8496, Loss: 173.77340698242188\n",
      "Epoch 62, Batch 8497, Loss: 172.96774291992188\n",
      "Epoch 62, Batch 8498, Loss: 171.15481567382812\n",
      "Epoch 62, Batch 8499, Loss: 172.4927215576172\n",
      "Epoch 62, Batch 8500, Loss: 171.39218139648438\n",
      "Epoch 62, Batch 8501, Loss: 162.33499145507812\n",
      "Epoch 62, Batch 8502, Loss: 164.77532958984375\n",
      "Epoch 62, Batch 8503, Loss: 176.46865844726562\n",
      "Epoch 62, Batch 8504, Loss: 166.89328002929688\n",
      "Epoch 62, Batch 8505, Loss: 178.70486450195312\n",
      "Epoch 62, Batch 8506, Loss: 181.07156372070312\n",
      "Epoch 62, Batch 8507, Loss: 177.75033569335938\n",
      "Epoch 62, Batch 8508, Loss: 169.9101104736328\n",
      "Epoch 62, Batch 8509, Loss: 174.93743896484375\n",
      "Epoch 62, Batch 8510, Loss: 180.4654083251953\n",
      "Epoch 62, Batch 8511, Loss: 174.18341064453125\n",
      "Epoch 62, Batch 8512, Loss: 187.6238555908203\n",
      "Epoch 62, Batch 8513, Loss: 187.98667907714844\n",
      "Epoch 62, Batch 8514, Loss: 160.63890075683594\n",
      "Epoch 62, Batch 8515, Loss: 157.7881317138672\n",
      "Epoch 62, Batch 8516, Loss: 164.3796844482422\n",
      "Epoch 62, Batch 8517, Loss: 174.83950805664062\n",
      "Epoch 62, Batch 8518, Loss: 178.15829467773438\n",
      "Epoch 62, Batch 8519, Loss: 169.3480682373047\n",
      "Epoch 62, Batch 8520, Loss: 178.7930145263672\n",
      "Epoch 62, Batch 8521, Loss: 180.38552856445312\n",
      "Epoch 62, Batch 8522, Loss: 167.29193115234375\n",
      "Epoch 62, Batch 8523, Loss: 175.03070068359375\n",
      "Epoch 62, Batch 8524, Loss: 159.7163543701172\n",
      "Epoch 62, Batch 8525, Loss: 180.32154846191406\n",
      "Epoch 62, Batch 8526, Loss: 166.61549377441406\n",
      "Epoch 62, Batch 8527, Loss: 185.60496520996094\n",
      "Epoch 62, Batch 8528, Loss: 182.29119873046875\n",
      "Epoch 62, Batch 8529, Loss: 159.5854949951172\n",
      "Epoch 62, Batch 8530, Loss: 163.23550415039062\n",
      "Epoch 62, Batch 8531, Loss: 187.4393768310547\n",
      "Epoch 62, Batch 8532, Loss: 156.24642944335938\n",
      "Epoch 62, Batch 8533, Loss: 182.15089416503906\n",
      "Epoch 62, Batch 8534, Loss: 167.50213623046875\n",
      "Epoch 62, Batch 8535, Loss: 171.01698303222656\n",
      "Epoch 62, Batch 8536, Loss: 180.00599670410156\n",
      "Epoch 62, Batch 8537, Loss: 170.74501037597656\n",
      "Epoch 62, Batch 8538, Loss: 178.08804321289062\n",
      "Epoch 62, Batch 8539, Loss: 168.93141174316406\n",
      "Epoch 62, Batch 8540, Loss: 163.32838439941406\n",
      "Epoch 62, Batch 8541, Loss: 178.46841430664062\n",
      "Epoch 62, Batch 8542, Loss: 168.25489807128906\n",
      "Epoch 62, Batch 8543, Loss: 182.10980224609375\n",
      "Epoch 62, Batch 8544, Loss: 164.348388671875\n",
      "Epoch 62, Batch 8545, Loss: 168.79464721679688\n",
      "Epoch 62, Batch 8546, Loss: 191.01589965820312\n",
      "Epoch 62, Batch 8547, Loss: 165.990478515625\n",
      "Epoch 62, Batch 8548, Loss: 182.58917236328125\n",
      "Epoch 62, Batch 8549, Loss: 159.09674072265625\n",
      "Epoch 62, Batch 8550, Loss: 166.68963623046875\n",
      "Epoch 62, Batch 8551, Loss: 177.09332275390625\n",
      "Epoch 62, Batch 8552, Loss: 180.07659912109375\n",
      "Epoch 62, Batch 8553, Loss: 175.11016845703125\n",
      "Epoch 62, Batch 8554, Loss: 173.27549743652344\n",
      "Epoch 62, Batch 8555, Loss: 161.88458251953125\n",
      "Epoch 62, Batch 8556, Loss: 187.45530700683594\n",
      "Epoch 62, Batch 8557, Loss: 178.2137908935547\n",
      "Epoch 62, Batch 8558, Loss: 163.6646270751953\n",
      "Epoch 62, Batch 8559, Loss: 177.60299682617188\n",
      "Epoch 62, Batch 8560, Loss: 174.83230590820312\n",
      "Epoch 62, Batch 8561, Loss: 164.9807891845703\n",
      "Epoch 62, Batch 8562, Loss: 165.6230926513672\n",
      "Epoch 62, Batch 8563, Loss: 161.31182861328125\n",
      "Epoch 62, Batch 8564, Loss: 177.88088989257812\n",
      "Epoch 62, Batch 8565, Loss: 179.86289978027344\n",
      "Epoch 62, Batch 8566, Loss: 177.55441284179688\n",
      "Epoch 62, Batch 8567, Loss: 185.28121948242188\n",
      "Epoch 62, Batch 8568, Loss: 177.15847778320312\n",
      "Epoch 62, Batch 8569, Loss: 181.51060485839844\n",
      "Epoch 62, Batch 8570, Loss: 165.3591766357422\n",
      "Epoch 62, Batch 8571, Loss: 172.2044677734375\n",
      "Epoch 62, Batch 8572, Loss: 179.1061553955078\n",
      "Epoch 62, Batch 8573, Loss: 165.4021759033203\n",
      "Epoch 62, Batch 8574, Loss: 164.9280242919922\n",
      "Epoch 62, Batch 8575, Loss: 170.6799774169922\n",
      "Epoch 62, Batch 8576, Loss: 199.61090087890625\n",
      "Epoch 62, Batch 8577, Loss: 150.74049377441406\n",
      "Epoch 62, Batch 8578, Loss: 162.71356201171875\n",
      "Epoch 62, Batch 8579, Loss: 169.0383758544922\n",
      "Epoch 62, Batch 8580, Loss: 171.54554748535156\n",
      "Epoch 62, Batch 8581, Loss: 179.28759765625\n",
      "Epoch 62, Batch 8582, Loss: 168.98641967773438\n",
      "Epoch 62, Batch 8583, Loss: 154.45591735839844\n",
      "Epoch 62, Batch 8584, Loss: 187.031005859375\n",
      "Epoch 62, Batch 8585, Loss: 159.106689453125\n",
      "Epoch 62, Batch 8586, Loss: 174.5384979248047\n",
      "Epoch 62, Batch 8587, Loss: 165.27613830566406\n",
      "Epoch 62, Batch 8588, Loss: 176.14967346191406\n",
      "Epoch 62, Batch 8589, Loss: 180.23153686523438\n",
      "Epoch 62, Batch 8590, Loss: 172.67929077148438\n",
      "Epoch 62, Batch 8591, Loss: 179.22555541992188\n",
      "Epoch 62, Batch 8592, Loss: 181.15899658203125\n",
      "Epoch 62, Batch 8593, Loss: 178.7439422607422\n",
      "Epoch 62, Batch 8594, Loss: 184.88320922851562\n",
      "Epoch 62, Batch 8595, Loss: 161.6232452392578\n",
      "Epoch 62, Batch 8596, Loss: 166.2769775390625\n",
      "Epoch 62, Batch 8597, Loss: 160.95425415039062\n",
      "Epoch 62, Batch 8598, Loss: 176.83670043945312\n",
      "Epoch 62, Batch 8599, Loss: 171.88197326660156\n",
      "Epoch 62, Batch 8600, Loss: 179.84263610839844\n",
      "Epoch 62, Batch 8601, Loss: 174.80545043945312\n",
      "Epoch 62, Batch 8602, Loss: 182.60231018066406\n",
      "Epoch 62, Batch 8603, Loss: 161.126708984375\n",
      "Epoch 62, Batch 8604, Loss: 155.802734375\n",
      "Epoch 62, Batch 8605, Loss: 172.4852294921875\n",
      "Epoch 62, Batch 8606, Loss: 175.70343017578125\n",
      "Epoch 62, Batch 8607, Loss: 177.90280151367188\n",
      "Epoch 62, Batch 8608, Loss: 181.85206604003906\n",
      "Epoch 62, Batch 8609, Loss: 162.88754272460938\n",
      "Epoch 62, Batch 8610, Loss: 179.4662322998047\n",
      "Epoch 62, Batch 8611, Loss: 162.69229125976562\n",
      "Epoch 62, Batch 8612, Loss: 163.94454956054688\n",
      "Epoch 62, Batch 8613, Loss: 181.3433074951172\n",
      "Epoch 62, Batch 8614, Loss: 176.37806701660156\n",
      "Epoch 62, Batch 8615, Loss: 176.34945678710938\n",
      "Epoch 62, Batch 8616, Loss: 192.3992919921875\n",
      "Epoch 62, Batch 8617, Loss: 172.16287231445312\n",
      "Epoch 62, Batch 8618, Loss: 163.40689086914062\n",
      "Epoch 62, Batch 8619, Loss: 155.57208251953125\n",
      "Epoch 62, Batch 8620, Loss: 176.6085968017578\n",
      "Epoch 62, Batch 8621, Loss: 175.29640197753906\n",
      "Epoch 62, Batch 8622, Loss: 179.08607482910156\n",
      "Epoch 62, Batch 8623, Loss: 178.9672088623047\n",
      "Epoch 62, Batch 8624, Loss: 194.53053283691406\n",
      "Epoch 62, Batch 8625, Loss: 175.9822998046875\n",
      "Epoch 62, Batch 8626, Loss: 170.1456298828125\n",
      "Epoch 62, Batch 8627, Loss: 164.81024169921875\n",
      "Epoch 62, Batch 8628, Loss: 180.8676300048828\n",
      "Epoch 62, Batch 8629, Loss: 176.763916015625\n",
      "Epoch 62, Batch 8630, Loss: 170.15899658203125\n",
      "Epoch 62, Batch 8631, Loss: 172.9217987060547\n",
      "Epoch 62, Batch 8632, Loss: 177.42857360839844\n",
      "Epoch 62, Batch 8633, Loss: 167.3138885498047\n",
      "Epoch 62, Batch 8634, Loss: 173.69119262695312\n",
      "Epoch 62, Batch 8635, Loss: 175.5118865966797\n",
      "Epoch 62, Batch 8636, Loss: 177.67080688476562\n",
      "Epoch 62, Batch 8637, Loss: 174.97787475585938\n",
      "Epoch 62, Batch 8638, Loss: 176.7234344482422\n",
      "Epoch 62, Batch 8639, Loss: 174.81399536132812\n",
      "Epoch 62, Batch 8640, Loss: 168.31068420410156\n",
      "Epoch 62, Batch 8641, Loss: 163.9207000732422\n",
      "Epoch 62, Batch 8642, Loss: 180.8927459716797\n",
      "Epoch 62, Batch 8643, Loss: 174.84689331054688\n",
      "Epoch 62, Batch 8644, Loss: 177.4754180908203\n",
      "Epoch 62, Batch 8645, Loss: 182.33644104003906\n",
      "Epoch 62, Batch 8646, Loss: 186.5209503173828\n",
      "Epoch 62, Batch 8647, Loss: 178.23570251464844\n",
      "Epoch 62, Batch 8648, Loss: 167.46095275878906\n",
      "Epoch 62, Batch 8649, Loss: 170.71798706054688\n",
      "Epoch 62, Batch 8650, Loss: 174.8420867919922\n",
      "Epoch 62, Batch 8651, Loss: 173.9092254638672\n",
      "Epoch 62, Batch 8652, Loss: 173.29083251953125\n",
      "Epoch 62, Batch 8653, Loss: 175.82876586914062\n",
      "Epoch 62, Batch 8654, Loss: 174.92588806152344\n",
      "Epoch 62, Batch 8655, Loss: 169.307861328125\n",
      "Epoch 62, Batch 8656, Loss: 173.8512725830078\n",
      "Epoch 62, Batch 8657, Loss: 168.84368896484375\n",
      "Epoch 62, Batch 8658, Loss: 177.3550567626953\n",
      "Epoch 62, Batch 8659, Loss: 177.28041076660156\n",
      "Epoch 62, Batch 8660, Loss: 174.92156982421875\n",
      "Epoch 62, Batch 8661, Loss: 176.7244415283203\n",
      "Epoch 62, Batch 8662, Loss: 152.00692749023438\n",
      "Epoch 62, Batch 8663, Loss: 162.94557189941406\n",
      "Epoch 62, Batch 8664, Loss: 168.673828125\n",
      "Epoch 62, Batch 8665, Loss: 181.5557098388672\n",
      "Epoch 62, Batch 8666, Loss: 170.67869567871094\n",
      "Epoch 62, Batch 8667, Loss: 196.75274658203125\n",
      "Epoch 62, Batch 8668, Loss: 161.04237365722656\n",
      "Epoch 62, Batch 8669, Loss: 152.99813842773438\n",
      "Epoch 62, Batch 8670, Loss: 172.12237548828125\n",
      "Epoch 62, Batch 8671, Loss: 182.75880432128906\n",
      "Epoch 62, Batch 8672, Loss: 184.60020446777344\n",
      "Epoch 62, Batch 8673, Loss: 168.41709899902344\n",
      "Epoch 62, Batch 8674, Loss: 157.68551635742188\n",
      "Epoch 62, Batch 8675, Loss: 167.1632537841797\n",
      "Epoch 62, Batch 8676, Loss: 163.9108428955078\n",
      "Epoch 62, Batch 8677, Loss: 173.01730346679688\n",
      "Epoch 62, Batch 8678, Loss: 175.10499572753906\n",
      "Epoch 62, Batch 8679, Loss: 174.3535919189453\n",
      "Epoch 62, Batch 8680, Loss: 178.98175048828125\n",
      "Epoch 62, Batch 8681, Loss: 173.86691284179688\n",
      "Epoch 62, Batch 8682, Loss: 171.0150909423828\n",
      "Epoch 62, Batch 8683, Loss: 176.62950134277344\n",
      "Epoch 62, Batch 8684, Loss: 161.23538208007812\n",
      "Epoch 62, Batch 8685, Loss: 161.821533203125\n",
      "Epoch 62, Batch 8686, Loss: 165.27386474609375\n",
      "Epoch 62, Batch 8687, Loss: 153.77838134765625\n",
      "Epoch 62, Batch 8688, Loss: 177.7716522216797\n",
      "Epoch 62, Batch 8689, Loss: 177.69744873046875\n",
      "Epoch 62, Batch 8690, Loss: 174.52401733398438\n",
      "Epoch 62, Batch 8691, Loss: 162.39366149902344\n",
      "Epoch 62, Batch 8692, Loss: 179.64962768554688\n",
      "Epoch 62, Batch 8693, Loss: 173.0986785888672\n",
      "Epoch 62, Batch 8694, Loss: 169.3408966064453\n",
      "Epoch 62, Batch 8695, Loss: 169.04103088378906\n",
      "Epoch 62, Batch 8696, Loss: 173.9810028076172\n",
      "Epoch 62, Batch 8697, Loss: 155.83059692382812\n",
      "Epoch 62, Batch 8698, Loss: 170.82260131835938\n",
      "Epoch 62, Batch 8699, Loss: 165.5818634033203\n",
      "Epoch 62, Batch 8700, Loss: 189.44070434570312\n",
      "Epoch 62, Batch 8701, Loss: 180.19686889648438\n",
      "Epoch 62, Batch 8702, Loss: 179.57254028320312\n",
      "Epoch 62, Batch 8703, Loss: 164.35888671875\n",
      "Epoch 62, Batch 8704, Loss: 153.099609375\n",
      "Epoch 62, Batch 8705, Loss: 168.36891174316406\n",
      "Epoch 62, Batch 8706, Loss: 167.29029846191406\n",
      "Epoch 62, Batch 8707, Loss: 162.92889404296875\n",
      "Epoch 62, Batch 8708, Loss: 189.68316650390625\n",
      "Epoch 62, Batch 8709, Loss: 174.5491180419922\n",
      "Epoch 62, Batch 8710, Loss: 187.44554138183594\n",
      "Epoch 62, Batch 8711, Loss: 176.35813903808594\n",
      "Epoch 62, Batch 8712, Loss: 166.14329528808594\n",
      "Epoch 62, Batch 8713, Loss: 161.85711669921875\n",
      "Epoch 62, Batch 8714, Loss: 161.60745239257812\n",
      "Epoch 62, Batch 8715, Loss: 173.90274047851562\n",
      "Epoch 62, Batch 8716, Loss: 166.2715606689453\n",
      "Epoch 62, Batch 8717, Loss: 161.98365783691406\n",
      "Epoch 62, Batch 8718, Loss: 164.8600311279297\n",
      "Epoch 62, Batch 8719, Loss: 173.7622528076172\n",
      "Epoch 62, Batch 8720, Loss: 179.7818603515625\n",
      "Epoch 62, Batch 8721, Loss: 178.7030792236328\n",
      "Epoch 62, Batch 8722, Loss: 172.15455627441406\n",
      "Epoch 62, Batch 8723, Loss: 157.3378448486328\n",
      "Epoch 62, Batch 8724, Loss: 167.03460693359375\n",
      "Epoch 62, Batch 8725, Loss: 169.0487518310547\n",
      "Epoch 62, Batch 8726, Loss: 178.5099334716797\n",
      "Epoch 62, Batch 8727, Loss: 170.36285400390625\n",
      "Epoch 62, Batch 8728, Loss: 186.25094604492188\n",
      "Epoch 62, Batch 8729, Loss: 181.54676818847656\n",
      "Epoch 62, Batch 8730, Loss: 167.4772186279297\n",
      "Epoch 62, Batch 8731, Loss: 166.4210968017578\n",
      "Epoch 62, Batch 8732, Loss: 165.40647888183594\n",
      "Epoch 62, Batch 8733, Loss: 164.76943969726562\n",
      "Epoch 62, Batch 8734, Loss: 181.77992248535156\n",
      "Epoch 62, Batch 8735, Loss: 160.06503295898438\n",
      "Epoch 62, Batch 8736, Loss: 160.5469207763672\n",
      "Epoch 62, Batch 8737, Loss: 161.29908752441406\n",
      "Epoch 62, Batch 8738, Loss: 164.8743896484375\n",
      "Epoch 62, Batch 8739, Loss: 166.05502319335938\n",
      "Epoch 62, Batch 8740, Loss: 163.1095428466797\n",
      "Epoch 62, Batch 8741, Loss: 168.3567352294922\n",
      "Epoch 62, Batch 8742, Loss: 180.287109375\n",
      "Epoch 62, Batch 8743, Loss: 177.208251953125\n",
      "Epoch 62, Batch 8744, Loss: 177.52401733398438\n",
      "Epoch 62, Batch 8745, Loss: 169.09051513671875\n",
      "Epoch 62, Batch 8746, Loss: 168.43824768066406\n",
      "Epoch 62, Batch 8747, Loss: 149.74009704589844\n",
      "Epoch 62, Batch 8748, Loss: 178.96641540527344\n",
      "Epoch 62, Batch 8749, Loss: 178.15074157714844\n",
      "Epoch 62, Batch 8750, Loss: 170.20411682128906\n",
      "Epoch 62, Batch 8751, Loss: 157.16200256347656\n",
      "Epoch 62, Batch 8752, Loss: 160.88951110839844\n",
      "Epoch 62, Batch 8753, Loss: 169.1941375732422\n",
      "Epoch 62, Batch 8754, Loss: 159.94036865234375\n",
      "Epoch 62, Batch 8755, Loss: 161.4340057373047\n",
      "Epoch 62, Batch 8756, Loss: 170.63380432128906\n",
      "Epoch 62, Batch 8757, Loss: 180.2291717529297\n",
      "Epoch 62, Batch 8758, Loss: 164.9091033935547\n",
      "Epoch 62, Batch 8759, Loss: 166.10073852539062\n",
      "Epoch 62, Batch 8760, Loss: 158.5675048828125\n",
      "Epoch 62, Batch 8761, Loss: 166.8773193359375\n",
      "Epoch 62, Batch 8762, Loss: 158.8766632080078\n",
      "Epoch 62, Batch 8763, Loss: 165.92037963867188\n",
      "Epoch 62, Batch 8764, Loss: 181.36582946777344\n",
      "Epoch 62, Batch 8765, Loss: 165.4602508544922\n",
      "Epoch 62, Batch 8766, Loss: 162.20262145996094\n",
      "Epoch 62, Batch 8767, Loss: 195.0492706298828\n",
      "Epoch 62, Batch 8768, Loss: 159.61903381347656\n",
      "Epoch 62, Batch 8769, Loss: 190.38766479492188\n",
      "Epoch 62, Batch 8770, Loss: 177.54330444335938\n",
      "Epoch 62, Batch 8771, Loss: 147.9077911376953\n",
      "Epoch 62, Batch 8772, Loss: 187.22303771972656\n",
      "Epoch 62, Batch 8773, Loss: 188.88427734375\n",
      "Epoch 62, Batch 8774, Loss: 173.1162872314453\n",
      "Epoch 62, Batch 8775, Loss: 168.0985870361328\n",
      "Epoch 62, Batch 8776, Loss: 173.5333709716797\n",
      "Epoch 62, Batch 8777, Loss: 179.8468780517578\n",
      "Epoch 62, Batch 8778, Loss: 177.49415588378906\n",
      "Epoch 62, Batch 8779, Loss: 173.0534210205078\n",
      "Epoch 62, Batch 8780, Loss: 166.04989624023438\n",
      "Epoch 62, Batch 8781, Loss: 163.25196838378906\n",
      "Epoch 62, Batch 8782, Loss: 167.76368713378906\n",
      "Epoch 62, Batch 8783, Loss: 163.891357421875\n",
      "Epoch 62, Batch 8784, Loss: 185.19534301757812\n",
      "Epoch 62, Batch 8785, Loss: 159.92140197753906\n",
      "Epoch 62, Batch 8786, Loss: 179.62020874023438\n",
      "Epoch 62, Batch 8787, Loss: 177.83018493652344\n",
      "Epoch 62, Batch 8788, Loss: 184.8274383544922\n",
      "Epoch 62, Batch 8789, Loss: 174.59263610839844\n",
      "Epoch 62, Batch 8790, Loss: 160.9816436767578\n",
      "Epoch 62, Batch 8791, Loss: 190.3848419189453\n",
      "Epoch 62, Batch 8792, Loss: 171.56109619140625\n",
      "Epoch 62, Batch 8793, Loss: 184.69790649414062\n",
      "Epoch 62, Batch 8794, Loss: 162.54428100585938\n",
      "Epoch 62, Batch 8795, Loss: 182.56385803222656\n",
      "Epoch 62, Batch 8796, Loss: 156.14816284179688\n",
      "Epoch 62, Batch 8797, Loss: 182.86146545410156\n",
      "Epoch 62, Batch 8798, Loss: 200.3795928955078\n",
      "Epoch 62, Batch 8799, Loss: 156.10220336914062\n",
      "Epoch 62, Batch 8800, Loss: 180.4226837158203\n",
      "Epoch 62, Batch 8801, Loss: 176.37130737304688\n",
      "Epoch 62, Batch 8802, Loss: 180.44581604003906\n",
      "Epoch 62, Batch 8803, Loss: 176.51461791992188\n",
      "Epoch 62, Batch 8804, Loss: 185.4977569580078\n",
      "Epoch 62, Batch 8805, Loss: 179.5003662109375\n",
      "Epoch 62, Batch 8806, Loss: 181.66607666015625\n",
      "Epoch 62, Batch 8807, Loss: 159.69171142578125\n",
      "Epoch 62, Batch 8808, Loss: 179.3811492919922\n",
      "Epoch 62, Batch 8809, Loss: 177.6978759765625\n",
      "Epoch 62, Batch 8810, Loss: 182.5845489501953\n",
      "Epoch 62, Batch 8811, Loss: 164.8102264404297\n",
      "Epoch 62, Batch 8812, Loss: 181.33070373535156\n",
      "Epoch 62, Batch 8813, Loss: 184.00140380859375\n",
      "Epoch 62, Batch 8814, Loss: 178.18052673339844\n",
      "Epoch 62, Batch 8815, Loss: 191.4752960205078\n",
      "Epoch 62, Batch 8816, Loss: 182.49249267578125\n",
      "Epoch 62, Batch 8817, Loss: 163.62608337402344\n",
      "Epoch 62, Batch 8818, Loss: 173.76739501953125\n",
      "Epoch 62, Batch 8819, Loss: 182.88304138183594\n",
      "Epoch 62, Batch 8820, Loss: 167.22164916992188\n",
      "Epoch 62, Batch 8821, Loss: 165.13632202148438\n",
      "Epoch 62, Batch 8822, Loss: 163.8870849609375\n",
      "Epoch 62, Batch 8823, Loss: 173.6539764404297\n",
      "Epoch 62, Batch 8824, Loss: 162.50418090820312\n",
      "Epoch 62, Batch 8825, Loss: 171.48507690429688\n",
      "Epoch 62, Batch 8826, Loss: 177.9491424560547\n",
      "Epoch 62, Batch 8827, Loss: 178.92393493652344\n",
      "Epoch 62, Batch 8828, Loss: 192.2089385986328\n",
      "Epoch 62, Batch 8829, Loss: 167.30616760253906\n",
      "Epoch 62, Batch 8830, Loss: 183.67233276367188\n",
      "Epoch 62, Batch 8831, Loss: 162.3687744140625\n",
      "Epoch 62, Batch 8832, Loss: 178.40066528320312\n",
      "Epoch 62, Batch 8833, Loss: 154.60194396972656\n",
      "Epoch 62, Batch 8834, Loss: 154.041015625\n",
      "Epoch 62, Batch 8835, Loss: 167.15380859375\n",
      "Epoch 62, Batch 8836, Loss: 166.41773986816406\n",
      "Epoch 62, Batch 8837, Loss: 176.45896911621094\n",
      "Epoch 62, Batch 8838, Loss: 166.5641632080078\n",
      "Epoch 62, Batch 8839, Loss: 179.2734375\n",
      "Epoch 62, Batch 8840, Loss: 182.25906372070312\n",
      "Epoch 62, Batch 8841, Loss: 179.048583984375\n",
      "Epoch 62, Batch 8842, Loss: 162.0833740234375\n",
      "Epoch 62, Batch 8843, Loss: 173.2076416015625\n",
      "Epoch 62, Batch 8844, Loss: 171.7982177734375\n",
      "Epoch 62, Batch 8845, Loss: 161.1150360107422\n",
      "Epoch 62, Batch 8846, Loss: 177.4131622314453\n",
      "Epoch 62, Batch 8847, Loss: 176.8170166015625\n",
      "Epoch 62, Batch 8848, Loss: 179.5114288330078\n",
      "Epoch 62, Batch 8849, Loss: 174.7663116455078\n",
      "Epoch 62, Batch 8850, Loss: 165.20118713378906\n",
      "Epoch 62, Batch 8851, Loss: 174.46044921875\n",
      "Epoch 62, Batch 8852, Loss: 185.67214965820312\n",
      "Epoch 62, Batch 8853, Loss: 163.82154846191406\n",
      "Epoch 62, Batch 8854, Loss: 182.65542602539062\n",
      "Epoch 62, Batch 8855, Loss: 164.3896942138672\n",
      "Epoch 62, Batch 8856, Loss: 168.74119567871094\n",
      "Epoch 62, Batch 8857, Loss: 183.645751953125\n",
      "Epoch 62, Batch 8858, Loss: 168.2314453125\n",
      "Epoch 62, Batch 8859, Loss: 180.85293579101562\n",
      "Epoch 62, Batch 8860, Loss: 179.56268310546875\n",
      "Epoch 62, Batch 8861, Loss: 165.6433563232422\n",
      "Epoch 62, Batch 8862, Loss: 164.7410430908203\n",
      "Epoch 62, Batch 8863, Loss: 174.2135467529297\n",
      "Epoch 62, Batch 8864, Loss: 168.51231384277344\n",
      "Epoch 62, Batch 8865, Loss: 168.38829040527344\n",
      "Epoch 62, Batch 8866, Loss: 172.3478546142578\n",
      "Epoch 62, Batch 8867, Loss: 204.50448608398438\n",
      "Epoch 62, Batch 8868, Loss: 160.28350830078125\n",
      "Epoch 62, Batch 8869, Loss: 165.14964294433594\n",
      "Epoch 62, Batch 8870, Loss: 192.8316192626953\n",
      "Epoch 62, Batch 8871, Loss: 172.4315185546875\n",
      "Epoch 62, Batch 8872, Loss: 157.7425537109375\n",
      "Epoch 62, Batch 8873, Loss: 176.97727966308594\n",
      "Epoch 62, Batch 8874, Loss: 168.5143585205078\n",
      "Epoch 62, Batch 8875, Loss: 181.2852020263672\n",
      "Epoch 62, Batch 8876, Loss: 162.69808959960938\n",
      "Epoch 62, Batch 8877, Loss: 188.4948272705078\n",
      "Epoch 62, Batch 8878, Loss: 180.71234130859375\n",
      "Epoch 62, Batch 8879, Loss: 157.55758666992188\n",
      "Epoch 62, Batch 8880, Loss: 199.1247100830078\n",
      "Epoch 62, Batch 8881, Loss: 179.12681579589844\n",
      "Epoch 62, Batch 8882, Loss: 173.0342559814453\n",
      "Epoch 62, Batch 8883, Loss: 169.6408233642578\n",
      "Epoch 62, Batch 8884, Loss: 178.35421752929688\n",
      "Epoch 62, Batch 8885, Loss: 181.0094757080078\n",
      "Epoch 62, Batch 8886, Loss: 175.8490447998047\n",
      "Epoch 62, Batch 8887, Loss: 158.95187377929688\n",
      "Epoch 62, Batch 8888, Loss: 163.6698760986328\n",
      "Epoch 62, Batch 8889, Loss: 154.38992309570312\n",
      "Epoch 62, Batch 8890, Loss: 179.36444091796875\n",
      "Epoch 62, Batch 8891, Loss: 173.64686584472656\n",
      "Epoch 62, Batch 8892, Loss: 171.69175720214844\n",
      "Epoch 62, Batch 8893, Loss: 171.46974182128906\n",
      "Epoch 62, Batch 8894, Loss: 164.8225860595703\n",
      "Epoch 62, Batch 8895, Loss: 160.03079223632812\n",
      "Epoch 62, Batch 8896, Loss: 162.46438598632812\n",
      "Epoch 62, Batch 8897, Loss: 169.45787048339844\n",
      "Epoch 62, Batch 8898, Loss: 157.98277282714844\n",
      "Epoch 62, Batch 8899, Loss: 151.9702606201172\n",
      "Epoch 62, Batch 8900, Loss: 167.47276306152344\n",
      "Epoch 62, Batch 8901, Loss: 188.38351440429688\n",
      "Epoch 62, Batch 8902, Loss: 165.41525268554688\n",
      "Epoch 62, Batch 8903, Loss: 159.16111755371094\n",
      "Epoch 62, Batch 8904, Loss: 179.15167236328125\n",
      "Epoch 62, Batch 8905, Loss: 181.14923095703125\n",
      "Epoch 62, Batch 8906, Loss: 166.42269897460938\n",
      "Epoch 62, Batch 8907, Loss: 198.40699768066406\n",
      "Epoch 62, Batch 8908, Loss: 187.48304748535156\n",
      "Epoch 62, Batch 8909, Loss: 174.37411499023438\n",
      "Epoch 62, Batch 8910, Loss: 180.83468627929688\n",
      "Epoch 62, Batch 8911, Loss: 167.53176879882812\n",
      "Epoch 62, Batch 8912, Loss: 181.718017578125\n",
      "Epoch 62, Batch 8913, Loss: 179.04652404785156\n",
      "Epoch 62, Batch 8914, Loss: 179.0742950439453\n",
      "Epoch 62, Batch 8915, Loss: 173.95748901367188\n",
      "Epoch 62, Batch 8916, Loss: 173.64610290527344\n",
      "Epoch 62, Batch 8917, Loss: 158.5874786376953\n",
      "Epoch 62, Batch 8918, Loss: 177.96981811523438\n",
      "Epoch 62, Batch 8919, Loss: 172.62417602539062\n",
      "Epoch 62, Batch 8920, Loss: 173.57553100585938\n",
      "Epoch 62, Batch 8921, Loss: 169.65176391601562\n",
      "Epoch 62, Batch 8922, Loss: 177.1602325439453\n",
      "Epoch 62, Batch 8923, Loss: 171.98362731933594\n",
      "Epoch 62, Batch 8924, Loss: 172.86093139648438\n",
      "Epoch 62, Batch 8925, Loss: 188.39447021484375\n",
      "Epoch 62, Batch 8926, Loss: 183.8331756591797\n",
      "Epoch 62, Batch 8927, Loss: 171.58563232421875\n",
      "Epoch 62, Batch 8928, Loss: 161.005859375\n",
      "Epoch 62, Batch 8929, Loss: 176.29263305664062\n",
      "Epoch 62, Batch 8930, Loss: 161.561279296875\n",
      "Epoch 62, Batch 8931, Loss: 176.63385009765625\n",
      "Epoch 62, Batch 8932, Loss: 173.07460021972656\n",
      "Epoch 62, Batch 8933, Loss: 182.13113403320312\n",
      "Epoch 62, Batch 8934, Loss: 178.0996856689453\n",
      "Epoch 62, Batch 8935, Loss: 163.7475128173828\n",
      "Epoch 62, Batch 8936, Loss: 162.99598693847656\n",
      "Epoch 62, Batch 8937, Loss: 173.64117431640625\n",
      "Epoch 62, Batch 8938, Loss: 168.56434631347656\n",
      "Epoch 62, Batch 8939, Loss: 161.7754364013672\n",
      "Epoch 62, Batch 8940, Loss: 182.04110717773438\n",
      "Epoch 62, Batch 8941, Loss: 173.64341735839844\n",
      "Epoch 62, Batch 8942, Loss: 171.42018127441406\n",
      "Epoch 62, Batch 8943, Loss: 180.79225158691406\n",
      "Epoch 62, Batch 8944, Loss: 181.52243041992188\n",
      "Epoch 62, Batch 8945, Loss: 169.68655395507812\n",
      "Epoch 62, Batch 8946, Loss: 171.7201690673828\n",
      "Epoch 62, Batch 8947, Loss: 172.7696075439453\n",
      "Epoch 62, Batch 8948, Loss: 175.16004943847656\n",
      "Epoch 62, Batch 8949, Loss: 170.7503204345703\n",
      "Epoch 62, Batch 8950, Loss: 152.75062561035156\n",
      "Epoch 62, Batch 8951, Loss: 165.4271240234375\n",
      "Epoch 62, Batch 8952, Loss: 172.09182739257812\n",
      "Epoch 62, Batch 8953, Loss: 172.14991760253906\n",
      "Epoch 62, Batch 8954, Loss: 161.50228881835938\n",
      "Epoch 62, Batch 8955, Loss: 175.85838317871094\n",
      "Epoch 62, Batch 8956, Loss: 174.5809326171875\n",
      "Epoch 62, Batch 8957, Loss: 183.16017150878906\n",
      "Epoch 62, Batch 8958, Loss: 160.47991943359375\n",
      "Epoch 62, Batch 8959, Loss: 189.07028198242188\n",
      "Epoch 62, Batch 8960, Loss: 174.5812225341797\n",
      "Epoch 62, Batch 8961, Loss: 175.6473388671875\n",
      "Epoch 62, Batch 8962, Loss: 166.36117553710938\n",
      "Epoch 62, Batch 8963, Loss: 151.3352813720703\n",
      "Epoch 62, Batch 8964, Loss: 182.76171875\n",
      "Epoch 62, Batch 8965, Loss: 167.28860473632812\n",
      "Epoch 62, Batch 8966, Loss: 162.86105346679688\n",
      "Epoch 62, Batch 8967, Loss: 169.65797424316406\n",
      "Epoch 62, Batch 8968, Loss: 184.55938720703125\n",
      "Epoch 62, Batch 8969, Loss: 169.03421020507812\n",
      "Epoch 62, Batch 8970, Loss: 160.1776885986328\n",
      "Epoch 62, Batch 8971, Loss: 161.9891357421875\n",
      "Epoch 62, Batch 8972, Loss: 169.1968536376953\n",
      "Epoch 62, Batch 8973, Loss: 171.50743103027344\n",
      "Epoch 62, Batch 8974, Loss: 168.01296997070312\n",
      "Epoch 62, Batch 8975, Loss: 189.24427795410156\n",
      "Epoch 62, Batch 8976, Loss: 180.04220581054688\n",
      "Epoch 62, Batch 8977, Loss: 188.4779052734375\n",
      "Epoch 62, Batch 8978, Loss: 185.42466735839844\n",
      "Epoch 62, Batch 8979, Loss: 180.93807983398438\n",
      "Epoch 62, Batch 8980, Loss: 185.30015563964844\n",
      "Epoch 62, Batch 8981, Loss: 162.54563903808594\n",
      "Epoch 62, Batch 8982, Loss: 168.56736755371094\n",
      "Epoch 62, Batch 8983, Loss: 169.74960327148438\n",
      "Epoch 62, Batch 8984, Loss: 181.55120849609375\n",
      "Epoch 62, Batch 8985, Loss: 174.23223876953125\n",
      "Epoch 62, Batch 8986, Loss: 208.75204467773438\n",
      "Epoch 62, Batch 8987, Loss: 180.76370239257812\n",
      "Epoch 62, Batch 8988, Loss: 170.31109619140625\n",
      "Epoch 62, Batch 8989, Loss: 173.6123504638672\n",
      "Epoch 62, Batch 8990, Loss: 171.1511688232422\n",
      "Epoch 62, Batch 8991, Loss: 182.99270629882812\n",
      "Epoch 62, Batch 8992, Loss: 166.26133728027344\n",
      "Epoch 62, Batch 8993, Loss: 187.9901885986328\n",
      "Epoch 62, Batch 8994, Loss: 177.19677734375\n",
      "Epoch 62, Batch 8995, Loss: 168.73733520507812\n",
      "Epoch 62, Batch 8996, Loss: 163.72349548339844\n",
      "Epoch 62, Batch 8997, Loss: 158.7899627685547\n",
      "Epoch 62, Batch 8998, Loss: 181.48825073242188\n",
      "Epoch 62, Batch 8999, Loss: 184.15869140625\n",
      "Epoch 62, Batch 9000, Loss: 167.04135131835938\n",
      "Epoch 62, Batch 9001, Loss: 194.0190887451172\n",
      "Epoch 62, Batch 9002, Loss: 164.2544708251953\n",
      "Epoch 62, Batch 9003, Loss: 163.44332885742188\n",
      "Epoch 62, Batch 9004, Loss: 197.06024169921875\n",
      "Epoch 62, Batch 9005, Loss: 175.1826171875\n",
      "Epoch 62, Batch 9006, Loss: 164.8041534423828\n",
      "Epoch 62, Batch 9007, Loss: 168.78460693359375\n",
      "Epoch 62, Batch 9008, Loss: 191.4083251953125\n",
      "Epoch 62, Batch 9009, Loss: 165.8035888671875\n",
      "Epoch 62, Batch 9010, Loss: 168.68182373046875\n",
      "Epoch 62, Batch 9011, Loss: 163.46572875976562\n",
      "Epoch 62, Batch 9012, Loss: 153.70401000976562\n",
      "Epoch 62, Batch 9013, Loss: 174.67086791992188\n",
      "Epoch 62, Batch 9014, Loss: 181.86614990234375\n",
      "Epoch 62, Batch 9015, Loss: 161.32518005371094\n",
      "Epoch 62, Batch 9016, Loss: 166.68197631835938\n",
      "Epoch 62, Batch 9017, Loss: 185.3857879638672\n",
      "Epoch 62, Batch 9018, Loss: 185.690673828125\n",
      "Epoch 62, Batch 9019, Loss: 161.3946533203125\n",
      "Epoch 62, Batch 9020, Loss: 161.98658752441406\n",
      "Epoch 62, Batch 9021, Loss: 171.41770935058594\n",
      "Epoch 62, Batch 9022, Loss: 170.25555419921875\n",
      "Epoch 62, Batch 9023, Loss: 159.88473510742188\n",
      "Epoch 62, Batch 9024, Loss: 187.81130981445312\n",
      "Epoch 62, Batch 9025, Loss: 173.0143585205078\n",
      "Epoch 62, Batch 9026, Loss: 163.60980224609375\n",
      "Epoch 62, Batch 9027, Loss: 175.50767517089844\n",
      "Epoch 62, Batch 9028, Loss: 167.61123657226562\n",
      "Epoch 62, Batch 9029, Loss: 178.65672302246094\n",
      "Epoch 62, Batch 9030, Loss: 159.90087890625\n",
      "Epoch 62, Batch 9031, Loss: 162.05177307128906\n",
      "Epoch 62, Batch 9032, Loss: 182.99497985839844\n",
      "Epoch 62, Batch 9033, Loss: 178.00991821289062\n",
      "Epoch 62, Batch 9034, Loss: 178.55226135253906\n",
      "Epoch 62, Batch 9035, Loss: 175.86598205566406\n",
      "Epoch 62, Batch 9036, Loss: 163.31394958496094\n",
      "Epoch 62, Batch 9037, Loss: 165.2133331298828\n",
      "Epoch 62, Batch 9038, Loss: 188.8668212890625\n",
      "Epoch 62, Batch 9039, Loss: 172.10169982910156\n",
      "Epoch 62, Batch 9040, Loss: 195.40281677246094\n",
      "Epoch 62, Batch 9041, Loss: 173.37242126464844\n",
      "Epoch 62, Batch 9042, Loss: 186.4727020263672\n",
      "Epoch 62, Batch 9043, Loss: 167.62155151367188\n",
      "Epoch 62, Batch 9044, Loss: 154.46542358398438\n",
      "Epoch 62, Batch 9045, Loss: 162.91329956054688\n",
      "Epoch 62, Batch 9046, Loss: 174.65721130371094\n",
      "Epoch 62, Batch 9047, Loss: 170.916748046875\n",
      "Epoch 62, Batch 9048, Loss: 178.3998260498047\n",
      "Epoch 62, Batch 9049, Loss: 169.69735717773438\n",
      "Epoch 62, Batch 9050, Loss: 162.39157104492188\n",
      "Epoch 62, Batch 9051, Loss: 187.02285766601562\n",
      "Epoch 62, Batch 9052, Loss: 156.34764099121094\n",
      "Epoch 62, Batch 9053, Loss: 163.71495056152344\n",
      "Epoch 62, Batch 9054, Loss: 167.60203552246094\n",
      "Epoch 62, Batch 9055, Loss: 162.9311981201172\n",
      "Epoch 62, Batch 9056, Loss: 186.18995666503906\n",
      "Epoch 62, Batch 9057, Loss: 201.5918426513672\n",
      "Epoch 62, Batch 9058, Loss: 182.25241088867188\n",
      "Epoch 62, Batch 9059, Loss: 159.48272705078125\n",
      "Epoch 62, Batch 9060, Loss: 189.0579071044922\n",
      "Epoch 62, Batch 9061, Loss: 164.41070556640625\n",
      "Epoch 62, Batch 9062, Loss: 152.535400390625\n",
      "Epoch 62, Batch 9063, Loss: 169.92404174804688\n",
      "Epoch 62, Batch 9064, Loss: 168.40476989746094\n",
      "Epoch 62, Batch 9065, Loss: 185.19273376464844\n",
      "Epoch 62, Batch 9066, Loss: 171.6551055908203\n",
      "Epoch 62, Batch 9067, Loss: 167.51548767089844\n",
      "Epoch 62, Batch 9068, Loss: 166.11961364746094\n",
      "Epoch 62, Batch 9069, Loss: 191.7954559326172\n",
      "Epoch 62, Batch 9070, Loss: 184.64813232421875\n",
      "Epoch 62, Batch 9071, Loss: 162.9251251220703\n",
      "Epoch 62, Batch 9072, Loss: 176.44662475585938\n",
      "Epoch 62, Batch 9073, Loss: 160.49609375\n",
      "Epoch 62, Batch 9074, Loss: 176.2029266357422\n",
      "Epoch 62, Batch 9075, Loss: 159.2779083251953\n",
      "Epoch 62, Batch 9076, Loss: 184.42575073242188\n",
      "Epoch 62, Batch 9077, Loss: 165.96360778808594\n",
      "Epoch 62, Batch 9078, Loss: 184.31112670898438\n",
      "Epoch 62, Batch 9079, Loss: 153.59490966796875\n",
      "Epoch 62, Batch 9080, Loss: 161.14491271972656\n",
      "Epoch 62, Batch 9081, Loss: 179.8626251220703\n",
      "Epoch 62, Batch 9082, Loss: 173.17010498046875\n",
      "Epoch 62, Batch 9083, Loss: 182.30035400390625\n",
      "Epoch 62, Batch 9084, Loss: 181.75308227539062\n",
      "Epoch 62, Batch 9085, Loss: 179.0340576171875\n",
      "Epoch 62, Batch 9086, Loss: 164.1591796875\n",
      "Epoch 62, Batch 9087, Loss: 184.01527404785156\n",
      "Epoch 62, Batch 9088, Loss: 177.39796447753906\n",
      "Epoch 62, Batch 9089, Loss: 174.40084838867188\n",
      "Epoch 62, Batch 9090, Loss: 174.29612731933594\n",
      "Epoch 62, Batch 9091, Loss: 156.89114379882812\n",
      "Epoch 62, Batch 9092, Loss: 170.1104278564453\n",
      "Epoch 62, Batch 9093, Loss: 187.10719299316406\n",
      "Epoch 62, Batch 9094, Loss: 162.23184204101562\n",
      "Epoch 62, Batch 9095, Loss: 159.0054473876953\n",
      "Epoch 62, Batch 9096, Loss: 181.57965087890625\n",
      "Epoch 62, Batch 9097, Loss: 166.0572967529297\n",
      "Epoch 62, Batch 9098, Loss: 158.892822265625\n",
      "Epoch 62, Batch 9099, Loss: 166.39361572265625\n",
      "Epoch 62, Batch 9100, Loss: 157.10012817382812\n",
      "Epoch 62, Batch 9101, Loss: 193.4075164794922\n",
      "Epoch 62, Batch 9102, Loss: 170.97146606445312\n",
      "Epoch 62, Batch 9103, Loss: 166.03248596191406\n",
      "Epoch 62, Batch 9104, Loss: 156.04849243164062\n",
      "Epoch 62, Batch 9105, Loss: 175.5534210205078\n",
      "Epoch 62, Batch 9106, Loss: 171.9705352783203\n",
      "Epoch 62, Batch 9107, Loss: 151.922607421875\n",
      "Epoch 62, Batch 9108, Loss: 188.4868927001953\n",
      "Epoch 62, Batch 9109, Loss: 173.6422119140625\n",
      "Epoch 62, Batch 9110, Loss: 181.5172576904297\n",
      "Epoch 62, Batch 9111, Loss: 169.5652313232422\n",
      "Epoch 62, Batch 9112, Loss: 185.15452575683594\n",
      "Epoch 62, Batch 9113, Loss: 179.0284423828125\n",
      "Epoch 62, Batch 9114, Loss: 182.37123107910156\n",
      "Epoch 62, Batch 9115, Loss: 166.89529418945312\n",
      "Epoch 62, Batch 9116, Loss: 169.5351104736328\n",
      "Epoch 62, Batch 9117, Loss: 159.88414001464844\n",
      "Epoch 62, Batch 9118, Loss: 168.32334899902344\n",
      "Epoch 62, Batch 9119, Loss: 174.29525756835938\n",
      "Epoch 62, Batch 9120, Loss: 185.06210327148438\n",
      "Epoch 62, Batch 9121, Loss: 168.4669189453125\n",
      "Epoch 62, Batch 9122, Loss: 169.08335876464844\n",
      "Epoch 62, Batch 9123, Loss: 178.2331085205078\n",
      "Epoch 62, Batch 9124, Loss: 173.1592559814453\n",
      "Epoch 62, Batch 9125, Loss: 162.0885467529297\n",
      "Epoch 62, Batch 9126, Loss: 184.85118103027344\n",
      "Epoch 62, Batch 9127, Loss: 179.52589416503906\n",
      "Epoch 62, Batch 9128, Loss: 177.08700561523438\n",
      "Epoch 62, Batch 9129, Loss: 163.04196166992188\n",
      "Epoch 62, Batch 9130, Loss: 179.53443908691406\n",
      "Epoch 62, Batch 9131, Loss: 183.67715454101562\n",
      "Epoch 62, Batch 9132, Loss: 181.1642608642578\n",
      "Epoch 62, Batch 9133, Loss: 179.00039672851562\n",
      "Epoch 62, Batch 9134, Loss: 173.31361389160156\n",
      "Epoch 62, Batch 9135, Loss: 193.26625061035156\n",
      "Epoch 62, Batch 9136, Loss: 169.45465087890625\n",
      "Epoch 62, Batch 9137, Loss: 195.45623779296875\n",
      "Epoch 62, Batch 9138, Loss: 173.77601623535156\n",
      "Epoch 62, Batch 9139, Loss: 178.8040313720703\n",
      "Epoch 62, Batch 9140, Loss: 163.1440887451172\n",
      "Epoch 62, Batch 9141, Loss: 169.97303771972656\n",
      "Epoch 62, Batch 9142, Loss: 163.25196838378906\n",
      "Epoch 62, Batch 9143, Loss: 171.8595428466797\n",
      "Epoch 62, Batch 9144, Loss: 166.95272827148438\n",
      "Epoch 62, Batch 9145, Loss: 194.47291564941406\n",
      "Epoch 62, Batch 9146, Loss: 184.52867126464844\n",
      "Epoch 62, Batch 9147, Loss: 163.87109375\n",
      "Epoch 62, Batch 9148, Loss: 145.5694580078125\n",
      "Epoch 62, Batch 9149, Loss: 156.85421752929688\n",
      "Epoch 62, Batch 9150, Loss: 167.86988830566406\n",
      "Epoch 62, Batch 9151, Loss: 157.7682647705078\n",
      "Epoch 62, Batch 9152, Loss: 159.91192626953125\n",
      "Epoch 62, Batch 9153, Loss: 160.15530395507812\n",
      "Epoch 62, Batch 9154, Loss: 175.61685180664062\n",
      "Epoch 62, Batch 9155, Loss: 165.5988006591797\n",
      "Epoch 62, Batch 9156, Loss: 166.76670837402344\n",
      "Epoch 62, Batch 9157, Loss: 171.9704132080078\n",
      "Epoch 62, Batch 9158, Loss: 167.29275512695312\n",
      "Epoch 62, Batch 9159, Loss: 174.12217712402344\n",
      "Epoch 62, Batch 9160, Loss: 181.91114807128906\n",
      "Epoch 62, Batch 9161, Loss: 169.1749267578125\n",
      "Epoch 62, Batch 9162, Loss: 164.44976806640625\n",
      "Epoch 62, Batch 9163, Loss: 171.07357788085938\n",
      "Epoch 62, Batch 9164, Loss: 164.44606018066406\n",
      "Epoch 62, Batch 9165, Loss: 176.86866760253906\n",
      "Epoch 62, Batch 9166, Loss: 179.78868103027344\n",
      "Epoch 62, Batch 9167, Loss: 184.1917266845703\n",
      "Epoch 62, Batch 9168, Loss: 148.64596557617188\n",
      "Epoch 62, Batch 9169, Loss: 155.2633056640625\n",
      "Epoch 62, Batch 9170, Loss: 181.1204833984375\n",
      "Epoch 62, Batch 9171, Loss: 185.6883087158203\n",
      "Epoch 62, Batch 9172, Loss: 179.77621459960938\n",
      "Epoch 62, Batch 9173, Loss: 167.91259765625\n",
      "Epoch 62, Batch 9174, Loss: 179.94187927246094\n",
      "Epoch 62, Batch 9175, Loss: 194.84661865234375\n",
      "Epoch 62, Batch 9176, Loss: 170.90476989746094\n",
      "Epoch 62, Batch 9177, Loss: 180.2975311279297\n",
      "Epoch 62, Batch 9178, Loss: 172.42166137695312\n",
      "Epoch 62, Batch 9179, Loss: 178.0221405029297\n",
      "Epoch 62, Batch 9180, Loss: 168.688720703125\n",
      "Epoch 62, Batch 9181, Loss: 174.3859100341797\n",
      "Epoch 62, Batch 9182, Loss: 185.14907836914062\n",
      "Epoch 62, Batch 9183, Loss: 170.80538940429688\n",
      "Epoch 62, Batch 9184, Loss: 161.9168701171875\n",
      "Epoch 62, Batch 9185, Loss: 186.18951416015625\n",
      "Epoch 62, Batch 9186, Loss: 178.27066040039062\n",
      "Epoch 62, Batch 9187, Loss: 179.48487854003906\n",
      "Epoch 62, Batch 9188, Loss: 193.23013305664062\n",
      "Epoch 62, Batch 9189, Loss: 191.700927734375\n",
      "Epoch 62, Batch 9190, Loss: 179.6765899658203\n",
      "Epoch 62, Batch 9191, Loss: 174.3427734375\n",
      "Epoch 62, Batch 9192, Loss: 166.06394958496094\n",
      "Epoch 62, Batch 9193, Loss: 172.57064819335938\n",
      "Epoch 62, Batch 9194, Loss: 169.28146362304688\n",
      "Epoch 62, Batch 9195, Loss: 175.68301391601562\n",
      "Epoch 62, Batch 9196, Loss: 162.00779724121094\n",
      "Epoch 62, Batch 9197, Loss: 162.366455078125\n",
      "Epoch 62, Batch 9198, Loss: 156.5075225830078\n",
      "Epoch 62, Batch 9199, Loss: 183.6992950439453\n",
      "Epoch 62, Batch 9200, Loss: 167.5715789794922\n",
      "Epoch 62, Batch 9201, Loss: 177.80052185058594\n",
      "Epoch 62, Batch 9202, Loss: 186.41461181640625\n",
      "Epoch 62, Batch 9203, Loss: 169.443115234375\n",
      "Epoch 62, Batch 9204, Loss: 183.95355224609375\n",
      "Epoch 62, Batch 9205, Loss: 161.7213592529297\n",
      "Epoch 62, Batch 9206, Loss: 181.11585998535156\n",
      "Epoch 62, Batch 9207, Loss: 181.02655029296875\n",
      "Epoch 62, Batch 9208, Loss: 164.700927734375\n",
      "Epoch 62, Batch 9209, Loss: 169.22799682617188\n",
      "Epoch 62, Batch 9210, Loss: 204.65777587890625\n",
      "Epoch 62, Batch 9211, Loss: 172.12513732910156\n",
      "Epoch 62, Batch 9212, Loss: 175.31832885742188\n",
      "Epoch 62, Batch 9213, Loss: 168.8291015625\n",
      "Epoch 62, Batch 9214, Loss: 180.40966796875\n",
      "Epoch 62, Batch 9215, Loss: 174.8888702392578\n",
      "Epoch 62, Batch 9216, Loss: 175.8107147216797\n",
      "Epoch 62, Batch 9217, Loss: 183.77230834960938\n",
      "Epoch 62, Batch 9218, Loss: 174.24676513671875\n",
      "Epoch 62, Batch 9219, Loss: 179.77740478515625\n",
      "Epoch 62, Batch 9220, Loss: 181.40322875976562\n",
      "Epoch 62, Batch 9221, Loss: 164.50022888183594\n",
      "Epoch 62, Batch 9222, Loss: 165.3638916015625\n",
      "Epoch 62, Batch 9223, Loss: 158.82484436035156\n",
      "Epoch 62, Batch 9224, Loss: 168.40631103515625\n",
      "Epoch 62, Batch 9225, Loss: 163.8775177001953\n",
      "Epoch 62, Batch 9226, Loss: 175.6251220703125\n",
      "Epoch 62, Batch 9227, Loss: 173.79994201660156\n",
      "Epoch 62, Batch 9228, Loss: 169.72203063964844\n",
      "Epoch 62, Batch 9229, Loss: 176.9466552734375\n",
      "Epoch 62, Batch 9230, Loss: 165.002685546875\n",
      "Epoch 62, Batch 9231, Loss: 174.96543884277344\n",
      "Epoch 62, Batch 9232, Loss: 178.16781616210938\n",
      "Epoch 62, Batch 9233, Loss: 155.2274169921875\n",
      "Epoch 62, Batch 9234, Loss: 168.9063262939453\n",
      "Epoch 62, Batch 9235, Loss: 173.9253692626953\n",
      "Epoch 62, Batch 9236, Loss: 173.17665100097656\n",
      "Epoch 62, Batch 9237, Loss: 163.62142944335938\n",
      "Epoch 62, Batch 9238, Loss: 179.26022338867188\n",
      "Epoch 62, Batch 9239, Loss: 168.89715576171875\n",
      "Epoch 62, Batch 9240, Loss: 162.1376495361328\n",
      "Epoch 62, Batch 9241, Loss: 167.66867065429688\n",
      "Epoch 62, Batch 9242, Loss: 166.45559692382812\n",
      "Epoch 62, Batch 9243, Loss: 175.62753295898438\n",
      "Epoch 62, Batch 9244, Loss: 184.1505584716797\n",
      "Epoch 62, Batch 9245, Loss: 183.45533752441406\n",
      "Epoch 62, Batch 9246, Loss: 175.0750732421875\n",
      "Epoch 62, Batch 9247, Loss: 165.0193328857422\n",
      "Epoch 62, Batch 9248, Loss: 175.5609130859375\n",
      "Epoch 62, Batch 9249, Loss: 173.35635375976562\n",
      "Epoch 62, Batch 9250, Loss: 174.27967834472656\n",
      "Epoch 62, Batch 9251, Loss: 163.08358764648438\n",
      "Epoch 62, Batch 9252, Loss: 167.5590362548828\n",
      "Epoch 62, Batch 9253, Loss: 171.75511169433594\n",
      "Epoch 62, Batch 9254, Loss: 167.9158935546875\n",
      "Epoch 62, Batch 9255, Loss: 165.65675354003906\n",
      "Epoch 62, Batch 9256, Loss: 166.3375701904297\n",
      "Epoch 62, Batch 9257, Loss: 176.54443359375\n",
      "Epoch 62, Batch 9258, Loss: 175.361083984375\n",
      "Epoch 62, Batch 9259, Loss: 166.35369873046875\n",
      "Epoch 62, Batch 9260, Loss: 166.9524383544922\n",
      "Epoch 62, Batch 9261, Loss: 172.3032684326172\n",
      "Epoch 62, Batch 9262, Loss: 173.57266235351562\n",
      "Epoch 62, Batch 9263, Loss: 166.43392944335938\n",
      "Epoch 62, Batch 9264, Loss: 174.67491149902344\n",
      "Epoch 62, Batch 9265, Loss: 166.3974609375\n",
      "Epoch 62, Batch 9266, Loss: 160.6453857421875\n",
      "Epoch 62, Batch 9267, Loss: 199.2124786376953\n",
      "Epoch 62, Batch 9268, Loss: 173.5991973876953\n",
      "Epoch 62, Batch 9269, Loss: 168.83396911621094\n",
      "Epoch 62, Batch 9270, Loss: 167.5828094482422\n",
      "Epoch 62, Batch 9271, Loss: 168.7760772705078\n",
      "Epoch 62, Batch 9272, Loss: 160.59226989746094\n",
      "Epoch 62, Batch 9273, Loss: 164.88388061523438\n",
      "Epoch 62, Batch 9274, Loss: 184.23719787597656\n",
      "Epoch 62, Batch 9275, Loss: 171.95330810546875\n",
      "Epoch 62, Batch 9276, Loss: 158.3332977294922\n",
      "Epoch 62, Batch 9277, Loss: 190.10366821289062\n",
      "Epoch 62, Batch 9278, Loss: 192.61837768554688\n",
      "Epoch 62, Batch 9279, Loss: 165.3404541015625\n",
      "Epoch 62, Batch 9280, Loss: 180.65420532226562\n",
      "Epoch 62, Batch 9281, Loss: 187.9052734375\n",
      "Epoch 62, Batch 9282, Loss: 185.02476501464844\n",
      "Epoch 62, Batch 9283, Loss: 173.3732452392578\n",
      "Epoch 62, Batch 9284, Loss: 156.43922424316406\n",
      "Epoch 62, Batch 9285, Loss: 154.2068634033203\n",
      "Epoch 62, Batch 9286, Loss: 172.2384796142578\n",
      "Epoch 62, Batch 9287, Loss: 176.3319091796875\n",
      "Epoch 62, Batch 9288, Loss: 203.89332580566406\n",
      "Epoch 62, Batch 9289, Loss: 175.33753967285156\n",
      "Epoch 62, Batch 9290, Loss: 199.6013946533203\n",
      "Epoch 62, Batch 9291, Loss: 170.60923767089844\n",
      "Epoch 62, Batch 9292, Loss: 170.62481689453125\n",
      "Epoch 62, Batch 9293, Loss: 167.0626220703125\n",
      "Epoch 62, Batch 9294, Loss: 172.09071350097656\n",
      "Epoch 62, Batch 9295, Loss: 174.7290496826172\n",
      "Epoch 62, Batch 9296, Loss: 181.55870056152344\n",
      "Epoch 62, Batch 9297, Loss: 166.2202606201172\n",
      "Epoch 62, Batch 9298, Loss: 184.07257080078125\n",
      "Epoch 62, Batch 9299, Loss: 151.55274963378906\n",
      "Epoch 62, Batch 9300, Loss: 177.9310760498047\n",
      "Epoch 62, Batch 9301, Loss: 157.4217987060547\n",
      "Epoch 62, Batch 9302, Loss: 166.91102600097656\n",
      "Epoch 62, Batch 9303, Loss: 183.8277130126953\n",
      "Epoch 62, Batch 9304, Loss: 166.0683135986328\n",
      "Epoch 62, Batch 9305, Loss: 178.13278198242188\n",
      "Epoch 62, Batch 9306, Loss: 185.4046173095703\n",
      "Epoch 62, Batch 9307, Loss: 166.17318725585938\n",
      "Epoch 62, Batch 9308, Loss: 169.95912170410156\n",
      "Epoch 62, Batch 9309, Loss: 173.67672729492188\n",
      "Epoch 62, Batch 9310, Loss: 169.9192657470703\n",
      "Epoch 62, Batch 9311, Loss: 165.246826171875\n",
      "Epoch 62, Batch 9312, Loss: 170.04676818847656\n",
      "Epoch 62, Batch 9313, Loss: 171.93467712402344\n",
      "Epoch 62, Batch 9314, Loss: 162.26766967773438\n",
      "Epoch 62, Batch 9315, Loss: 166.2086944580078\n",
      "Epoch 62, Batch 9316, Loss: 161.74273681640625\n",
      "Epoch 62, Batch 9317, Loss: 170.93545532226562\n",
      "Epoch 62, Batch 9318, Loss: 168.37857055664062\n",
      "Epoch 62, Batch 9319, Loss: 159.83641052246094\n",
      "Epoch 62, Batch 9320, Loss: 172.4730987548828\n",
      "Epoch 62, Batch 9321, Loss: 173.90579223632812\n",
      "Epoch 62, Batch 9322, Loss: 159.79249572753906\n",
      "Epoch 62, Batch 9323, Loss: 194.77565002441406\n",
      "Epoch 62, Batch 9324, Loss: 152.8787841796875\n",
      "Epoch 62, Batch 9325, Loss: 164.1011199951172\n",
      "Epoch 62, Batch 9326, Loss: 173.4278106689453\n",
      "Epoch 62, Batch 9327, Loss: 174.72950744628906\n",
      "Epoch 62, Batch 9328, Loss: 176.23109436035156\n",
      "Epoch 62, Batch 9329, Loss: 170.12905883789062\n",
      "Epoch 62, Batch 9330, Loss: 170.787353515625\n",
      "Epoch 62, Batch 9331, Loss: 166.45468139648438\n",
      "Epoch 62, Batch 9332, Loss: 170.30792236328125\n",
      "Epoch 62, Batch 9333, Loss: 170.1997833251953\n",
      "Epoch 62, Batch 9334, Loss: 167.35963439941406\n",
      "Epoch 62, Batch 9335, Loss: 175.74095153808594\n",
      "Epoch 62, Batch 9336, Loss: 173.91969299316406\n",
      "Epoch 62, Batch 9337, Loss: 187.26263427734375\n",
      "Epoch 62, Batch 9338, Loss: 183.46002197265625\n",
      "Epoch 62, Batch 9339, Loss: 172.31866455078125\n",
      "Epoch 62, Batch 9340, Loss: 152.14108276367188\n",
      "Epoch 62, Batch 9341, Loss: 172.85194396972656\n",
      "Epoch 62, Batch 9342, Loss: 197.4326171875\n",
      "Epoch 62, Batch 9343, Loss: 196.11404418945312\n",
      "Epoch 62, Batch 9344, Loss: 166.65977478027344\n",
      "Epoch 62, Batch 9345, Loss: 180.0749969482422\n",
      "Epoch 62, Batch 9346, Loss: 179.81837463378906\n",
      "Epoch 62, Batch 9347, Loss: 185.3584747314453\n",
      "Epoch 62, Batch 9348, Loss: 159.94918823242188\n",
      "Epoch 62, Batch 9349, Loss: 164.9327392578125\n",
      "Epoch 62, Batch 9350, Loss: 168.491943359375\n",
      "Epoch 62, Batch 9351, Loss: 171.21595764160156\n",
      "Epoch 62, Batch 9352, Loss: 171.76699829101562\n",
      "Epoch 62, Batch 9353, Loss: 170.55577087402344\n",
      "Epoch 62, Batch 9354, Loss: 179.52239990234375\n",
      "Epoch 62, Batch 9355, Loss: 152.21630859375\n",
      "Epoch 62, Batch 9356, Loss: 162.26075744628906\n",
      "Epoch 62, Batch 9357, Loss: 158.77442932128906\n",
      "Epoch 62, Batch 9358, Loss: 174.69805908203125\n",
      "Epoch 62, Batch 9359, Loss: 174.2989044189453\n",
      "Epoch 62, Batch 9360, Loss: 183.61915588378906\n",
      "Epoch 62, Batch 9361, Loss: 178.3119659423828\n",
      "Epoch 62, Batch 9362, Loss: 176.33233642578125\n",
      "Epoch 62, Batch 9363, Loss: 175.91876220703125\n",
      "Epoch 62, Batch 9364, Loss: 184.0243377685547\n",
      "Epoch 62, Batch 9365, Loss: 159.8731231689453\n",
      "Epoch 62, Batch 9366, Loss: 175.71507263183594\n",
      "Epoch 62, Batch 9367, Loss: 180.40594482421875\n",
      "Epoch 62, Batch 9368, Loss: 194.70938110351562\n",
      "Epoch 62, Batch 9369, Loss: 165.81724548339844\n",
      "Epoch 62, Batch 9370, Loss: 189.7264862060547\n",
      "Epoch 62, Batch 9371, Loss: 162.3058624267578\n",
      "Epoch 62, Batch 9372, Loss: 166.88058471679688\n",
      "Epoch 62, Batch 9373, Loss: 180.2968292236328\n",
      "Epoch 62, Batch 9374, Loss: 185.74046325683594\n",
      "Epoch 62, Batch 9375, Loss: 174.68728637695312\n",
      "Epoch 62, Batch 9376, Loss: 163.41928100585938\n",
      "Epoch 62, Batch 9377, Loss: 176.6826171875\n",
      "Epoch 62, Batch 9378, Loss: 173.5133056640625\n",
      "Epoch 62, Batch 9379, Loss: 162.2459259033203\n",
      "Epoch 62, Batch 9380, Loss: 163.2040557861328\n",
      "Epoch 62, Batch 9381, Loss: 157.0142364501953\n",
      "Epoch 62, Batch 9382, Loss: 171.6768341064453\n",
      "Epoch 62, Batch 9383, Loss: 176.35018920898438\n",
      "Epoch 62, Batch 9384, Loss: 169.192138671875\n",
      "Epoch 62, Batch 9385, Loss: 171.43284606933594\n",
      "Epoch 62, Batch 9386, Loss: 177.24957275390625\n",
      "Epoch 62, Batch 9387, Loss: 183.24807739257812\n",
      "Epoch 62, Batch 9388, Loss: 171.32586669921875\n",
      "Epoch 62, Batch 9389, Loss: 158.2703857421875\n",
      "Epoch 62, Batch 9390, Loss: 173.0747528076172\n",
      "Epoch 62, Batch 9391, Loss: 165.7233123779297\n",
      "Epoch 62, Batch 9392, Loss: 176.49072265625\n",
      "Epoch 62, Batch 9393, Loss: 167.6158905029297\n",
      "Epoch 62, Batch 9394, Loss: 176.36679077148438\n",
      "Epoch 62, Batch 9395, Loss: 179.64332580566406\n",
      "Epoch 62, Batch 9396, Loss: 166.81573486328125\n",
      "Epoch 62, Batch 9397, Loss: 162.1960906982422\n",
      "Epoch 62, Batch 9398, Loss: 172.41253662109375\n",
      "Epoch 62, Batch 9399, Loss: 177.38426208496094\n",
      "Epoch 62, Batch 9400, Loss: 165.66934204101562\n",
      "Epoch 62, Batch 9401, Loss: 186.7054901123047\n",
      "Epoch 62, Batch 9402, Loss: 161.28436279296875\n",
      "Epoch 62, Batch 9403, Loss: 187.4217071533203\n",
      "Epoch 62, Batch 9404, Loss: 171.6404571533203\n",
      "Epoch 62, Batch 9405, Loss: 182.683837890625\n",
      "Epoch 62, Batch 9406, Loss: 179.1387481689453\n",
      "Epoch 62, Batch 9407, Loss: 173.70237731933594\n",
      "Epoch 62, Batch 9408, Loss: 167.4524688720703\n",
      "Epoch 62, Batch 9409, Loss: 163.41143798828125\n",
      "Epoch 62, Batch 9410, Loss: 182.20074462890625\n",
      "Epoch 62, Batch 9411, Loss: 173.8306121826172\n",
      "Epoch 62, Batch 9412, Loss: 180.30055236816406\n",
      "Epoch 62, Batch 9413, Loss: 176.82855224609375\n",
      "Epoch 62, Batch 9414, Loss: 178.2534637451172\n",
      "Epoch 62, Batch 9415, Loss: 166.19815063476562\n",
      "Epoch 62, Batch 9416, Loss: 172.63076782226562\n",
      "Epoch 62, Batch 9417, Loss: 176.19796752929688\n",
      "Epoch 62, Batch 9418, Loss: 160.5789794921875\n",
      "Epoch 62, Batch 9419, Loss: 167.81503295898438\n",
      "Epoch 62, Batch 9420, Loss: 187.36077880859375\n",
      "Epoch 62, Batch 9421, Loss: 173.18165588378906\n",
      "Epoch 62, Batch 9422, Loss: 172.6034698486328\n",
      "Epoch 62, Batch 9423, Loss: 181.6298065185547\n",
      "Epoch 62, Batch 9424, Loss: 165.05929565429688\n",
      "Epoch 62, Batch 9425, Loss: 183.50831604003906\n",
      "Epoch 62, Batch 9426, Loss: 179.5491180419922\n",
      "Epoch 62, Batch 9427, Loss: 187.5950469970703\n",
      "Epoch 62, Batch 9428, Loss: 169.23094177246094\n",
      "Epoch 62, Batch 9429, Loss: 167.24266052246094\n",
      "Epoch 62, Batch 9430, Loss: 174.05145263671875\n",
      "Epoch 62, Batch 9431, Loss: 176.31289672851562\n",
      "Epoch 62, Batch 9432, Loss: 188.41455078125\n",
      "Epoch 62, Batch 9433, Loss: 172.4313507080078\n",
      "Epoch 62, Batch 9434, Loss: 178.72621154785156\n",
      "Epoch 62, Batch 9435, Loss: 175.51451110839844\n",
      "Epoch 62, Batch 9436, Loss: 187.0726318359375\n",
      "Epoch 62, Batch 9437, Loss: 180.6593780517578\n",
      "Epoch 62, Batch 9438, Loss: 171.9706268310547\n",
      "Epoch 62, Batch 9439, Loss: 166.39688110351562\n",
      "Epoch 62, Batch 9440, Loss: 172.5565948486328\n",
      "Epoch 62, Batch 9441, Loss: 170.1822052001953\n",
      "Epoch 62, Batch 9442, Loss: 163.8101348876953\n",
      "Epoch 62, Batch 9443, Loss: 175.1062469482422\n",
      "Epoch 62, Batch 9444, Loss: 171.5753936767578\n",
      "Epoch 62, Batch 9445, Loss: 191.23883056640625\n",
      "Epoch 62, Batch 9446, Loss: 180.41619873046875\n",
      "Epoch 62, Batch 9447, Loss: 159.64967346191406\n",
      "Epoch 62, Batch 9448, Loss: 177.46495056152344\n",
      "Epoch 62, Batch 9449, Loss: 162.9013671875\n",
      "Epoch 62, Batch 9450, Loss: 182.29507446289062\n",
      "Epoch 62, Batch 9451, Loss: 167.88873291015625\n",
      "Epoch 62, Batch 9452, Loss: 169.48097229003906\n",
      "Epoch 62, Batch 9453, Loss: 162.67581176757812\n",
      "Epoch 62, Batch 9454, Loss: 172.675537109375\n",
      "Epoch 62, Batch 9455, Loss: 177.7158203125\n",
      "Epoch 62, Batch 9456, Loss: 170.48033142089844\n",
      "Epoch 62, Batch 9457, Loss: 181.9177703857422\n",
      "Epoch 62, Batch 9458, Loss: 178.33799743652344\n",
      "Epoch 62, Batch 9459, Loss: 169.03118896484375\n",
      "Epoch 62, Batch 9460, Loss: 161.40548706054688\n",
      "Epoch 62, Batch 9461, Loss: 162.67526245117188\n",
      "Epoch 62, Batch 9462, Loss: 177.60000610351562\n",
      "Epoch 62, Batch 9463, Loss: 172.79046630859375\n",
      "Epoch 62, Batch 9464, Loss: 170.32472229003906\n",
      "Epoch 62, Batch 9465, Loss: 170.7783966064453\n",
      "Epoch 62, Batch 9466, Loss: 176.3155517578125\n",
      "Epoch 62, Batch 9467, Loss: 163.30307006835938\n",
      "Epoch 62, Batch 9468, Loss: 179.21780395507812\n",
      "Epoch 62, Batch 9469, Loss: 168.20401000976562\n",
      "Epoch 62, Batch 9470, Loss: 173.47171020507812\n",
      "Epoch 62, Batch 9471, Loss: 164.64703369140625\n",
      "Epoch 62, Batch 9472, Loss: 165.08670043945312\n",
      "Epoch 62, Batch 9473, Loss: 177.67681884765625\n",
      "Epoch 62, Batch 9474, Loss: 185.96078491210938\n",
      "Epoch 62, Batch 9475, Loss: 172.28648376464844\n",
      "Epoch 62, Batch 9476, Loss: 181.32229614257812\n",
      "Epoch 62, Batch 9477, Loss: 166.32937622070312\n",
      "Epoch 62, Batch 9478, Loss: 174.4005584716797\n",
      "Epoch 62, Batch 9479, Loss: 177.30006408691406\n",
      "Epoch 62, Batch 9480, Loss: 181.91586303710938\n",
      "Epoch 62, Batch 9481, Loss: 160.4140625\n",
      "Epoch 62, Batch 9482, Loss: 160.9303436279297\n",
      "Epoch 62, Batch 9483, Loss: 176.74624633789062\n",
      "Epoch 62, Batch 9484, Loss: 172.6312255859375\n",
      "Epoch 62, Batch 9485, Loss: 174.09397888183594\n",
      "Epoch 62, Batch 9486, Loss: 160.64137268066406\n",
      "Epoch 62, Batch 9487, Loss: 179.93060302734375\n",
      "Epoch 62, Batch 9488, Loss: 178.38491821289062\n",
      "Epoch 62, Batch 9489, Loss: 171.8108673095703\n",
      "Epoch 62, Batch 9490, Loss: 169.39480590820312\n",
      "Epoch 62, Batch 9491, Loss: 185.87330627441406\n",
      "Epoch 62, Batch 9492, Loss: 158.5227813720703\n",
      "Epoch 62, Batch 9493, Loss: 164.83816528320312\n",
      "Epoch 62, Batch 9494, Loss: 179.2721405029297\n",
      "Epoch 62, Batch 9495, Loss: 178.12847900390625\n",
      "Epoch 62, Batch 9496, Loss: 152.46865844726562\n",
      "Epoch 62, Batch 9497, Loss: 173.31422424316406\n",
      "Epoch 62, Batch 9498, Loss: 184.27561950683594\n",
      "Epoch 62, Batch 9499, Loss: 168.4491424560547\n",
      "Epoch 62, Batch 9500, Loss: 169.2247314453125\n",
      "Epoch 62, Batch 9501, Loss: 167.48423767089844\n",
      "Epoch 62, Batch 9502, Loss: 189.02975463867188\n",
      "Epoch 62, Batch 9503, Loss: 158.64894104003906\n",
      "Epoch 62, Batch 9504, Loss: 164.06265258789062\n",
      "Epoch 62, Batch 9505, Loss: 189.3561553955078\n",
      "Epoch 62, Batch 9506, Loss: 172.48489379882812\n",
      "Epoch 62, Batch 9507, Loss: 181.61900329589844\n",
      "Epoch 62, Batch 9508, Loss: 183.73544311523438\n",
      "Epoch 62, Batch 9509, Loss: 170.04261779785156\n",
      "Epoch 62, Batch 9510, Loss: 166.36219787597656\n",
      "Epoch 62, Batch 9511, Loss: 174.4183807373047\n",
      "Epoch 62, Batch 9512, Loss: 177.1163787841797\n",
      "Epoch 62, Batch 9513, Loss: 177.75869750976562\n",
      "Epoch 62, Batch 9514, Loss: 185.6533660888672\n",
      "Epoch 62, Batch 9515, Loss: 169.1661376953125\n",
      "Epoch 62, Batch 9516, Loss: 176.51626586914062\n",
      "Epoch 62, Batch 9517, Loss: 169.8874053955078\n",
      "Epoch 62, Batch 9518, Loss: 162.80039978027344\n",
      "Epoch 62, Batch 9519, Loss: 161.50457763671875\n",
      "Epoch 62, Batch 9520, Loss: 164.60687255859375\n",
      "Epoch 62, Batch 9521, Loss: 178.56924438476562\n",
      "Epoch 62, Batch 9522, Loss: 167.70013427734375\n",
      "Epoch 62, Batch 9523, Loss: 188.58164978027344\n",
      "Epoch 62, Batch 9524, Loss: 175.5913543701172\n",
      "Epoch 62, Batch 9525, Loss: 186.21389770507812\n",
      "Epoch 62, Batch 9526, Loss: 168.7853546142578\n",
      "Epoch 62, Batch 9527, Loss: 191.77828979492188\n",
      "Epoch 62, Batch 9528, Loss: 170.7987060546875\n",
      "Epoch 62, Batch 9529, Loss: 191.8345947265625\n",
      "Epoch 62, Batch 9530, Loss: 165.7706298828125\n",
      "Epoch 62, Batch 9531, Loss: 169.0763702392578\n",
      "Epoch 62, Batch 9532, Loss: 177.88368225097656\n",
      "Epoch 62, Batch 9533, Loss: 186.9632110595703\n",
      "Epoch 62, Batch 9534, Loss: 180.5444793701172\n",
      "Epoch 62, Batch 9535, Loss: 175.97056579589844\n",
      "Epoch 62, Batch 9536, Loss: 180.2295379638672\n",
      "Epoch 62, Batch 9537, Loss: 158.10394287109375\n",
      "Epoch 62, Batch 9538, Loss: 179.80557250976562\n",
      "Epoch 62, Batch 9539, Loss: 178.36585998535156\n",
      "Epoch 62, Batch 9540, Loss: 176.8242645263672\n",
      "Epoch 62, Batch 9541, Loss: 161.67391967773438\n",
      "Epoch 62, Batch 9542, Loss: 168.91380310058594\n",
      "Epoch 62, Batch 9543, Loss: 174.28553771972656\n",
      "Epoch 62, Batch 9544, Loss: 169.33375549316406\n",
      "Epoch 62, Batch 9545, Loss: 185.6010284423828\n",
      "Epoch 62, Batch 9546, Loss: 173.7947540283203\n",
      "Epoch 62, Batch 9547, Loss: 157.02886962890625\n",
      "Epoch 62, Batch 9548, Loss: 155.98634338378906\n",
      "Epoch 62, Batch 9549, Loss: 169.8965606689453\n",
      "Epoch 62, Batch 9550, Loss: 184.54962158203125\n",
      "Epoch 62, Batch 9551, Loss: 167.6364288330078\n",
      "Epoch 62, Batch 9552, Loss: 184.42909240722656\n",
      "Epoch 62, Batch 9553, Loss: 168.5080108642578\n",
      "Epoch 62, Batch 9554, Loss: 163.2902374267578\n",
      "Epoch 62, Batch 9555, Loss: 177.14215087890625\n",
      "Epoch 62, Batch 9556, Loss: 172.66542053222656\n",
      "Epoch 62, Batch 9557, Loss: 173.95858764648438\n",
      "Epoch 62, Batch 9558, Loss: 175.081787109375\n",
      "Epoch 62, Batch 9559, Loss: 165.3629608154297\n",
      "Epoch 62, Batch 9560, Loss: 163.65966796875\n",
      "Epoch 62, Batch 9561, Loss: 167.4383087158203\n",
      "Epoch 62, Batch 9562, Loss: 155.906982421875\n",
      "Epoch 62, Batch 9563, Loss: 166.60955810546875\n",
      "Epoch 62, Batch 9564, Loss: 170.6251983642578\n",
      "Epoch 62, Batch 9565, Loss: 160.55813598632812\n",
      "Epoch 62, Batch 9566, Loss: 172.30995178222656\n",
      "Epoch 62, Batch 9567, Loss: 167.22349548339844\n",
      "Epoch 62, Batch 9568, Loss: 160.0852813720703\n",
      "Epoch 62, Batch 9569, Loss: 159.6160125732422\n",
      "Epoch 62, Batch 9570, Loss: 161.1992950439453\n",
      "Epoch 62, Batch 9571, Loss: 154.70574951171875\n",
      "Epoch 62, Batch 9572, Loss: 165.4482879638672\n",
      "Epoch 62, Batch 9573, Loss: 183.05506896972656\n",
      "Epoch 62, Batch 9574, Loss: 173.68446350097656\n",
      "Epoch 62, Batch 9575, Loss: 162.54396057128906\n",
      "Epoch 62, Batch 9576, Loss: 164.0594482421875\n",
      "Epoch 62, Batch 9577, Loss: 156.14990234375\n",
      "Epoch 62, Batch 9578, Loss: 179.6117706298828\n",
      "Epoch 62, Batch 9579, Loss: 173.12001037597656\n",
      "Epoch 62, Batch 9580, Loss: 166.10260009765625\n",
      "Epoch 62, Batch 9581, Loss: 166.82037353515625\n",
      "Epoch 62, Batch 9582, Loss: 175.94744873046875\n",
      "Epoch 62, Batch 9583, Loss: 161.5195770263672\n",
      "Epoch 62, Batch 9584, Loss: 180.0547332763672\n",
      "Epoch 62, Batch 9585, Loss: 199.82257080078125\n",
      "Epoch 62, Batch 9586, Loss: 166.61566162109375\n",
      "Epoch 62, Batch 9587, Loss: 144.50839233398438\n",
      "Epoch 62, Batch 9588, Loss: 182.27340698242188\n",
      "Epoch 62, Batch 9589, Loss: 175.0553436279297\n",
      "Epoch 62, Batch 9590, Loss: 165.65943908691406\n",
      "Epoch 62, Batch 9591, Loss: 169.39825439453125\n",
      "Epoch 62, Batch 9592, Loss: 160.81761169433594\n",
      "Epoch 62, Batch 9593, Loss: 156.7512969970703\n",
      "Epoch 62, Batch 9594, Loss: 163.44691467285156\n",
      "Epoch 62, Batch 9595, Loss: 178.69956970214844\n",
      "Epoch 62, Batch 9596, Loss: 182.85096740722656\n",
      "Epoch 62, Batch 9597, Loss: 163.69073486328125\n",
      "Epoch 62, Batch 9598, Loss: 141.9372100830078\n",
      "Epoch 62, Batch 9599, Loss: 173.23910522460938\n",
      "Epoch 62, Batch 9600, Loss: 185.64646911621094\n",
      "Epoch 62, Batch 9601, Loss: 175.6942901611328\n",
      "Epoch 62, Batch 9602, Loss: 172.67938232421875\n",
      "Epoch 62, Batch 9603, Loss: 167.02987670898438\n",
      "Epoch 62, Batch 9604, Loss: 163.33609008789062\n",
      "Epoch 62, Batch 9605, Loss: 175.92550659179688\n",
      "Epoch 62, Batch 9606, Loss: 156.0576934814453\n",
      "Epoch 62, Batch 9607, Loss: 178.80816650390625\n",
      "Epoch 62, Batch 9608, Loss: 160.54466247558594\n",
      "Epoch 62, Batch 9609, Loss: 182.78253173828125\n",
      "Epoch 62, Batch 9610, Loss: 173.9931640625\n",
      "Epoch 62, Batch 9611, Loss: 185.7344512939453\n",
      "Epoch 62, Batch 9612, Loss: 168.9740447998047\n",
      "Epoch 62, Batch 9613, Loss: 174.8580780029297\n",
      "Epoch 62, Batch 9614, Loss: 178.37657165527344\n",
      "Epoch 62, Batch 9615, Loss: 178.829345703125\n",
      "Epoch 62, Batch 9616, Loss: 176.7623291015625\n",
      "Epoch 62, Batch 9617, Loss: 156.23072814941406\n",
      "Epoch 62, Batch 9618, Loss: 166.90255737304688\n",
      "Epoch 62, Batch 9619, Loss: 187.98184204101562\n",
      "Epoch 62, Batch 9620, Loss: 183.7964324951172\n",
      "Epoch 62, Batch 9621, Loss: 189.8641815185547\n",
      "Epoch 62, Batch 9622, Loss: 150.78968811035156\n",
      "Epoch 62, Batch 9623, Loss: 166.11997985839844\n",
      "Epoch 62, Batch 9624, Loss: 172.37185668945312\n",
      "Epoch 62, Batch 9625, Loss: 182.68885803222656\n",
      "Epoch 62, Batch 9626, Loss: 168.13192749023438\n",
      "Epoch 62, Batch 9627, Loss: 182.32225036621094\n",
      "Epoch 62, Batch 9628, Loss: 183.03329467773438\n",
      "Epoch 62, Batch 9629, Loss: 159.31285095214844\n",
      "Epoch 62, Batch 9630, Loss: 174.40138244628906\n",
      "Epoch 62, Batch 9631, Loss: 188.44248962402344\n",
      "Epoch 62, Batch 9632, Loss: 186.55816650390625\n",
      "Epoch 62, Batch 9633, Loss: 183.07260131835938\n",
      "Epoch 62, Batch 9634, Loss: 173.6809844970703\n",
      "Epoch 62, Batch 9635, Loss: 171.6055145263672\n",
      "Epoch 62, Batch 9636, Loss: 164.34304809570312\n",
      "Epoch 62, Batch 9637, Loss: 179.67860412597656\n",
      "Epoch 62, Batch 9638, Loss: 173.72872924804688\n",
      "Epoch 62, Batch 9639, Loss: 179.93211364746094\n",
      "Epoch 62, Batch 9640, Loss: 165.63259887695312\n",
      "Epoch 62, Batch 9641, Loss: 167.22727966308594\n",
      "Epoch 62, Batch 9642, Loss: 188.56771850585938\n",
      "Epoch 62, Batch 9643, Loss: 175.16311645507812\n",
      "Epoch 62, Batch 9644, Loss: 171.10643005371094\n",
      "Epoch 62, Batch 9645, Loss: 165.5400390625\n",
      "Epoch 62, Batch 9646, Loss: 153.07862854003906\n",
      "Epoch 62, Batch 9647, Loss: 184.34506225585938\n",
      "Epoch 62, Batch 9648, Loss: 184.03543090820312\n",
      "Epoch 62, Batch 9649, Loss: 159.76834106445312\n",
      "Epoch 62, Batch 9650, Loss: 174.56541442871094\n",
      "Epoch 62, Batch 9651, Loss: 170.62646484375\n",
      "Epoch 62, Batch 9652, Loss: 172.82583618164062\n",
      "Epoch 62, Batch 9653, Loss: 165.50062561035156\n",
      "Epoch 62, Batch 9654, Loss: 160.41159057617188\n",
      "Epoch 62, Batch 9655, Loss: 156.4029998779297\n",
      "Epoch 62, Batch 9656, Loss: 183.1998291015625\n",
      "Epoch 62, Batch 9657, Loss: 175.15200805664062\n",
      "Epoch 62, Batch 9658, Loss: 174.57115173339844\n",
      "Epoch 62, Batch 9659, Loss: 166.6855010986328\n",
      "Epoch 62, Batch 9660, Loss: 173.3764190673828\n",
      "Epoch 62, Batch 9661, Loss: 175.43336486816406\n",
      "Epoch 62, Batch 9662, Loss: 180.7882843017578\n",
      "Epoch 62, Batch 9663, Loss: 162.12033081054688\n",
      "Epoch 62, Batch 9664, Loss: 181.8404541015625\n",
      "Epoch 62, Batch 9665, Loss: 172.32508850097656\n",
      "Epoch 62, Batch 9666, Loss: 180.40533447265625\n",
      "Epoch 62, Batch 9667, Loss: 168.22857666015625\n",
      "Epoch 62, Batch 9668, Loss: 182.4113006591797\n",
      "Epoch 62, Batch 9669, Loss: 165.72219848632812\n",
      "Epoch 62, Batch 9670, Loss: 180.6082000732422\n",
      "Epoch 62, Batch 9671, Loss: 149.22056579589844\n",
      "Epoch 62, Batch 9672, Loss: 187.7483367919922\n",
      "Epoch 62, Batch 9673, Loss: 160.41387939453125\n",
      "Epoch 62, Batch 9674, Loss: 185.49354553222656\n",
      "Epoch 62, Batch 9675, Loss: 186.02000427246094\n",
      "Epoch 62, Batch 9676, Loss: 179.84051513671875\n",
      "Epoch 62, Batch 9677, Loss: 174.71200561523438\n",
      "Epoch 62, Batch 9678, Loss: 169.1431884765625\n",
      "Epoch 62, Batch 9679, Loss: 164.46372985839844\n",
      "Epoch 62, Batch 9680, Loss: 169.0783233642578\n",
      "Epoch 62, Batch 9681, Loss: 171.3016357421875\n",
      "Epoch 62, Batch 9682, Loss: 160.3885498046875\n",
      "Epoch 62, Batch 9683, Loss: 178.5538787841797\n",
      "Epoch 62, Batch 9684, Loss: 167.61891174316406\n",
      "Epoch 62, Batch 9685, Loss: 175.6645050048828\n",
      "Epoch 62, Batch 9686, Loss: 178.9366455078125\n",
      "Epoch 62, Batch 9687, Loss: 161.0224151611328\n",
      "Epoch 62, Batch 9688, Loss: 172.48179626464844\n",
      "Epoch 62, Batch 9689, Loss: 186.90496826171875\n",
      "Epoch 62, Batch 9690, Loss: 180.360107421875\n",
      "Epoch 62, Batch 9691, Loss: 170.77072143554688\n",
      "Epoch 62, Batch 9692, Loss: 171.83009338378906\n",
      "Epoch 62, Batch 9693, Loss: 159.56358337402344\n",
      "Epoch 62, Batch 9694, Loss: 175.5123748779297\n",
      "Epoch 62, Batch 9695, Loss: 175.96910095214844\n",
      "Epoch 62, Batch 9696, Loss: 153.97909545898438\n",
      "Epoch 62, Batch 9697, Loss: 167.45994567871094\n",
      "Epoch 62, Batch 9698, Loss: 174.26490783691406\n",
      "Epoch 62, Batch 9699, Loss: 170.02423095703125\n",
      "Epoch 62, Batch 9700, Loss: 178.68313598632812\n",
      "Epoch 62, Batch 9701, Loss: 179.08436584472656\n",
      "Epoch 62, Batch 9702, Loss: 167.1696014404297\n",
      "Epoch 62, Batch 9703, Loss: 174.239013671875\n",
      "Epoch 62, Batch 9704, Loss: 175.90573120117188\n",
      "Epoch 62, Batch 9705, Loss: 171.505615234375\n",
      "Epoch 62, Batch 9706, Loss: 168.6251220703125\n",
      "Epoch 62, Batch 9707, Loss: 166.47410583496094\n",
      "Epoch 62, Batch 9708, Loss: 168.58013916015625\n",
      "Epoch 62, Batch 9709, Loss: 159.2918701171875\n",
      "Epoch 62, Batch 9710, Loss: 175.31915283203125\n",
      "Epoch 62, Batch 9711, Loss: 170.60293579101562\n",
      "Epoch 62, Batch 9712, Loss: 152.83517456054688\n",
      "Epoch 62, Batch 9713, Loss: 160.47439575195312\n",
      "Epoch 62, Batch 9714, Loss: 190.34384155273438\n",
      "Epoch 62, Batch 9715, Loss: 164.8175506591797\n",
      "Epoch 62, Batch 9716, Loss: 167.67898559570312\n",
      "Epoch 62, Batch 9717, Loss: 169.5813446044922\n",
      "Epoch 62, Batch 9718, Loss: 172.97604370117188\n",
      "Epoch 62, Batch 9719, Loss: 195.7755584716797\n",
      "Epoch 62, Batch 9720, Loss: 159.81370544433594\n",
      "Epoch 62, Batch 9721, Loss: 171.5188751220703\n",
      "Epoch 62, Batch 9722, Loss: 164.360107421875\n",
      "Epoch 62, Batch 9723, Loss: 186.5519561767578\n",
      "Epoch 62, Batch 9724, Loss: 174.8519744873047\n",
      "Epoch 62, Batch 9725, Loss: 181.4729766845703\n",
      "Epoch 62, Batch 9726, Loss: 186.0117645263672\n",
      "Epoch 62, Batch 9727, Loss: 168.62428283691406\n",
      "Epoch 62, Batch 9728, Loss: 184.43174743652344\n",
      "Epoch 62, Batch 9729, Loss: 165.684326171875\n",
      "Epoch 62, Batch 9730, Loss: 181.68919372558594\n",
      "Epoch 62, Batch 9731, Loss: 174.7370147705078\n",
      "Epoch 62, Batch 9732, Loss: 169.69898986816406\n",
      "Epoch 62, Batch 9733, Loss: 184.22900390625\n",
      "Epoch 62, Batch 9734, Loss: 172.64154052734375\n",
      "Epoch 62, Batch 9735, Loss: 146.7169952392578\n",
      "Epoch 62, Batch 9736, Loss: 175.77764892578125\n",
      "Epoch 62, Batch 9737, Loss: 171.62637329101562\n",
      "Epoch 62, Batch 9738, Loss: 158.29588317871094\n",
      "Epoch 62, Batch 9739, Loss: 172.97589111328125\n",
      "Epoch 62, Batch 9740, Loss: 168.67967224121094\n",
      "Epoch 62, Batch 9741, Loss: 175.8962860107422\n",
      "Epoch 62, Batch 9742, Loss: 164.02842712402344\n",
      "Epoch 62, Batch 9743, Loss: 190.3818817138672\n",
      "Epoch 62, Batch 9744, Loss: 191.52552795410156\n",
      "Epoch 62, Batch 9745, Loss: 173.82823181152344\n",
      "Epoch 62, Batch 9746, Loss: 168.43690490722656\n",
      "Epoch 62, Batch 9747, Loss: 167.42971801757812\n",
      "Epoch 62, Batch 9748, Loss: 171.11151123046875\n",
      "Epoch 62, Batch 9749, Loss: 158.6837158203125\n",
      "Epoch 62, Batch 9750, Loss: 175.1806182861328\n",
      "Epoch 62, Batch 9751, Loss: 171.74563598632812\n",
      "Epoch 62, Batch 9752, Loss: 176.17868041992188\n",
      "Epoch 62, Batch 9753, Loss: 176.99017333984375\n",
      "Epoch 62, Batch 9754, Loss: 182.86239624023438\n",
      "Epoch 62, Batch 9755, Loss: 173.9895477294922\n",
      "Epoch 62, Batch 9756, Loss: 162.2648162841797\n",
      "Epoch 62, Batch 9757, Loss: 169.6353759765625\n",
      "Epoch 62, Batch 9758, Loss: 190.33140563964844\n",
      "Epoch 62, Batch 9759, Loss: 169.41604614257812\n",
      "Epoch 62, Batch 9760, Loss: 179.50889587402344\n",
      "Epoch 62, Batch 9761, Loss: 183.19554138183594\n",
      "Epoch 62, Batch 9762, Loss: 181.34568786621094\n",
      "Epoch 62, Batch 9763, Loss: 174.75613403320312\n",
      "Epoch 62, Batch 9764, Loss: 157.21376037597656\n",
      "Epoch 62, Batch 9765, Loss: 163.99945068359375\n",
      "Epoch 62, Batch 9766, Loss: 193.43966674804688\n",
      "Epoch 62, Batch 9767, Loss: 170.99037170410156\n",
      "Epoch 62, Batch 9768, Loss: 172.56365966796875\n",
      "Epoch 62, Batch 9769, Loss: 158.90646362304688\n",
      "Epoch 62, Batch 9770, Loss: 168.01904296875\n",
      "Epoch 62, Batch 9771, Loss: 164.9940185546875\n",
      "Epoch 62, Batch 9772, Loss: 160.6784210205078\n",
      "Epoch 62, Batch 9773, Loss: 161.87718200683594\n",
      "Epoch 62, Batch 9774, Loss: 182.09193420410156\n",
      "Epoch 62, Batch 9775, Loss: 167.84922790527344\n",
      "Epoch 62, Batch 9776, Loss: 168.62200927734375\n",
      "Epoch 62, Batch 9777, Loss: 169.0757293701172\n",
      "Epoch 62, Batch 9778, Loss: 152.44671630859375\n",
      "Epoch 62, Batch 9779, Loss: 193.2864532470703\n",
      "Epoch 62, Batch 9780, Loss: 169.3235321044922\n",
      "Epoch 62, Batch 9781, Loss: 173.45643615722656\n",
      "Epoch 62, Batch 9782, Loss: 155.94427490234375\n",
      "Epoch 62, Batch 9783, Loss: 174.7269744873047\n",
      "Epoch 62, Batch 9784, Loss: 167.5803680419922\n",
      "Epoch 62, Batch 9785, Loss: 170.05592346191406\n",
      "Epoch 62, Batch 9786, Loss: 173.7687530517578\n",
      "Epoch 62, Batch 9787, Loss: 183.66407775878906\n",
      "Epoch 62, Batch 9788, Loss: 159.99603271484375\n",
      "Epoch 62, Batch 9789, Loss: 172.679931640625\n",
      "Epoch 62, Batch 9790, Loss: 178.13365173339844\n",
      "Epoch 62, Batch 9791, Loss: 162.98941040039062\n",
      "Epoch 62, Batch 9792, Loss: 180.3504638671875\n",
      "Epoch 62, Batch 9793, Loss: 160.11181640625\n",
      "Epoch 62, Batch 9794, Loss: 171.1503448486328\n",
      "Epoch 62, Batch 9795, Loss: 184.02264404296875\n",
      "Epoch 62, Batch 9796, Loss: 169.80026245117188\n",
      "Epoch 62, Batch 9797, Loss: 178.41957092285156\n",
      "Epoch 62, Batch 9798, Loss: 172.4500732421875\n",
      "Epoch 62, Batch 9799, Loss: 195.31484985351562\n",
      "Epoch 62, Batch 9800, Loss: 175.48033142089844\n",
      "Epoch 62, Batch 9801, Loss: 184.0978546142578\n",
      "Epoch 62, Batch 9802, Loss: 171.42428588867188\n",
      "Epoch 62, Batch 9803, Loss: 162.399658203125\n",
      "Epoch 62, Batch 9804, Loss: 180.1508331298828\n",
      "Epoch 62, Batch 9805, Loss: 171.32102966308594\n",
      "Epoch 62, Batch 9806, Loss: 163.96702575683594\n",
      "Epoch 62, Batch 9807, Loss: 159.25341796875\n",
      "Epoch 62, Batch 9808, Loss: 163.99093627929688\n",
      "Epoch 62, Batch 9809, Loss: 174.4130096435547\n",
      "Epoch 62, Batch 9810, Loss: 160.50747680664062\n",
      "Epoch 62, Batch 9811, Loss: 170.7020721435547\n",
      "Epoch 62, Batch 9812, Loss: 174.7147216796875\n",
      "Epoch 62, Batch 9813, Loss: 155.67886352539062\n",
      "Epoch 62, Batch 9814, Loss: 182.5760498046875\n",
      "Epoch 62, Batch 9815, Loss: 177.0189666748047\n",
      "Epoch 62, Batch 9816, Loss: 173.677734375\n",
      "Epoch 62, Batch 9817, Loss: 163.3035125732422\n",
      "Epoch 62, Batch 9818, Loss: 177.75189208984375\n",
      "Epoch 62, Batch 9819, Loss: 183.39370727539062\n",
      "Epoch 62, Batch 9820, Loss: 181.21484375\n",
      "Epoch 62, Batch 9821, Loss: 171.37738037109375\n",
      "Epoch 62, Batch 9822, Loss: 174.4143829345703\n",
      "Epoch 62, Batch 9823, Loss: 163.73477172851562\n",
      "Epoch 62, Batch 9824, Loss: 187.9912872314453\n",
      "Epoch 62, Batch 9825, Loss: 162.2464599609375\n",
      "Epoch 62, Batch 9826, Loss: 182.42123413085938\n",
      "Epoch 62, Batch 9827, Loss: 184.36813354492188\n",
      "Epoch 62, Batch 9828, Loss: 198.66845703125\n",
      "Epoch 62, Batch 9829, Loss: 191.11331176757812\n",
      "Epoch 62, Batch 9830, Loss: 176.28836059570312\n",
      "Epoch 62, Batch 9831, Loss: 191.73159790039062\n",
      "Epoch 62, Batch 9832, Loss: 193.94439697265625\n",
      "Epoch 62, Batch 9833, Loss: 186.3488006591797\n",
      "Epoch 62, Batch 9834, Loss: 181.1934051513672\n",
      "Epoch 62, Batch 9835, Loss: 179.903564453125\n",
      "Epoch 62, Batch 9836, Loss: 163.10714721679688\n",
      "Epoch 62, Batch 9837, Loss: 169.39096069335938\n",
      "Epoch 62, Batch 9838, Loss: 169.22869873046875\n",
      "Epoch 62, Batch 9839, Loss: 155.40667724609375\n",
      "Epoch 62, Batch 9840, Loss: 176.92538452148438\n",
      "Epoch 62, Batch 9841, Loss: 187.193115234375\n",
      "Epoch 62, Batch 9842, Loss: 156.0391387939453\n",
      "Epoch 62, Batch 9843, Loss: 172.2210693359375\n",
      "Epoch 62, Batch 9844, Loss: 179.21176147460938\n",
      "Epoch 62, Batch 9845, Loss: 191.50877380371094\n",
      "Epoch 62, Batch 9846, Loss: 166.16213989257812\n",
      "Epoch 62, Batch 9847, Loss: 156.99839782714844\n",
      "Epoch 62, Batch 9848, Loss: 164.06781005859375\n",
      "Epoch 62, Batch 9849, Loss: 164.9186553955078\n",
      "Epoch 62, Batch 9850, Loss: 183.5193328857422\n",
      "Epoch 62, Batch 9851, Loss: 164.33753967285156\n",
      "Epoch 62, Batch 9852, Loss: 165.59877014160156\n",
      "Epoch 62, Batch 9853, Loss: 174.95542907714844\n",
      "Epoch 62, Batch 9854, Loss: 187.3934326171875\n",
      "Epoch 62, Batch 9855, Loss: 170.41888427734375\n",
      "Epoch 62, Batch 9856, Loss: 180.47181701660156\n",
      "Epoch 62, Batch 9857, Loss: 195.08328247070312\n",
      "Epoch 62, Batch 9858, Loss: 185.7822723388672\n",
      "Epoch 62, Batch 9859, Loss: 182.78086853027344\n",
      "Epoch 62, Batch 9860, Loss: 169.41952514648438\n",
      "Epoch 62, Batch 9861, Loss: 185.9706268310547\n",
      "Epoch 62, Batch 9862, Loss: 187.76026916503906\n",
      "Epoch 62, Batch 9863, Loss: 174.9432373046875\n",
      "Epoch 62, Batch 9864, Loss: 179.2394256591797\n",
      "Epoch 62, Batch 9865, Loss: 164.3839569091797\n",
      "Epoch 62, Batch 9866, Loss: 171.72422790527344\n",
      "Epoch 62, Batch 9867, Loss: 184.66018676757812\n",
      "Epoch 62, Batch 9868, Loss: 187.00701904296875\n",
      "Epoch 62, Batch 9869, Loss: 186.2279052734375\n",
      "Epoch 62, Batch 9870, Loss: 186.19754028320312\n",
      "Epoch 62, Batch 9871, Loss: 178.90493774414062\n",
      "Epoch 62, Batch 9872, Loss: 173.86019897460938\n",
      "Epoch 62, Batch 9873, Loss: 170.75291442871094\n",
      "Epoch 62, Batch 9874, Loss: 172.77255249023438\n",
      "Epoch 62, Batch 9875, Loss: 173.6492919921875\n",
      "Epoch 62, Batch 9876, Loss: 169.23031616210938\n",
      "Epoch 62, Batch 9877, Loss: 167.7482147216797\n",
      "Epoch 62, Batch 9878, Loss: 175.28990173339844\n",
      "Epoch 62, Batch 9879, Loss: 168.51576232910156\n",
      "Epoch 62, Batch 9880, Loss: 154.0426483154297\n",
      "Epoch 62, Batch 9881, Loss: 177.1609649658203\n",
      "Epoch 62, Batch 9882, Loss: 173.209716796875\n",
      "Epoch 62, Batch 9883, Loss: 167.84640502929688\n",
      "Epoch 62, Batch 9884, Loss: 173.4243927001953\n",
      "Epoch 62, Batch 9885, Loss: 184.1327667236328\n",
      "Epoch 62, Batch 9886, Loss: 177.72296142578125\n",
      "Epoch 62, Batch 9887, Loss: 173.87254333496094\n",
      "Epoch 62, Batch 9888, Loss: 175.41268920898438\n",
      "Epoch 62, Batch 9889, Loss: 155.47279357910156\n",
      "Epoch 62, Batch 9890, Loss: 181.05447387695312\n",
      "Epoch 62, Batch 9891, Loss: 174.48486328125\n",
      "Epoch 62, Batch 9892, Loss: 181.65435791015625\n",
      "Epoch 62, Batch 9893, Loss: 190.94387817382812\n",
      "Epoch 62, Batch 9894, Loss: 183.08456420898438\n",
      "Epoch 62, Batch 9895, Loss: 172.66156005859375\n",
      "Epoch 62, Batch 9896, Loss: 175.69447326660156\n",
      "Epoch 62, Batch 9897, Loss: 170.79837036132812\n",
      "Epoch 62, Batch 9898, Loss: 171.93478393554688\n",
      "Epoch 62, Batch 9899, Loss: 159.46163940429688\n",
      "Epoch 62, Batch 9900, Loss: 167.38526916503906\n",
      "Epoch 62, Batch 9901, Loss: 182.1833038330078\n",
      "Epoch 62, Batch 9902, Loss: 161.67861938476562\n",
      "Epoch 62, Batch 9903, Loss: 186.553466796875\n",
      "Epoch 62, Batch 9904, Loss: 179.90061950683594\n",
      "Epoch 62, Batch 9905, Loss: 183.62814331054688\n",
      "Epoch 62, Batch 9906, Loss: 159.60191345214844\n",
      "Epoch 62, Batch 9907, Loss: 173.63119506835938\n",
      "Epoch 62, Batch 9908, Loss: 169.49539184570312\n",
      "Epoch 62, Batch 9909, Loss: 175.30169677734375\n",
      "Epoch 62, Batch 9910, Loss: 189.31964111328125\n",
      "Epoch 62, Batch 9911, Loss: 177.28366088867188\n",
      "Epoch 62, Batch 9912, Loss: 186.2824249267578\n",
      "Epoch 62, Batch 9913, Loss: 195.01881408691406\n",
      "Epoch 62, Batch 9914, Loss: 176.52638244628906\n",
      "Epoch 62, Batch 9915, Loss: 181.07904052734375\n",
      "Epoch 62, Batch 9916, Loss: 177.9774932861328\n",
      "Epoch 62, Batch 9917, Loss: 177.2168731689453\n",
      "Epoch 62, Batch 9918, Loss: 188.26278686523438\n",
      "Epoch 62, Batch 9919, Loss: 158.18658447265625\n",
      "Epoch 62, Batch 9920, Loss: 162.30470275878906\n",
      "Epoch 62, Batch 9921, Loss: 161.76698303222656\n",
      "Epoch 62, Batch 9922, Loss: 183.0384979248047\n",
      "Epoch 62, Batch 9923, Loss: 177.5777587890625\n",
      "Epoch 62, Batch 9924, Loss: 145.87283325195312\n",
      "Epoch 62, Batch 9925, Loss: 159.4134979248047\n",
      "Epoch 62, Batch 9926, Loss: 156.80702209472656\n",
      "Epoch 62, Batch 9927, Loss: 171.87149047851562\n",
      "Epoch 62, Batch 9928, Loss: 163.98977661132812\n",
      "Epoch 62, Batch 9929, Loss: 170.725830078125\n",
      "Epoch 62, Batch 9930, Loss: 189.8383331298828\n",
      "Epoch 62, Batch 9931, Loss: 164.034912109375\n",
      "Epoch 62, Batch 9932, Loss: 164.228271484375\n",
      "Epoch 62, Batch 9933, Loss: 167.4352264404297\n",
      "Epoch 62, Batch 9934, Loss: 167.8730926513672\n",
      "Epoch 62, Batch 9935, Loss: 177.47903442382812\n",
      "Epoch 62, Batch 9936, Loss: 161.28424072265625\n",
      "Epoch 62, Batch 9937, Loss: 156.0607452392578\n",
      "Epoch 62, Batch 9938, Loss: 178.438232421875\n",
      "Epoch 62, Batch 9939, Loss: 182.1329345703125\n",
      "Epoch 62, Batch 9940, Loss: 184.8568572998047\n",
      "Epoch 62, Batch 9941, Loss: 184.58538818359375\n",
      "Epoch 62, Batch 9942, Loss: 175.3837127685547\n",
      "Epoch 62, Batch 9943, Loss: 168.56961059570312\n",
      "Epoch 62, Batch 9944, Loss: 167.17584228515625\n",
      "Epoch 62, Batch 9945, Loss: 182.5244903564453\n",
      "Epoch 62, Batch 9946, Loss: 150.21627807617188\n",
      "Epoch 62, Batch 9947, Loss: 179.59042358398438\n",
      "Epoch 62, Batch 9948, Loss: 163.76846313476562\n",
      "Epoch 62, Batch 9949, Loss: 173.80015563964844\n",
      "Epoch 62, Batch 9950, Loss: 171.50955200195312\n",
      "Epoch 62, Batch 9951, Loss: 167.95974731445312\n",
      "Epoch 62, Batch 9952, Loss: 157.8968048095703\n",
      "Epoch 62, Batch 9953, Loss: 170.34947204589844\n",
      "Epoch 62, Batch 9954, Loss: 188.4070587158203\n",
      "Epoch 62, Batch 9955, Loss: 169.4158477783203\n",
      "Epoch 62, Batch 9956, Loss: 165.38682556152344\n",
      "Epoch 62, Batch 9957, Loss: 189.7147216796875\n",
      "Epoch 62, Batch 9958, Loss: 176.95095825195312\n",
      "Epoch 62, Batch 9959, Loss: 162.19334411621094\n",
      "Epoch 62, Batch 9960, Loss: 184.76417541503906\n",
      "Epoch 62, Batch 9961, Loss: 168.84255981445312\n",
      "Epoch 62, Batch 9962, Loss: 183.38906860351562\n",
      "Epoch 62, Batch 9963, Loss: 175.114013671875\n",
      "Epoch 62, Batch 9964, Loss: 177.7564239501953\n",
      "Epoch 62, Batch 9965, Loss: 168.0594940185547\n",
      "Epoch 62, Batch 9966, Loss: 172.0558624267578\n",
      "Epoch 62, Batch 9967, Loss: 171.08473205566406\n",
      "Epoch 62, Batch 9968, Loss: 180.73699951171875\n",
      "Epoch 62, Batch 9969, Loss: 167.09788513183594\n",
      "Epoch 62, Batch 9970, Loss: 161.98495483398438\n",
      "Epoch 62, Batch 9971, Loss: 173.39295959472656\n",
      "Epoch 62, Batch 9972, Loss: 172.0103759765625\n",
      "Epoch 62, Batch 9973, Loss: 169.10003662109375\n",
      "Epoch 62, Batch 9974, Loss: 174.71490478515625\n",
      "Epoch 62, Batch 9975, Loss: 179.88107299804688\n",
      "Epoch 62, Batch 9976, Loss: 171.0928955078125\n",
      "Epoch 62, Batch 9977, Loss: 178.16542053222656\n",
      "Epoch 62, Batch 9978, Loss: 172.5454559326172\n",
      "Epoch 62, Batch 9979, Loss: 174.41119384765625\n",
      "Epoch 62, Batch 9980, Loss: 165.5551300048828\n",
      "Epoch 62, Batch 9981, Loss: 164.85528564453125\n",
      "Epoch 62, Batch 9982, Loss: 175.85911560058594\n",
      "Epoch 62, Batch 9983, Loss: 176.56234741210938\n",
      "Epoch 62, Batch 9984, Loss: 170.733154296875\n",
      "Epoch 62, Batch 9985, Loss: 155.32159423828125\n",
      "Epoch 62, Batch 9986, Loss: 161.48519897460938\n",
      "Epoch 62, Batch 9987, Loss: 167.82386779785156\n",
      "Epoch 62, Batch 9988, Loss: 174.45111083984375\n",
      "Epoch 62, Batch 9989, Loss: 181.60275268554688\n",
      "Epoch 62, Batch 9990, Loss: 176.77444458007812\n",
      "Epoch 62, Batch 9991, Loss: 170.63357543945312\n",
      "Epoch 62, Batch 9992, Loss: 181.5690460205078\n",
      "Epoch 62, Batch 9993, Loss: 175.7507781982422\n",
      "Epoch 62, Batch 9994, Loss: 164.63339233398438\n",
      "Epoch 62, Batch 9995, Loss: 170.15274047851562\n",
      "Epoch 62, Batch 9996, Loss: 182.90289306640625\n",
      "Epoch 62, Batch 9997, Loss: 180.4938201904297\n",
      "Epoch 62, Batch 9998, Loss: 167.50726318359375\n",
      "Epoch 62, Batch 9999, Loss: 183.1627655029297\n",
      "Epoch 62, Batch 10000, Loss: 205.31004333496094\n",
      "Epoch 62, Batch 10001, Loss: 151.7325439453125\n",
      "Epoch 62, Batch 10002, Loss: 153.55657958984375\n",
      "Epoch 62, Batch 10003, Loss: 194.03419494628906\n",
      "Epoch 62, Batch 10004, Loss: 158.9784698486328\n",
      "Epoch 62, Batch 10005, Loss: 173.15614318847656\n",
      "Epoch 62, Batch 10006, Loss: 157.41128540039062\n",
      "Epoch 62, Batch 10007, Loss: 165.96890258789062\n",
      "Epoch 62, Batch 10008, Loss: 183.88656616210938\n",
      "Epoch 62, Batch 10009, Loss: 185.5784912109375\n",
      "Epoch 62, Batch 10010, Loss: 172.41038513183594\n",
      "Epoch 62, Batch 10011, Loss: 186.3749542236328\n",
      "Epoch 62, Batch 10012, Loss: 168.46173095703125\n",
      "Epoch 62, Batch 10013, Loss: 189.43650817871094\n",
      "Epoch 62, Batch 10014, Loss: 168.6527862548828\n",
      "Epoch 62, Batch 10015, Loss: 157.27090454101562\n",
      "Epoch 62, Batch 10016, Loss: 168.50701904296875\n",
      "Epoch 62, Batch 10017, Loss: 190.40191650390625\n",
      "Epoch 62, Batch 10018, Loss: 189.2794189453125\n",
      "Epoch 62, Batch 10019, Loss: 169.74049377441406\n",
      "Epoch 62, Batch 10020, Loss: 179.0361785888672\n",
      "Epoch 62, Batch 10021, Loss: 154.27810668945312\n",
      "Epoch 62, Batch 10022, Loss: 175.14166259765625\n",
      "Epoch 62, Batch 10023, Loss: 177.1764678955078\n",
      "Epoch 62, Batch 10024, Loss: 157.9647674560547\n",
      "Epoch 62, Batch 10025, Loss: 162.1368865966797\n",
      "Epoch 62, Batch 10026, Loss: 178.84371948242188\n",
      "Epoch 62, Batch 10027, Loss: 169.2796630859375\n",
      "Epoch 62, Batch 10028, Loss: 169.3587188720703\n",
      "Epoch 62, Batch 10029, Loss: 178.87205505371094\n",
      "Epoch 62, Batch 10030, Loss: 166.52432250976562\n",
      "Epoch 62, Batch 10031, Loss: 182.75271606445312\n",
      "Epoch 62, Batch 10032, Loss: 168.03302001953125\n",
      "Epoch 62, Batch 10033, Loss: 169.59320068359375\n",
      "Epoch 62, Batch 10034, Loss: 175.2827911376953\n",
      "Epoch 62, Batch 10035, Loss: 168.77195739746094\n",
      "Epoch 62, Batch 10036, Loss: 177.15625\n",
      "Epoch 62, Batch 10037, Loss: 193.35499572753906\n",
      "Epoch 62, Batch 10038, Loss: 166.05838012695312\n",
      "Epoch 62, Batch 10039, Loss: 164.64012145996094\n",
      "Epoch 62, Batch 10040, Loss: 185.92259216308594\n",
      "Epoch 62, Batch 10041, Loss: 164.92384338378906\n",
      "Epoch 62, Batch 10042, Loss: 168.5306396484375\n",
      "Epoch 62, Batch 10043, Loss: 179.00486755371094\n",
      "Epoch 62, Batch 10044, Loss: 163.8187713623047\n",
      "Epoch 62, Batch 10045, Loss: 179.2073211669922\n",
      "Epoch 62, Batch 10046, Loss: 180.05995178222656\n",
      "Epoch 62, Batch 10047, Loss: 170.4989013671875\n",
      "Epoch 62, Batch 10048, Loss: 183.93873596191406\n",
      "Epoch 62, Batch 10049, Loss: 161.92288208007812\n",
      "Epoch 62, Batch 10050, Loss: 177.36497497558594\n",
      "Epoch 62, Batch 10051, Loss: 168.5011444091797\n",
      "Epoch 62, Batch 10052, Loss: 206.18130493164062\n",
      "Epoch 62, Batch 10053, Loss: 189.4727020263672\n",
      "Epoch 62, Batch 10054, Loss: 168.19461059570312\n",
      "Epoch 62, Batch 10055, Loss: 174.77825927734375\n",
      "Epoch 62, Batch 10056, Loss: 170.49700927734375\n",
      "Epoch 62, Batch 10057, Loss: 176.1690673828125\n",
      "Epoch 62, Batch 10058, Loss: 178.04922485351562\n",
      "Epoch 62, Batch 10059, Loss: 175.6712188720703\n",
      "Epoch 62, Batch 10060, Loss: 172.29522705078125\n",
      "Epoch 62, Batch 10061, Loss: 161.7802734375\n",
      "Epoch 62, Batch 10062, Loss: 157.27626037597656\n",
      "Epoch 62, Batch 10063, Loss: 197.32740783691406\n",
      "Epoch 62, Batch 10064, Loss: 157.6810760498047\n",
      "Epoch 62, Batch 10065, Loss: 176.7615203857422\n",
      "Epoch 62, Batch 10066, Loss: 170.70809936523438\n",
      "Epoch 62, Batch 10067, Loss: 172.71780395507812\n",
      "Epoch 62, Batch 10068, Loss: 164.2096405029297\n",
      "Epoch 62, Batch 10069, Loss: 158.09274291992188\n",
      "Epoch 62, Batch 10070, Loss: 169.7714080810547\n",
      "Epoch 62, Batch 10071, Loss: 175.07833862304688\n",
      "Epoch 62, Batch 10072, Loss: 163.06727600097656\n",
      "Epoch 62, Batch 10073, Loss: 181.79212951660156\n",
      "Epoch 62, Batch 10074, Loss: 166.90785217285156\n",
      "Epoch 62, Batch 10075, Loss: 187.07672119140625\n",
      "Epoch 62, Batch 10076, Loss: 170.91981506347656\n",
      "Epoch 62, Batch 10077, Loss: 172.73130798339844\n",
      "Epoch 62, Batch 10078, Loss: 157.64486694335938\n",
      "Epoch 62, Batch 10079, Loss: 171.5214385986328\n",
      "Epoch 62, Batch 10080, Loss: 160.0238800048828\n",
      "Epoch 62, Batch 10081, Loss: 172.1222381591797\n",
      "Epoch 62, Batch 10082, Loss: 168.31809997558594\n",
      "Epoch 62, Batch 10083, Loss: 165.4075164794922\n",
      "Epoch 62, Batch 10084, Loss: 170.32235717773438\n",
      "Epoch 62, Batch 10085, Loss: 181.83787536621094\n",
      "Epoch 62, Batch 10086, Loss: 167.3164825439453\n",
      "Epoch 62, Batch 10087, Loss: 181.6674041748047\n",
      "Epoch 62, Batch 10088, Loss: 173.2345733642578\n",
      "Epoch 62, Batch 10089, Loss: 172.83953857421875\n",
      "Epoch 62, Batch 10090, Loss: 178.2096405029297\n",
      "Epoch 62, Batch 10091, Loss: 163.02774047851562\n",
      "Epoch 62, Batch 10092, Loss: 176.7837371826172\n",
      "Epoch 62, Batch 10093, Loss: 178.4871063232422\n",
      "Epoch 62, Batch 10094, Loss: 162.33184814453125\n",
      "Epoch 62, Batch 10095, Loss: 154.82850646972656\n",
      "Epoch 62, Batch 10096, Loss: 172.39053344726562\n",
      "Epoch 62, Batch 10097, Loss: 186.62005615234375\n",
      "Epoch 62, Batch 10098, Loss: 162.94175720214844\n",
      "Epoch 62, Batch 10099, Loss: 172.0301055908203\n",
      "Epoch 62, Batch 10100, Loss: 192.98094177246094\n",
      "Epoch 62, Batch 10101, Loss: 166.68115234375\n",
      "Epoch 62, Batch 10102, Loss: 186.89389038085938\n",
      "Epoch 62, Batch 10103, Loss: 184.8166961669922\n",
      "Epoch 62, Batch 10104, Loss: 154.2047119140625\n",
      "Epoch 62, Batch 10105, Loss: 181.6788787841797\n",
      "Epoch 62, Batch 10106, Loss: 174.2230682373047\n",
      "Epoch 62, Batch 10107, Loss: 183.8828887939453\n",
      "Epoch 62, Batch 10108, Loss: 184.28961181640625\n",
      "Epoch 62, Batch 10109, Loss: 164.13084411621094\n",
      "Epoch 62, Batch 10110, Loss: 172.84120178222656\n",
      "Epoch 62, Batch 10111, Loss: 171.46762084960938\n",
      "Epoch 62, Batch 10112, Loss: 186.3587646484375\n",
      "Epoch 62, Batch 10113, Loss: 164.83615112304688\n",
      "Epoch 62, Batch 10114, Loss: 161.92684936523438\n",
      "Epoch 62, Batch 10115, Loss: 163.1949005126953\n",
      "Epoch 62, Batch 10116, Loss: 154.92703247070312\n",
      "Epoch 62, Batch 10117, Loss: 166.83920288085938\n",
      "Epoch 62, Batch 10118, Loss: 173.65301513671875\n",
      "Epoch 62, Batch 10119, Loss: 158.84376525878906\n",
      "Epoch 62, Batch 10120, Loss: 172.01368713378906\n",
      "Epoch 62, Batch 10121, Loss: 174.69033813476562\n",
      "Epoch 62, Batch 10122, Loss: 152.1798095703125\n",
      "Epoch 62, Batch 10123, Loss: 158.74444580078125\n",
      "Epoch 62, Batch 10124, Loss: 173.6247100830078\n",
      "Epoch 62, Batch 10125, Loss: 169.96905517578125\n",
      "Epoch 62, Batch 10126, Loss: 174.60430908203125\n",
      "Epoch 62, Batch 10127, Loss: 163.3512420654297\n",
      "Epoch 62, Batch 10128, Loss: 175.17079162597656\n",
      "Epoch 62, Batch 10129, Loss: 175.84144592285156\n",
      "Epoch 62, Batch 10130, Loss: 182.3363037109375\n",
      "Epoch 62, Batch 10131, Loss: 175.5225830078125\n",
      "Epoch 62, Batch 10132, Loss: 174.6864471435547\n",
      "Epoch 62, Batch 10133, Loss: 176.98268127441406\n",
      "Epoch 62, Batch 10134, Loss: 169.06541442871094\n",
      "Epoch 62, Batch 10135, Loss: 184.56581115722656\n",
      "Epoch 62, Batch 10136, Loss: 162.41233825683594\n",
      "Epoch 62, Batch 10137, Loss: 176.8154296875\n",
      "Epoch 62, Batch 10138, Loss: 164.3729705810547\n",
      "Epoch 62, Batch 10139, Loss: 179.08763122558594\n",
      "Epoch 62, Batch 10140, Loss: 171.05824279785156\n",
      "Epoch 62, Batch 10141, Loss: 159.3175048828125\n",
      "Epoch 62, Batch 10142, Loss: 182.4970245361328\n",
      "Epoch 62, Batch 10143, Loss: 185.7166290283203\n",
      "Epoch 62, Batch 10144, Loss: 157.63670349121094\n",
      "Epoch 62, Batch 10145, Loss: 171.14735412597656\n",
      "Epoch 62, Batch 10146, Loss: 170.8201446533203\n",
      "Epoch 62, Batch 10147, Loss: 175.23472595214844\n",
      "Epoch 62, Batch 10148, Loss: 148.1378936767578\n",
      "Epoch 62, Batch 10149, Loss: 171.871337890625\n",
      "Epoch 62, Batch 10150, Loss: 162.14129638671875\n",
      "Epoch 62, Batch 10151, Loss: 176.95587158203125\n",
      "Epoch 62, Batch 10152, Loss: 179.4501190185547\n",
      "Epoch 62, Batch 10153, Loss: 151.9801025390625\n",
      "Epoch 62, Batch 10154, Loss: 169.4901123046875\n",
      "Epoch 62, Batch 10155, Loss: 181.98941040039062\n",
      "Epoch 62, Batch 10156, Loss: 163.61083984375\n",
      "Epoch 62, Batch 10157, Loss: 171.8164520263672\n",
      "Epoch 62, Batch 10158, Loss: 164.399169921875\n",
      "Epoch 62, Batch 10159, Loss: 167.2579803466797\n",
      "Epoch 62, Batch 10160, Loss: 173.20797729492188\n",
      "Epoch 62, Batch 10161, Loss: 166.98529052734375\n",
      "Epoch 62, Batch 10162, Loss: 179.98048400878906\n",
      "Epoch 62, Batch 10163, Loss: 161.58917236328125\n",
      "Epoch 62, Batch 10164, Loss: 171.75927734375\n",
      "Epoch 62, Batch 10165, Loss: 188.15969848632812\n",
      "Epoch 62, Batch 10166, Loss: 169.00030517578125\n",
      "Epoch 62, Batch 10167, Loss: 165.1410675048828\n",
      "Epoch 62, Batch 10168, Loss: 181.9979705810547\n",
      "Epoch 62, Batch 10169, Loss: 170.53956604003906\n",
      "Epoch 62, Batch 10170, Loss: 165.54478454589844\n",
      "Epoch 62, Batch 10171, Loss: 173.9271697998047\n",
      "Epoch 62, Batch 10172, Loss: 184.04779052734375\n",
      "Epoch 62, Batch 10173, Loss: 148.391357421875\n",
      "Epoch 62, Batch 10174, Loss: 168.41934204101562\n",
      "Epoch 62, Batch 10175, Loss: 182.18994140625\n",
      "Epoch 62, Batch 10176, Loss: 186.0287628173828\n",
      "Epoch 62, Batch 10177, Loss: 193.4703826904297\n",
      "Epoch 62, Batch 10178, Loss: 187.42518615722656\n",
      "Epoch 62, Batch 10179, Loss: 190.70213317871094\n",
      "Epoch 62, Batch 10180, Loss: 175.09429931640625\n",
      "Epoch 62, Batch 10181, Loss: 170.11776733398438\n",
      "Epoch 62, Batch 10182, Loss: 167.9072265625\n",
      "Epoch 62, Batch 10183, Loss: 162.89781188964844\n",
      "Epoch 62, Batch 10184, Loss: 171.77569580078125\n",
      "Epoch 62, Batch 10185, Loss: 162.87648010253906\n",
      "Epoch 62, Batch 10186, Loss: 172.83645629882812\n",
      "Epoch 62, Batch 10187, Loss: 162.4990234375\n",
      "Epoch 62, Batch 10188, Loss: 173.22972106933594\n",
      "Epoch 62, Batch 10189, Loss: 173.46824645996094\n",
      "Epoch 62, Batch 10190, Loss: 175.5813446044922\n",
      "Epoch 62, Batch 10191, Loss: 169.9899139404297\n",
      "Epoch 62, Batch 10192, Loss: 159.62696838378906\n",
      "Epoch 62, Batch 10193, Loss: 178.33494567871094\n",
      "Epoch 62, Batch 10194, Loss: 175.166015625\n",
      "Epoch 62, Batch 10195, Loss: 179.19393920898438\n",
      "Epoch 62, Batch 10196, Loss: 163.84634399414062\n",
      "Epoch 62, Batch 10197, Loss: 175.08050537109375\n",
      "Epoch 62, Batch 10198, Loss: 165.36724853515625\n",
      "Epoch 62, Batch 10199, Loss: 176.92958068847656\n",
      "Epoch 62, Batch 10200, Loss: 146.84201049804688\n",
      "Epoch 62, Batch 10201, Loss: 173.7642059326172\n",
      "Epoch 62, Batch 10202, Loss: 171.20187377929688\n",
      "Epoch 62, Batch 10203, Loss: 184.5961151123047\n",
      "Epoch 62, Batch 10204, Loss: 172.24464416503906\n",
      "Epoch 62, Batch 10205, Loss: 174.05096435546875\n",
      "Epoch 62, Batch 10206, Loss: 166.09060668945312\n",
      "Epoch 62, Batch 10207, Loss: 203.89012145996094\n",
      "Epoch 62, Batch 10208, Loss: 180.6068115234375\n",
      "Epoch 62, Batch 10209, Loss: 186.2904510498047\n",
      "Epoch 62, Batch 10210, Loss: 158.6632080078125\n",
      "Epoch 62, Batch 10211, Loss: 189.25234985351562\n",
      "Epoch 62, Batch 10212, Loss: 186.8336639404297\n",
      "Epoch 62, Batch 10213, Loss: 169.01841735839844\n",
      "Epoch 62, Batch 10214, Loss: 167.44390869140625\n",
      "Epoch 62, Batch 10215, Loss: 156.0601348876953\n",
      "Epoch 62, Batch 10216, Loss: 174.3301544189453\n",
      "Epoch 62, Batch 10217, Loss: 164.88992309570312\n",
      "Epoch 62, Batch 10218, Loss: 173.00270080566406\n",
      "Epoch 62, Batch 10219, Loss: 162.96408081054688\n",
      "Epoch 62, Batch 10220, Loss: 170.87257385253906\n",
      "Epoch 62, Batch 10221, Loss: 166.1897430419922\n",
      "Epoch 62, Batch 10222, Loss: 156.65988159179688\n",
      "Epoch 62, Batch 10223, Loss: 144.95851135253906\n",
      "Epoch 62, Batch 10224, Loss: 180.07391357421875\n",
      "Epoch 62, Batch 10225, Loss: 166.61537170410156\n",
      "Epoch 62, Batch 10226, Loss: 184.1686248779297\n",
      "Epoch 62, Batch 10227, Loss: 168.76373291015625\n",
      "Epoch 62, Batch 10228, Loss: 173.84764099121094\n",
      "Epoch 62, Batch 10229, Loss: 172.12393188476562\n",
      "Epoch 62, Batch 10230, Loss: 169.48411560058594\n",
      "Epoch 62, Batch 10231, Loss: 186.25909423828125\n",
      "Epoch 62, Batch 10232, Loss: 177.85926818847656\n",
      "Epoch 62, Batch 10233, Loss: 169.3937225341797\n",
      "Epoch 62, Batch 10234, Loss: 171.05751037597656\n",
      "Epoch 62, Batch 10235, Loss: 177.50132751464844\n",
      "Epoch 62, Batch 10236, Loss: 171.2952117919922\n",
      "Epoch 62, Batch 10237, Loss: 175.14797973632812\n",
      "Epoch 62, Batch 10238, Loss: 177.9370574951172\n",
      "Epoch 62, Batch 10239, Loss: 173.08506774902344\n",
      "Epoch 62, Batch 10240, Loss: 186.75833129882812\n",
      "Epoch 62, Batch 10241, Loss: 172.27120971679688\n",
      "Epoch 62, Batch 10242, Loss: 176.2854766845703\n",
      "Epoch 62, Batch 10243, Loss: 179.03929138183594\n",
      "Epoch 62, Batch 10244, Loss: 163.24636840820312\n",
      "Epoch 62, Batch 10245, Loss: 185.1428680419922\n",
      "Epoch 62, Batch 10246, Loss: 190.6513671875\n",
      "Epoch 62, Batch 10247, Loss: 178.3072052001953\n",
      "Epoch 62, Batch 10248, Loss: 162.03463745117188\n",
      "Epoch 62, Batch 10249, Loss: 170.73495483398438\n",
      "Epoch 62, Batch 10250, Loss: 180.96734619140625\n",
      "Epoch 62, Batch 10251, Loss: 148.57577514648438\n",
      "Epoch 62, Batch 10252, Loss: 185.06503295898438\n",
      "Epoch 62, Batch 10253, Loss: 166.2101593017578\n",
      "Epoch 62, Batch 10254, Loss: 167.89227294921875\n",
      "Epoch 62, Batch 10255, Loss: 172.9224853515625\n",
      "Epoch 62, Batch 10256, Loss: 180.8909912109375\n",
      "Epoch 62, Batch 10257, Loss: 175.9073028564453\n",
      "Epoch 62, Batch 10258, Loss: 211.69097900390625\n",
      "Epoch 62, Batch 10259, Loss: 170.17715454101562\n",
      "Epoch 62, Batch 10260, Loss: 174.21067810058594\n",
      "Epoch 62, Batch 10261, Loss: 166.507080078125\n",
      "Epoch 62, Batch 10262, Loss: 171.44248962402344\n",
      "Epoch 62, Batch 10263, Loss: 163.39125061035156\n",
      "Epoch 62, Batch 10264, Loss: 169.99398803710938\n",
      "Epoch 62, Batch 10265, Loss: 175.10379028320312\n",
      "Epoch 62, Batch 10266, Loss: 169.6011199951172\n",
      "Epoch 62, Batch 10267, Loss: 171.45361328125\n",
      "Epoch 62, Batch 10268, Loss: 174.60464477539062\n",
      "Epoch 62, Batch 10269, Loss: 183.52081298828125\n",
      "Epoch 62, Batch 10270, Loss: 161.53280639648438\n",
      "Epoch 62, Batch 10271, Loss: 183.98802185058594\n",
      "Epoch 62, Batch 10272, Loss: 186.1916961669922\n",
      "Epoch 62, Batch 10273, Loss: 166.5863037109375\n",
      "Epoch 62, Batch 10274, Loss: 172.33676147460938\n",
      "Epoch 62, Batch 10275, Loss: 151.3176727294922\n",
      "Epoch 62, Batch 10276, Loss: 193.39569091796875\n",
      "Epoch 62, Batch 10277, Loss: 182.0352783203125\n",
      "Epoch 62, Batch 10278, Loss: 164.4960174560547\n",
      "Epoch 62, Batch 10279, Loss: 174.10009765625\n",
      "Epoch 62, Batch 10280, Loss: 169.04071044921875\n",
      "Epoch 62, Batch 10281, Loss: 168.3746795654297\n",
      "Epoch 62, Batch 10282, Loss: 194.9902801513672\n",
      "Epoch 62, Batch 10283, Loss: 177.9663543701172\n",
      "Epoch 62, Batch 10284, Loss: 165.3052978515625\n",
      "Epoch 62, Batch 10285, Loss: 175.524658203125\n",
      "Epoch 62, Batch 10286, Loss: 183.61158752441406\n",
      "Epoch 62, Batch 10287, Loss: 188.95094299316406\n",
      "Epoch 62, Batch 10288, Loss: 174.4500732421875\n",
      "Epoch 62, Batch 10289, Loss: 169.05335998535156\n",
      "Epoch 62, Batch 10290, Loss: 161.88673400878906\n",
      "Epoch 62, Batch 10291, Loss: 170.68592834472656\n",
      "Epoch 62, Batch 10292, Loss: 171.46543884277344\n",
      "Epoch 62, Batch 10293, Loss: 169.92857360839844\n",
      "Epoch 62, Batch 10294, Loss: 187.69915771484375\n",
      "Epoch 62, Batch 10295, Loss: 168.8673095703125\n",
      "Epoch 62, Batch 10296, Loss: 160.9583282470703\n",
      "Epoch 62, Batch 10297, Loss: 161.96484375\n",
      "Epoch 62, Batch 10298, Loss: 177.40061950683594\n",
      "Epoch 62, Batch 10299, Loss: 162.9072723388672\n",
      "Epoch 62, Batch 10300, Loss: 177.97195434570312\n",
      "Epoch 62, Batch 10301, Loss: 185.32217407226562\n",
      "Epoch 62, Batch 10302, Loss: 171.93231201171875\n",
      "Epoch 62, Batch 10303, Loss: 180.19602966308594\n",
      "Epoch 62, Batch 10304, Loss: 175.65614318847656\n",
      "Epoch 62, Batch 10305, Loss: 172.266845703125\n",
      "Epoch 62, Batch 10306, Loss: 158.9521942138672\n",
      "Epoch 62, Batch 10307, Loss: 181.02525329589844\n",
      "Epoch 62, Batch 10308, Loss: 163.3684844970703\n",
      "Epoch 62, Batch 10309, Loss: 173.7265167236328\n",
      "Epoch 62, Batch 10310, Loss: 166.92384338378906\n",
      "Epoch 62, Batch 10311, Loss: 161.08273315429688\n",
      "Epoch 62, Batch 10312, Loss: 179.26617431640625\n",
      "Epoch 62, Batch 10313, Loss: 172.7220001220703\n",
      "Epoch 62, Batch 10314, Loss: 161.9090576171875\n",
      "Epoch 62, Batch 10315, Loss: 166.33258056640625\n",
      "Epoch 62, Batch 10316, Loss: 173.58033752441406\n",
      "Epoch 62, Batch 10317, Loss: 167.40982055664062\n",
      "Epoch 62, Batch 10318, Loss: 174.34518432617188\n",
      "Epoch 62, Batch 10319, Loss: 179.2624053955078\n",
      "Epoch 62, Batch 10320, Loss: 181.97662353515625\n",
      "Epoch 62, Batch 10321, Loss: 172.88780212402344\n",
      "Epoch 62, Batch 10322, Loss: 162.32171630859375\n",
      "Epoch 62, Batch 10323, Loss: 170.5048370361328\n",
      "Epoch 62, Batch 10324, Loss: 169.45458984375\n",
      "Epoch 62, Batch 10325, Loss: 170.5164031982422\n",
      "Epoch 62, Batch 10326, Loss: 174.3780975341797\n",
      "Epoch 62, Batch 10327, Loss: 199.3601837158203\n",
      "Epoch 62, Batch 10328, Loss: 177.81857299804688\n",
      "Epoch 62, Batch 10329, Loss: 171.2957000732422\n",
      "Epoch 62, Batch 10330, Loss: 181.56756591796875\n",
      "Epoch 62, Batch 10331, Loss: 169.91299438476562\n",
      "Epoch 62, Batch 10332, Loss: 181.69203186035156\n",
      "Epoch 62, Batch 10333, Loss: 178.29495239257812\n",
      "Epoch 62, Batch 10334, Loss: 165.1112823486328\n",
      "Epoch 62, Batch 10335, Loss: 180.884033203125\n",
      "Epoch 62, Batch 10336, Loss: 172.30050659179688\n",
      "Epoch 62, Batch 10337, Loss: 184.1599884033203\n",
      "Epoch 62, Batch 10338, Loss: 176.3879852294922\n",
      "Epoch 62, Batch 10339, Loss: 165.5649871826172\n",
      "Epoch 62, Batch 10340, Loss: 201.446533203125\n",
      "Epoch 62, Batch 10341, Loss: 173.5339813232422\n",
      "Epoch 62, Batch 10342, Loss: 167.35699462890625\n",
      "Epoch 62, Batch 10343, Loss: 156.76329040527344\n",
      "Epoch 62, Batch 10344, Loss: 174.65383911132812\n",
      "Epoch 62, Batch 10345, Loss: 167.58233642578125\n",
      "Epoch 62, Batch 10346, Loss: 177.8993377685547\n",
      "Epoch 62, Batch 10347, Loss: 167.75660705566406\n",
      "Epoch 62, Batch 10348, Loss: 165.8273162841797\n",
      "Epoch 62, Batch 10349, Loss: 196.5924530029297\n",
      "Epoch 62, Batch 10350, Loss: 175.0017547607422\n",
      "Epoch 62, Batch 10351, Loss: 172.30381774902344\n",
      "Epoch 62, Batch 10352, Loss: 167.06094360351562\n",
      "Epoch 62, Batch 10353, Loss: 176.25909423828125\n",
      "Epoch 62, Batch 10354, Loss: 173.15818786621094\n",
      "Epoch 62, Batch 10355, Loss: 168.26031494140625\n",
      "Epoch 62, Batch 10356, Loss: 177.49917602539062\n",
      "Epoch 62, Batch 10357, Loss: 157.50363159179688\n",
      "Epoch 62, Batch 10358, Loss: 169.12747192382812\n",
      "Epoch 62, Batch 10359, Loss: 156.55149841308594\n",
      "Epoch 62, Batch 10360, Loss: 161.84255981445312\n",
      "Epoch 62, Batch 10361, Loss: 165.39523315429688\n",
      "Epoch 62, Batch 10362, Loss: 148.86587524414062\n",
      "Epoch 62, Batch 10363, Loss: 150.83953857421875\n",
      "Epoch 62, Batch 10364, Loss: 169.11521911621094\n",
      "Epoch 62, Batch 10365, Loss: 157.52362060546875\n",
      "Epoch 62, Batch 10366, Loss: 184.36151123046875\n",
      "Epoch 62, Batch 10367, Loss: 163.812255859375\n",
      "Epoch 62, Batch 10368, Loss: 176.0809326171875\n",
      "Epoch 62, Batch 10369, Loss: 185.86964416503906\n",
      "Epoch 62, Batch 10370, Loss: 171.56088256835938\n",
      "Epoch 62, Batch 10371, Loss: 167.6393585205078\n",
      "Epoch 62, Batch 10372, Loss: 173.0877227783203\n",
      "Epoch 62, Batch 10373, Loss: 180.30397033691406\n",
      "Epoch 62, Batch 10374, Loss: 182.79139709472656\n",
      "Epoch 62, Batch 10375, Loss: 176.31137084960938\n",
      "Epoch 62, Batch 10376, Loss: 167.7614288330078\n",
      "Epoch 62, Batch 10377, Loss: 182.10140991210938\n",
      "Epoch 62, Batch 10378, Loss: 161.63230895996094\n",
      "Epoch 62, Batch 10379, Loss: 177.79153442382812\n",
      "Epoch 62, Batch 10380, Loss: 187.60206604003906\n",
      "Epoch 62, Batch 10381, Loss: 175.9083251953125\n",
      "Epoch 62, Batch 10382, Loss: 180.13880920410156\n",
      "Epoch 62, Batch 10383, Loss: 190.2134246826172\n",
      "Epoch 62, Batch 10384, Loss: 161.419677734375\n",
      "Epoch 62, Batch 10385, Loss: 164.86831665039062\n",
      "Epoch 62, Batch 10386, Loss: 178.9770965576172\n",
      "Epoch 62, Batch 10387, Loss: 161.74948120117188\n",
      "Epoch 62, Batch 10388, Loss: 177.29116821289062\n",
      "Epoch 62, Batch 10389, Loss: 155.2584686279297\n",
      "Epoch 62, Batch 10390, Loss: 173.08090209960938\n",
      "Epoch 62, Batch 10391, Loss: 162.03619384765625\n",
      "Epoch 62, Batch 10392, Loss: 173.9158935546875\n",
      "Epoch 62, Batch 10393, Loss: 168.7283935546875\n",
      "Epoch 62, Batch 10394, Loss: 182.75979614257812\n",
      "Epoch 62, Batch 10395, Loss: 178.41845703125\n",
      "Epoch 62, Batch 10396, Loss: 170.74295043945312\n",
      "Epoch 62, Batch 10397, Loss: 174.85955810546875\n",
      "Epoch 62, Batch 10398, Loss: 157.08067321777344\n",
      "Epoch 62, Batch 10399, Loss: 169.01519775390625\n",
      "Epoch 62, Batch 10400, Loss: 177.5881805419922\n",
      "Epoch 62, Batch 10401, Loss: 183.0111846923828\n",
      "Epoch 62, Batch 10402, Loss: 165.65895080566406\n",
      "Epoch 62, Batch 10403, Loss: 182.21136474609375\n",
      "Epoch 62, Batch 10404, Loss: 175.12295532226562\n",
      "Epoch 62, Batch 10405, Loss: 167.73233032226562\n",
      "Epoch 62, Batch 10406, Loss: 182.10765075683594\n",
      "Epoch 62, Batch 10407, Loss: 162.26663208007812\n",
      "Epoch 62, Batch 10408, Loss: 174.92234802246094\n",
      "Epoch 62, Batch 10409, Loss: 179.05979919433594\n",
      "Epoch 62, Batch 10410, Loss: 168.35032653808594\n",
      "Epoch 62, Batch 10411, Loss: 160.14967346191406\n",
      "Epoch 62, Batch 10412, Loss: 171.60430908203125\n",
      "Epoch 62, Batch 10413, Loss: 154.16964721679688\n",
      "Epoch 62, Batch 10414, Loss: 165.4644317626953\n",
      "Epoch 62, Batch 10415, Loss: 192.65719604492188\n",
      "Epoch 62, Batch 10416, Loss: 162.6005096435547\n",
      "Epoch 62, Batch 10417, Loss: 167.75572204589844\n",
      "Epoch 62, Batch 10418, Loss: 186.10690307617188\n",
      "Epoch 62, Batch 10419, Loss: 179.79295349121094\n",
      "Epoch 62, Batch 10420, Loss: 170.7592010498047\n",
      "Epoch 62, Batch 10421, Loss: 166.74851989746094\n",
      "Epoch 62, Batch 10422, Loss: 174.000732421875\n",
      "Epoch 62, Batch 10423, Loss: 164.67947387695312\n",
      "Epoch 62, Batch 10424, Loss: 152.5880126953125\n",
      "Epoch 62, Batch 10425, Loss: 169.5447540283203\n",
      "Epoch 62, Batch 10426, Loss: 180.51632690429688\n",
      "Epoch 62, Batch 10427, Loss: 172.9153594970703\n",
      "Epoch 62, Batch 10428, Loss: 190.5740509033203\n",
      "Epoch 62, Batch 10429, Loss: 171.0833282470703\n",
      "Epoch 62, Batch 10430, Loss: 170.91961669921875\n",
      "Epoch 62, Batch 10431, Loss: 174.05319213867188\n",
      "Epoch 62, Batch 10432, Loss: 175.8151092529297\n",
      "Epoch 62, Batch 10433, Loss: 167.66783142089844\n",
      "Epoch 62, Batch 10434, Loss: 174.45370483398438\n",
      "Epoch 62, Batch 10435, Loss: 164.81690979003906\n",
      "Epoch 62, Batch 10436, Loss: 158.3916778564453\n",
      "Epoch 62, Batch 10437, Loss: 185.08143615722656\n",
      "Epoch 62, Batch 10438, Loss: 182.10459899902344\n",
      "Epoch 62, Batch 10439, Loss: 180.1925048828125\n",
      "Epoch 62, Batch 10440, Loss: 166.09181213378906\n",
      "Epoch 62, Batch 10441, Loss: 168.3037567138672\n",
      "Epoch 62, Batch 10442, Loss: 176.19198608398438\n",
      "Epoch 62, Batch 10443, Loss: 181.9690399169922\n",
      "Epoch 62, Batch 10444, Loss: 177.25413513183594\n",
      "Epoch 62, Batch 10445, Loss: 177.88153076171875\n",
      "Epoch 62, Batch 10446, Loss: 168.2343292236328\n",
      "Epoch 62, Batch 10447, Loss: 168.8960723876953\n",
      "Epoch 62, Batch 10448, Loss: 190.20355224609375\n",
      "Epoch 62, Batch 10449, Loss: 172.00587463378906\n",
      "Epoch 62, Batch 10450, Loss: 182.6371307373047\n",
      "Epoch 62, Batch 10451, Loss: 169.8787078857422\n",
      "Epoch 62, Batch 10452, Loss: 182.70657348632812\n",
      "Epoch 62, Batch 10453, Loss: 181.91212463378906\n",
      "Epoch 62, Batch 10454, Loss: 163.60130310058594\n",
      "Epoch 62, Batch 10455, Loss: 158.65052795410156\n",
      "Epoch 62, Batch 10456, Loss: 163.9069366455078\n",
      "Epoch 62, Batch 10457, Loss: 177.48643493652344\n",
      "Epoch 62, Batch 10458, Loss: 182.12136840820312\n",
      "Epoch 62, Batch 10459, Loss: 171.21075439453125\n",
      "Epoch 62, Batch 10460, Loss: 168.85816955566406\n",
      "Epoch 62, Batch 10461, Loss: 172.8494873046875\n",
      "Epoch 62, Batch 10462, Loss: 167.1139678955078\n",
      "Epoch 62, Batch 10463, Loss: 158.6414794921875\n",
      "Epoch 62, Batch 10464, Loss: 163.14566040039062\n",
      "Epoch 62, Batch 10465, Loss: 179.49569702148438\n",
      "Epoch 62, Batch 10466, Loss: 179.96438598632812\n",
      "Epoch 62, Batch 10467, Loss: 155.23439025878906\n",
      "Epoch 62, Batch 10468, Loss: 180.97921752929688\n",
      "Epoch 62, Batch 10469, Loss: 180.70791625976562\n",
      "Epoch 62, Batch 10470, Loss: 182.31065368652344\n",
      "Epoch 62, Batch 10471, Loss: 160.2027587890625\n",
      "Epoch 62, Batch 10472, Loss: 153.53799438476562\n",
      "Epoch 62, Batch 10473, Loss: 164.5769500732422\n",
      "Epoch 62, Batch 10474, Loss: 187.36032104492188\n",
      "Epoch 62, Batch 10475, Loss: 172.48243713378906\n",
      "Epoch 62, Batch 10476, Loss: 160.9639434814453\n",
      "Epoch 62, Batch 10477, Loss: 173.9640655517578\n",
      "Epoch 62, Batch 10478, Loss: 192.50103759765625\n",
      "Epoch 62, Batch 10479, Loss: 176.18753051757812\n",
      "Epoch 62, Batch 10480, Loss: 170.03016662597656\n",
      "Epoch 62, Batch 10481, Loss: 163.56527709960938\n",
      "Epoch 62, Batch 10482, Loss: 162.09722900390625\n",
      "Epoch 62, Batch 10483, Loss: 172.1595916748047\n",
      "Epoch 62, Batch 10484, Loss: 191.8030548095703\n",
      "Epoch 62, Batch 10485, Loss: 164.56204223632812\n",
      "Epoch 62, Batch 10486, Loss: 168.3866424560547\n",
      "Epoch 62, Batch 10487, Loss: 172.44705200195312\n",
      "Epoch 62, Batch 10488, Loss: 173.5356903076172\n",
      "Epoch 62, Batch 10489, Loss: 180.29310607910156\n",
      "Epoch 62, Batch 10490, Loss: 178.17164611816406\n",
      "Epoch 62, Batch 10491, Loss: 154.82456970214844\n",
      "Epoch 62, Batch 10492, Loss: 175.7639617919922\n",
      "Epoch 62, Batch 10493, Loss: 163.1944580078125\n",
      "Epoch 62, Batch 10494, Loss: 159.4332275390625\n",
      "Epoch 62, Batch 10495, Loss: 186.64369201660156\n",
      "Epoch 62, Batch 10496, Loss: 175.19729614257812\n",
      "Epoch 62, Batch 10497, Loss: 174.74766540527344\n",
      "Epoch 62, Batch 10498, Loss: 168.93222045898438\n",
      "Epoch 62, Batch 10499, Loss: 167.01678466796875\n",
      "Epoch 62, Batch 10500, Loss: 167.6750946044922\n",
      "Epoch 62, Batch 10501, Loss: 167.75733947753906\n",
      "Epoch 62, Batch 10502, Loss: 184.68882751464844\n",
      "Epoch 62, Batch 10503, Loss: 187.79061889648438\n",
      "Epoch 62, Batch 10504, Loss: 159.50665283203125\n",
      "Epoch 62, Batch 10505, Loss: 174.19778442382812\n",
      "Epoch 62, Batch 10506, Loss: 168.7845916748047\n",
      "Epoch 62, Batch 10507, Loss: 155.4572296142578\n",
      "Epoch 62, Batch 10508, Loss: 157.09002685546875\n",
      "Epoch 62, Batch 10509, Loss: 187.4079132080078\n",
      "Epoch 62, Batch 10510, Loss: 172.36013793945312\n",
      "Epoch 62, Batch 10511, Loss: 174.33322143554688\n",
      "Epoch 62, Batch 10512, Loss: 182.8417510986328\n",
      "Epoch 62, Batch 10513, Loss: 163.6814727783203\n",
      "Epoch 62, Batch 10514, Loss: 170.7133026123047\n",
      "Epoch 62, Batch 10515, Loss: 164.4220733642578\n",
      "Epoch 62, Batch 10516, Loss: 179.3720245361328\n",
      "Epoch 62, Batch 10517, Loss: 167.93927001953125\n",
      "Epoch 62, Batch 10518, Loss: 165.61952209472656\n",
      "Epoch 62, Batch 10519, Loss: 162.2647247314453\n",
      "Epoch 62, Batch 10520, Loss: 173.45655822753906\n",
      "Epoch 62, Batch 10521, Loss: 165.35025024414062\n",
      "Epoch 62, Batch 10522, Loss: 169.2236785888672\n",
      "Epoch 62, Batch 10523, Loss: 177.03909301757812\n",
      "Epoch 62, Batch 10524, Loss: 165.47264099121094\n",
      "Epoch 62, Batch 10525, Loss: 176.94847106933594\n",
      "Epoch 62, Batch 10526, Loss: 180.6204833984375\n",
      "Epoch 62, Batch 10527, Loss: 171.51150512695312\n",
      "Epoch 62, Batch 10528, Loss: 173.23110961914062\n",
      "Epoch 62, Batch 10529, Loss: 199.44822692871094\n",
      "Epoch 62, Batch 10530, Loss: 167.32740783691406\n",
      "Epoch 62, Batch 10531, Loss: 177.128173828125\n",
      "Epoch 62, Batch 10532, Loss: 167.73109436035156\n",
      "Epoch 62, Batch 10533, Loss: 164.7495574951172\n",
      "Epoch 62, Batch 10534, Loss: 159.90640258789062\n",
      "Epoch 62, Batch 10535, Loss: 178.3997344970703\n",
      "Epoch 62, Batch 10536, Loss: 162.36102294921875\n",
      "Epoch 62, Batch 10537, Loss: 177.36019897460938\n",
      "Epoch 62, Batch 10538, Loss: 154.70054626464844\n",
      "Epoch 62, Batch 10539, Loss: 180.6437225341797\n",
      "Epoch 62, Batch 10540, Loss: 175.9498748779297\n",
      "Epoch 62, Batch 10541, Loss: 176.370361328125\n",
      "Epoch 62, Batch 10542, Loss: 176.0828857421875\n",
      "Epoch 62, Batch 10543, Loss: 166.13150024414062\n",
      "Epoch 62, Batch 10544, Loss: 174.09091186523438\n",
      "Epoch 62, Batch 10545, Loss: 167.53329467773438\n",
      "Epoch 62, Batch 10546, Loss: 180.5082550048828\n",
      "Epoch 62, Batch 10547, Loss: 171.80738830566406\n",
      "Epoch 62, Batch 10548, Loss: 180.99337768554688\n",
      "Epoch 62, Batch 10549, Loss: 180.84146118164062\n",
      "Epoch 62, Batch 10550, Loss: 174.9189910888672\n",
      "Epoch 62, Batch 10551, Loss: 164.0758056640625\n",
      "Epoch 62, Batch 10552, Loss: 172.34791564941406\n",
      "Epoch 62, Batch 10553, Loss: 162.29983520507812\n",
      "Epoch 62, Batch 10554, Loss: 164.1702880859375\n",
      "Epoch 62, Batch 10555, Loss: 157.43310546875\n",
      "Epoch 62, Batch 10556, Loss: 169.83355712890625\n",
      "Epoch 62, Batch 10557, Loss: 173.7502899169922\n",
      "Epoch 62, Batch 10558, Loss: 171.54916381835938\n",
      "Epoch 62, Batch 10559, Loss: 177.51133728027344\n",
      "Epoch 62, Batch 10560, Loss: 165.59095764160156\n",
      "Epoch 62, Batch 10561, Loss: 170.25352478027344\n",
      "Epoch 62, Batch 10562, Loss: 181.7452392578125\n",
      "Epoch 62, Batch 10563, Loss: 166.0299835205078\n",
      "Epoch 62, Batch 10564, Loss: 187.38595581054688\n",
      "Epoch 62, Batch 10565, Loss: 177.4809112548828\n",
      "Epoch 62, Batch 10566, Loss: 174.63340759277344\n",
      "Epoch 62, Batch 10567, Loss: 175.59231567382812\n",
      "Epoch 62, Batch 10568, Loss: 185.500732421875\n",
      "Epoch 62, Batch 10569, Loss: 182.6574249267578\n",
      "Epoch 62, Batch 10570, Loss: 175.173828125\n",
      "Epoch 62, Batch 10571, Loss: 174.4503173828125\n",
      "Epoch 62, Batch 10572, Loss: 161.68138122558594\n",
      "Epoch 62, Batch 10573, Loss: 180.24594116210938\n",
      "Epoch 62, Batch 10574, Loss: 173.6858673095703\n",
      "Epoch 62, Batch 10575, Loss: 157.82882690429688\n",
      "Epoch 62, Batch 10576, Loss: 140.30105590820312\n",
      "Epoch 62, Batch 10577, Loss: 165.86244201660156\n",
      "Epoch 62, Batch 10578, Loss: 177.2117919921875\n",
      "Epoch 62, Batch 10579, Loss: 160.90707397460938\n",
      "Epoch 62, Batch 10580, Loss: 166.192626953125\n",
      "Epoch 62, Batch 10581, Loss: 178.81768798828125\n",
      "Epoch 62, Batch 10582, Loss: 181.84317016601562\n",
      "Epoch 62, Batch 10583, Loss: 169.0535125732422\n",
      "Epoch 62, Batch 10584, Loss: 156.78797912597656\n",
      "Epoch 62, Batch 10585, Loss: 171.1632080078125\n",
      "Epoch 62, Batch 10586, Loss: 174.771484375\n",
      "Epoch 62, Batch 10587, Loss: 179.6102752685547\n",
      "Epoch 62, Batch 10588, Loss: 172.6020965576172\n",
      "Epoch 62, Batch 10589, Loss: 182.25350952148438\n",
      "Epoch 62, Batch 10590, Loss: 172.1634521484375\n",
      "Epoch 62, Batch 10591, Loss: 162.5960693359375\n",
      "Epoch 62, Batch 10592, Loss: 180.23036193847656\n",
      "Epoch 62, Batch 10593, Loss: 185.37017822265625\n",
      "Epoch 62, Batch 10594, Loss: 189.49595642089844\n",
      "Epoch 62, Batch 10595, Loss: 182.884033203125\n",
      "Epoch 62, Batch 10596, Loss: 185.21975708007812\n",
      "Epoch 62, Batch 10597, Loss: 175.46482849121094\n",
      "Epoch 62, Batch 10598, Loss: 177.10804748535156\n",
      "Epoch 62, Batch 10599, Loss: 185.04615783691406\n",
      "Epoch 62, Batch 10600, Loss: 172.85336303710938\n",
      "Epoch 62, Batch 10601, Loss: 167.230712890625\n",
      "Epoch 62, Batch 10602, Loss: 182.0752716064453\n",
      "Epoch 62, Batch 10603, Loss: 144.3478546142578\n",
      "Epoch 62, Batch 10604, Loss: 177.26258850097656\n",
      "Epoch 62, Batch 10605, Loss: 184.1835479736328\n",
      "Epoch 62, Batch 10606, Loss: 176.53736877441406\n",
      "Epoch 62, Batch 10607, Loss: 170.1915283203125\n",
      "Epoch 62, Batch 10608, Loss: 161.60647583007812\n",
      "Epoch 62, Batch 10609, Loss: 183.47328186035156\n",
      "Epoch 62, Batch 10610, Loss: 171.96609497070312\n",
      "Epoch 62, Batch 10611, Loss: 166.66244506835938\n",
      "Epoch 62, Batch 10612, Loss: 156.12847900390625\n",
      "Epoch 62, Batch 10613, Loss: 163.2759552001953\n",
      "Epoch 62, Batch 10614, Loss: 167.40213012695312\n",
      "Epoch 62, Batch 10615, Loss: 181.0452880859375\n",
      "Epoch 62, Batch 10616, Loss: 180.9523162841797\n",
      "Epoch 62, Batch 10617, Loss: 162.2357635498047\n",
      "Epoch 62, Batch 10618, Loss: 147.8776092529297\n",
      "Epoch 62, Batch 10619, Loss: 168.96377563476562\n",
      "Epoch 62, Batch 10620, Loss: 173.06182861328125\n",
      "Epoch 62, Batch 10621, Loss: 164.1338348388672\n",
      "Epoch 62, Batch 10622, Loss: 164.00613403320312\n",
      "Epoch 62, Batch 10623, Loss: 189.21401977539062\n",
      "Epoch 62, Batch 10624, Loss: 193.77325439453125\n",
      "Epoch 62, Batch 10625, Loss: 178.56410217285156\n",
      "Epoch 62, Batch 10626, Loss: 181.29931640625\n",
      "Epoch 62, Batch 10627, Loss: 165.69606018066406\n",
      "Epoch 62, Batch 10628, Loss: 157.42642211914062\n",
      "Epoch 62, Batch 10629, Loss: 194.88330078125\n",
      "Epoch 62, Batch 10630, Loss: 191.0062255859375\n",
      "Epoch 62, Batch 10631, Loss: 176.09371948242188\n",
      "Epoch 62, Batch 10632, Loss: 174.4081268310547\n",
      "Epoch 62, Batch 10633, Loss: 165.0253143310547\n",
      "Epoch 62, Batch 10634, Loss: 170.8289794921875\n",
      "Epoch 62, Batch 10635, Loss: 165.07896423339844\n",
      "Epoch 62, Batch 10636, Loss: 180.57131958007812\n",
      "Epoch 62, Batch 10637, Loss: 186.95498657226562\n",
      "Epoch 62, Batch 10638, Loss: 158.50515747070312\n",
      "Epoch 62, Batch 10639, Loss: 148.50555419921875\n",
      "Epoch 62, Batch 10640, Loss: 189.57627868652344\n",
      "Epoch 62, Batch 10641, Loss: 172.68714904785156\n",
      "Epoch 62, Batch 10642, Loss: 165.31451416015625\n",
      "Epoch 62, Batch 10643, Loss: 182.7905731201172\n",
      "Epoch 62, Batch 10644, Loss: 174.18482971191406\n",
      "Epoch 62, Batch 10645, Loss: 180.65855407714844\n",
      "Epoch 62, Batch 10646, Loss: 169.880859375\n",
      "Epoch 62, Batch 10647, Loss: 162.01373291015625\n",
      "Epoch 62, Batch 10648, Loss: 165.6919403076172\n",
      "Epoch 62, Batch 10649, Loss: 161.91172790527344\n",
      "Epoch 62, Batch 10650, Loss: 166.44493103027344\n",
      "Epoch 62, Batch 10651, Loss: 179.39483642578125\n",
      "Epoch 62, Batch 10652, Loss: 162.86875915527344\n",
      "Epoch 62, Batch 10653, Loss: 183.1392059326172\n",
      "Epoch 62, Batch 10654, Loss: 177.96630859375\n",
      "Epoch 62, Batch 10655, Loss: 173.96127319335938\n",
      "Epoch 62, Batch 10656, Loss: 173.75025939941406\n",
      "Epoch 62, Batch 10657, Loss: 162.6582794189453\n",
      "Epoch 62, Batch 10658, Loss: 160.1535186767578\n",
      "Epoch 62, Batch 10659, Loss: 178.33213806152344\n",
      "Epoch 62, Batch 10660, Loss: 173.31101989746094\n",
      "Epoch 62, Batch 10661, Loss: 178.10162353515625\n",
      "Epoch 62, Batch 10662, Loss: 188.5275421142578\n",
      "Epoch 62, Batch 10663, Loss: 160.758056640625\n",
      "Epoch 62, Batch 10664, Loss: 175.13014221191406\n",
      "Epoch 62, Batch 10665, Loss: 163.72108459472656\n",
      "Epoch 62, Batch 10666, Loss: 189.67922973632812\n",
      "Epoch 62, Batch 10667, Loss: 171.42984008789062\n",
      "Epoch 62, Batch 10668, Loss: 162.98666381835938\n",
      "Epoch 62, Batch 10669, Loss: 159.33908081054688\n",
      "Epoch 62, Batch 10670, Loss: 168.57647705078125\n",
      "Epoch 62, Batch 10671, Loss: 165.7505645751953\n",
      "Epoch 62, Batch 10672, Loss: 164.70401000976562\n",
      "Epoch 62, Batch 10673, Loss: 183.16737365722656\n",
      "Epoch 62, Batch 10674, Loss: 160.02833557128906\n",
      "Epoch 62, Batch 10675, Loss: 173.44769287109375\n",
      "Epoch 62, Batch 10676, Loss: 160.56582641601562\n",
      "Epoch 62, Batch 10677, Loss: 192.636962890625\n",
      "Epoch 62, Batch 10678, Loss: 169.4396514892578\n",
      "Epoch 62, Batch 10679, Loss: 166.31309509277344\n",
      "Epoch 62, Batch 10680, Loss: 171.46656799316406\n",
      "Epoch 62, Batch 10681, Loss: 168.37411499023438\n",
      "Epoch 62, Batch 10682, Loss: 186.6334686279297\n",
      "Epoch 62, Batch 10683, Loss: 164.5345916748047\n",
      "Epoch 62, Batch 10684, Loss: 172.13751220703125\n",
      "Epoch 62, Batch 10685, Loss: 180.63101196289062\n",
      "Epoch 62, Batch 10686, Loss: 169.2406463623047\n",
      "Epoch 62, Batch 10687, Loss: 178.6822967529297\n",
      "Epoch 62, Batch 10688, Loss: 185.85269165039062\n",
      "Epoch 62, Batch 10689, Loss: 168.3092803955078\n",
      "Epoch 62, Batch 10690, Loss: 171.03726196289062\n",
      "Epoch 62, Batch 10691, Loss: 171.77606201171875\n",
      "Epoch 62, Batch 10692, Loss: 173.96139526367188\n",
      "Epoch 62, Batch 10693, Loss: 178.7556610107422\n",
      "Epoch 62, Batch 10694, Loss: 169.43714904785156\n",
      "Epoch 62, Batch 10695, Loss: 168.0701446533203\n",
      "Epoch 62, Batch 10696, Loss: 181.81468200683594\n",
      "Epoch 62, Batch 10697, Loss: 177.34310913085938\n",
      "Epoch 62, Batch 10698, Loss: 155.0358428955078\n",
      "Epoch 62, Batch 10699, Loss: 177.32583618164062\n",
      "Epoch 62, Batch 10700, Loss: 179.29493713378906\n",
      "Epoch 62, Batch 10701, Loss: 171.0007781982422\n",
      "Epoch 62, Batch 10702, Loss: 176.7991485595703\n",
      "Epoch 62, Batch 10703, Loss: 161.79588317871094\n",
      "Epoch 62, Batch 10704, Loss: 171.38880920410156\n",
      "Epoch 62, Batch 10705, Loss: 171.81561279296875\n",
      "Epoch 62, Batch 10706, Loss: 171.39796447753906\n",
      "Epoch 62, Batch 10707, Loss: 180.22315979003906\n",
      "Epoch 62, Batch 10708, Loss: 170.52684020996094\n",
      "Epoch 62, Batch 10709, Loss: 174.0125732421875\n",
      "Epoch 62, Batch 10710, Loss: 177.64883422851562\n",
      "Epoch 62, Batch 10711, Loss: 161.91891479492188\n",
      "Epoch 62, Batch 10712, Loss: 194.02011108398438\n",
      "Epoch 62, Batch 10713, Loss: 199.36019897460938\n",
      "Epoch 62, Batch 10714, Loss: 167.44041442871094\n",
      "Epoch 62, Batch 10715, Loss: 184.44659423828125\n",
      "Epoch 62, Batch 10716, Loss: 159.9984893798828\n",
      "Epoch 62, Batch 10717, Loss: 175.656494140625\n",
      "Epoch 62, Batch 10718, Loss: 180.6593475341797\n",
      "Epoch 62, Batch 10719, Loss: 182.494873046875\n",
      "Epoch 62, Batch 10720, Loss: 173.7603759765625\n",
      "Epoch 62, Batch 10721, Loss: 169.2135772705078\n",
      "Epoch 62, Batch 10722, Loss: 183.18844604492188\n",
      "Epoch 62, Batch 10723, Loss: 166.4501953125\n",
      "Epoch 62, Batch 10724, Loss: 180.6751251220703\n",
      "Epoch 62, Batch 10725, Loss: 184.2236785888672\n",
      "Epoch 62, Batch 10726, Loss: 158.18170166015625\n",
      "Epoch 62, Batch 10727, Loss: 157.33445739746094\n",
      "Epoch 62, Batch 10728, Loss: 173.7067413330078\n",
      "Epoch 62, Batch 10729, Loss: 176.6857147216797\n",
      "Epoch 62, Batch 10730, Loss: 170.8592529296875\n",
      "Epoch 62, Batch 10731, Loss: 179.18215942382812\n",
      "Epoch 62, Batch 10732, Loss: 184.94656372070312\n",
      "Epoch 62, Batch 10733, Loss: 173.587890625\n",
      "Epoch 62, Batch 10734, Loss: 193.51499938964844\n",
      "Epoch 62, Batch 10735, Loss: 170.8019256591797\n",
      "Epoch 62, Batch 10736, Loss: 177.11610412597656\n",
      "Epoch 62, Batch 10737, Loss: 164.40606689453125\n",
      "Epoch 62, Batch 10738, Loss: 172.79209899902344\n",
      "Epoch 62, Batch 10739, Loss: 166.9033660888672\n",
      "Epoch 62, Batch 10740, Loss: 183.92750549316406\n",
      "Epoch 62, Batch 10741, Loss: 170.6282501220703\n",
      "Epoch 62, Batch 10742, Loss: 173.70590209960938\n",
      "Epoch 62, Batch 10743, Loss: 185.52537536621094\n",
      "Epoch 62, Batch 10744, Loss: 170.7601318359375\n",
      "Epoch 62, Batch 10745, Loss: 172.85105895996094\n",
      "Epoch 62, Batch 10746, Loss: 171.46336364746094\n",
      "Epoch 62, Batch 10747, Loss: 166.16500854492188\n",
      "Epoch 62, Batch 10748, Loss: 185.41880798339844\n",
      "Epoch 62, Batch 10749, Loss: 157.11480712890625\n",
      "Epoch 62, Batch 10750, Loss: 166.83238220214844\n",
      "Epoch 62, Batch 10751, Loss: 178.056884765625\n",
      "Epoch 62, Batch 10752, Loss: 183.4442901611328\n",
      "Epoch 62, Batch 10753, Loss: 167.7992401123047\n",
      "Epoch 62, Batch 10754, Loss: 169.57894897460938\n",
      "Epoch 62, Batch 10755, Loss: 180.23599243164062\n",
      "Epoch 62, Batch 10756, Loss: 172.3935089111328\n",
      "Epoch 62, Batch 10757, Loss: 175.73519897460938\n",
      "Epoch 62, Batch 10758, Loss: 179.2008056640625\n",
      "Epoch 62, Batch 10759, Loss: 167.4125213623047\n",
      "Epoch 62, Batch 10760, Loss: 162.67327880859375\n",
      "Epoch 62, Batch 10761, Loss: 190.7977294921875\n",
      "Epoch 62, Batch 10762, Loss: 174.12217712402344\n",
      "Epoch 62, Batch 10763, Loss: 177.5944061279297\n",
      "Epoch 62, Batch 10764, Loss: 172.0509033203125\n",
      "Epoch 62, Batch 10765, Loss: 176.512939453125\n",
      "Epoch 62, Batch 10766, Loss: 169.10049438476562\n",
      "Epoch 62, Batch 10767, Loss: 171.41659545898438\n",
      "Epoch 62, Batch 10768, Loss: 173.89967346191406\n",
      "Epoch 62, Batch 10769, Loss: 177.2482147216797\n",
      "Epoch 62, Batch 10770, Loss: 160.72015380859375\n",
      "Epoch 62, Batch 10771, Loss: 170.6603240966797\n",
      "Epoch 62, Batch 10772, Loss: 180.35784912109375\n",
      "Epoch 62, Batch 10773, Loss: 195.90354919433594\n",
      "Epoch 62, Batch 10774, Loss: 167.7866668701172\n",
      "Epoch 62, Batch 10775, Loss: 166.13497924804688\n",
      "Epoch 62, Batch 10776, Loss: 179.47386169433594\n",
      "Epoch 62, Batch 10777, Loss: 166.07159423828125\n",
      "Epoch 62, Batch 10778, Loss: 174.1475830078125\n",
      "Epoch 62, Batch 10779, Loss: 184.61524963378906\n",
      "Epoch 62, Batch 10780, Loss: 176.79981994628906\n",
      "Epoch 62, Batch 10781, Loss: 177.15481567382812\n",
      "Epoch 62, Batch 10782, Loss: 157.95213317871094\n",
      "Epoch 62, Batch 10783, Loss: 172.3397216796875\n",
      "Epoch 62, Batch 10784, Loss: 154.9718780517578\n",
      "Epoch 62, Batch 10785, Loss: 166.03024291992188\n",
      "Epoch 62, Batch 10786, Loss: 171.07293701171875\n",
      "Epoch 62, Batch 10787, Loss: 177.95631408691406\n",
      "Epoch 62, Batch 10788, Loss: 151.9254913330078\n",
      "Epoch 62, Batch 10789, Loss: 159.32664489746094\n",
      "Epoch 62, Batch 10790, Loss: 174.20973205566406\n",
      "Epoch 62, Batch 10791, Loss: 182.3597412109375\n",
      "Epoch 62, Batch 10792, Loss: 177.5174102783203\n",
      "Epoch 62, Batch 10793, Loss: 164.89242553710938\n",
      "Epoch 62, Batch 10794, Loss: 166.29698181152344\n",
      "Epoch 62, Batch 10795, Loss: 164.84735107421875\n",
      "Epoch 62, Batch 10796, Loss: 171.3986358642578\n",
      "Epoch 62, Batch 10797, Loss: 166.8701171875\n",
      "Epoch 62, Batch 10798, Loss: 174.7951202392578\n",
      "Epoch 62, Batch 10799, Loss: 159.67884826660156\n",
      "Epoch 62, Batch 10800, Loss: 172.1253204345703\n",
      "Epoch 62, Batch 10801, Loss: 177.73583984375\n",
      "Epoch 62, Batch 10802, Loss: 192.2728271484375\n",
      "Epoch 62, Batch 10803, Loss: 162.25262451171875\n",
      "Epoch 62, Batch 10804, Loss: 170.78073120117188\n",
      "Epoch 62, Batch 10805, Loss: 159.39382934570312\n",
      "Epoch 62, Batch 10806, Loss: 166.99163818359375\n",
      "Epoch 62, Batch 10807, Loss: 169.48287963867188\n",
      "Epoch 62, Batch 10808, Loss: 173.75437927246094\n",
      "Epoch 62, Batch 10809, Loss: 169.61758422851562\n",
      "Epoch 62, Batch 10810, Loss: 170.0529327392578\n",
      "Epoch 62, Batch 10811, Loss: 179.73123168945312\n",
      "Epoch 62, Batch 10812, Loss: 176.947998046875\n",
      "Epoch 62, Batch 10813, Loss: 175.7292938232422\n",
      "Epoch 62, Batch 10814, Loss: 176.44493103027344\n",
      "Epoch 62, Batch 10815, Loss: 177.7708282470703\n",
      "Epoch 62, Batch 10816, Loss: 168.26644897460938\n",
      "Epoch 62, Batch 10817, Loss: 184.52232360839844\n",
      "Epoch 62, Batch 10818, Loss: 165.05809020996094\n",
      "Epoch 62, Batch 10819, Loss: 166.90806579589844\n",
      "Epoch 62, Batch 10820, Loss: 199.67454528808594\n",
      "Epoch 62, Batch 10821, Loss: 176.9353790283203\n",
      "Epoch 62, Batch 10822, Loss: 180.82435607910156\n",
      "Epoch 62, Batch 10823, Loss: 169.45306396484375\n",
      "Epoch 62, Batch 10824, Loss: 168.7247772216797\n",
      "Epoch 62, Batch 10825, Loss: 185.5019073486328\n",
      "Epoch 62, Batch 10826, Loss: 163.7813262939453\n",
      "Epoch 62, Batch 10827, Loss: 168.08042907714844\n",
      "Epoch 62, Batch 10828, Loss: 184.4226531982422\n",
      "Epoch 62, Batch 10829, Loss: 160.9182586669922\n",
      "Epoch 62, Batch 10830, Loss: 174.40206909179688\n",
      "Epoch 62, Batch 10831, Loss: 184.0980987548828\n",
      "Epoch 62, Batch 10832, Loss: 165.27984619140625\n",
      "Epoch 62, Batch 10833, Loss: 177.0520782470703\n",
      "Epoch 62, Batch 10834, Loss: 162.71595764160156\n",
      "Epoch 62, Batch 10835, Loss: 162.68362426757812\n",
      "Epoch 62, Batch 10836, Loss: 164.2571258544922\n",
      "Epoch 62, Batch 10837, Loss: 170.23681640625\n",
      "Epoch 62, Batch 10838, Loss: 177.74935913085938\n",
      "Epoch 62, Batch 10839, Loss: 175.53680419921875\n",
      "Epoch 62, Batch 10840, Loss: 179.00860595703125\n",
      "Epoch 62, Batch 10841, Loss: 177.61376953125\n",
      "Epoch 62, Batch 10842, Loss: 173.1126251220703\n",
      "Epoch 62, Batch 10843, Loss: 185.61859130859375\n",
      "Epoch 62, Batch 10844, Loss: 170.6645050048828\n",
      "Epoch 62, Batch 10845, Loss: 180.45957946777344\n",
      "Epoch 62, Batch 10846, Loss: 172.12977600097656\n",
      "Epoch 62, Batch 10847, Loss: 173.0143585205078\n",
      "Epoch 62, Batch 10848, Loss: 180.7049560546875\n",
      "Epoch 62, Batch 10849, Loss: 169.29409790039062\n",
      "Epoch 62, Batch 10850, Loss: 168.06207275390625\n",
      "Epoch 62, Batch 10851, Loss: 178.4700164794922\n",
      "Epoch 62, Batch 10852, Loss: 187.9241180419922\n",
      "Epoch 62, Batch 10853, Loss: 170.85108947753906\n",
      "Epoch 62, Batch 10854, Loss: 173.38319396972656\n",
      "Epoch 62, Batch 10855, Loss: 178.06655883789062\n",
      "Epoch 62, Batch 10856, Loss: 188.89418029785156\n",
      "Epoch 62, Batch 10857, Loss: 161.41151428222656\n",
      "Epoch 62, Batch 10858, Loss: 190.1668243408203\n",
      "Epoch 62, Batch 10859, Loss: 175.359375\n",
      "Epoch 62, Batch 10860, Loss: 167.0426788330078\n",
      "Epoch 62, Batch 10861, Loss: 163.2891845703125\n",
      "Epoch 62, Batch 10862, Loss: 182.09735107421875\n",
      "Epoch 62, Batch 10863, Loss: 181.08889770507812\n",
      "Epoch 62, Batch 10864, Loss: 177.77291870117188\n",
      "Epoch 62, Batch 10865, Loss: 167.8270263671875\n",
      "Epoch 62, Batch 10866, Loss: 182.5731964111328\n",
      "Epoch 62, Batch 10867, Loss: 193.03707885742188\n",
      "Epoch 62, Batch 10868, Loss: 170.1448211669922\n",
      "Epoch 62, Batch 10869, Loss: 179.16099548339844\n",
      "Epoch 62, Batch 10870, Loss: 175.7754669189453\n",
      "Epoch 62, Batch 10871, Loss: 177.25680541992188\n",
      "Epoch 62, Batch 10872, Loss: 170.61044311523438\n",
      "Epoch 62, Batch 10873, Loss: 167.6416473388672\n",
      "Epoch 62, Batch 10874, Loss: 170.2413330078125\n",
      "Epoch 62, Batch 10875, Loss: 182.09263610839844\n",
      "Epoch 62, Batch 10876, Loss: 172.13279724121094\n",
      "Epoch 62, Batch 10877, Loss: 187.0644989013672\n",
      "Epoch 62, Batch 10878, Loss: 180.14683532714844\n",
      "Epoch 62, Batch 10879, Loss: 172.20535278320312\n",
      "Epoch 62, Batch 10880, Loss: 178.20501708984375\n",
      "Epoch 62, Batch 10881, Loss: 178.15248107910156\n",
      "Epoch 62, Batch 10882, Loss: 175.9555206298828\n",
      "Epoch 62, Batch 10883, Loss: 176.0999755859375\n",
      "Epoch 62, Batch 10884, Loss: 178.9700164794922\n",
      "Epoch 62, Batch 10885, Loss: 166.84332275390625\n",
      "Epoch 62, Batch 10886, Loss: 179.91798400878906\n",
      "Epoch 62, Batch 10887, Loss: 181.1886444091797\n",
      "Epoch 62, Batch 10888, Loss: 169.50498962402344\n",
      "Epoch 62, Batch 10889, Loss: 153.8618621826172\n",
      "Epoch 62, Batch 10890, Loss: 163.91493225097656\n",
      "Epoch 62, Batch 10891, Loss: 170.0917510986328\n",
      "Epoch 62, Batch 10892, Loss: 184.66265869140625\n",
      "Epoch 62, Batch 10893, Loss: 165.69888305664062\n",
      "Epoch 62, Batch 10894, Loss: 178.5609893798828\n",
      "Epoch 62, Batch 10895, Loss: 170.27972412109375\n",
      "Epoch 62, Batch 10896, Loss: 171.77545166015625\n",
      "Epoch 62, Batch 10897, Loss: 186.626708984375\n",
      "Epoch 62, Batch 10898, Loss: 166.5660858154297\n",
      "Epoch 62, Batch 10899, Loss: 177.14886474609375\n",
      "Epoch 62, Batch 10900, Loss: 176.37652587890625\n",
      "Epoch 62, Batch 10901, Loss: 166.94793701171875\n",
      "Epoch 62, Batch 10902, Loss: 167.6429443359375\n",
      "Epoch 62, Batch 10903, Loss: 164.89491271972656\n",
      "Epoch 62, Batch 10904, Loss: 173.89321899414062\n",
      "Epoch 62, Batch 10905, Loss: 188.48458862304688\n",
      "Epoch 62, Batch 10906, Loss: 184.36453247070312\n",
      "Epoch 62, Batch 10907, Loss: 168.96478271484375\n",
      "Epoch 62, Batch 10908, Loss: 159.37850952148438\n",
      "Epoch 62, Batch 10909, Loss: 175.3332977294922\n",
      "Epoch 62, Batch 10910, Loss: 170.8712921142578\n",
      "Epoch 62, Batch 10911, Loss: 170.91677856445312\n",
      "Epoch 62, Batch 10912, Loss: 181.18174743652344\n",
      "Epoch 62, Batch 10913, Loss: 180.5678253173828\n",
      "Epoch 62, Batch 10914, Loss: 163.98141479492188\n",
      "Epoch 62, Batch 10915, Loss: 176.7997589111328\n",
      "Epoch 62, Batch 10916, Loss: 172.3531951904297\n",
      "Epoch 62, Batch 10917, Loss: 177.1367645263672\n",
      "Epoch 62, Batch 10918, Loss: 164.60841369628906\n",
      "Epoch 62, Batch 10919, Loss: 175.72833251953125\n",
      "Epoch 62, Batch 10920, Loss: 155.54541015625\n",
      "Epoch 62, Batch 10921, Loss: 174.06153869628906\n",
      "Epoch 62, Batch 10922, Loss: 164.4016571044922\n",
      "Epoch 62, Batch 10923, Loss: 171.46798706054688\n",
      "Epoch 62, Batch 10924, Loss: 163.3900604248047\n",
      "Epoch 62, Batch 10925, Loss: 156.15158081054688\n",
      "Epoch 62, Batch 10926, Loss: 175.10604858398438\n",
      "Epoch 62, Batch 10927, Loss: 176.33131408691406\n",
      "Epoch 62, Batch 10928, Loss: 166.66537475585938\n",
      "Epoch 62, Batch 10929, Loss: 162.5157012939453\n",
      "Epoch 62, Batch 10930, Loss: 162.6136474609375\n",
      "Epoch 62, Batch 10931, Loss: 181.17442321777344\n",
      "Epoch 62, Batch 10932, Loss: 171.2816925048828\n",
      "Epoch 62, Batch 10933, Loss: 175.11087036132812\n",
      "Epoch 62, Batch 10934, Loss: 154.14620971679688\n",
      "Epoch 62, Batch 10935, Loss: 184.248779296875\n",
      "Epoch 62, Batch 10936, Loss: 191.83935546875\n",
      "Epoch 62, Batch 10937, Loss: 166.15548706054688\n",
      "Epoch 62, Batch 10938, Loss: 169.1105194091797\n",
      "Epoch 62, Batch 10939, Loss: 158.04388427734375\n",
      "Epoch 62, Batch 10940, Loss: 167.98123168945312\n",
      "Epoch 62, Batch 10941, Loss: 176.94960021972656\n",
      "Epoch 62, Batch 10942, Loss: 164.48765563964844\n",
      "Epoch 62, Batch 10943, Loss: 165.57789611816406\n",
      "Epoch 62, Batch 10944, Loss: 171.6588134765625\n",
      "Epoch 62, Batch 10945, Loss: 159.7591094970703\n",
      "Epoch 62, Batch 10946, Loss: 173.86289978027344\n",
      "Epoch 62, Batch 10947, Loss: 182.33660888671875\n",
      "Epoch 62, Batch 10948, Loss: 180.89767456054688\n",
      "Epoch 62, Batch 10949, Loss: 174.62644958496094\n",
      "Epoch 62, Batch 10950, Loss: 177.12725830078125\n",
      "Epoch 62, Batch 10951, Loss: 191.40884399414062\n",
      "Epoch 62, Batch 10952, Loss: 175.8570098876953\n",
      "Epoch 62, Batch 10953, Loss: 175.7192840576172\n",
      "Epoch 62, Batch 10954, Loss: 158.9522247314453\n",
      "Epoch 62, Batch 10955, Loss: 172.0400390625\n",
      "Epoch 62, Batch 10956, Loss: 180.66163635253906\n",
      "Epoch 62, Batch 10957, Loss: 157.9322967529297\n",
      "Epoch 62, Batch 10958, Loss: 164.53224182128906\n",
      "Epoch 62, Batch 10959, Loss: 178.24493408203125\n",
      "Epoch 62, Batch 10960, Loss: 174.00222778320312\n",
      "Epoch 62, Batch 10961, Loss: 188.91671752929688\n",
      "Epoch 62, Batch 10962, Loss: 177.18679809570312\n",
      "Epoch 62, Batch 10963, Loss: 161.46575927734375\n",
      "Epoch 62, Batch 10964, Loss: 155.6219482421875\n",
      "Epoch 62, Batch 10965, Loss: 177.4458770751953\n",
      "Epoch 62, Batch 10966, Loss: 159.73553466796875\n",
      "Epoch 62, Batch 10967, Loss: 184.81195068359375\n",
      "Epoch 62, Batch 10968, Loss: 185.54833984375\n",
      "Epoch 62, Batch 10969, Loss: 156.59109497070312\n",
      "Epoch 62, Batch 10970, Loss: 178.82469177246094\n",
      "Epoch 62, Batch 10971, Loss: 175.2551727294922\n",
      "Epoch 62, Batch 10972, Loss: 171.7050323486328\n",
      "Epoch 62, Batch 10973, Loss: 169.71092224121094\n",
      "Epoch 62, Batch 10974, Loss: 153.40792846679688\n",
      "Epoch 62, Batch 10975, Loss: 176.9844207763672\n",
      "Epoch 62, Batch 10976, Loss: 162.39773559570312\n",
      "Epoch 62, Batch 10977, Loss: 165.19419860839844\n",
      "Epoch 62, Batch 10978, Loss: 152.1395721435547\n",
      "Epoch 62, Batch 10979, Loss: 181.9669952392578\n",
      "Epoch 62, Batch 10980, Loss: 173.37844848632812\n",
      "Epoch 62, Batch 10981, Loss: 159.65359497070312\n",
      "Epoch 62, Batch 10982, Loss: 173.33303833007812\n",
      "Epoch 62, Batch 10983, Loss: 168.11325073242188\n",
      "Epoch 62, Batch 10984, Loss: 164.9622039794922\n",
      "Epoch 62, Batch 10985, Loss: 160.300537109375\n",
      "Epoch 62, Batch 10986, Loss: 152.44638061523438\n",
      "Epoch 62, Batch 10987, Loss: 172.7858428955078\n",
      "Epoch 62, Batch 10988, Loss: 160.46202087402344\n",
      "Epoch 62, Batch 10989, Loss: 185.4686279296875\n",
      "Epoch 62, Batch 10990, Loss: 192.31365966796875\n",
      "Epoch 62, Batch 10991, Loss: 156.9221649169922\n",
      "Epoch 62, Batch 10992, Loss: 163.00405883789062\n",
      "Epoch 62, Batch 10993, Loss: 166.33995056152344\n",
      "Epoch 62, Batch 10994, Loss: 169.62515258789062\n",
      "Epoch 62, Batch 10995, Loss: 164.2480926513672\n",
      "Epoch 62, Batch 10996, Loss: 178.14846801757812\n",
      "Epoch 62, Batch 10997, Loss: 167.0186309814453\n",
      "Epoch 62, Batch 10998, Loss: 185.33299255371094\n",
      "Epoch 62, Batch 10999, Loss: 179.65000915527344\n",
      "Epoch 62, Batch 11000, Loss: 174.0154571533203\n",
      "Epoch 62, Batch 11001, Loss: 191.04254150390625\n",
      "Epoch 62, Batch 11002, Loss: 184.20957946777344\n",
      "Epoch 62, Batch 11003, Loss: 169.99314880371094\n",
      "Epoch 62, Batch 11004, Loss: 164.5988311767578\n",
      "Epoch 62, Batch 11005, Loss: 189.9939422607422\n",
      "Epoch 62, Batch 11006, Loss: 180.94163513183594\n",
      "Epoch 62, Batch 11007, Loss: 182.65086364746094\n",
      "Epoch 62, Batch 11008, Loss: 190.63172912597656\n",
      "Epoch 62, Batch 11009, Loss: 172.7451171875\n",
      "Epoch 62, Batch 11010, Loss: 163.00096130371094\n",
      "Epoch 62, Batch 11011, Loss: 169.43328857421875\n",
      "Epoch 62, Batch 11012, Loss: 177.50387573242188\n",
      "Epoch 62, Batch 11013, Loss: 170.46078491210938\n",
      "Epoch 62, Batch 11014, Loss: 182.69273376464844\n",
      "Epoch 62, Batch 11015, Loss: 182.18540954589844\n",
      "Epoch 62, Batch 11016, Loss: 175.86663818359375\n",
      "Epoch 62, Batch 11017, Loss: 184.9868927001953\n",
      "Epoch 62, Batch 11018, Loss: 173.7614288330078\n",
      "Epoch 62, Batch 11019, Loss: 175.816162109375\n",
      "Epoch 62, Batch 11020, Loss: 177.79421997070312\n",
      "Epoch 62, Batch 11021, Loss: 186.4095458984375\n",
      "Epoch 62, Batch 11022, Loss: 173.92987060546875\n",
      "Epoch 62, Batch 11023, Loss: 160.4344940185547\n",
      "Epoch 62, Batch 11024, Loss: 153.45068359375\n",
      "Epoch 62, Batch 11025, Loss: 174.88699340820312\n",
      "Epoch 62, Batch 11026, Loss: 163.2196807861328\n",
      "Epoch 62, Batch 11027, Loss: 162.4420928955078\n",
      "Epoch 62, Batch 11028, Loss: 164.1963653564453\n",
      "Epoch 62, Batch 11029, Loss: 182.95138549804688\n",
      "Epoch 62, Batch 11030, Loss: 178.89447021484375\n",
      "Epoch 62, Batch 11031, Loss: 164.51841735839844\n",
      "Epoch 62, Batch 11032, Loss: 170.74452209472656\n",
      "Epoch 62, Batch 11033, Loss: 178.3055419921875\n",
      "Epoch 62, Batch 11034, Loss: 175.35009765625\n",
      "Epoch 62, Batch 11035, Loss: 169.9228515625\n",
      "Epoch 62, Batch 11036, Loss: 163.2510223388672\n",
      "Epoch 62, Batch 11037, Loss: 162.171630859375\n",
      "Epoch 62, Batch 11038, Loss: 178.3670654296875\n",
      "Epoch 62, Batch 11039, Loss: 164.8167266845703\n",
      "Epoch 62, Batch 11040, Loss: 178.77395629882812\n",
      "Epoch 62, Batch 11041, Loss: 184.15269470214844\n",
      "Epoch 62, Batch 11042, Loss: 148.7519989013672\n",
      "Epoch 62, Batch 11043, Loss: 175.48553466796875\n",
      "Epoch 62, Batch 11044, Loss: 172.97825622558594\n",
      "Epoch 62, Batch 11045, Loss: 179.5396270751953\n",
      "Epoch 62, Batch 11046, Loss: 165.0115966796875\n",
      "Epoch 62, Batch 11047, Loss: 166.0878143310547\n",
      "Epoch 62, Batch 11048, Loss: 196.9458465576172\n",
      "Epoch 62, Batch 11049, Loss: 156.84226989746094\n",
      "Epoch 62, Batch 11050, Loss: 154.25965881347656\n",
      "Epoch 62, Batch 11051, Loss: 169.56752014160156\n",
      "Epoch 62, Batch 11052, Loss: 188.87940979003906\n",
      "Epoch 62, Batch 11053, Loss: 167.07676696777344\n",
      "Epoch 62, Batch 11054, Loss: 195.37112426757812\n",
      "Epoch 62, Batch 11055, Loss: 171.052490234375\n",
      "Epoch 62, Batch 11056, Loss: 171.74224853515625\n",
      "Epoch 62, Batch 11057, Loss: 180.84042358398438\n",
      "Epoch 62, Batch 11058, Loss: 170.26739501953125\n",
      "Epoch 62, Batch 11059, Loss: 189.37435913085938\n",
      "Epoch 62, Batch 11060, Loss: 157.27328491210938\n",
      "Epoch 62, Batch 11061, Loss: 172.31170654296875\n",
      "Epoch 62, Batch 11062, Loss: 179.2781219482422\n",
      "Epoch 62, Batch 11063, Loss: 177.0216827392578\n",
      "Epoch 62, Batch 11064, Loss: 176.5316162109375\n",
      "Epoch 62, Batch 11065, Loss: 195.1709442138672\n",
      "Epoch 62, Batch 11066, Loss: 181.15065002441406\n",
      "Epoch 62, Batch 11067, Loss: 147.9774932861328\n",
      "Epoch 62, Batch 11068, Loss: 168.4624786376953\n",
      "Epoch 62, Batch 11069, Loss: 168.08920288085938\n",
      "Epoch 62, Batch 11070, Loss: 176.72372436523438\n",
      "Epoch 62, Batch 11071, Loss: 176.39601135253906\n",
      "Epoch 62, Batch 11072, Loss: 162.58804321289062\n",
      "Epoch 62, Batch 11073, Loss: 178.22520446777344\n",
      "Epoch 62, Batch 11074, Loss: 169.55076599121094\n",
      "Epoch 62, Batch 11075, Loss: 187.51022338867188\n",
      "Epoch 62, Batch 11076, Loss: 171.8661651611328\n",
      "Epoch 62, Batch 11077, Loss: 183.0527801513672\n",
      "Epoch 62, Batch 11078, Loss: 153.20213317871094\n",
      "Epoch 62, Batch 11079, Loss: 166.63963317871094\n",
      "Epoch 62, Batch 11080, Loss: 181.91390991210938\n",
      "Epoch 62, Batch 11081, Loss: 169.82565307617188\n",
      "Epoch 62, Batch 11082, Loss: 176.11585998535156\n",
      "Epoch 62, Batch 11083, Loss: 158.9994354248047\n",
      "Epoch 62, Batch 11084, Loss: 191.0746612548828\n",
      "Epoch 62, Batch 11085, Loss: 167.9657440185547\n",
      "Epoch 62, Batch 11086, Loss: 168.2073974609375\n",
      "Epoch 62, Batch 11087, Loss: 173.93649291992188\n",
      "Epoch 62, Batch 11088, Loss: 159.8612060546875\n",
      "Epoch 62, Batch 11089, Loss: 164.25975036621094\n",
      "Epoch 62, Batch 11090, Loss: 163.12496948242188\n",
      "Epoch 62, Batch 11091, Loss: 178.1499481201172\n",
      "Epoch 62, Batch 11092, Loss: 180.2761688232422\n",
      "Epoch 62, Batch 11093, Loss: 164.9795684814453\n",
      "Epoch 62, Batch 11094, Loss: 189.14366149902344\n",
      "Epoch 62, Batch 11095, Loss: 176.93455505371094\n",
      "Epoch 62, Batch 11096, Loss: 171.0574493408203\n",
      "Epoch 62, Batch 11097, Loss: 165.65206909179688\n",
      "Epoch 62, Batch 11098, Loss: 180.7408905029297\n",
      "Epoch 62, Batch 11099, Loss: 179.09483337402344\n",
      "Epoch 62, Batch 11100, Loss: 173.31488037109375\n",
      "Epoch 62, Batch 11101, Loss: 163.02195739746094\n",
      "Epoch 62, Batch 11102, Loss: 169.2014617919922\n",
      "Epoch 62, Batch 11103, Loss: 185.65902709960938\n",
      "Epoch 62, Batch 11104, Loss: 158.6215362548828\n",
      "Epoch 62, Batch 11105, Loss: 171.99525451660156\n",
      "Epoch 62, Batch 11106, Loss: 178.80992126464844\n",
      "Epoch 62, Batch 11107, Loss: 192.8106231689453\n",
      "Epoch 62, Batch 11108, Loss: 160.39598083496094\n",
      "Epoch 62, Batch 11109, Loss: 168.11622619628906\n",
      "Epoch 62, Batch 11110, Loss: 168.65496826171875\n",
      "Epoch 62, Batch 11111, Loss: 172.40521240234375\n",
      "Epoch 62, Batch 11112, Loss: 174.3549041748047\n",
      "Epoch 62, Batch 11113, Loss: 191.11831665039062\n",
      "Epoch 62, Batch 11114, Loss: 165.52996826171875\n",
      "Epoch 62, Batch 11115, Loss: 177.9325408935547\n",
      "Epoch 62, Batch 11116, Loss: 172.93104553222656\n",
      "Epoch 62, Batch 11117, Loss: 183.9386749267578\n",
      "Epoch 62, Batch 11118, Loss: 188.69297790527344\n",
      "Epoch 62, Batch 11119, Loss: 162.6151885986328\n",
      "Epoch 62, Batch 11120, Loss: 167.1646270751953\n",
      "Epoch 62, Batch 11121, Loss: 191.90823364257812\n",
      "Epoch 62, Batch 11122, Loss: 165.56788635253906\n",
      "Epoch 62, Batch 11123, Loss: 180.7063751220703\n",
      "Epoch 62, Batch 11124, Loss: 173.1964874267578\n",
      "Epoch 62, Batch 11125, Loss: 167.6122283935547\n",
      "Epoch 62, Batch 11126, Loss: 161.56634521484375\n",
      "Epoch 62, Batch 11127, Loss: 186.4357147216797\n",
      "Epoch 62, Batch 11128, Loss: 179.433349609375\n",
      "Epoch 62, Batch 11129, Loss: 174.48675537109375\n",
      "Epoch 62, Batch 11130, Loss: 171.09280395507812\n",
      "Epoch 62, Batch 11131, Loss: 170.56298828125\n",
      "Epoch 62, Batch 11132, Loss: 164.81582641601562\n",
      "Epoch 62, Batch 11133, Loss: 181.42115783691406\n",
      "Epoch 62, Batch 11134, Loss: 172.1112823486328\n",
      "Epoch 62, Batch 11135, Loss: 170.8378143310547\n",
      "Epoch 62, Batch 11136, Loss: 166.51768493652344\n",
      "Epoch 62, Batch 11137, Loss: 168.94924926757812\n",
      "Epoch 62, Batch 11138, Loss: 181.76113891601562\n",
      "Epoch 62, Batch 11139, Loss: 162.79318237304688\n",
      "Epoch 62, Batch 11140, Loss: 165.75379943847656\n",
      "Epoch 62, Batch 11141, Loss: 176.43443298339844\n",
      "Epoch 62, Batch 11142, Loss: 179.42481994628906\n",
      "Epoch 62, Batch 11143, Loss: 164.49942016601562\n",
      "Epoch 62, Batch 11144, Loss: 175.41954040527344\n",
      "Epoch 62, Batch 11145, Loss: 175.07716369628906\n",
      "Epoch 62, Batch 11146, Loss: 169.67311096191406\n",
      "Epoch 62, Batch 11147, Loss: 173.9070281982422\n",
      "Epoch 62, Batch 11148, Loss: 164.04490661621094\n",
      "Epoch 62, Batch 11149, Loss: 169.33929443359375\n",
      "Epoch 62, Batch 11150, Loss: 165.1443328857422\n",
      "Epoch 62, Batch 11151, Loss: 185.8108673095703\n",
      "Epoch 62, Batch 11152, Loss: 182.67494201660156\n",
      "Epoch 62, Batch 11153, Loss: 176.45449829101562\n",
      "Epoch 62, Batch 11154, Loss: 162.23109436035156\n",
      "Epoch 62, Batch 11155, Loss: 174.22183227539062\n",
      "Epoch 62, Batch 11156, Loss: 172.70089721679688\n",
      "Epoch 62, Batch 11157, Loss: 166.22103881835938\n",
      "Epoch 62, Batch 11158, Loss: 158.5796661376953\n",
      "Epoch 62, Batch 11159, Loss: 192.5011749267578\n",
      "Epoch 62, Batch 11160, Loss: 178.86000061035156\n",
      "Epoch 62, Batch 11161, Loss: 165.76663208007812\n",
      "Epoch 62, Batch 11162, Loss: 179.4672393798828\n",
      "Epoch 62, Batch 11163, Loss: 170.3019256591797\n",
      "Epoch 62, Batch 11164, Loss: 185.09974670410156\n",
      "Epoch 62, Batch 11165, Loss: 148.36305236816406\n",
      "Epoch 62, Batch 11166, Loss: 167.36322021484375\n",
      "Epoch 62, Batch 11167, Loss: 182.2381591796875\n",
      "Epoch 62, Batch 11168, Loss: 175.8866424560547\n",
      "Epoch 62, Batch 11169, Loss: 168.1107635498047\n",
      "Epoch 62, Batch 11170, Loss: 171.73873901367188\n",
      "Epoch 62, Batch 11171, Loss: 160.4091796875\n",
      "Epoch 62, Batch 11172, Loss: 169.58560180664062\n",
      "Epoch 62, Batch 11173, Loss: 173.18006896972656\n",
      "Epoch 62, Batch 11174, Loss: 170.41659545898438\n",
      "Epoch 62, Batch 11175, Loss: 169.67288208007812\n",
      "Epoch 62, Batch 11176, Loss: 181.6187286376953\n",
      "Epoch 62, Batch 11177, Loss: 182.8533172607422\n",
      "Epoch 62, Batch 11178, Loss: 184.7611083984375\n",
      "Epoch 62, Batch 11179, Loss: 164.86033630371094\n",
      "Epoch 62, Batch 11180, Loss: 167.61265563964844\n",
      "Epoch 62, Batch 11181, Loss: 160.01426696777344\n",
      "Epoch 62, Batch 11182, Loss: 174.62814331054688\n",
      "Epoch 62, Batch 11183, Loss: 175.49716186523438\n",
      "Epoch 62, Batch 11184, Loss: 155.7079620361328\n",
      "Epoch 62, Batch 11185, Loss: 177.53965759277344\n",
      "Epoch 62, Batch 11186, Loss: 176.22439575195312\n",
      "Epoch 62, Batch 11187, Loss: 148.43014526367188\n",
      "Epoch 62, Batch 11188, Loss: 179.43704223632812\n",
      "Epoch 62, Batch 11189, Loss: 172.73861694335938\n",
      "Epoch 62, Batch 11190, Loss: 181.70968627929688\n",
      "Epoch 62, Batch 11191, Loss: 179.6182861328125\n",
      "Epoch 62, Batch 11192, Loss: 157.04067993164062\n",
      "Epoch 62, Batch 11193, Loss: 158.4578399658203\n",
      "Epoch 62, Batch 11194, Loss: 168.65548706054688\n",
      "Epoch 62, Batch 11195, Loss: 176.75787353515625\n",
      "Epoch 62, Batch 11196, Loss: 158.23394775390625\n",
      "Epoch 62, Batch 11197, Loss: 177.29185485839844\n",
      "Epoch 62, Batch 11198, Loss: 178.07864379882812\n",
      "Epoch 62, Batch 11199, Loss: 179.53102111816406\n",
      "Epoch 62, Batch 11200, Loss: 170.27377319335938\n",
      "Epoch 62, Batch 11201, Loss: 176.09193420410156\n",
      "Epoch 62, Batch 11202, Loss: 191.4630889892578\n",
      "Epoch 62, Batch 11203, Loss: 192.57687377929688\n",
      "Epoch 62, Batch 11204, Loss: 194.60675048828125\n",
      "Epoch 62, Batch 11205, Loss: 188.893798828125\n",
      "Epoch 62, Batch 11206, Loss: 187.10887145996094\n",
      "Epoch 62, Batch 11207, Loss: 163.9429473876953\n",
      "Epoch 62, Batch 11208, Loss: 169.725830078125\n",
      "Epoch 62, Batch 11209, Loss: 164.77476501464844\n",
      "Epoch 62, Batch 11210, Loss: 166.60484313964844\n",
      "Epoch 62, Batch 11211, Loss: 171.2677764892578\n",
      "Epoch 62, Batch 11212, Loss: 177.0235595703125\n",
      "Epoch 62, Batch 11213, Loss: 153.7400665283203\n",
      "Epoch 62, Batch 11214, Loss: 179.26515197753906\n",
      "Epoch 62, Batch 11215, Loss: 158.16851806640625\n",
      "Epoch 62, Batch 11216, Loss: 184.1880645751953\n",
      "Epoch 62, Batch 11217, Loss: 189.8704071044922\n",
      "Epoch 62, Batch 11218, Loss: 165.64341735839844\n",
      "Epoch 62, Batch 11219, Loss: 176.7611541748047\n",
      "Epoch 62, Batch 11220, Loss: 174.70394897460938\n",
      "Epoch 62, Batch 11221, Loss: 196.84616088867188\n",
      "Epoch 62, Batch 11222, Loss: 171.98069763183594\n",
      "Epoch 62, Batch 11223, Loss: 184.7255859375\n",
      "Epoch 62, Batch 11224, Loss: 172.71783447265625\n",
      "Epoch 62, Batch 11225, Loss: 163.8832550048828\n",
      "Epoch 62, Batch 11226, Loss: 168.0286407470703\n",
      "Epoch 62, Batch 11227, Loss: 160.75650024414062\n",
      "Epoch 62, Batch 11228, Loss: 156.80430603027344\n",
      "Epoch 62, Batch 11229, Loss: 174.4740447998047\n",
      "Epoch 62, Batch 11230, Loss: 192.73788452148438\n",
      "Epoch 62, Batch 11231, Loss: 180.79620361328125\n",
      "Epoch 62, Batch 11232, Loss: 165.09695434570312\n",
      "Epoch 62, Batch 11233, Loss: 192.95191955566406\n",
      "Epoch 62, Batch 11234, Loss: 160.3833770751953\n",
      "Epoch 62, Batch 11235, Loss: 169.1350555419922\n",
      "Epoch 62, Batch 11236, Loss: 157.6846160888672\n",
      "Epoch 62, Batch 11237, Loss: 176.58787536621094\n",
      "Epoch 62, Batch 11238, Loss: 166.6433563232422\n",
      "Epoch 62, Batch 11239, Loss: 167.4586639404297\n",
      "Epoch 62, Batch 11240, Loss: 177.21664428710938\n",
      "Epoch 62, Batch 11241, Loss: 173.80604553222656\n",
      "Epoch 62, Batch 11242, Loss: 177.978271484375\n",
      "Epoch 62, Batch 11243, Loss: 171.10311889648438\n",
      "Epoch 62, Batch 11244, Loss: 161.45086669921875\n",
      "Epoch 62, Batch 11245, Loss: 173.0899658203125\n",
      "Epoch 62, Batch 11246, Loss: 180.37657165527344\n",
      "Epoch 62, Batch 11247, Loss: 163.6891326904297\n",
      "Epoch 62, Batch 11248, Loss: 179.614501953125\n",
      "Epoch 62, Batch 11249, Loss: 181.07933044433594\n",
      "Epoch 62, Batch 11250, Loss: 170.2235565185547\n",
      "Epoch 62, Batch 11251, Loss: 172.6068572998047\n",
      "Epoch 62, Batch 11252, Loss: 192.47718811035156\n",
      "Epoch 62, Batch 11253, Loss: 188.58250427246094\n",
      "Epoch 62, Batch 11254, Loss: 161.28480529785156\n",
      "Epoch 62, Batch 11255, Loss: 182.5463409423828\n",
      "Epoch 62, Batch 11256, Loss: 176.7629852294922\n",
      "Epoch 62, Batch 11257, Loss: 166.26927185058594\n",
      "Epoch 62, Batch 11258, Loss: 187.96871948242188\n",
      "Epoch 62, Batch 11259, Loss: 186.07667541503906\n",
      "Epoch 62, Batch 11260, Loss: 177.86346435546875\n",
      "Epoch 62, Batch 11261, Loss: 175.9646453857422\n",
      "Epoch 62, Batch 11262, Loss: 191.82054138183594\n",
      "Epoch 62, Batch 11263, Loss: 171.88133239746094\n",
      "Epoch 62, Batch 11264, Loss: 164.6296844482422\n",
      "Epoch 62, Batch 11265, Loss: 178.42613220214844\n",
      "Epoch 62, Batch 11266, Loss: 183.63629150390625\n",
      "Epoch 62, Batch 11267, Loss: 171.9176788330078\n",
      "Epoch 62, Batch 11268, Loss: 166.85145568847656\n",
      "Epoch 62, Batch 11269, Loss: 183.5032958984375\n",
      "Epoch 62, Batch 11270, Loss: 173.35963439941406\n",
      "Epoch 62, Batch 11271, Loss: 170.100341796875\n",
      "Epoch 62, Batch 11272, Loss: 173.42987060546875\n",
      "Epoch 62, Batch 11273, Loss: 170.491455078125\n",
      "Epoch 62, Batch 11274, Loss: 169.83802795410156\n",
      "Epoch 62, Batch 11275, Loss: 163.56097412109375\n",
      "Epoch 62, Batch 11276, Loss: 177.4209747314453\n",
      "Epoch 62, Batch 11277, Loss: 171.32211303710938\n",
      "Epoch 62, Batch 11278, Loss: 154.24609375\n",
      "Epoch 62, Batch 11279, Loss: 174.32725524902344\n",
      "Epoch 62, Batch 11280, Loss: 170.51229858398438\n",
      "Epoch 62, Batch 11281, Loss: 167.34571838378906\n",
      "Epoch 62, Batch 11282, Loss: 176.71484375\n",
      "Epoch 62, Batch 11283, Loss: 171.71859741210938\n",
      "Epoch 62, Batch 11284, Loss: 177.39601135253906\n",
      "Epoch 62, Batch 11285, Loss: 147.09271240234375\n",
      "Epoch 62, Batch 11286, Loss: 189.3748321533203\n",
      "Epoch 62, Batch 11287, Loss: 173.0895538330078\n",
      "Epoch 62, Batch 11288, Loss: 187.1858673095703\n",
      "Epoch 62, Batch 11289, Loss: 172.99021911621094\n",
      "Epoch 62, Batch 11290, Loss: 172.1456298828125\n",
      "Epoch 62, Batch 11291, Loss: 160.8312530517578\n",
      "Epoch 62, Batch 11292, Loss: 175.089599609375\n",
      "Epoch 62, Batch 11293, Loss: 172.8689422607422\n",
      "Epoch 62, Batch 11294, Loss: 170.2589874267578\n",
      "Epoch 62, Batch 11295, Loss: 164.01808166503906\n",
      "Epoch 62, Batch 11296, Loss: 174.47726440429688\n",
      "Epoch 62, Batch 11297, Loss: 173.11293029785156\n",
      "Epoch 62, Batch 11298, Loss: 172.92755126953125\n",
      "Epoch 62, Batch 11299, Loss: 173.72872924804688\n",
      "Epoch 62, Batch 11300, Loss: 169.57701110839844\n",
      "Epoch 62, Batch 11301, Loss: 177.47476196289062\n",
      "Epoch 62, Batch 11302, Loss: 172.2935028076172\n",
      "Epoch 62, Batch 11303, Loss: 165.2063751220703\n",
      "Epoch 62, Batch 11304, Loss: 181.1163787841797\n",
      "Epoch 62, Batch 11305, Loss: 166.01840209960938\n",
      "Epoch 62, Batch 11306, Loss: 162.76922607421875\n",
      "Epoch 62, Batch 11307, Loss: 181.49752807617188\n",
      "Epoch 62, Batch 11308, Loss: 180.70846557617188\n",
      "Epoch 62, Batch 11309, Loss: 165.90798950195312\n",
      "Epoch 62, Batch 11310, Loss: 181.0803985595703\n",
      "Epoch 62, Batch 11311, Loss: 177.67103576660156\n",
      "Epoch 62, Batch 11312, Loss: 170.8233184814453\n",
      "Epoch 62, Batch 11313, Loss: 183.00428771972656\n",
      "Epoch 62, Batch 11314, Loss: 181.673828125\n",
      "Epoch 62, Batch 11315, Loss: 177.14085388183594\n",
      "Epoch 62, Batch 11316, Loss: 168.1779022216797\n",
      "Epoch 62, Batch 11317, Loss: 168.9589385986328\n",
      "Epoch 62, Batch 11318, Loss: 165.49098205566406\n",
      "Epoch 62, Batch 11319, Loss: 171.09927368164062\n",
      "Epoch 62, Batch 11320, Loss: 184.05023193359375\n",
      "Epoch 62, Batch 11321, Loss: 183.90748596191406\n",
      "Epoch 62, Batch 11322, Loss: 162.59197998046875\n",
      "Epoch 62, Batch 11323, Loss: 176.83177185058594\n",
      "Epoch 62, Batch 11324, Loss: 167.430419921875\n",
      "Epoch 62, Batch 11325, Loss: 154.40652465820312\n",
      "Epoch 62, Batch 11326, Loss: 191.5640106201172\n",
      "Epoch 62, Batch 11327, Loss: 170.91928100585938\n",
      "Epoch 62, Batch 11328, Loss: 193.29685974121094\n",
      "Epoch 62, Batch 11329, Loss: 181.86532592773438\n",
      "Epoch 62, Batch 11330, Loss: 178.88339233398438\n",
      "Epoch 62, Batch 11331, Loss: 178.56866455078125\n",
      "Epoch 62, Batch 11332, Loss: 189.35760498046875\n",
      "Epoch 62, Batch 11333, Loss: 181.19363403320312\n",
      "Epoch 62, Batch 11334, Loss: 182.61380004882812\n",
      "Epoch 62, Batch 11335, Loss: 170.6883087158203\n",
      "Epoch 62, Batch 11336, Loss: 194.6039276123047\n",
      "Epoch 62, Batch 11337, Loss: 183.88470458984375\n",
      "Epoch 62, Batch 11338, Loss: 164.8076934814453\n",
      "Epoch 62, Batch 11339, Loss: 165.70455932617188\n",
      "Epoch 62, Batch 11340, Loss: 183.48507690429688\n",
      "Epoch 62, Batch 11341, Loss: 171.94175720214844\n",
      "Epoch 62, Batch 11342, Loss: 169.281494140625\n",
      "Epoch 62, Batch 11343, Loss: 174.02952575683594\n",
      "Epoch 62, Batch 11344, Loss: 175.7245635986328\n",
      "Epoch 62, Batch 11345, Loss: 157.69839477539062\n",
      "Epoch 62, Batch 11346, Loss: 172.73219299316406\n",
      "Epoch 62, Batch 11347, Loss: 185.18214416503906\n",
      "Epoch 62, Batch 11348, Loss: 192.7498779296875\n",
      "Epoch 62, Batch 11349, Loss: 172.5319061279297\n",
      "Epoch 62, Batch 11350, Loss: 200.51600646972656\n",
      "Epoch 62, Batch 11351, Loss: 164.59262084960938\n",
      "Epoch 62, Batch 11352, Loss: 168.2574005126953\n",
      "Epoch 62, Batch 11353, Loss: 164.25038146972656\n",
      "Epoch 62, Batch 11354, Loss: 175.89852905273438\n",
      "Epoch 62, Batch 11355, Loss: 169.11221313476562\n",
      "Epoch 62, Batch 11356, Loss: 152.63783264160156\n",
      "Epoch 62, Batch 11357, Loss: 182.60360717773438\n",
      "Epoch 62, Batch 11358, Loss: 195.8341522216797\n",
      "Epoch 62, Batch 11359, Loss: 177.07217407226562\n",
      "Epoch 62, Batch 11360, Loss: 176.30332946777344\n",
      "Epoch 62, Batch 11361, Loss: 174.59239196777344\n",
      "Epoch 62, Batch 11362, Loss: 178.57400512695312\n",
      "Epoch 62, Batch 11363, Loss: 178.04519653320312\n",
      "Epoch 62, Batch 11364, Loss: 174.714111328125\n",
      "Epoch 62, Batch 11365, Loss: 171.34359741210938\n",
      "Epoch 62, Batch 11366, Loss: 180.77708435058594\n",
      "Epoch 62, Batch 11367, Loss: 172.4225616455078\n",
      "Epoch 62, Batch 11368, Loss: 162.93809509277344\n",
      "Epoch 62, Batch 11369, Loss: 183.5215606689453\n",
      "Epoch 62, Batch 11370, Loss: 166.97515869140625\n",
      "Epoch 62, Batch 11371, Loss: 176.3790740966797\n",
      "Epoch 62, Batch 11372, Loss: 169.77706909179688\n",
      "Epoch 62, Batch 11373, Loss: 177.5859375\n",
      "Epoch 62, Batch 11374, Loss: 164.93081665039062\n",
      "Epoch 62, Batch 11375, Loss: 184.68328857421875\n",
      "Epoch 62, Batch 11376, Loss: 177.70098876953125\n",
      "Epoch 62, Batch 11377, Loss: 174.60227966308594\n",
      "Epoch 62, Batch 11378, Loss: 175.73287963867188\n",
      "Epoch 62, Batch 11379, Loss: 174.56495666503906\n",
      "Epoch 62, Batch 11380, Loss: 183.6432647705078\n",
      "Epoch 62, Batch 11381, Loss: 185.73037719726562\n",
      "Epoch 62, Batch 11382, Loss: 199.22442626953125\n",
      "Epoch 62, Batch 11383, Loss: 169.62351989746094\n",
      "Epoch 62, Batch 11384, Loss: 176.56954956054688\n",
      "Epoch 62, Batch 11385, Loss: 163.22576904296875\n",
      "Epoch 62, Batch 11386, Loss: 194.4353485107422\n",
      "Epoch 62, Batch 11387, Loss: 168.6564483642578\n",
      "Epoch 62, Batch 11388, Loss: 168.8662567138672\n",
      "Epoch 62, Batch 11389, Loss: 174.44390869140625\n",
      "Epoch 62, Batch 11390, Loss: 172.73277282714844\n",
      "Epoch 62, Batch 11391, Loss: 184.94168090820312\n",
      "Epoch 62, Batch 11392, Loss: 184.0096435546875\n",
      "Epoch 62, Batch 11393, Loss: 178.60813903808594\n",
      "Epoch 62, Batch 11394, Loss: 172.20138549804688\n",
      "Epoch 62, Batch 11395, Loss: 168.780029296875\n",
      "Epoch 62, Batch 11396, Loss: 173.9615020751953\n",
      "Epoch 62, Batch 11397, Loss: 159.91468811035156\n",
      "Epoch 62, Batch 11398, Loss: 179.99325561523438\n",
      "Epoch 62, Batch 11399, Loss: 170.4134521484375\n",
      "Epoch 62, Batch 11400, Loss: 179.72396850585938\n",
      "Epoch 62, Batch 11401, Loss: 174.48614501953125\n",
      "Epoch 62, Batch 11402, Loss: 172.3442840576172\n",
      "Epoch 62, Batch 11403, Loss: 168.19338989257812\n",
      "Epoch 62, Batch 11404, Loss: 187.92764282226562\n",
      "Epoch 62, Batch 11405, Loss: 165.71585083007812\n",
      "Epoch 62, Batch 11406, Loss: 173.85287475585938\n",
      "Epoch 62, Batch 11407, Loss: 172.2550506591797\n",
      "Epoch 62, Batch 11408, Loss: 178.1658172607422\n",
      "Epoch 62, Batch 11409, Loss: 180.8218994140625\n",
      "Epoch 62, Batch 11410, Loss: 156.83633422851562\n",
      "Epoch 62, Batch 11411, Loss: 179.88829040527344\n",
      "Epoch 62, Batch 11412, Loss: 172.92416381835938\n",
      "Epoch 62, Batch 11413, Loss: 189.11181640625\n",
      "Epoch 62, Batch 11414, Loss: 157.38363647460938\n",
      "Epoch 62, Batch 11415, Loss: 179.75843811035156\n",
      "Epoch 62, Batch 11416, Loss: 173.5410614013672\n",
      "Epoch 62, Batch 11417, Loss: 173.8807830810547\n",
      "Epoch 62, Batch 11418, Loss: 164.42230224609375\n",
      "Epoch 62, Batch 11419, Loss: 166.71163940429688\n",
      "Epoch 62, Batch 11420, Loss: 183.08840942382812\n",
      "Epoch 62, Batch 11421, Loss: 151.35330200195312\n",
      "Epoch 62, Batch 11422, Loss: 161.09957885742188\n",
      "Epoch 62, Batch 11423, Loss: 169.443359375\n",
      "Epoch 62, Batch 11424, Loss: 165.25770568847656\n",
      "Epoch 62, Batch 11425, Loss: 182.8384246826172\n",
      "Epoch 62, Batch 11426, Loss: 193.719970703125\n",
      "Epoch 62, Batch 11427, Loss: 166.60092163085938\n",
      "Epoch 62, Batch 11428, Loss: 175.40924072265625\n",
      "Epoch 62, Batch 11429, Loss: 158.26473999023438\n",
      "Epoch 62, Batch 11430, Loss: 174.8150177001953\n",
      "Epoch 62, Batch 11431, Loss: 170.1142120361328\n",
      "Epoch 62, Batch 11432, Loss: 173.54136657714844\n",
      "Epoch 62, Batch 11433, Loss: 181.08981323242188\n",
      "Epoch 62, Batch 11434, Loss: 170.4522247314453\n",
      "Epoch 62, Batch 11435, Loss: 161.70680236816406\n",
      "Epoch 62, Batch 11436, Loss: 183.17868041992188\n",
      "Epoch 62, Batch 11437, Loss: 166.7021942138672\n",
      "Epoch 62, Batch 11438, Loss: 158.78338623046875\n",
      "Epoch 62, Batch 11439, Loss: 176.90899658203125\n",
      "Epoch 62, Batch 11440, Loss: 178.01303100585938\n",
      "Epoch 62, Batch 11441, Loss: 167.58860778808594\n",
      "Epoch 62, Batch 11442, Loss: 179.8417510986328\n",
      "Epoch 62, Batch 11443, Loss: 164.66073608398438\n",
      "Epoch 62, Batch 11444, Loss: 177.86033630371094\n",
      "Epoch 62, Batch 11445, Loss: 188.16249084472656\n",
      "Epoch 62, Batch 11446, Loss: 156.19139099121094\n",
      "Epoch 62, Batch 11447, Loss: 184.05792236328125\n",
      "Epoch 62, Batch 11448, Loss: 166.0431671142578\n",
      "Epoch 62, Batch 11449, Loss: 186.58058166503906\n",
      "Epoch 62, Batch 11450, Loss: 165.46852111816406\n",
      "Epoch 62, Batch 11451, Loss: 175.25949096679688\n",
      "Epoch 62, Batch 11452, Loss: 181.12928771972656\n",
      "Epoch 62, Batch 11453, Loss: 154.05641174316406\n",
      "Epoch 62, Batch 11454, Loss: 185.72915649414062\n",
      "Epoch 62, Batch 11455, Loss: 164.68247985839844\n",
      "Epoch 62, Batch 11456, Loss: 165.95956420898438\n",
      "Epoch 62, Batch 11457, Loss: 178.22621154785156\n",
      "Epoch 62, Batch 11458, Loss: 166.87362670898438\n",
      "Epoch 62, Batch 11459, Loss: 170.0834503173828\n",
      "Epoch 62, Batch 11460, Loss: 165.5882110595703\n",
      "Epoch 62, Batch 11461, Loss: 184.0315704345703\n",
      "Epoch 62, Batch 11462, Loss: 158.1455841064453\n",
      "Epoch 62, Batch 11463, Loss: 172.02664184570312\n",
      "Epoch 62, Batch 11464, Loss: 164.00418090820312\n",
      "Epoch 62, Batch 11465, Loss: 165.51708984375\n",
      "Epoch 62, Batch 11466, Loss: 183.1387481689453\n",
      "Epoch 62, Batch 11467, Loss: 185.01702880859375\n",
      "Epoch 62, Batch 11468, Loss: 181.95339965820312\n",
      "Epoch 62, Batch 11469, Loss: 171.68301391601562\n",
      "Epoch 62, Batch 11470, Loss: 161.62535095214844\n",
      "Epoch 62, Batch 11471, Loss: 200.56570434570312\n",
      "Epoch 62, Batch 11472, Loss: 168.16885375976562\n",
      "Epoch 62, Batch 11473, Loss: 180.07357788085938\n",
      "Epoch 62, Batch 11474, Loss: 179.64187622070312\n",
      "Epoch 62, Batch 11475, Loss: 183.38876342773438\n",
      "Epoch 62, Batch 11476, Loss: 178.08563232421875\n",
      "Epoch 62, Batch 11477, Loss: 170.43309020996094\n",
      "Epoch 62, Batch 11478, Loss: 193.3386993408203\n",
      "Epoch 62, Batch 11479, Loss: 162.02792358398438\n",
      "Epoch 62, Batch 11480, Loss: 171.32313537597656\n",
      "Epoch 62, Batch 11481, Loss: 162.59664916992188\n",
      "Epoch 62, Batch 11482, Loss: 180.49806213378906\n",
      "Epoch 62, Batch 11483, Loss: 171.0919952392578\n",
      "Epoch 62, Batch 11484, Loss: 180.608642578125\n",
      "Epoch 62, Batch 11485, Loss: 186.78839111328125\n",
      "Epoch 62, Batch 11486, Loss: 163.36151123046875\n",
      "Epoch 62, Batch 11487, Loss: 163.69053649902344\n",
      "Epoch 62, Batch 11488, Loss: 162.216796875\n",
      "Epoch 62, Batch 11489, Loss: 170.6081085205078\n",
      "Epoch 62, Batch 11490, Loss: 174.04685974121094\n",
      "Epoch 62, Batch 11491, Loss: 158.0146026611328\n",
      "Epoch 62, Batch 11492, Loss: 159.91297912597656\n",
      "Epoch 62, Batch 11493, Loss: 175.7417755126953\n",
      "Epoch 62, Batch 11494, Loss: 163.72096252441406\n",
      "Epoch 62, Batch 11495, Loss: 167.07864379882812\n",
      "Epoch 62, Batch 11496, Loss: 165.11135864257812\n",
      "Epoch 62, Batch 11497, Loss: 162.3630828857422\n",
      "Epoch 62, Batch 11498, Loss: 166.3663330078125\n",
      "Epoch 62, Batch 11499, Loss: 178.99212646484375\n",
      "Epoch 62, Batch 11500, Loss: 172.7626953125\n",
      "Epoch 62, Batch 11501, Loss: 176.3220977783203\n",
      "Epoch 62, Batch 11502, Loss: 169.0096435546875\n",
      "Epoch 62, Batch 11503, Loss: 181.9447784423828\n",
      "Epoch 62, Batch 11504, Loss: 173.7384033203125\n",
      "Epoch 62, Batch 11505, Loss: 182.40505981445312\n",
      "Epoch 62, Batch 11506, Loss: 160.82174682617188\n",
      "Epoch 62, Batch 11507, Loss: 185.87741088867188\n",
      "Epoch 62, Batch 11508, Loss: 162.724853515625\n",
      "Epoch 62, Batch 11509, Loss: 167.18045043945312\n",
      "Epoch 62, Batch 11510, Loss: 157.3245391845703\n",
      "Epoch 62, Batch 11511, Loss: 180.54739379882812\n",
      "Epoch 62, Batch 11512, Loss: 184.57373046875\n",
      "Epoch 62, Batch 11513, Loss: 156.5146026611328\n",
      "Epoch 62, Batch 11514, Loss: 165.5985870361328\n",
      "Epoch 62, Batch 11515, Loss: 182.69981384277344\n",
      "Epoch 62, Batch 11516, Loss: 166.62411499023438\n",
      "Epoch 62, Batch 11517, Loss: 164.31752014160156\n",
      "Epoch 62, Batch 11518, Loss: 178.87692260742188\n",
      "Epoch 62, Batch 11519, Loss: 158.52101135253906\n",
      "Epoch 62, Batch 11520, Loss: 156.48477172851562\n",
      "Epoch 62, Batch 11521, Loss: 182.18020629882812\n",
      "Epoch 62, Batch 11522, Loss: 166.4631805419922\n",
      "Epoch 62, Batch 11523, Loss: 178.66989135742188\n",
      "Epoch 62, Batch 11524, Loss: 173.04739379882812\n",
      "Epoch 62, Batch 11525, Loss: 172.2855987548828\n",
      "Epoch 62, Batch 11526, Loss: 187.96746826171875\n",
      "Epoch 62, Batch 11527, Loss: 165.9818115234375\n",
      "Epoch 62, Batch 11528, Loss: 159.34840393066406\n",
      "Epoch 62, Batch 11529, Loss: 169.68617248535156\n",
      "Epoch 62, Batch 11530, Loss: 181.4258575439453\n",
      "Epoch 62, Batch 11531, Loss: 172.49705505371094\n",
      "Epoch 62, Batch 11532, Loss: 182.78192138671875\n",
      "Epoch 62, Batch 11533, Loss: 192.96275329589844\n",
      "Epoch 62, Batch 11534, Loss: 167.2455291748047\n",
      "Epoch 62, Batch 11535, Loss: 158.9176025390625\n",
      "Epoch 62, Batch 11536, Loss: 199.37518310546875\n",
      "Epoch 62, Batch 11537, Loss: 192.18231201171875\n",
      "Epoch 62, Batch 11538, Loss: 171.37950134277344\n",
      "Epoch 62, Batch 11539, Loss: 161.24046325683594\n",
      "Epoch 62, Batch 11540, Loss: 171.43026733398438\n",
      "Epoch 62, Batch 11541, Loss: 188.49440002441406\n",
      "Epoch 62, Batch 11542, Loss: 166.10203552246094\n",
      "Epoch 62, Batch 11543, Loss: 175.26016235351562\n",
      "Epoch 62, Batch 11544, Loss: 163.6968231201172\n",
      "Epoch 62, Batch 11545, Loss: 184.99972534179688\n",
      "Epoch 62, Batch 11546, Loss: 170.6020050048828\n",
      "Epoch 62, Batch 11547, Loss: 173.35626220703125\n",
      "Epoch 62, Batch 11548, Loss: 159.77764892578125\n",
      "Epoch 62, Batch 11549, Loss: 160.3313446044922\n",
      "Epoch 62, Batch 11550, Loss: 174.95474243164062\n",
      "Epoch 62, Batch 11551, Loss: 159.6610565185547\n",
      "Epoch 62, Batch 11552, Loss: 171.12477111816406\n",
      "Epoch 62, Batch 11553, Loss: 176.3111572265625\n",
      "Epoch 62, Batch 11554, Loss: 166.65122985839844\n",
      "Epoch 62, Batch 11555, Loss: 168.64199829101562\n",
      "Epoch 62, Batch 11556, Loss: 178.43386840820312\n",
      "Epoch 62, Batch 11557, Loss: 172.01971435546875\n",
      "Epoch 62, Batch 11558, Loss: 175.73106384277344\n",
      "Epoch 62, Batch 11559, Loss: 173.22694396972656\n",
      "Epoch 62, Batch 11560, Loss: 170.62408447265625\n",
      "Epoch 62, Batch 11561, Loss: 183.3282012939453\n",
      "Epoch 62, Batch 11562, Loss: 162.45083618164062\n",
      "Epoch 62, Batch 11563, Loss: 176.38595581054688\n",
      "Epoch 62, Batch 11564, Loss: 172.31158447265625\n",
      "Epoch 62, Batch 11565, Loss: 171.05081176757812\n",
      "Epoch 62, Batch 11566, Loss: 174.3006591796875\n",
      "Epoch 62, Batch 11567, Loss: 183.601806640625\n",
      "Epoch 62, Batch 11568, Loss: 177.16952514648438\n",
      "Epoch 62, Batch 11569, Loss: 171.9793701171875\n",
      "Epoch 62, Batch 11570, Loss: 183.4740447998047\n",
      "Epoch 62, Batch 11571, Loss: 166.49209594726562\n",
      "Epoch 62, Batch 11572, Loss: 163.99427795410156\n",
      "Epoch 62, Batch 11573, Loss: 177.12686157226562\n",
      "Epoch 62, Batch 11574, Loss: 171.8639373779297\n",
      "Epoch 62, Batch 11575, Loss: 182.306640625\n",
      "Epoch 62, Batch 11576, Loss: 180.78175354003906\n",
      "Epoch 62, Batch 11577, Loss: 165.12681579589844\n",
      "Epoch 62, Batch 11578, Loss: 174.19332885742188\n",
      "Epoch 62, Batch 11579, Loss: 181.47491455078125\n",
      "Epoch 62, Batch 11580, Loss: 164.451904296875\n",
      "Epoch 62, Batch 11581, Loss: 175.02232360839844\n",
      "Epoch 62, Batch 11582, Loss: 161.45758056640625\n",
      "Epoch 62, Batch 11583, Loss: 177.01170349121094\n",
      "Epoch 62, Batch 11584, Loss: 181.37026977539062\n",
      "Epoch 62, Batch 11585, Loss: 177.45159912109375\n",
      "Epoch 62, Batch 11586, Loss: 160.6756134033203\n",
      "Epoch 62, Batch 11587, Loss: 176.65219116210938\n",
      "Epoch 62, Batch 11588, Loss: 173.1861114501953\n",
      "Epoch 62, Batch 11589, Loss: 173.81869506835938\n",
      "Epoch 62, Batch 11590, Loss: 176.4266357421875\n",
      "Epoch 62, Batch 11591, Loss: 152.8337860107422\n",
      "Epoch 62, Batch 11592, Loss: 161.72276306152344\n",
      "Epoch 62, Batch 11593, Loss: 162.4042510986328\n",
      "Epoch 62, Batch 11594, Loss: 172.0716552734375\n",
      "Epoch 62, Batch 11595, Loss: 159.83551025390625\n",
      "Epoch 62, Batch 11596, Loss: 174.05039978027344\n",
      "Epoch 62, Batch 11597, Loss: 180.94088745117188\n",
      "Epoch 62, Batch 11598, Loss: 180.28030395507812\n",
      "Epoch 62, Batch 11599, Loss: 161.24215698242188\n",
      "Epoch 62, Batch 11600, Loss: 189.5500030517578\n",
      "Epoch 62, Batch 11601, Loss: 178.0206298828125\n",
      "Epoch 62, Batch 11602, Loss: 198.26593017578125\n",
      "Epoch 62, Batch 11603, Loss: 167.02676391601562\n",
      "Epoch 62, Batch 11604, Loss: 171.8800506591797\n",
      "Epoch 62, Batch 11605, Loss: 166.50454711914062\n",
      "Epoch 62, Batch 11606, Loss: 151.22996520996094\n",
      "Epoch 62, Batch 11607, Loss: 169.50328063964844\n",
      "Epoch 62, Batch 11608, Loss: 157.5094757080078\n",
      "Epoch 62, Batch 11609, Loss: 185.13731384277344\n",
      "Epoch 62, Batch 11610, Loss: 166.92996215820312\n",
      "Epoch 62, Batch 11611, Loss: 181.01231384277344\n",
      "Epoch 62, Batch 11612, Loss: 184.22088623046875\n",
      "Epoch 62, Batch 11613, Loss: 182.34548950195312\n",
      "Epoch 62, Batch 11614, Loss: 181.1707305908203\n",
      "Epoch 62, Batch 11615, Loss: 172.77561950683594\n",
      "Epoch 62, Batch 11616, Loss: 165.27252197265625\n",
      "Epoch 62, Batch 11617, Loss: 179.01625061035156\n",
      "Epoch 62, Batch 11618, Loss: 180.0546875\n",
      "Epoch 62, Batch 11619, Loss: 170.16111755371094\n",
      "Epoch 62, Batch 11620, Loss: 171.10166931152344\n",
      "Epoch 62, Batch 11621, Loss: 164.5052947998047\n",
      "Epoch 62, Batch 11622, Loss: 162.87681579589844\n",
      "Epoch 62, Batch 11623, Loss: 169.9494171142578\n",
      "Epoch 62, Batch 11624, Loss: 162.7606201171875\n",
      "Epoch 62, Batch 11625, Loss: 156.79513549804688\n",
      "Epoch 62, Batch 11626, Loss: 176.11578369140625\n",
      "Epoch 62, Batch 11627, Loss: 169.52450561523438\n",
      "Epoch 62, Batch 11628, Loss: 162.07818603515625\n",
      "Epoch 62, Batch 11629, Loss: 179.3070831298828\n",
      "Epoch 62, Batch 11630, Loss: 181.43759155273438\n",
      "Epoch 62, Batch 11631, Loss: 172.36068725585938\n",
      "Epoch 62, Batch 11632, Loss: 187.8987274169922\n",
      "Epoch 62, Batch 11633, Loss: 176.34486389160156\n",
      "Epoch 62, Batch 11634, Loss: 163.88648986816406\n",
      "Epoch 62, Batch 11635, Loss: 171.40390014648438\n",
      "Epoch 62, Batch 11636, Loss: 175.3502197265625\n",
      "Epoch 62, Batch 11637, Loss: 153.72340393066406\n",
      "Epoch 62, Batch 11638, Loss: 174.42282104492188\n",
      "Epoch 62, Batch 11639, Loss: 190.87191772460938\n",
      "Epoch 62, Batch 11640, Loss: 150.46844482421875\n",
      "Epoch 62, Batch 11641, Loss: 180.09812927246094\n",
      "Epoch 62, Batch 11642, Loss: 168.53306579589844\n",
      "Epoch 62, Batch 11643, Loss: 179.42124938964844\n",
      "Epoch 62, Batch 11644, Loss: 155.7066650390625\n",
      "Epoch 62, Batch 11645, Loss: 167.89141845703125\n",
      "Epoch 62, Batch 11646, Loss: 167.48687744140625\n",
      "Epoch 62, Batch 11647, Loss: 187.0511932373047\n",
      "Epoch 62, Batch 11648, Loss: 185.45462036132812\n",
      "Epoch 62, Batch 11649, Loss: 173.51361083984375\n",
      "Epoch 62, Batch 11650, Loss: 171.6869659423828\n",
      "Epoch 62, Batch 11651, Loss: 174.09571838378906\n",
      "Epoch 62, Batch 11652, Loss: 177.00997924804688\n",
      "Epoch 62, Batch 11653, Loss: 182.82705688476562\n",
      "Epoch 62, Batch 11654, Loss: 172.08270263671875\n",
      "Epoch 62, Batch 11655, Loss: 187.21566772460938\n",
      "Epoch 62, Batch 11656, Loss: 165.3090057373047\n",
      "Epoch 62, Batch 11657, Loss: 182.29991149902344\n",
      "Epoch 62, Batch 11658, Loss: 157.42025756835938\n",
      "Epoch 62, Batch 11659, Loss: 187.7748260498047\n",
      "Epoch 62, Batch 11660, Loss: 179.72235107421875\n",
      "Epoch 62, Batch 11661, Loss: 157.68104553222656\n",
      "Epoch 62, Batch 11662, Loss: 171.1752166748047\n",
      "Epoch 62, Batch 11663, Loss: 163.5665740966797\n",
      "Epoch 62, Batch 11664, Loss: 167.77371215820312\n",
      "Epoch 62, Batch 11665, Loss: 172.05841064453125\n",
      "Epoch 62, Batch 11666, Loss: 168.14456176757812\n",
      "Epoch 62, Batch 11667, Loss: 185.8894500732422\n",
      "Epoch 62, Batch 11668, Loss: 166.91656494140625\n",
      "Epoch 62, Batch 11669, Loss: 167.6794891357422\n",
      "Epoch 62, Batch 11670, Loss: 180.17645263671875\n",
      "Epoch 62, Batch 11671, Loss: 167.66189575195312\n",
      "Epoch 62, Batch 11672, Loss: 168.4965362548828\n",
      "Epoch 62, Batch 11673, Loss: 176.586181640625\n",
      "Epoch 62, Batch 11674, Loss: 170.78594970703125\n",
      "Epoch 62, Batch 11675, Loss: 154.9603271484375\n",
      "Epoch 62, Batch 11676, Loss: 189.60690307617188\n",
      "Epoch 62, Batch 11677, Loss: 159.53916931152344\n",
      "Epoch 62, Batch 11678, Loss: 160.36192321777344\n",
      "Epoch 62, Batch 11679, Loss: 177.37255859375\n",
      "Epoch 62, Batch 11680, Loss: 164.3346405029297\n",
      "Epoch 62, Batch 11681, Loss: 171.3378448486328\n",
      "Epoch 62, Batch 11682, Loss: 182.9515380859375\n",
      "Epoch 62, Batch 11683, Loss: 200.42718505859375\n",
      "Epoch 62, Batch 11684, Loss: 193.38140869140625\n",
      "Epoch 62, Batch 11685, Loss: 172.85414123535156\n",
      "Epoch 62, Batch 11686, Loss: 167.18133544921875\n",
      "Epoch 62, Batch 11687, Loss: 165.65008544921875\n",
      "Epoch 62, Batch 11688, Loss: 170.60006713867188\n",
      "Epoch 62, Batch 11689, Loss: 170.98251342773438\n",
      "Epoch 62, Batch 11690, Loss: 174.38340759277344\n",
      "Epoch 62, Batch 11691, Loss: 175.23193359375\n",
      "Epoch 62, Batch 11692, Loss: 167.1509552001953\n",
      "Epoch 62, Batch 11693, Loss: 174.1533203125\n",
      "Epoch 62, Batch 11694, Loss: 191.99432373046875\n",
      "Epoch 62, Batch 11695, Loss: 163.89683532714844\n",
      "Epoch 62, Batch 11696, Loss: 171.659423828125\n",
      "Epoch 62, Batch 11697, Loss: 172.61669921875\n",
      "Epoch 62, Batch 11698, Loss: 168.41000366210938\n",
      "Epoch 62, Batch 11699, Loss: 156.12112426757812\n",
      "Epoch 62, Batch 11700, Loss: 194.53160095214844\n",
      "Epoch 62, Batch 11701, Loss: 185.0732421875\n",
      "Epoch 62, Batch 11702, Loss: 189.5732879638672\n",
      "Epoch 62, Batch 11703, Loss: 184.30667114257812\n",
      "Epoch 62, Batch 11704, Loss: 176.62416076660156\n",
      "Epoch 62, Batch 11705, Loss: 190.10009765625\n",
      "Epoch 62, Batch 11706, Loss: 171.91839599609375\n",
      "Epoch 62, Batch 11707, Loss: 154.48504638671875\n",
      "Epoch 62, Batch 11708, Loss: 176.9831085205078\n",
      "Epoch 62, Batch 11709, Loss: 185.68174743652344\n",
      "Epoch 62, Batch 11710, Loss: 172.11630249023438\n",
      "Epoch 62, Batch 11711, Loss: 185.0853271484375\n",
      "Epoch 62, Batch 11712, Loss: 187.16709899902344\n",
      "Epoch 62, Batch 11713, Loss: 178.1680450439453\n",
      "Epoch 62, Batch 11714, Loss: 163.5355987548828\n",
      "Epoch 62, Batch 11715, Loss: 179.72125244140625\n",
      "Epoch 62, Batch 11716, Loss: 154.30508422851562\n",
      "Epoch 62, Batch 11717, Loss: 175.6349334716797\n",
      "Epoch 62, Batch 11718, Loss: 184.86305236816406\n",
      "Epoch 62, Batch 11719, Loss: 148.08566284179688\n",
      "Epoch 62, Batch 11720, Loss: 180.78282165527344\n",
      "Epoch 62, Batch 11721, Loss: 181.76158142089844\n",
      "Epoch 62, Batch 11722, Loss: 191.22720336914062\n",
      "Epoch 62, Batch 11723, Loss: 185.05715942382812\n",
      "Epoch 62, Batch 11724, Loss: 168.73971557617188\n",
      "Epoch 62, Batch 11725, Loss: 179.61050415039062\n",
      "Epoch 62, Batch 11726, Loss: 177.27122497558594\n",
      "Epoch 62, Batch 11727, Loss: 175.1739501953125\n",
      "Epoch 62, Batch 11728, Loss: 174.2126007080078\n",
      "Epoch 62, Batch 11729, Loss: 171.27691650390625\n",
      "Epoch 62, Batch 11730, Loss: 177.99972534179688\n",
      "Epoch 62, Batch 11731, Loss: 175.76031494140625\n",
      "Epoch 62, Batch 11732, Loss: 176.41786193847656\n",
      "Epoch 62, Batch 11733, Loss: 167.92041015625\n",
      "Epoch 62, Batch 11734, Loss: 174.75997924804688\n",
      "Epoch 62, Batch 11735, Loss: 166.34039306640625\n",
      "Epoch 62, Batch 11736, Loss: 175.86651611328125\n",
      "Epoch 62, Batch 11737, Loss: 180.75730895996094\n",
      "Epoch 62, Batch 11738, Loss: 172.72637939453125\n",
      "Epoch 62, Batch 11739, Loss: 165.82786560058594\n",
      "Epoch 62, Batch 11740, Loss: 147.94883728027344\n",
      "Epoch 62, Batch 11741, Loss: 164.1317138671875\n",
      "Epoch 62, Batch 11742, Loss: 188.5673065185547\n",
      "Epoch 62, Batch 11743, Loss: 171.35775756835938\n",
      "Epoch 62, Batch 11744, Loss: 177.00350952148438\n",
      "Epoch 62, Batch 11745, Loss: 175.0883331298828\n",
      "Epoch 62, Batch 11746, Loss: 170.22691345214844\n",
      "Epoch 62, Batch 11747, Loss: 168.21115112304688\n",
      "Epoch 62, Batch 11748, Loss: 171.53981018066406\n",
      "Epoch 62, Batch 11749, Loss: 175.91522216796875\n",
      "Epoch 62, Batch 11750, Loss: 165.19227600097656\n",
      "Epoch 62, Batch 11751, Loss: 178.6050262451172\n",
      "Epoch 62, Batch 11752, Loss: 178.76907348632812\n",
      "Epoch 62, Batch 11753, Loss: 177.94642639160156\n",
      "Epoch 62, Batch 11754, Loss: 164.09556579589844\n",
      "Epoch 62, Batch 11755, Loss: 185.43174743652344\n",
      "Epoch 62, Batch 11756, Loss: 167.47181701660156\n",
      "Epoch 62, Batch 11757, Loss: 161.1889190673828\n",
      "Epoch 62, Batch 11758, Loss: 163.33963012695312\n",
      "Epoch 62, Batch 11759, Loss: 162.6931610107422\n",
      "Epoch 62, Batch 11760, Loss: 174.2449188232422\n",
      "Epoch 62, Batch 11761, Loss: 175.9816131591797\n",
      "Epoch 62, Batch 11762, Loss: 164.71630859375\n",
      "Epoch 62, Batch 11763, Loss: 193.56466674804688\n",
      "Epoch 62, Batch 11764, Loss: 183.82943725585938\n",
      "Epoch 62, Batch 11765, Loss: 171.3191680908203\n",
      "Epoch 62, Batch 11766, Loss: 180.5015869140625\n",
      "Epoch 62, Batch 11767, Loss: 163.37387084960938\n",
      "Epoch 62, Batch 11768, Loss: 162.7441864013672\n",
      "Epoch 62, Batch 11769, Loss: 164.47068786621094\n",
      "Epoch 62, Batch 11770, Loss: 173.02272033691406\n",
      "Epoch 62, Batch 11771, Loss: 176.68055725097656\n",
      "Epoch 62, Batch 11772, Loss: 169.6377716064453\n",
      "Epoch 62, Batch 11773, Loss: 178.4296417236328\n",
      "Epoch 62, Batch 11774, Loss: 170.68844604492188\n",
      "Epoch 62, Batch 11775, Loss: 170.87440490722656\n",
      "Epoch 62, Batch 11776, Loss: 165.67611694335938\n",
      "Epoch 62, Batch 11777, Loss: 178.4380340576172\n",
      "Epoch 62, Batch 11778, Loss: 160.00840759277344\n",
      "Epoch 62, Batch 11779, Loss: 190.26138305664062\n",
      "Epoch 62, Batch 11780, Loss: 173.1996307373047\n",
      "Epoch 62, Batch 11781, Loss: 176.60940551757812\n",
      "Epoch 62, Batch 11782, Loss: 157.02967834472656\n",
      "Epoch 62, Batch 11783, Loss: 174.12937927246094\n",
      "Epoch 62, Batch 11784, Loss: 169.43472290039062\n",
      "Epoch 62, Batch 11785, Loss: 183.28695678710938\n",
      "Epoch 62, Batch 11786, Loss: 185.30917358398438\n",
      "Epoch 62, Batch 11787, Loss: 164.80621337890625\n",
      "Epoch 62, Batch 11788, Loss: 181.8251495361328\n",
      "Epoch 62, Batch 11789, Loss: 168.42823791503906\n",
      "Epoch 62, Batch 11790, Loss: 164.31776428222656\n",
      "Epoch 62, Batch 11791, Loss: 161.9916229248047\n",
      "Epoch 62, Batch 11792, Loss: 175.84002685546875\n",
      "Epoch 62, Batch 11793, Loss: 187.5106201171875\n",
      "Epoch 62, Batch 11794, Loss: 176.41368103027344\n",
      "Epoch 62, Batch 11795, Loss: 149.9200897216797\n",
      "Epoch 62, Batch 11796, Loss: 170.5008544921875\n",
      "Epoch 62, Batch 11797, Loss: 159.01573181152344\n",
      "Epoch 62, Batch 11798, Loss: 190.6121063232422\n",
      "Epoch 62, Batch 11799, Loss: 196.9867401123047\n",
      "Epoch 62, Batch 11800, Loss: 167.7134246826172\n",
      "Epoch 62, Batch 11801, Loss: 174.94668579101562\n",
      "Epoch 62, Batch 11802, Loss: 166.0536651611328\n",
      "Epoch 62, Batch 11803, Loss: 164.68125915527344\n",
      "Epoch 62, Batch 11804, Loss: 173.42234802246094\n",
      "Epoch 62, Batch 11805, Loss: 184.61558532714844\n",
      "Epoch 62, Batch 11806, Loss: 166.0148468017578\n",
      "Epoch 62, Batch 11807, Loss: 155.27113342285156\n",
      "Epoch 62, Batch 11808, Loss: 175.2101593017578\n",
      "Epoch 62, Batch 11809, Loss: 190.5364532470703\n",
      "Epoch 62, Batch 11810, Loss: 177.2706298828125\n",
      "Epoch 62, Batch 11811, Loss: 156.0052947998047\n",
      "Epoch 62, Batch 11812, Loss: 178.93673706054688\n",
      "Epoch 62, Batch 11813, Loss: 161.76296997070312\n",
      "Epoch 62, Batch 11814, Loss: 180.2481231689453\n",
      "Epoch 62, Batch 11815, Loss: 174.7608642578125\n",
      "Epoch 62, Batch 11816, Loss: 166.51553344726562\n",
      "Epoch 62, Batch 11817, Loss: 168.466552734375\n",
      "Epoch 62, Batch 11818, Loss: 184.41574096679688\n",
      "Epoch 62, Batch 11819, Loss: 152.3390350341797\n",
      "Epoch 62, Batch 11820, Loss: 176.7569580078125\n",
      "Epoch 62, Batch 11821, Loss: 164.73350524902344\n",
      "Epoch 62, Batch 11822, Loss: 175.58047485351562\n",
      "Epoch 62, Batch 11823, Loss: 181.98048400878906\n",
      "Epoch 62, Batch 11824, Loss: 182.72178649902344\n",
      "Epoch 62, Batch 11825, Loss: 175.84939575195312\n",
      "Epoch 62, Batch 11826, Loss: 179.647216796875\n",
      "Epoch 62, Batch 11827, Loss: 184.883056640625\n",
      "Epoch 62, Batch 11828, Loss: 190.7427215576172\n",
      "Epoch 62, Batch 11829, Loss: 183.13600158691406\n",
      "Epoch 62, Batch 11830, Loss: 184.53717041015625\n",
      "Epoch 62, Batch 11831, Loss: 180.09524536132812\n",
      "Epoch 62, Batch 11832, Loss: 185.80259704589844\n",
      "Epoch 62, Batch 11833, Loss: 181.2867431640625\n",
      "Epoch 62, Batch 11834, Loss: 176.34091186523438\n",
      "Epoch 62, Batch 11835, Loss: 166.9771270751953\n",
      "Epoch 62, Batch 11836, Loss: 173.7103271484375\n",
      "Epoch 62, Batch 11837, Loss: 178.1150665283203\n",
      "Epoch 62, Batch 11838, Loss: 170.37889099121094\n",
      "Epoch 62, Batch 11839, Loss: 171.99710083007812\n",
      "Epoch 62, Batch 11840, Loss: 157.41128540039062\n",
      "Epoch 62, Batch 11841, Loss: 192.28787231445312\n",
      "Epoch 62, Batch 11842, Loss: 170.5144805908203\n",
      "Epoch 62, Batch 11843, Loss: 181.4451141357422\n",
      "Epoch 62, Batch 11844, Loss: 181.34254455566406\n",
      "Epoch 62, Batch 11845, Loss: 180.6326446533203\n",
      "Epoch 62, Batch 11846, Loss: 188.58155822753906\n",
      "Epoch 62, Batch 11847, Loss: 182.9276580810547\n",
      "Epoch 62, Batch 11848, Loss: 166.56822204589844\n",
      "Epoch 62, Batch 11849, Loss: 174.74880981445312\n",
      "Epoch 62, Batch 11850, Loss: 174.31802368164062\n",
      "Epoch 62, Batch 11851, Loss: 178.5984649658203\n",
      "Epoch 62, Batch 11852, Loss: 178.31527709960938\n",
      "Epoch 62, Batch 11853, Loss: 160.32965087890625\n",
      "Epoch 62, Batch 11854, Loss: 179.59396362304688\n",
      "Epoch 62, Batch 11855, Loss: 160.75140380859375\n",
      "Epoch 62, Batch 11856, Loss: 155.3096160888672\n",
      "Epoch 62, Batch 11857, Loss: 184.12399291992188\n",
      "Epoch 62, Batch 11858, Loss: 169.71905517578125\n",
      "Epoch 62, Batch 11859, Loss: 180.19517517089844\n",
      "Epoch 62, Batch 11860, Loss: 173.18858337402344\n",
      "Epoch 62, Batch 11861, Loss: 173.82810974121094\n",
      "Epoch 62, Batch 11862, Loss: 168.31837463378906\n",
      "Epoch 62, Batch 11863, Loss: 172.10693359375\n",
      "Epoch 62, Batch 11864, Loss: 175.51097106933594\n",
      "Epoch 62, Batch 11865, Loss: 164.61912536621094\n",
      "Epoch 62, Batch 11866, Loss: 160.64141845703125\n",
      "Epoch 62, Batch 11867, Loss: 178.63095092773438\n",
      "Epoch 62, Batch 11868, Loss: 170.00216674804688\n",
      "Epoch 62, Batch 11869, Loss: 156.8414764404297\n",
      "Epoch 62, Batch 11870, Loss: 175.3880615234375\n",
      "Epoch 62, Batch 11871, Loss: 171.4701385498047\n",
      "Epoch 62, Batch 11872, Loss: 182.50253295898438\n",
      "Epoch 62, Batch 11873, Loss: 157.09298706054688\n",
      "Epoch 62, Batch 11874, Loss: 171.277587890625\n",
      "Epoch 62, Batch 11875, Loss: 181.46200561523438\n",
      "Epoch 62, Batch 11876, Loss: 164.77430725097656\n",
      "Epoch 62, Batch 11877, Loss: 180.61859130859375\n",
      "Epoch 62, Batch 11878, Loss: 165.79261779785156\n",
      "Epoch 62, Batch 11879, Loss: 171.3255157470703\n",
      "Epoch 62, Batch 11880, Loss: 168.8133087158203\n",
      "Epoch 62, Batch 11881, Loss: 166.9752197265625\n",
      "Epoch 62, Batch 11882, Loss: 163.57147216796875\n",
      "Epoch 62, Batch 11883, Loss: 173.1721954345703\n",
      "Epoch 62, Batch 11884, Loss: 160.60675048828125\n",
      "Epoch 62, Batch 11885, Loss: 177.89901733398438\n",
      "Epoch 62, Batch 11886, Loss: 174.73007202148438\n",
      "Epoch 62, Batch 11887, Loss: 184.22291564941406\n",
      "Epoch 62, Batch 11888, Loss: 179.27163696289062\n",
      "Epoch 62, Batch 11889, Loss: 173.8283233642578\n",
      "Epoch 62, Batch 11890, Loss: 194.2664794921875\n",
      "Epoch 62, Batch 11891, Loss: 174.79852294921875\n",
      "Epoch 62, Batch 11892, Loss: 164.55686950683594\n",
      "Epoch 62, Batch 11893, Loss: 173.12173461914062\n",
      "Epoch 62, Batch 11894, Loss: 177.58279418945312\n",
      "Epoch 62, Batch 11895, Loss: 195.768798828125\n",
      "Epoch 62, Batch 11896, Loss: 164.76953125\n",
      "Epoch 62, Batch 11897, Loss: 187.433837890625\n",
      "Epoch 62, Batch 11898, Loss: 175.8625946044922\n",
      "Epoch 62, Batch 11899, Loss: 186.87283325195312\n",
      "Epoch 62, Batch 11900, Loss: 175.76052856445312\n",
      "Epoch 62, Batch 11901, Loss: 185.40440368652344\n",
      "Epoch 62, Batch 11902, Loss: 156.51608276367188\n",
      "Epoch 62, Batch 11903, Loss: 167.06297302246094\n",
      "Epoch 62, Batch 11904, Loss: 164.7317657470703\n",
      "Epoch 62, Batch 11905, Loss: 189.2483673095703\n",
      "Epoch 62, Batch 11906, Loss: 170.27940368652344\n",
      "Epoch 62, Batch 11907, Loss: 183.62713623046875\n",
      "Epoch 62, Batch 11908, Loss: 172.05001831054688\n",
      "Epoch 62, Batch 11909, Loss: 172.50747680664062\n",
      "Epoch 62, Batch 11910, Loss: 170.63458251953125\n",
      "Epoch 62, Batch 11911, Loss: 161.61038208007812\n",
      "Epoch 62, Batch 11912, Loss: 170.03436279296875\n",
      "Epoch 62, Batch 11913, Loss: 199.70343017578125\n",
      "Epoch 62, Batch 11914, Loss: 163.62635803222656\n",
      "Epoch 62, Batch 11915, Loss: 183.08164978027344\n",
      "Epoch 62, Batch 11916, Loss: 164.15560913085938\n",
      "Epoch 62, Batch 11917, Loss: 170.51419067382812\n",
      "Epoch 62, Batch 11918, Loss: 166.9228973388672\n",
      "Epoch 62, Batch 11919, Loss: 168.10946655273438\n",
      "Epoch 62, Batch 11920, Loss: 178.16082763671875\n",
      "Epoch 62, Batch 11921, Loss: 175.47463989257812\n",
      "Epoch 62, Batch 11922, Loss: 163.57301330566406\n",
      "Epoch 62, Batch 11923, Loss: 167.95379638671875\n",
      "Epoch 62, Batch 11924, Loss: 161.02984619140625\n",
      "Epoch 62, Batch 11925, Loss: 175.2763671875\n",
      "Epoch 62, Batch 11926, Loss: 198.18048095703125\n",
      "Epoch 62, Batch 11927, Loss: 180.58139038085938\n",
      "Epoch 62, Batch 11928, Loss: 173.19723510742188\n",
      "Epoch 62, Batch 11929, Loss: 176.18751525878906\n",
      "Epoch 62, Batch 11930, Loss: 174.75489807128906\n",
      "Epoch 62, Batch 11931, Loss: 168.3770751953125\n",
      "Epoch 62, Batch 11932, Loss: 167.3732147216797\n",
      "Epoch 62, Batch 11933, Loss: 175.06521606445312\n",
      "Epoch 62, Batch 11934, Loss: 171.9229736328125\n",
      "Epoch 62, Batch 11935, Loss: 186.16714477539062\n",
      "Epoch 62, Batch 11936, Loss: 175.5907745361328\n",
      "Epoch 62, Batch 11937, Loss: 177.63978576660156\n",
      "Epoch 62, Batch 11938, Loss: 175.35853576660156\n",
      "Epoch 62, Batch 11939, Loss: 162.0964813232422\n",
      "Epoch 62, Batch 11940, Loss: 172.81407165527344\n",
      "Epoch 62, Batch 11941, Loss: 169.21734619140625\n",
      "Epoch 62, Batch 11942, Loss: 173.2283935546875\n",
      "Epoch 62, Batch 11943, Loss: 195.7112579345703\n",
      "Epoch 62, Batch 11944, Loss: 189.56390380859375\n",
      "Epoch 62, Batch 11945, Loss: 186.03309631347656\n",
      "Epoch 62, Batch 11946, Loss: 165.05711364746094\n",
      "Epoch 62, Batch 11947, Loss: 170.31044006347656\n",
      "Epoch 62, Batch 11948, Loss: 160.3208770751953\n",
      "Epoch 62, Batch 11949, Loss: 173.128173828125\n",
      "Epoch 62, Batch 11950, Loss: 179.42884826660156\n",
      "Epoch 62, Batch 11951, Loss: 181.415283203125\n",
      "Epoch 62, Batch 11952, Loss: 168.48680114746094\n",
      "Epoch 62, Batch 11953, Loss: 179.7439727783203\n",
      "Epoch 62, Batch 11954, Loss: 156.66465759277344\n",
      "Epoch 62, Batch 11955, Loss: 184.432861328125\n",
      "Epoch 62, Batch 11956, Loss: 164.43763732910156\n",
      "Epoch 62, Batch 11957, Loss: 159.9759979248047\n",
      "Epoch 62, Batch 11958, Loss: 195.478515625\n",
      "Epoch 62, Batch 11959, Loss: 156.06692504882812\n",
      "Epoch 62, Batch 11960, Loss: 150.2185821533203\n",
      "Epoch 62, Batch 11961, Loss: 171.397705078125\n",
      "Epoch 62, Batch 11962, Loss: 180.22833251953125\n",
      "Epoch 62, Batch 11963, Loss: 175.66552734375\n",
      "Epoch 62, Batch 11964, Loss: 176.12680053710938\n",
      "Epoch 62, Batch 11965, Loss: 200.63812255859375\n",
      "Epoch 62, Batch 11966, Loss: 166.77249145507812\n",
      "Epoch 62, Batch 11967, Loss: 181.23721313476562\n",
      "Epoch 62, Batch 11968, Loss: 165.19163513183594\n",
      "Epoch 62, Batch 11969, Loss: 165.36756896972656\n",
      "Epoch 62, Batch 11970, Loss: 169.08575439453125\n",
      "Epoch 62, Batch 11971, Loss: 169.97520446777344\n",
      "Epoch 62, Batch 11972, Loss: 172.69703674316406\n",
      "Epoch 62, Batch 11973, Loss: 176.54150390625\n",
      "Epoch 62, Batch 11974, Loss: 165.79547119140625\n",
      "Epoch 62, Batch 11975, Loss: 163.3414764404297\n",
      "Epoch 62, Batch 11976, Loss: 169.16078186035156\n",
      "Epoch 62, Batch 11977, Loss: 151.4031219482422\n",
      "Epoch 62, Batch 11978, Loss: 146.71006774902344\n",
      "Epoch 62, Batch 11979, Loss: 181.69378662109375\n",
      "Epoch 62, Batch 11980, Loss: 156.52322387695312\n",
      "Epoch 62, Batch 11981, Loss: 164.56675720214844\n",
      "Epoch 62, Batch 11982, Loss: 165.23483276367188\n",
      "Epoch 62, Batch 11983, Loss: 179.52330017089844\n",
      "Epoch 62, Batch 11984, Loss: 180.22193908691406\n",
      "Epoch 62, Batch 11985, Loss: 168.26295471191406\n",
      "Epoch 62, Batch 11986, Loss: 175.5977325439453\n",
      "Epoch 62, Batch 11987, Loss: 177.10438537597656\n",
      "Epoch 62, Batch 11988, Loss: 183.10162353515625\n",
      "Epoch 62, Batch 11989, Loss: 179.90927124023438\n",
      "Epoch 62, Batch 11990, Loss: 159.10263061523438\n",
      "Epoch 62, Batch 11991, Loss: 168.12017822265625\n",
      "Epoch 62, Batch 11992, Loss: 170.41444396972656\n",
      "Epoch 62, Batch 11993, Loss: 164.0970916748047\n",
      "Epoch 62, Batch 11994, Loss: 182.50018310546875\n",
      "Epoch 62, Batch 11995, Loss: 160.3433837890625\n",
      "Epoch 62, Batch 11996, Loss: 175.11480712890625\n",
      "Epoch 62, Batch 11997, Loss: 159.43310546875\n",
      "Epoch 62, Batch 11998, Loss: 179.61326599121094\n",
      "Epoch 62, Batch 11999, Loss: 179.05865478515625\n",
      "Epoch 62, Batch 12000, Loss: 173.2740936279297\n",
      "Epoch 62, Batch 12001, Loss: 170.45803833007812\n",
      "Epoch 62, Batch 12002, Loss: 166.6151580810547\n",
      "Epoch 62, Batch 12003, Loss: 167.38937377929688\n",
      "Epoch 62, Batch 12004, Loss: 160.83287048339844\n",
      "Epoch 62, Batch 12005, Loss: 164.72509765625\n",
      "Epoch 62, Batch 12006, Loss: 163.52928161621094\n",
      "Epoch 62, Batch 12007, Loss: 173.974365234375\n",
      "Epoch 62, Batch 12008, Loss: 168.07183837890625\n",
      "Epoch 62, Batch 12009, Loss: 177.1968536376953\n",
      "Epoch 62, Batch 12010, Loss: 180.68832397460938\n",
      "Epoch 62, Batch 12011, Loss: 152.5888214111328\n",
      "Epoch 62, Batch 12012, Loss: 166.2442626953125\n",
      "Epoch 62, Batch 12013, Loss: 176.97634887695312\n",
      "Epoch 62, Batch 12014, Loss: 176.49134826660156\n",
      "Epoch 62, Batch 12015, Loss: 173.654296875\n",
      "Epoch 62, Batch 12016, Loss: 176.74085998535156\n",
      "Epoch 62, Batch 12017, Loss: 165.81883239746094\n",
      "Epoch 62, Batch 12018, Loss: 170.76644897460938\n",
      "Epoch 62, Batch 12019, Loss: 177.0009765625\n",
      "Epoch 62, Batch 12020, Loss: 167.2108154296875\n",
      "Epoch 62, Batch 12021, Loss: 166.1749725341797\n",
      "Epoch 62, Batch 12022, Loss: 175.1094970703125\n",
      "Epoch 62, Batch 12023, Loss: 167.62840270996094\n",
      "Epoch 62, Batch 12024, Loss: 176.1584014892578\n",
      "Epoch 62, Batch 12025, Loss: 172.90518188476562\n",
      "Epoch 62, Batch 12026, Loss: 190.60862731933594\n",
      "Epoch 62, Batch 12027, Loss: 170.8574981689453\n",
      "Epoch 62, Batch 12028, Loss: 158.1316375732422\n",
      "Epoch 62, Batch 12029, Loss: 160.68324279785156\n",
      "Epoch 62, Batch 12030, Loss: 177.84417724609375\n",
      "Epoch 62, Batch 12031, Loss: 169.7899932861328\n",
      "Epoch 62, Batch 12032, Loss: 172.4276885986328\n",
      "Epoch 62, Batch 12033, Loss: 182.14744567871094\n",
      "Epoch 62, Batch 12034, Loss: 197.89859008789062\n",
      "Epoch 62, Batch 12035, Loss: 173.1077880859375\n",
      "Epoch 62, Batch 12036, Loss: 163.9964599609375\n",
      "Epoch 62, Batch 12037, Loss: 170.43670654296875\n",
      "Epoch 62, Batch 12038, Loss: 149.7391357421875\n",
      "Epoch 62, Batch 12039, Loss: 165.2135772705078\n",
      "Epoch 62, Batch 12040, Loss: 172.27708435058594\n",
      "Epoch 62, Batch 12041, Loss: 169.2281494140625\n",
      "Epoch 62, Batch 12042, Loss: 185.12940979003906\n",
      "Epoch 62, Batch 12043, Loss: 181.80320739746094\n",
      "Epoch 62, Batch 12044, Loss: 181.10581970214844\n",
      "Epoch 62, Batch 12045, Loss: 173.2505340576172\n",
      "Epoch 62, Batch 12046, Loss: 165.21170043945312\n",
      "Epoch 62, Batch 12047, Loss: 172.3785400390625\n",
      "Epoch 62, Batch 12048, Loss: 159.46730041503906\n",
      "Epoch 62, Batch 12049, Loss: 172.8043670654297\n",
      "Epoch 62, Batch 12050, Loss: 178.69822692871094\n",
      "Epoch 62, Batch 12051, Loss: 175.99044799804688\n",
      "Epoch 62, Batch 12052, Loss: 177.07501220703125\n",
      "Epoch 62, Batch 12053, Loss: 178.905517578125\n",
      "Epoch 62, Batch 12054, Loss: 162.68186950683594\n",
      "Epoch 62, Batch 12055, Loss: 172.58261108398438\n",
      "Epoch 62, Batch 12056, Loss: 170.3184814453125\n",
      "Epoch 62, Batch 12057, Loss: 166.46896362304688\n",
      "Epoch 62, Batch 12058, Loss: 184.9481658935547\n",
      "Epoch 62, Batch 12059, Loss: 181.69224548339844\n",
      "Epoch 62, Batch 12060, Loss: 164.9878692626953\n",
      "Epoch 62, Batch 12061, Loss: 173.56875610351562\n",
      "Epoch 62, Batch 12062, Loss: 170.3553924560547\n",
      "Epoch 62, Batch 12063, Loss: 182.0145721435547\n",
      "Epoch 62, Batch 12064, Loss: 185.6300811767578\n",
      "Epoch 62, Batch 12065, Loss: 167.5933837890625\n",
      "Epoch 62, Batch 12066, Loss: 178.10903930664062\n",
      "Epoch 62, Batch 12067, Loss: 163.82363891601562\n",
      "Epoch 62, Batch 12068, Loss: 170.65890502929688\n",
      "Epoch 62, Batch 12069, Loss: 157.68861389160156\n",
      "Epoch 62, Batch 12070, Loss: 168.8734588623047\n",
      "Epoch 62, Batch 12071, Loss: 179.49972534179688\n",
      "Epoch 62, Batch 12072, Loss: 170.12399291992188\n",
      "Epoch 62, Batch 12073, Loss: 171.57740783691406\n",
      "Epoch 62, Batch 12074, Loss: 169.85861206054688\n",
      "Epoch 62, Batch 12075, Loss: 158.13568115234375\n",
      "Epoch 62, Batch 12076, Loss: 155.72781372070312\n",
      "Epoch 62, Batch 12077, Loss: 185.776611328125\n",
      "Epoch 62, Batch 12078, Loss: 173.0738067626953\n",
      "Epoch 62, Batch 12079, Loss: 189.44088745117188\n",
      "Epoch 62, Batch 12080, Loss: 179.16844177246094\n",
      "Epoch 62, Batch 12081, Loss: 178.1920623779297\n",
      "Epoch 62, Batch 12082, Loss: 174.8903045654297\n",
      "Epoch 62, Batch 12083, Loss: 161.4734649658203\n",
      "Epoch 62, Batch 12084, Loss: 177.47750854492188\n",
      "Epoch 62, Batch 12085, Loss: 163.01327514648438\n",
      "Epoch 62, Batch 12086, Loss: 174.88560485839844\n",
      "Epoch 62, Batch 12087, Loss: 174.70703125\n",
      "Epoch 62, Batch 12088, Loss: 163.4515838623047\n",
      "Epoch 62, Batch 12089, Loss: 162.05972290039062\n",
      "Epoch 62, Batch 12090, Loss: 169.00486755371094\n",
      "Epoch 62, Batch 12091, Loss: 166.9716796875\n",
      "Epoch 62, Batch 12092, Loss: 160.7283477783203\n",
      "Epoch 62, Batch 12093, Loss: 169.73867797851562\n",
      "Epoch 62, Batch 12094, Loss: 162.41883850097656\n",
      "Epoch 62, Batch 12095, Loss: 196.11927795410156\n",
      "Epoch 62, Batch 12096, Loss: 180.2684326171875\n",
      "Epoch 62, Batch 12097, Loss: 196.1703643798828\n",
      "Epoch 62, Batch 12098, Loss: 195.31942749023438\n",
      "Epoch 62, Batch 12099, Loss: 183.32916259765625\n",
      "Epoch 62, Batch 12100, Loss: 176.03660583496094\n",
      "Epoch 62, Batch 12101, Loss: 167.09048461914062\n",
      "Epoch 62, Batch 12102, Loss: 175.15733337402344\n",
      "Epoch 62, Batch 12103, Loss: 164.62535095214844\n",
      "Epoch 62, Batch 12104, Loss: 183.24505615234375\n",
      "Epoch 62, Batch 12105, Loss: 156.3476104736328\n",
      "Epoch 62, Batch 12106, Loss: 174.94862365722656\n",
      "Epoch 62, Batch 12107, Loss: 174.563720703125\n",
      "Epoch 62, Batch 12108, Loss: 171.47463989257812\n",
      "Epoch 62, Batch 12109, Loss: 166.09718322753906\n",
      "Epoch 62, Batch 12110, Loss: 160.89532470703125\n",
      "Epoch 62, Batch 12111, Loss: 182.79005432128906\n",
      "Epoch 62, Batch 12112, Loss: 161.8321533203125\n",
      "Epoch 62, Batch 12113, Loss: 150.45407104492188\n",
      "Epoch 62, Batch 12114, Loss: 175.4070587158203\n",
      "Epoch 62, Batch 12115, Loss: 174.29283142089844\n",
      "Epoch 62, Batch 12116, Loss: 175.38888549804688\n",
      "Epoch 62, Batch 12117, Loss: 189.94154357910156\n",
      "Epoch 62, Batch 12118, Loss: 177.75315856933594\n",
      "Epoch 62, Batch 12119, Loss: 179.61614990234375\n",
      "Epoch 62, Batch 12120, Loss: 173.78553771972656\n",
      "Epoch 62, Batch 12121, Loss: 174.0657501220703\n",
      "Epoch 62, Batch 12122, Loss: 178.6559600830078\n",
      "Epoch 62, Batch 12123, Loss: 178.2928009033203\n",
      "Epoch 62, Batch 12124, Loss: 182.57659912109375\n",
      "Epoch 62, Batch 12125, Loss: 169.58648681640625\n",
      "Epoch 62, Batch 12126, Loss: 183.38137817382812\n",
      "Epoch 62, Batch 12127, Loss: 168.01226806640625\n",
      "Epoch 62, Batch 12128, Loss: 158.52207946777344\n",
      "Epoch 62, Batch 12129, Loss: 177.3996124267578\n",
      "Epoch 62, Batch 12130, Loss: 184.443115234375\n",
      "Epoch 62, Batch 12131, Loss: 179.610595703125\n",
      "Epoch 62, Batch 12132, Loss: 179.06082153320312\n",
      "Epoch 62, Batch 12133, Loss: 165.11483764648438\n",
      "Epoch 62, Batch 12134, Loss: 175.88528442382812\n",
      "Epoch 62, Batch 12135, Loss: 184.70706176757812\n",
      "Epoch 62, Batch 12136, Loss: 160.38999938964844\n",
      "Epoch 62, Batch 12137, Loss: 169.65028381347656\n",
      "Epoch 62, Batch 12138, Loss: 178.1221466064453\n",
      "Epoch 62, Batch 12139, Loss: 175.98516845703125\n",
      "Epoch 62, Batch 12140, Loss: 164.26312255859375\n",
      "Epoch 62, Batch 12141, Loss: 190.6455841064453\n",
      "Epoch 62, Batch 12142, Loss: 190.56777954101562\n",
      "Epoch 62, Batch 12143, Loss: 168.18223571777344\n",
      "Epoch 62, Batch 12144, Loss: 179.87734985351562\n",
      "Epoch 62, Batch 12145, Loss: 155.45196533203125\n",
      "Epoch 62, Batch 12146, Loss: 174.4016876220703\n",
      "Epoch 62, Batch 12147, Loss: 169.17164611816406\n",
      "Epoch 62, Batch 12148, Loss: 167.19581604003906\n",
      "Epoch 62, Batch 12149, Loss: 172.83978271484375\n",
      "Epoch 62, Batch 12150, Loss: 178.23580932617188\n",
      "Epoch 62, Batch 12151, Loss: 164.2015838623047\n",
      "Epoch 62, Batch 12152, Loss: 175.4344024658203\n",
      "Epoch 62, Batch 12153, Loss: 167.0921630859375\n",
      "Epoch 62, Batch 12154, Loss: 180.49853515625\n",
      "Epoch 62, Batch 12155, Loss: 155.9856719970703\n",
      "Epoch 62, Batch 12156, Loss: 198.18951416015625\n",
      "Epoch 62, Batch 12157, Loss: 181.42672729492188\n",
      "Epoch 62, Batch 12158, Loss: 171.52540588378906\n",
      "Epoch 62, Batch 12159, Loss: 180.31588745117188\n",
      "Epoch 62, Batch 12160, Loss: 171.37303161621094\n",
      "Epoch 62, Batch 12161, Loss: 171.84967041015625\n",
      "Epoch 62, Batch 12162, Loss: 172.0948028564453\n",
      "Epoch 62, Batch 12163, Loss: 163.4564208984375\n",
      "Epoch 62, Batch 12164, Loss: 180.0601806640625\n",
      "Epoch 62, Batch 12165, Loss: 167.74447631835938\n",
      "Epoch 62, Batch 12166, Loss: 191.18870544433594\n",
      "Epoch 62, Batch 12167, Loss: 197.43125915527344\n",
      "Epoch 62, Batch 12168, Loss: 187.9310760498047\n",
      "Epoch 62, Batch 12169, Loss: 179.56658935546875\n",
      "Epoch 62, Batch 12170, Loss: 179.90713500976562\n",
      "Epoch 62, Batch 12171, Loss: 175.4340057373047\n",
      "Epoch 62, Batch 12172, Loss: 179.5886993408203\n",
      "Epoch 62, Batch 12173, Loss: 189.51019287109375\n",
      "Epoch 62, Batch 12174, Loss: 184.53350830078125\n",
      "Epoch 62, Batch 12175, Loss: 179.5711212158203\n",
      "Epoch 62, Batch 12176, Loss: 176.48495483398438\n",
      "Epoch 62, Batch 12177, Loss: 163.203857421875\n",
      "Epoch 62, Batch 12178, Loss: 163.3446502685547\n",
      "Epoch 62, Batch 12179, Loss: 189.22120666503906\n",
      "Epoch 62, Batch 12180, Loss: 169.65640258789062\n",
      "Epoch 62, Batch 12181, Loss: 182.06930541992188\n",
      "Epoch 62, Batch 12182, Loss: 178.83595275878906\n",
      "Epoch 62, Batch 12183, Loss: 181.49569702148438\n",
      "Epoch 62, Batch 12184, Loss: 174.4076690673828\n",
      "Epoch 62, Batch 12185, Loss: 163.2366180419922\n",
      "Epoch 62, Batch 12186, Loss: 162.9134979248047\n",
      "Epoch 62, Batch 12187, Loss: 178.2678680419922\n",
      "Epoch 62, Batch 12188, Loss: 165.13555908203125\n",
      "Epoch 62, Batch 12189, Loss: 182.37106323242188\n",
      "Epoch 62, Batch 12190, Loss: 170.34620666503906\n",
      "Epoch 62, Batch 12191, Loss: 167.07704162597656\n",
      "Epoch 62, Batch 12192, Loss: 176.02084350585938\n",
      "Epoch 62, Batch 12193, Loss: 173.99916076660156\n",
      "Epoch 62, Batch 12194, Loss: 165.611328125\n",
      "Epoch 62, Batch 12195, Loss: 167.12838745117188\n",
      "Epoch 62, Batch 12196, Loss: 185.16839599609375\n",
      "Epoch 62, Batch 12197, Loss: 178.5443115234375\n",
      "Epoch 62, Batch 12198, Loss: 163.71229553222656\n",
      "Epoch 62, Batch 12199, Loss: 168.55039978027344\n",
      "Epoch 62, Batch 12200, Loss: 173.6943359375\n",
      "Epoch 62, Batch 12201, Loss: 174.06956481933594\n",
      "Epoch 62, Batch 12202, Loss: 174.34071350097656\n",
      "Epoch 62, Batch 12203, Loss: 195.9216766357422\n",
      "Epoch 62, Batch 12204, Loss: 189.11404418945312\n",
      "Epoch 62, Batch 12205, Loss: 186.290283203125\n",
      "Epoch 62, Batch 12206, Loss: 170.1782989501953\n",
      "Epoch 62, Batch 12207, Loss: 180.3424835205078\n",
      "Epoch 62, Batch 12208, Loss: 175.26065063476562\n",
      "Epoch 62, Batch 12209, Loss: 179.52366638183594\n",
      "Epoch 62, Batch 12210, Loss: 173.15611267089844\n",
      "Epoch 62, Batch 12211, Loss: 170.17860412597656\n",
      "Epoch 62, Batch 12212, Loss: 152.71148681640625\n",
      "Epoch 62, Batch 12213, Loss: 180.79344177246094\n",
      "Epoch 62, Batch 12214, Loss: 157.83563232421875\n",
      "Epoch 62, Batch 12215, Loss: 173.84454345703125\n",
      "Epoch 62, Batch 12216, Loss: 155.43594360351562\n",
      "Epoch 62, Batch 12217, Loss: 183.43740844726562\n",
      "Epoch 62, Batch 12218, Loss: 184.6057586669922\n",
      "Epoch 62, Batch 12219, Loss: 165.1499481201172\n",
      "Epoch 62, Batch 12220, Loss: 168.1160125732422\n",
      "Epoch 62, Batch 12221, Loss: 181.16786193847656\n",
      "Epoch 62, Batch 12222, Loss: 165.50900268554688\n",
      "Epoch 62, Batch 12223, Loss: 164.40585327148438\n",
      "Epoch 62, Batch 12224, Loss: 157.55474853515625\n",
      "Epoch 62, Batch 12225, Loss: 181.59005737304688\n",
      "Epoch 62, Batch 12226, Loss: 168.8532257080078\n",
      "Epoch 62, Batch 12227, Loss: 170.17433166503906\n",
      "Epoch 62, Batch 12228, Loss: 182.8697967529297\n",
      "Epoch 62, Batch 12229, Loss: 161.68215942382812\n",
      "Epoch 62, Batch 12230, Loss: 191.31219482421875\n",
      "Epoch 62, Batch 12231, Loss: 186.8216552734375\n",
      "Epoch 62, Batch 12232, Loss: 168.488525390625\n",
      "Epoch 62, Batch 12233, Loss: 173.94363403320312\n",
      "Epoch 62, Batch 12234, Loss: 187.53781127929688\n",
      "Epoch 62, Batch 12235, Loss: 164.26947021484375\n",
      "Epoch 62, Batch 12236, Loss: 190.84120178222656\n",
      "Epoch 62, Batch 12237, Loss: 172.35641479492188\n",
      "Epoch 62, Batch 12238, Loss: 186.3402099609375\n",
      "Epoch 62, Batch 12239, Loss: 155.3158721923828\n",
      "Epoch 62, Batch 12240, Loss: 161.50901794433594\n",
      "Epoch 62, Batch 12241, Loss: 169.97528076171875\n",
      "Epoch 62, Batch 12242, Loss: 172.49996948242188\n",
      "Epoch 62, Batch 12243, Loss: 168.6020965576172\n",
      "Epoch 62, Batch 12244, Loss: 160.83245849609375\n",
      "Epoch 62, Batch 12245, Loss: 156.310546875\n",
      "Epoch 62, Batch 12246, Loss: 180.3714599609375\n",
      "Epoch 62, Batch 12247, Loss: 171.74459838867188\n",
      "Epoch 62, Batch 12248, Loss: 167.8480987548828\n",
      "Epoch 62, Batch 12249, Loss: 171.63304138183594\n",
      "Epoch 62, Batch 12250, Loss: 179.02146911621094\n",
      "Epoch 62, Batch 12251, Loss: 178.64031982421875\n",
      "Epoch 62, Batch 12252, Loss: 158.06553649902344\n",
      "Epoch 62, Batch 12253, Loss: 173.69065856933594\n",
      "Epoch 62, Batch 12254, Loss: 168.19476318359375\n",
      "Epoch 62, Batch 12255, Loss: 185.48446655273438\n",
      "Epoch 62, Batch 12256, Loss: 187.6707000732422\n",
      "Epoch 62, Batch 12257, Loss: 170.0692138671875\n",
      "Epoch 62, Batch 12258, Loss: 182.0250701904297\n",
      "Epoch 62, Batch 12259, Loss: 159.34982299804688\n",
      "Epoch 62, Batch 12260, Loss: 161.95602416992188\n",
      "Epoch 62, Batch 12261, Loss: 169.798828125\n",
      "Epoch 62, Batch 12262, Loss: 187.38575744628906\n",
      "Epoch 62, Batch 12263, Loss: 166.44317626953125\n",
      "Epoch 62, Batch 12264, Loss: 160.51791381835938\n",
      "Epoch 62, Batch 12265, Loss: 196.0878143310547\n",
      "Epoch 62, Batch 12266, Loss: 178.83236694335938\n",
      "Epoch 62, Batch 12267, Loss: 166.63502502441406\n",
      "Epoch 62, Batch 12268, Loss: 182.30104064941406\n",
      "Epoch 62, Batch 12269, Loss: 171.1312713623047\n",
      "Epoch 62, Batch 12270, Loss: 174.6031494140625\n",
      "Epoch 62, Batch 12271, Loss: 163.5888214111328\n",
      "Epoch 62, Batch 12272, Loss: 152.43722534179688\n",
      "Epoch 62, Batch 12273, Loss: 180.18080139160156\n",
      "Epoch 62, Batch 12274, Loss: 178.37989807128906\n",
      "Epoch 62, Batch 12275, Loss: 184.0750732421875\n",
      "Epoch 62, Batch 12276, Loss: 181.74252319335938\n",
      "Epoch 62, Batch 12277, Loss: 160.58407592773438\n",
      "Epoch 62, Batch 12278, Loss: 172.16331481933594\n",
      "Epoch 62, Batch 12279, Loss: 179.22787475585938\n",
      "Epoch 62, Batch 12280, Loss: 161.4456329345703\n",
      "Epoch 62, Batch 12281, Loss: 174.82168579101562\n",
      "Epoch 62, Batch 12282, Loss: 167.8107452392578\n",
      "Epoch 62, Batch 12283, Loss: 178.9267578125\n",
      "Epoch 62, Batch 12284, Loss: 180.0065155029297\n",
      "Epoch 62, Batch 12285, Loss: 183.6885223388672\n",
      "Epoch 62, Batch 12286, Loss: 177.87059020996094\n",
      "Epoch 62, Batch 12287, Loss: 166.2688446044922\n",
      "Epoch 62, Batch 12288, Loss: 173.0099639892578\n",
      "Epoch 62, Batch 12289, Loss: 183.5419464111328\n",
      "Epoch 62, Batch 12290, Loss: 175.11610412597656\n",
      "Epoch 62, Batch 12291, Loss: 174.68081665039062\n",
      "Epoch 62, Batch 12292, Loss: 168.07391357421875\n",
      "Epoch 62, Batch 12293, Loss: 174.10147094726562\n",
      "Epoch 62, Batch 12294, Loss: 163.2154541015625\n",
      "Epoch 62, Batch 12295, Loss: 172.7581787109375\n",
      "Epoch 62, Batch 12296, Loss: 179.52207946777344\n",
      "Epoch 62, Batch 12297, Loss: 160.21878051757812\n",
      "Epoch 62, Batch 12298, Loss: 172.67019653320312\n",
      "Epoch 62, Batch 12299, Loss: 182.1616973876953\n",
      "Epoch 62, Batch 12300, Loss: 171.9877166748047\n",
      "Epoch 62, Batch 12301, Loss: 195.38528442382812\n",
      "Epoch 62, Batch 12302, Loss: 163.9713897705078\n",
      "Epoch 62, Batch 12303, Loss: 169.9276885986328\n",
      "Epoch 62, Batch 12304, Loss: 170.7271728515625\n",
      "Epoch 62, Batch 12305, Loss: 172.27659606933594\n",
      "Epoch 62, Batch 12306, Loss: 165.6882781982422\n",
      "Epoch 62, Batch 12307, Loss: 164.88116455078125\n",
      "Epoch 62, Batch 12308, Loss: 173.81207275390625\n",
      "Epoch 62, Batch 12309, Loss: 167.2839813232422\n",
      "Epoch 62, Batch 12310, Loss: 181.56715393066406\n",
      "Epoch 62, Batch 12311, Loss: 164.14480590820312\n",
      "Epoch 62, Batch 12312, Loss: 173.83160400390625\n",
      "Epoch 62, Batch 12313, Loss: 168.7023162841797\n",
      "Epoch 62, Batch 12314, Loss: 174.2643585205078\n",
      "Epoch 62, Batch 12315, Loss: 165.44790649414062\n",
      "Epoch 62, Batch 12316, Loss: 174.31106567382812\n",
      "Epoch 62, Batch 12317, Loss: 169.02362060546875\n",
      "Epoch 62, Batch 12318, Loss: 177.673828125\n",
      "Epoch 62, Batch 12319, Loss: 161.05789184570312\n",
      "Epoch 62, Batch 12320, Loss: 183.64483642578125\n",
      "Epoch 62, Batch 12321, Loss: 181.7843017578125\n",
      "Epoch 62, Batch 12322, Loss: 169.67051696777344\n",
      "Epoch 62, Batch 12323, Loss: 176.294677734375\n",
      "Epoch 62, Batch 12324, Loss: 175.5482940673828\n",
      "Epoch 62, Batch 12325, Loss: 186.78118896484375\n",
      "Epoch 62, Batch 12326, Loss: 183.8811798095703\n",
      "Epoch 62, Batch 12327, Loss: 170.2796173095703\n",
      "Epoch 62, Batch 12328, Loss: 184.55824279785156\n",
      "Epoch 62, Batch 12329, Loss: 154.6947021484375\n",
      "Epoch 62, Batch 12330, Loss: 154.3859100341797\n",
      "Epoch 62, Batch 12331, Loss: 173.4976043701172\n",
      "Epoch 62, Batch 12332, Loss: 170.89378356933594\n",
      "Epoch 62, Batch 12333, Loss: 161.01918029785156\n",
      "Epoch 62, Batch 12334, Loss: 167.09027099609375\n",
      "Epoch 62, Batch 12335, Loss: 171.07594299316406\n",
      "Epoch 62, Batch 12336, Loss: 181.679443359375\n",
      "Epoch 62, Batch 12337, Loss: 180.01609802246094\n",
      "Epoch 62, Batch 12338, Loss: 189.6992645263672\n",
      "Epoch 62, Batch 12339, Loss: 159.9076690673828\n",
      "Epoch 62, Batch 12340, Loss: 178.70327758789062\n",
      "Epoch 62, Batch 12341, Loss: 172.4383087158203\n",
      "Epoch 62, Batch 12342, Loss: 170.57183837890625\n",
      "Epoch 62, Batch 12343, Loss: 175.5130615234375\n",
      "Epoch 62, Batch 12344, Loss: 165.92552185058594\n",
      "Epoch 62, Batch 12345, Loss: 164.13839721679688\n",
      "Epoch 62, Batch 12346, Loss: 172.93988037109375\n",
      "Epoch 62, Batch 12347, Loss: 186.07005310058594\n",
      "Epoch 62, Batch 12348, Loss: 184.27288818359375\n",
      "Epoch 62, Batch 12349, Loss: 168.90982055664062\n",
      "Epoch 62, Batch 12350, Loss: 167.64747619628906\n",
      "Epoch 62, Batch 12351, Loss: 180.78529357910156\n",
      "Epoch 62, Batch 12352, Loss: 182.05174255371094\n",
      "Epoch 62, Batch 12353, Loss: 174.4772491455078\n",
      "Epoch 62, Batch 12354, Loss: 175.822509765625\n",
      "Epoch 62, Batch 12355, Loss: 182.5089111328125\n",
      "Epoch 62, Batch 12356, Loss: 189.49241638183594\n",
      "Epoch 62, Batch 12357, Loss: 188.1814727783203\n",
      "Epoch 62, Batch 12358, Loss: 188.60108947753906\n",
      "Epoch 62, Batch 12359, Loss: 160.28659057617188\n",
      "Epoch 62, Batch 12360, Loss: 168.50244140625\n",
      "Epoch 62, Batch 12361, Loss: 170.49630737304688\n",
      "Epoch 62, Batch 12362, Loss: 195.38754272460938\n",
      "Epoch 62, Batch 12363, Loss: 161.93142700195312\n",
      "Epoch 62, Batch 12364, Loss: 181.91653442382812\n",
      "Epoch 62, Batch 12365, Loss: 166.96871948242188\n",
      "Epoch 62, Batch 12366, Loss: 179.38206481933594\n",
      "Epoch 62, Batch 12367, Loss: 163.09519958496094\n",
      "Epoch 62, Batch 12368, Loss: 175.6429901123047\n",
      "Epoch 62, Batch 12369, Loss: 160.80807495117188\n",
      "Epoch 62, Batch 12370, Loss: 187.43540954589844\n",
      "Epoch 62, Batch 12371, Loss: 171.60157775878906\n",
      "Epoch 62, Batch 12372, Loss: 162.71983337402344\n",
      "Epoch 62, Batch 12373, Loss: 157.79754638671875\n",
      "Epoch 62, Batch 12374, Loss: 179.45077514648438\n",
      "Epoch 62, Batch 12375, Loss: 160.57423400878906\n",
      "Epoch 62, Batch 12376, Loss: 174.54574584960938\n",
      "Epoch 62, Batch 12377, Loss: 182.34359741210938\n",
      "Epoch 62, Batch 12378, Loss: 171.04676818847656\n",
      "Epoch 62, Batch 12379, Loss: 173.90191650390625\n",
      "Epoch 62, Batch 12380, Loss: 157.36146545410156\n",
      "Epoch 62, Batch 12381, Loss: 176.46949768066406\n",
      "Epoch 62, Batch 12382, Loss: 157.3608856201172\n",
      "Epoch 62, Batch 12383, Loss: 149.12384033203125\n",
      "Epoch 62, Batch 12384, Loss: 161.5248565673828\n",
      "Epoch 62, Batch 12385, Loss: 178.8828887939453\n",
      "Epoch 62, Batch 12386, Loss: 179.7344207763672\n",
      "Epoch 62, Batch 12387, Loss: 180.93885803222656\n",
      "Epoch 62, Batch 12388, Loss: 174.94400024414062\n",
      "Epoch 62, Batch 12389, Loss: 171.48072814941406\n",
      "Epoch 62, Batch 12390, Loss: 159.7745819091797\n",
      "Epoch 62, Batch 12391, Loss: 182.72457885742188\n",
      "Epoch 62, Batch 12392, Loss: 181.54440307617188\n",
      "Epoch 62, Batch 12393, Loss: 173.88055419921875\n",
      "Epoch 62, Batch 12394, Loss: 170.82501220703125\n",
      "Epoch 62, Batch 12395, Loss: 172.0643768310547\n",
      "Epoch 62, Batch 12396, Loss: 191.97842407226562\n",
      "Epoch 62, Batch 12397, Loss: 181.51353454589844\n",
      "Epoch 62, Batch 12398, Loss: 161.70245361328125\n",
      "Epoch 62, Batch 12399, Loss: 181.16400146484375\n",
      "Epoch 62, Batch 12400, Loss: 170.9905242919922\n",
      "Epoch 62, Batch 12401, Loss: 158.5242462158203\n",
      "Epoch 62, Batch 12402, Loss: 165.48532104492188\n",
      "Epoch 62, Batch 12403, Loss: 187.0465545654297\n",
      "Epoch 62, Batch 12404, Loss: 175.2466583251953\n",
      "Epoch 62, Batch 12405, Loss: 165.2892608642578\n",
      "Epoch 62, Batch 12406, Loss: 158.67947387695312\n",
      "Epoch 62, Batch 12407, Loss: 179.76829528808594\n",
      "Epoch 62, Batch 12408, Loss: 170.62193298339844\n",
      "Epoch 62, Batch 12409, Loss: 183.5220184326172\n",
      "Epoch 62, Batch 12410, Loss: 166.5331573486328\n",
      "Epoch 62, Batch 12411, Loss: 181.32223510742188\n",
      "Epoch 62, Batch 12412, Loss: 162.5818634033203\n",
      "Epoch 62, Batch 12413, Loss: 173.06509399414062\n",
      "Epoch 62, Batch 12414, Loss: 178.3563232421875\n",
      "Epoch 62, Batch 12415, Loss: 164.4266815185547\n",
      "Epoch 62, Batch 12416, Loss: 176.90145874023438\n",
      "Epoch 62, Batch 12417, Loss: 160.4379119873047\n",
      "Epoch 62, Batch 12418, Loss: 179.27664184570312\n",
      "Epoch 62, Batch 12419, Loss: 173.3103790283203\n",
      "Epoch 62, Batch 12420, Loss: 159.7027587890625\n",
      "Epoch 62, Batch 12421, Loss: 165.76654052734375\n",
      "Epoch 62, Batch 12422, Loss: 167.18991088867188\n",
      "Epoch 62, Batch 12423, Loss: 164.75924682617188\n",
      "Epoch 62, Batch 12424, Loss: 164.8261260986328\n",
      "Epoch 62, Batch 12425, Loss: 173.824462890625\n",
      "Epoch 62, Batch 12426, Loss: 186.1708526611328\n",
      "Epoch 62, Batch 12427, Loss: 161.17236328125\n",
      "Epoch 62, Batch 12428, Loss: 166.8456268310547\n",
      "Epoch 62, Batch 12429, Loss: 181.58969116210938\n",
      "Epoch 62, Batch 12430, Loss: 174.1988983154297\n",
      "Epoch 62, Batch 12431, Loss: 169.48953247070312\n",
      "Epoch 62, Batch 12432, Loss: 170.6623992919922\n",
      "Epoch 62, Batch 12433, Loss: 189.35955810546875\n",
      "Epoch 62, Batch 12434, Loss: 150.0520782470703\n",
      "Epoch 62, Batch 12435, Loss: 181.78614807128906\n",
      "Epoch 62, Batch 12436, Loss: 163.62831115722656\n",
      "Epoch 62, Batch 12437, Loss: 172.57334899902344\n",
      "Epoch 62, Batch 12438, Loss: 160.68325805664062\n",
      "Epoch 62, Batch 12439, Loss: 160.83758544921875\n",
      "Epoch 62, Batch 12440, Loss: 181.56130981445312\n",
      "Epoch 62, Batch 12441, Loss: 181.05003356933594\n",
      "Epoch 62, Batch 12442, Loss: 170.24154663085938\n",
      "Epoch 62, Batch 12443, Loss: 151.62501525878906\n",
      "Epoch 62, Batch 12444, Loss: 172.29696655273438\n",
      "Epoch 62, Batch 12445, Loss: 163.52557373046875\n",
      "Epoch 62, Batch 12446, Loss: 175.46585083007812\n",
      "Epoch 62, Batch 12447, Loss: 161.85911560058594\n",
      "Epoch 62, Batch 12448, Loss: 178.42173767089844\n",
      "Epoch 62, Batch 12449, Loss: 175.64389038085938\n",
      "Epoch 62, Batch 12450, Loss: 185.08328247070312\n",
      "Epoch 62, Batch 12451, Loss: 161.6431121826172\n",
      "Epoch 62, Batch 12452, Loss: 170.7774658203125\n",
      "Epoch 62, Batch 12453, Loss: 172.64024353027344\n",
      "Epoch 62, Batch 12454, Loss: 159.6657257080078\n",
      "Epoch 62, Batch 12455, Loss: 173.5060577392578\n",
      "Epoch 62, Batch 12456, Loss: 183.39187622070312\n",
      "Epoch 62, Batch 12457, Loss: 175.2090301513672\n",
      "Epoch 62, Batch 12458, Loss: 178.5216064453125\n",
      "Epoch 62, Batch 12459, Loss: 172.89483642578125\n",
      "Epoch 62, Batch 12460, Loss: 182.60508728027344\n",
      "Epoch 62, Batch 12461, Loss: 168.12257385253906\n",
      "Epoch 62, Batch 12462, Loss: 168.7864227294922\n",
      "Epoch 62, Batch 12463, Loss: 167.37619018554688\n",
      "Epoch 62, Batch 12464, Loss: 202.4881591796875\n",
      "Epoch 62, Batch 12465, Loss: 176.75970458984375\n",
      "Epoch 62, Batch 12466, Loss: 176.80587768554688\n",
      "Epoch 62, Batch 12467, Loss: 180.17950439453125\n",
      "Epoch 62, Batch 12468, Loss: 173.41680908203125\n",
      "Epoch 62, Batch 12469, Loss: 178.3497314453125\n",
      "Epoch 62, Batch 12470, Loss: 156.7020721435547\n",
      "Epoch 62, Batch 12471, Loss: 172.29685974121094\n",
      "Epoch 62, Batch 12472, Loss: 191.2216796875\n",
      "Epoch 62, Batch 12473, Loss: 172.3129119873047\n",
      "Epoch 62, Batch 12474, Loss: 158.46775817871094\n",
      "Epoch 62, Batch 12475, Loss: 174.7867431640625\n",
      "Epoch 62, Batch 12476, Loss: 174.60641479492188\n",
      "Epoch 62, Batch 12477, Loss: 166.52146911621094\n",
      "Epoch 62, Batch 12478, Loss: 165.75778198242188\n",
      "Epoch 62, Batch 12479, Loss: 192.302001953125\n",
      "Epoch 62, Batch 12480, Loss: 168.7904052734375\n",
      "Epoch 62, Batch 12481, Loss: 168.1246337890625\n",
      "Epoch 62, Batch 12482, Loss: 165.46949768066406\n",
      "Epoch 62, Batch 12483, Loss: 187.75253295898438\n",
      "Epoch 62, Batch 12484, Loss: 173.35960388183594\n",
      "Epoch 62, Batch 12485, Loss: 163.3498077392578\n",
      "Epoch 62, Batch 12486, Loss: 177.27685546875\n",
      "Epoch 62, Batch 12487, Loss: 169.45159912109375\n",
      "Epoch 62, Batch 12488, Loss: 177.62164306640625\n",
      "Epoch 62, Batch 12489, Loss: 184.04132080078125\n",
      "Epoch 62, Batch 12490, Loss: 170.49696350097656\n",
      "Epoch 62, Batch 12491, Loss: 166.65052795410156\n",
      "Epoch 62, Batch 12492, Loss: 169.9644012451172\n",
      "Epoch 62, Batch 12493, Loss: 165.22686767578125\n",
      "Epoch 62, Batch 12494, Loss: 183.8969268798828\n",
      "Epoch 62, Batch 12495, Loss: 162.01487731933594\n",
      "Epoch 62, Batch 12496, Loss: 176.80224609375\n",
      "Epoch 62, Batch 12497, Loss: 177.125\n",
      "Epoch 62, Batch 12498, Loss: 169.1627197265625\n",
      "Epoch 62, Batch 12499, Loss: 187.2793731689453\n",
      "Epoch 62, Batch 12500, Loss: 160.79603576660156\n",
      "Epoch 62, Batch 12501, Loss: 167.37820434570312\n",
      "Epoch 62, Batch 12502, Loss: 177.70176696777344\n",
      "Epoch 62, Batch 12503, Loss: 173.1988983154297\n",
      "Epoch 62, Batch 12504, Loss: 167.5623779296875\n",
      "Epoch 62, Batch 12505, Loss: 182.5440216064453\n",
      "Epoch 62, Batch 12506, Loss: 162.4110870361328\n",
      "Epoch 62, Batch 12507, Loss: 171.02354431152344\n",
      "Epoch 62, Batch 12508, Loss: 165.22802734375\n",
      "Epoch 62, Batch 12509, Loss: 194.23699951171875\n",
      "Epoch 62, Batch 12510, Loss: 188.25381469726562\n",
      "Epoch 62, Batch 12511, Loss: 182.4839630126953\n",
      "Epoch 62, Batch 12512, Loss: 160.82449340820312\n",
      "Epoch 62, Batch 12513, Loss: 178.7930450439453\n",
      "Epoch 62, Batch 12514, Loss: 177.0779266357422\n",
      "Epoch 62, Batch 12515, Loss: 185.91879272460938\n",
      "Epoch 62, Batch 12516, Loss: 181.31739807128906\n",
      "Epoch 62, Batch 12517, Loss: 182.2634735107422\n",
      "Epoch 62, Batch 12518, Loss: 173.1529541015625\n",
      "Epoch 62, Batch 12519, Loss: 161.63633728027344\n",
      "Epoch 62, Batch 12520, Loss: 175.9900360107422\n",
      "Epoch 62, Batch 12521, Loss: 174.75942993164062\n",
      "Epoch 62, Batch 12522, Loss: 164.76834106445312\n",
      "Epoch 62, Batch 12523, Loss: 174.3809051513672\n",
      "Epoch 62, Batch 12524, Loss: 173.5760498046875\n",
      "Epoch 62, Batch 12525, Loss: 166.1503448486328\n",
      "Epoch 62, Batch 12526, Loss: 177.4757080078125\n",
      "Epoch 62, Batch 12527, Loss: 175.0251007080078\n",
      "Epoch 62, Batch 12528, Loss: 163.2960968017578\n",
      "Epoch 62, Batch 12529, Loss: 163.40357971191406\n",
      "Epoch 62, Batch 12530, Loss: 157.385009765625\n",
      "Epoch 62, Batch 12531, Loss: 185.9627227783203\n",
      "Epoch 62, Batch 12532, Loss: 179.82489013671875\n",
      "Epoch 62, Batch 12533, Loss: 161.7089080810547\n",
      "Epoch 62, Batch 12534, Loss: 176.25575256347656\n",
      "Epoch 62, Batch 12535, Loss: 184.75875854492188\n",
      "Epoch 62, Batch 12536, Loss: 177.1908416748047\n",
      "Epoch 62, Batch 12537, Loss: 181.79953002929688\n",
      "Epoch 62, Batch 12538, Loss: 176.20176696777344\n",
      "Epoch 62, Batch 12539, Loss: 168.40733337402344\n",
      "Epoch 62, Batch 12540, Loss: 173.01426696777344\n",
      "Epoch 62, Batch 12541, Loss: 159.86956787109375\n",
      "Epoch 62, Batch 12542, Loss: 175.0476837158203\n",
      "Epoch 62, Batch 12543, Loss: 180.87069702148438\n",
      "Epoch 62, Batch 12544, Loss: 164.3836212158203\n",
      "Epoch 62, Batch 12545, Loss: 167.66641235351562\n",
      "Epoch 62, Batch 12546, Loss: 153.3312530517578\n",
      "Epoch 62, Batch 12547, Loss: 174.3579864501953\n",
      "Epoch 62, Batch 12548, Loss: 167.03424072265625\n",
      "Epoch 62, Batch 12549, Loss: 158.728515625\n",
      "Epoch 62, Batch 12550, Loss: 195.04066467285156\n",
      "Epoch 62, Batch 12551, Loss: 191.5393524169922\n",
      "Epoch 62, Batch 12552, Loss: 177.87220764160156\n",
      "Epoch 62, Batch 12553, Loss: 167.99598693847656\n",
      "Epoch 62, Batch 12554, Loss: 176.46409606933594\n",
      "Epoch 62, Batch 12555, Loss: 159.15847778320312\n",
      "Epoch 62, Batch 12556, Loss: 172.16549682617188\n",
      "Epoch 62, Batch 12557, Loss: 164.17333984375\n",
      "Epoch 62, Batch 12558, Loss: 148.22634887695312\n",
      "Epoch 62, Batch 12559, Loss: 160.35415649414062\n",
      "Epoch 62, Batch 12560, Loss: 201.5802764892578\n",
      "Epoch 62, Batch 12561, Loss: 170.05133056640625\n",
      "Epoch 62, Batch 12562, Loss: 168.5340576171875\n",
      "Epoch 62, Batch 12563, Loss: 167.99732971191406\n",
      "Epoch 62, Batch 12564, Loss: 172.54920959472656\n",
      "Epoch 62, Batch 12565, Loss: 164.73472595214844\n",
      "Epoch 62, Batch 12566, Loss: 162.10340881347656\n",
      "Epoch 62, Batch 12567, Loss: 164.75888061523438\n",
      "Epoch 62, Batch 12568, Loss: 167.5989227294922\n",
      "Epoch 62, Batch 12569, Loss: 172.97836303710938\n",
      "Epoch 62, Batch 12570, Loss: 170.17076110839844\n",
      "Epoch 62, Batch 12571, Loss: 169.6763458251953\n",
      "Epoch 62, Batch 12572, Loss: 177.5470733642578\n",
      "Epoch 62, Batch 12573, Loss: 178.8483428955078\n",
      "Epoch 62, Batch 12574, Loss: 192.22142028808594\n",
      "Epoch 62, Batch 12575, Loss: 165.5567169189453\n",
      "Epoch 62, Batch 12576, Loss: 165.7440948486328\n",
      "Epoch 62, Batch 12577, Loss: 173.16845703125\n",
      "Epoch 62, Batch 12578, Loss: 164.39468383789062\n",
      "Epoch 62, Batch 12579, Loss: 165.31988525390625\n",
      "Epoch 62, Batch 12580, Loss: 171.66268920898438\n",
      "Epoch 62, Batch 12581, Loss: 177.04933166503906\n",
      "Epoch 62, Batch 12582, Loss: 187.1820526123047\n",
      "Epoch 62, Batch 12583, Loss: 166.4231719970703\n",
      "Epoch 62, Batch 12584, Loss: 160.6431427001953\n",
      "Epoch 62, Batch 12585, Loss: 161.9725799560547\n",
      "Epoch 62, Batch 12586, Loss: 167.72171020507812\n",
      "Epoch 62, Batch 12587, Loss: 176.48387145996094\n",
      "Epoch 62, Batch 12588, Loss: 167.45806884765625\n",
      "Epoch 62, Batch 12589, Loss: 165.76983642578125\n",
      "Epoch 62, Batch 12590, Loss: 176.4332733154297\n",
      "Epoch 62, Batch 12591, Loss: 188.8380889892578\n",
      "Epoch 62, Batch 12592, Loss: 186.69898986816406\n",
      "Epoch 62, Batch 12593, Loss: 162.38827514648438\n",
      "Epoch 62, Batch 12594, Loss: 162.58383178710938\n",
      "Epoch 62, Batch 12595, Loss: 165.57740783691406\n",
      "Epoch 62, Batch 12596, Loss: 170.40077209472656\n",
      "Epoch 62, Batch 12597, Loss: 167.7791290283203\n",
      "Epoch 62, Batch 12598, Loss: 175.8253631591797\n",
      "Epoch 62, Batch 12599, Loss: 152.22824096679688\n",
      "Epoch 62, Batch 12600, Loss: 162.24339294433594\n",
      "Epoch 62, Batch 12601, Loss: 168.26754760742188\n",
      "Epoch 62, Batch 12602, Loss: 178.14134216308594\n",
      "Epoch 62, Batch 12603, Loss: 178.91455078125\n",
      "Epoch 62, Batch 12604, Loss: 174.6407928466797\n",
      "Epoch 62, Batch 12605, Loss: 187.1455535888672\n",
      "Epoch 62, Batch 12606, Loss: 173.69808959960938\n",
      "Epoch 62, Batch 12607, Loss: 182.5622100830078\n",
      "Epoch 62, Batch 12608, Loss: 172.07501220703125\n",
      "Epoch 62, Batch 12609, Loss: 168.14935302734375\n",
      "Epoch 62, Batch 12610, Loss: 177.00856018066406\n",
      "Epoch 62, Batch 12611, Loss: 157.85012817382812\n",
      "Epoch 62, Batch 12612, Loss: 190.4842529296875\n",
      "Epoch 62, Batch 12613, Loss: 170.0589599609375\n",
      "Epoch 62, Batch 12614, Loss: 168.9049530029297\n",
      "Epoch 62, Batch 12615, Loss: 173.5313262939453\n",
      "Epoch 62, Batch 12616, Loss: 195.86622619628906\n",
      "Epoch 62, Batch 12617, Loss: 167.12130737304688\n",
      "Epoch 62, Batch 12618, Loss: 187.5775909423828\n",
      "Epoch 62, Batch 12619, Loss: 174.37490844726562\n",
      "Epoch 62, Batch 12620, Loss: 177.5157012939453\n",
      "Epoch 62, Batch 12621, Loss: 186.34852600097656\n",
      "Epoch 62, Batch 12622, Loss: 155.4864959716797\n",
      "Epoch 62, Batch 12623, Loss: 188.24537658691406\n",
      "Epoch 62, Batch 12624, Loss: 173.42800903320312\n",
      "Epoch 62, Batch 12625, Loss: 172.8166961669922\n",
      "Epoch 62, Batch 12626, Loss: 186.09896850585938\n",
      "Epoch 62, Batch 12627, Loss: 185.9175262451172\n",
      "Epoch 62, Batch 12628, Loss: 182.19302368164062\n",
      "Epoch 62, Batch 12629, Loss: 171.4102325439453\n",
      "Epoch 62, Batch 12630, Loss: 159.22451782226562\n",
      "Epoch 62, Batch 12631, Loss: 163.79403686523438\n",
      "Epoch 62, Batch 12632, Loss: 174.57162475585938\n",
      "Epoch 62, Batch 12633, Loss: 157.7528839111328\n",
      "Epoch 62, Batch 12634, Loss: 176.61767578125\n",
      "Epoch 62, Batch 12635, Loss: 179.2787628173828\n",
      "Epoch 62, Batch 12636, Loss: 188.57913208007812\n",
      "Epoch 62, Batch 12637, Loss: 164.6873779296875\n",
      "Epoch 62, Batch 12638, Loss: 179.62249755859375\n",
      "Epoch 62, Batch 12639, Loss: 173.53543090820312\n",
      "Epoch 62, Batch 12640, Loss: 181.93919372558594\n",
      "Epoch 62, Batch 12641, Loss: 174.45150756835938\n",
      "Epoch 62, Batch 12642, Loss: 176.580078125\n",
      "Epoch 62, Batch 12643, Loss: 171.46347045898438\n",
      "Epoch 62, Batch 12644, Loss: 168.09658813476562\n",
      "Epoch 62, Batch 12645, Loss: 162.16514587402344\n",
      "Epoch 62, Batch 12646, Loss: 178.9160919189453\n",
      "Epoch 62, Batch 12647, Loss: 183.71133422851562\n",
      "Epoch 62, Batch 12648, Loss: 179.0087432861328\n",
      "Epoch 62, Batch 12649, Loss: 180.4527587890625\n",
      "Epoch 62, Batch 12650, Loss: 181.76657104492188\n",
      "Epoch 62, Batch 12651, Loss: 178.44390869140625\n",
      "Epoch 62, Batch 12652, Loss: 185.56088256835938\n",
      "Epoch 62, Batch 12653, Loss: 184.36289978027344\n",
      "Epoch 62, Batch 12654, Loss: 164.01351928710938\n",
      "Epoch 62, Batch 12655, Loss: 151.9325408935547\n",
      "Epoch 62, Batch 12656, Loss: 184.7544403076172\n",
      "Epoch 62, Batch 12657, Loss: 167.9665069580078\n",
      "Epoch 62, Batch 12658, Loss: 180.77423095703125\n",
      "Epoch 62, Batch 12659, Loss: 182.9879913330078\n",
      "Epoch 62, Batch 12660, Loss: 158.75601196289062\n",
      "Epoch 62, Batch 12661, Loss: 181.141357421875\n",
      "Epoch 62, Batch 12662, Loss: 180.6713409423828\n",
      "Epoch 62, Batch 12663, Loss: 180.8472137451172\n",
      "Epoch 62, Batch 12664, Loss: 179.55795288085938\n",
      "Epoch 62, Batch 12665, Loss: 153.67031860351562\n",
      "Epoch 62, Batch 12666, Loss: 176.50892639160156\n",
      "Epoch 62, Batch 12667, Loss: 169.8108673095703\n",
      "Epoch 62, Batch 12668, Loss: 162.1540069580078\n",
      "Epoch 62, Batch 12669, Loss: 182.00323486328125\n",
      "Epoch 62, Batch 12670, Loss: 179.28773498535156\n",
      "Epoch 62, Batch 12671, Loss: 155.3166046142578\n",
      "Epoch 62, Batch 12672, Loss: 165.9501190185547\n",
      "Epoch 62, Batch 12673, Loss: 162.34164428710938\n",
      "Epoch 62, Batch 12674, Loss: 186.093505859375\n",
      "Epoch 62, Batch 12675, Loss: 176.4672088623047\n",
      "Epoch 62, Batch 12676, Loss: 192.8098907470703\n",
      "Epoch 62, Batch 12677, Loss: 183.90367126464844\n",
      "Epoch 62, Batch 12678, Loss: 174.12158203125\n",
      "Epoch 62, Batch 12679, Loss: 169.1187286376953\n",
      "Epoch 62, Batch 12680, Loss: 171.75341796875\n",
      "Epoch 62, Batch 12681, Loss: 169.18331909179688\n",
      "Epoch 62, Batch 12682, Loss: 159.80044555664062\n",
      "Epoch 62, Batch 12683, Loss: 153.29718017578125\n",
      "Epoch 62, Batch 12684, Loss: 176.90406799316406\n",
      "Epoch 62, Batch 12685, Loss: 160.49424743652344\n",
      "Epoch 62, Batch 12686, Loss: 164.8690185546875\n",
      "Epoch 62, Batch 12687, Loss: 171.37493896484375\n",
      "Epoch 62, Batch 12688, Loss: 154.4315948486328\n",
      "Epoch 62, Batch 12689, Loss: 174.15379333496094\n",
      "Epoch 62, Batch 12690, Loss: 172.95030212402344\n",
      "Epoch 62, Batch 12691, Loss: 186.37083435058594\n",
      "Epoch 62, Batch 12692, Loss: 170.32781982421875\n",
      "Epoch 62, Batch 12693, Loss: 182.2585906982422\n",
      "Epoch 62, Batch 12694, Loss: 180.2725372314453\n",
      "Epoch 62, Batch 12695, Loss: 152.363525390625\n",
      "Epoch 62, Batch 12696, Loss: 177.37811279296875\n",
      "Epoch 62, Batch 12697, Loss: 176.24127197265625\n",
      "Epoch 62, Batch 12698, Loss: 186.73094177246094\n",
      "Epoch 62, Batch 12699, Loss: 171.78079223632812\n",
      "Epoch 62, Batch 12700, Loss: 162.6868896484375\n",
      "Epoch 62, Batch 12701, Loss: 176.8603973388672\n",
      "Epoch 62, Batch 12702, Loss: 173.02915954589844\n",
      "Epoch 62, Batch 12703, Loss: 178.9584197998047\n",
      "Epoch 62, Batch 12704, Loss: 174.69100952148438\n",
      "Epoch 62, Batch 12705, Loss: 183.7010040283203\n",
      "Epoch 62, Batch 12706, Loss: 164.65370178222656\n",
      "Epoch 62, Batch 12707, Loss: 165.94180297851562\n",
      "Epoch 62, Batch 12708, Loss: 175.76475524902344\n",
      "Epoch 62, Batch 12709, Loss: 172.25099182128906\n",
      "Epoch 62, Batch 12710, Loss: 182.8060760498047\n",
      "Epoch 62, Batch 12711, Loss: 171.7482452392578\n",
      "Epoch 62, Batch 12712, Loss: 160.16734313964844\n",
      "Epoch 62, Batch 12713, Loss: 179.7748565673828\n",
      "Epoch 62, Batch 12714, Loss: 187.27740478515625\n",
      "Epoch 62, Batch 12715, Loss: 172.01048278808594\n",
      "Epoch 62, Batch 12716, Loss: 167.56851196289062\n",
      "Epoch 62, Batch 12717, Loss: 184.51345825195312\n",
      "Epoch 62, Batch 12718, Loss: 160.84930419921875\n",
      "Epoch 62, Batch 12719, Loss: 181.27821350097656\n",
      "Epoch 62, Batch 12720, Loss: 169.68971252441406\n",
      "Epoch 62, Batch 12721, Loss: 158.90684509277344\n",
      "Epoch 62, Batch 12722, Loss: 179.37156677246094\n",
      "Epoch 62, Batch 12723, Loss: 190.59164428710938\n",
      "Epoch 62, Batch 12724, Loss: 165.50511169433594\n",
      "Epoch 62, Batch 12725, Loss: 173.80526733398438\n",
      "Epoch 62, Batch 12726, Loss: 189.65234375\n",
      "Epoch 62, Batch 12727, Loss: 181.68310546875\n",
      "Epoch 62, Batch 12728, Loss: 175.89349365234375\n",
      "Epoch 62, Batch 12729, Loss: 162.5279541015625\n",
      "Epoch 62, Batch 12730, Loss: 178.7354736328125\n",
      "Epoch 62, Batch 12731, Loss: 181.370361328125\n",
      "Epoch 62, Batch 12732, Loss: 168.6477813720703\n",
      "Epoch 62, Batch 12733, Loss: 158.45721435546875\n",
      "Epoch 62, Batch 12734, Loss: 156.18658447265625\n",
      "Epoch 62, Batch 12735, Loss: 171.20513916015625\n",
      "Epoch 62, Batch 12736, Loss: 171.99742126464844\n",
      "Epoch 62, Batch 12737, Loss: 174.1238250732422\n",
      "Epoch 62, Batch 12738, Loss: 167.44285583496094\n",
      "Epoch 62, Batch 12739, Loss: 168.7360076904297\n",
      "Epoch 62, Batch 12740, Loss: 180.22900390625\n",
      "Epoch 62, Batch 12741, Loss: 179.57302856445312\n",
      "Epoch 62, Batch 12742, Loss: 180.94142150878906\n",
      "Epoch 62, Batch 12743, Loss: 172.1375732421875\n",
      "Epoch 62, Batch 12744, Loss: 185.27346801757812\n",
      "Epoch 62, Batch 12745, Loss: 180.97666931152344\n",
      "Epoch 62, Batch 12746, Loss: 163.64930725097656\n",
      "Epoch 62, Batch 12747, Loss: 159.64227294921875\n",
      "Epoch 62, Batch 12748, Loss: 185.91366577148438\n",
      "Epoch 62, Batch 12749, Loss: 171.34744262695312\n",
      "Epoch 62, Batch 12750, Loss: 176.74131774902344\n",
      "Epoch 62, Batch 12751, Loss: 191.14743041992188\n",
      "Epoch 62, Batch 12752, Loss: 189.79022216796875\n",
      "Epoch 62, Batch 12753, Loss: 185.39395141601562\n",
      "Epoch 62, Batch 12754, Loss: 179.76242065429688\n",
      "Epoch 62, Batch 12755, Loss: 163.25723266601562\n",
      "Epoch 62, Batch 12756, Loss: 180.87014770507812\n",
      "Epoch 62, Batch 12757, Loss: 174.6234588623047\n",
      "Epoch 62, Batch 12758, Loss: 168.5470428466797\n",
      "Epoch 62, Batch 12759, Loss: 196.05825805664062\n",
      "Epoch 62, Batch 12760, Loss: 177.80215454101562\n",
      "Epoch 62, Batch 12761, Loss: 161.11549377441406\n",
      "Epoch 62, Batch 12762, Loss: 178.31878662109375\n",
      "Epoch 62, Batch 12763, Loss: 180.63140869140625\n",
      "Epoch 62, Batch 12764, Loss: 191.16802978515625\n",
      "Epoch 62, Batch 12765, Loss: 173.36737060546875\n",
      "Epoch 62, Batch 12766, Loss: 166.08372497558594\n",
      "Epoch 62, Batch 12767, Loss: 182.207275390625\n",
      "Epoch 62, Batch 12768, Loss: 173.02450561523438\n",
      "Epoch 62, Batch 12769, Loss: 181.61441040039062\n",
      "Epoch 62, Batch 12770, Loss: 169.7078094482422\n",
      "Epoch 62, Batch 12771, Loss: 178.82858276367188\n",
      "Epoch 62, Batch 12772, Loss: 169.13076782226562\n",
      "Epoch 62, Batch 12773, Loss: 175.51844787597656\n",
      "Epoch 62, Batch 12774, Loss: 185.44769287109375\n",
      "Epoch 62, Batch 12775, Loss: 168.2578582763672\n",
      "Epoch 62, Batch 12776, Loss: 161.95994567871094\n",
      "Epoch 62, Batch 12777, Loss: 173.40301513671875\n",
      "Epoch 62, Batch 12778, Loss: 177.63839721679688\n",
      "Epoch 62, Batch 12779, Loss: 169.0885772705078\n",
      "Epoch 62, Batch 12780, Loss: 185.66314697265625\n",
      "Epoch 62, Batch 12781, Loss: 180.183349609375\n",
      "Epoch 62, Batch 12782, Loss: 169.58860778808594\n",
      "Epoch 62, Batch 12783, Loss: 167.1703643798828\n",
      "Epoch 62, Batch 12784, Loss: 183.71327209472656\n",
      "Epoch 62, Batch 12785, Loss: 171.62442016601562\n",
      "Epoch 62, Batch 12786, Loss: 163.6050567626953\n",
      "Epoch 62, Batch 12787, Loss: 176.61375427246094\n",
      "Epoch 62, Batch 12788, Loss: 170.02871704101562\n",
      "Epoch 62, Batch 12789, Loss: 186.2751922607422\n",
      "Epoch 62, Batch 12790, Loss: 185.02622985839844\n",
      "Epoch 62, Batch 12791, Loss: 183.61358642578125\n",
      "Epoch 62, Batch 12792, Loss: 170.5867919921875\n",
      "Epoch 62, Batch 12793, Loss: 179.61204528808594\n",
      "Epoch 62, Batch 12794, Loss: 162.65167236328125\n",
      "Epoch 62, Batch 12795, Loss: 166.77206420898438\n",
      "Epoch 62, Batch 12796, Loss: 156.7265625\n",
      "Epoch 62, Batch 12797, Loss: 167.05789184570312\n",
      "Epoch 62, Batch 12798, Loss: 157.0899658203125\n",
      "Epoch 62, Batch 12799, Loss: 170.08372497558594\n",
      "Epoch 62, Batch 12800, Loss: 164.3555145263672\n",
      "Epoch 62, Batch 12801, Loss: 166.07943725585938\n",
      "Epoch 62, Batch 12802, Loss: 165.41970825195312\n",
      "Epoch 62, Batch 12803, Loss: 167.80621337890625\n",
      "Epoch 62, Batch 12804, Loss: 176.4246063232422\n",
      "Epoch 62, Batch 12805, Loss: 160.530517578125\n",
      "Epoch 62, Batch 12806, Loss: 177.10804748535156\n",
      "Epoch 62, Batch 12807, Loss: 177.31948852539062\n",
      "Epoch 62, Batch 12808, Loss: 159.73338317871094\n",
      "Epoch 62, Batch 12809, Loss: 174.9656219482422\n",
      "Epoch 62, Batch 12810, Loss: 150.2364959716797\n",
      "Epoch 62, Batch 12811, Loss: 166.66224670410156\n",
      "Epoch 62, Batch 12812, Loss: 182.19412231445312\n",
      "Epoch 62, Batch 12813, Loss: 178.84146118164062\n",
      "Epoch 62, Batch 12814, Loss: 163.11770629882812\n",
      "Epoch 62, Batch 12815, Loss: 174.017578125\n",
      "Epoch 62, Batch 12816, Loss: 182.68528747558594\n",
      "Epoch 62, Batch 12817, Loss: 192.6477508544922\n",
      "Epoch 62, Batch 12818, Loss: 164.8693084716797\n",
      "Epoch 62, Batch 12819, Loss: 167.03469848632812\n",
      "Epoch 62, Batch 12820, Loss: 164.34255981445312\n",
      "Epoch 62, Batch 12821, Loss: 174.2892608642578\n",
      "Epoch 62, Batch 12822, Loss: 177.8374481201172\n",
      "Epoch 62, Batch 12823, Loss: 167.36927795410156\n",
      "Epoch 62, Batch 12824, Loss: 176.0992889404297\n",
      "Epoch 62, Batch 12825, Loss: 159.20462036132812\n",
      "Epoch 62, Batch 12826, Loss: 170.7757110595703\n",
      "Epoch 62, Batch 12827, Loss: 167.25941467285156\n",
      "Epoch 62, Batch 12828, Loss: 183.1432342529297\n",
      "Epoch 62, Batch 12829, Loss: 174.2577667236328\n",
      "Epoch 62, Batch 12830, Loss: 190.2173614501953\n",
      "Epoch 62, Batch 12831, Loss: 172.1212615966797\n",
      "Epoch 62, Batch 12832, Loss: 184.11500549316406\n",
      "Epoch 62, Batch 12833, Loss: 172.78900146484375\n",
      "Epoch 62, Batch 12834, Loss: 195.19630432128906\n",
      "Epoch 62, Batch 12835, Loss: 171.35768127441406\n",
      "Epoch 62, Batch 12836, Loss: 164.66259765625\n",
      "Epoch 62, Batch 12837, Loss: 161.12538146972656\n",
      "Epoch 62, Batch 12838, Loss: 171.08975219726562\n",
      "Epoch 62, Batch 12839, Loss: 169.64450073242188\n",
      "Epoch 62, Batch 12840, Loss: 154.2690887451172\n",
      "Epoch 62, Batch 12841, Loss: 158.94207763671875\n",
      "Epoch 62, Batch 12842, Loss: 166.71522521972656\n",
      "Epoch 62, Batch 12843, Loss: 182.3478546142578\n",
      "Epoch 62, Batch 12844, Loss: 169.80982971191406\n",
      "Epoch 62, Batch 12845, Loss: 174.572265625\n",
      "Epoch 62, Batch 12846, Loss: 172.0741729736328\n",
      "Epoch 62, Batch 12847, Loss: 168.50942993164062\n",
      "Epoch 62, Batch 12848, Loss: 179.24415588378906\n",
      "Epoch 62, Batch 12849, Loss: 165.8511199951172\n",
      "Epoch 62, Batch 12850, Loss: 165.55821228027344\n",
      "Epoch 62, Batch 12851, Loss: 163.90065002441406\n",
      "Epoch 62, Batch 12852, Loss: 168.9147491455078\n",
      "Epoch 62, Batch 12853, Loss: 186.65635681152344\n",
      "Epoch 62, Batch 12854, Loss: 160.16571044921875\n",
      "Epoch 62, Batch 12855, Loss: 171.8445587158203\n",
      "Epoch 62, Batch 12856, Loss: 173.4234161376953\n",
      "Epoch 62, Batch 12857, Loss: 171.62176513671875\n",
      "Epoch 62, Batch 12858, Loss: 174.2169647216797\n",
      "Epoch 62, Batch 12859, Loss: 170.3694610595703\n",
      "Epoch 62, Batch 12860, Loss: 183.21652221679688\n",
      "Epoch 62, Batch 12861, Loss: 164.02162170410156\n",
      "Epoch 62, Batch 12862, Loss: 177.6328125\n",
      "Epoch 62, Batch 12863, Loss: 164.06808471679688\n",
      "Epoch 62, Batch 12864, Loss: 165.1140899658203\n",
      "Epoch 62, Batch 12865, Loss: 178.3994903564453\n",
      "Epoch 62, Batch 12866, Loss: 179.95458984375\n",
      "Epoch 62, Batch 12867, Loss: 176.6044158935547\n",
      "Epoch 62, Batch 12868, Loss: 177.69691467285156\n",
      "Epoch 62, Batch 12869, Loss: 172.5601043701172\n",
      "Epoch 62, Batch 12870, Loss: 170.1477813720703\n",
      "Epoch 62, Batch 12871, Loss: 175.44212341308594\n",
      "Epoch 62, Batch 12872, Loss: 170.3727264404297\n",
      "Epoch 62, Batch 12873, Loss: 179.4855499267578\n",
      "Epoch 62, Batch 12874, Loss: 165.06271362304688\n",
      "Epoch 62, Batch 12875, Loss: 182.96041870117188\n",
      "Epoch 62, Batch 12876, Loss: 180.7198028564453\n",
      "Epoch 62, Batch 12877, Loss: 178.95816040039062\n",
      "Epoch 62, Batch 12878, Loss: 179.14907836914062\n",
      "Epoch 62, Batch 12879, Loss: 169.68435668945312\n",
      "Epoch 62, Batch 12880, Loss: 172.16534423828125\n",
      "Epoch 62, Batch 12881, Loss: 164.49533081054688\n",
      "Epoch 62, Batch 12882, Loss: 187.35597229003906\n",
      "Epoch 62, Batch 12883, Loss: 171.91123962402344\n",
      "Epoch 62, Batch 12884, Loss: 178.76744079589844\n",
      "Epoch 62, Batch 12885, Loss: 178.3794403076172\n",
      "Epoch 62, Batch 12886, Loss: 168.04981994628906\n",
      "Epoch 62, Batch 12887, Loss: 170.3003387451172\n",
      "Epoch 62, Batch 12888, Loss: 171.4562530517578\n",
      "Epoch 62, Batch 12889, Loss: 185.79473876953125\n",
      "Epoch 62, Batch 12890, Loss: 175.46127319335938\n",
      "Epoch 62, Batch 12891, Loss: 173.57638549804688\n",
      "Epoch 62, Batch 12892, Loss: 181.61422729492188\n",
      "Epoch 62, Batch 12893, Loss: 191.58934020996094\n",
      "Epoch 62, Batch 12894, Loss: 171.0255584716797\n",
      "Epoch 62, Batch 12895, Loss: 174.1365509033203\n",
      "Epoch 62, Batch 12896, Loss: 172.14508056640625\n",
      "Epoch 62, Batch 12897, Loss: 180.1900177001953\n",
      "Epoch 62, Batch 12898, Loss: 181.32081604003906\n",
      "Epoch 62, Batch 12899, Loss: 164.4734649658203\n",
      "Epoch 62, Batch 12900, Loss: 170.1881561279297\n",
      "Epoch 62, Batch 12901, Loss: 180.64181518554688\n",
      "Epoch 62, Batch 12902, Loss: 181.5899658203125\n",
      "Epoch 62, Batch 12903, Loss: 152.02450561523438\n",
      "Epoch 62, Batch 12904, Loss: 181.48301696777344\n",
      "Epoch 62, Batch 12905, Loss: 169.25169372558594\n",
      "Epoch 62, Batch 12906, Loss: 175.6942138671875\n",
      "Epoch 62, Batch 12907, Loss: 170.7438507080078\n",
      "Epoch 62, Batch 12908, Loss: 163.5655517578125\n",
      "Epoch 62, Batch 12909, Loss: 175.75730895996094\n",
      "Epoch 62, Batch 12910, Loss: 169.04698181152344\n",
      "Epoch 62, Batch 12911, Loss: 200.74566650390625\n",
      "Epoch 62, Batch 12912, Loss: 175.37208557128906\n",
      "Epoch 62, Batch 12913, Loss: 174.99586486816406\n",
      "Epoch 62, Batch 12914, Loss: 186.2142333984375\n",
      "Epoch 62, Batch 12915, Loss: 152.76539611816406\n",
      "Epoch 62, Batch 12916, Loss: 168.94049072265625\n",
      "Epoch 62, Batch 12917, Loss: 161.7506866455078\n",
      "Epoch 62, Batch 12918, Loss: 165.7924041748047\n",
      "Epoch 62, Batch 12919, Loss: 166.67727661132812\n",
      "Epoch 62, Batch 12920, Loss: 166.90533447265625\n",
      "Epoch 62, Batch 12921, Loss: 193.72857666015625\n",
      "Epoch 62, Batch 12922, Loss: 166.3941192626953\n",
      "Epoch 62, Batch 12923, Loss: 186.91580200195312\n",
      "Epoch 62, Batch 12924, Loss: 165.77957153320312\n",
      "Epoch 62, Batch 12925, Loss: 172.5770263671875\n",
      "Epoch 62, Batch 12926, Loss: 182.69149780273438\n",
      "Epoch 62, Batch 12927, Loss: 179.3206329345703\n",
      "Epoch 62, Batch 12928, Loss: 196.67715454101562\n",
      "Epoch 62, Batch 12929, Loss: 178.2935333251953\n",
      "Epoch 62, Batch 12930, Loss: 158.36680603027344\n",
      "Epoch 62, Batch 12931, Loss: 175.6458740234375\n",
      "Epoch 62, Batch 12932, Loss: 172.5745391845703\n",
      "Epoch 62, Batch 12933, Loss: 188.81475830078125\n",
      "Epoch 62, Batch 12934, Loss: 152.937744140625\n",
      "Epoch 62, Batch 12935, Loss: 157.2914276123047\n",
      "Epoch 62, Batch 12936, Loss: 199.71751403808594\n",
      "Epoch 62, Batch 12937, Loss: 188.73794555664062\n",
      "Epoch 62, Batch 12938, Loss: 169.62098693847656\n",
      "Epoch 62, Batch 12939, Loss: 169.72842407226562\n",
      "Epoch 62, Batch 12940, Loss: 165.94076538085938\n",
      "Epoch 62, Batch 12941, Loss: 166.11322021484375\n",
      "Epoch 62, Batch 12942, Loss: 184.2871856689453\n",
      "Epoch 62, Batch 12943, Loss: 174.70068359375\n",
      "Epoch 62, Batch 12944, Loss: 160.7728271484375\n",
      "Epoch 62, Batch 12945, Loss: 173.41307067871094\n",
      "Epoch 62, Batch 12946, Loss: 182.74217224121094\n",
      "Epoch 62, Batch 12947, Loss: 185.01361083984375\n",
      "Epoch 62, Batch 12948, Loss: 165.91783142089844\n",
      "Epoch 62, Batch 12949, Loss: 174.09217834472656\n",
      "Epoch 62, Batch 12950, Loss: 169.2384796142578\n",
      "Epoch 62, Batch 12951, Loss: 163.66751098632812\n",
      "Epoch 62, Batch 12952, Loss: 167.3553466796875\n",
      "Epoch 62, Batch 12953, Loss: 165.5322723388672\n",
      "Epoch 62, Batch 12954, Loss: 191.03366088867188\n",
      "Epoch 62, Batch 12955, Loss: 152.0558319091797\n",
      "Epoch 62, Batch 12956, Loss: 183.03790283203125\n",
      "Epoch 62, Batch 12957, Loss: 175.9493408203125\n",
      "Epoch 62, Batch 12958, Loss: 182.97288513183594\n",
      "Epoch 62, Batch 12959, Loss: 185.10377502441406\n",
      "Epoch 62, Batch 12960, Loss: 191.53273010253906\n",
      "Epoch 62, Batch 12961, Loss: 173.88722229003906\n",
      "Epoch 62, Batch 12962, Loss: 182.99746704101562\n",
      "Epoch 62, Batch 12963, Loss: 163.7582244873047\n",
      "Epoch 62, Batch 12964, Loss: 168.91627502441406\n",
      "Epoch 62, Batch 12965, Loss: 174.0676727294922\n",
      "Epoch 62, Batch 12966, Loss: 157.864990234375\n",
      "Epoch 62, Batch 12967, Loss: 186.88119506835938\n",
      "Epoch 62, Batch 12968, Loss: 178.8727569580078\n",
      "Epoch 62, Batch 12969, Loss: 182.87771606445312\n",
      "Epoch 62, Batch 12970, Loss: 183.9205780029297\n",
      "Epoch 62, Batch 12971, Loss: 184.70252990722656\n",
      "Epoch 62, Batch 12972, Loss: 185.89599609375\n",
      "Epoch 62, Batch 12973, Loss: 174.83787536621094\n",
      "Epoch 62, Batch 12974, Loss: 165.98684692382812\n",
      "Epoch 62, Batch 12975, Loss: 180.28536987304688\n",
      "Epoch 62, Batch 12976, Loss: 176.58302307128906\n",
      "Epoch 62, Batch 12977, Loss: 173.26962280273438\n",
      "Epoch 62, Batch 12978, Loss: 165.9117431640625\n",
      "Epoch 62, Batch 12979, Loss: 166.5565643310547\n",
      "Epoch 62, Batch 12980, Loss: 169.25320434570312\n",
      "Epoch 62, Batch 12981, Loss: 184.5972442626953\n",
      "Epoch 62, Batch 12982, Loss: 191.39305114746094\n",
      "Epoch 62, Batch 12983, Loss: 192.96786499023438\n",
      "Epoch 62, Batch 12984, Loss: 174.2230224609375\n",
      "Epoch 62, Batch 12985, Loss: 162.16159057617188\n",
      "Epoch 62, Batch 12986, Loss: 164.4852294921875\n",
      "Epoch 62, Batch 12987, Loss: 175.79969787597656\n",
      "Epoch 62, Batch 12988, Loss: 188.1358184814453\n",
      "Epoch 62, Batch 12989, Loss: 170.35397338867188\n",
      "Epoch 62, Batch 12990, Loss: 186.57691955566406\n",
      "Epoch 62, Batch 12991, Loss: 176.67433166503906\n",
      "Epoch 62, Batch 12992, Loss: 174.18247985839844\n",
      "Epoch 62, Batch 12993, Loss: 163.826171875\n",
      "Epoch 62, Batch 12994, Loss: 166.21514892578125\n",
      "Epoch 62, Batch 12995, Loss: 175.05502319335938\n",
      "Epoch 62, Batch 12996, Loss: 186.96192932128906\n",
      "Epoch 62, Batch 12997, Loss: 182.9990234375\n",
      "Epoch 62, Batch 12998, Loss: 165.1648712158203\n",
      "Epoch 62, Batch 12999, Loss: 174.65618896484375\n",
      "Epoch 62, Batch 13000, Loss: 178.0178680419922\n",
      "Epoch 62, Batch 13001, Loss: 185.9413604736328\n",
      "Epoch 62, Batch 13002, Loss: 181.06761169433594\n",
      "Epoch 62, Batch 13003, Loss: 192.97933959960938\n",
      "Epoch 62, Batch 13004, Loss: 162.8389434814453\n",
      "Epoch 62, Batch 13005, Loss: 156.79940795898438\n",
      "Epoch 62, Batch 13006, Loss: 185.87753295898438\n",
      "Epoch 62, Batch 13007, Loss: 186.87066650390625\n",
      "Epoch 62, Batch 13008, Loss: 174.59405517578125\n",
      "Epoch 62, Batch 13009, Loss: 171.5697479248047\n",
      "Epoch 62, Batch 13010, Loss: 161.815185546875\n",
      "Epoch 62, Batch 13011, Loss: 189.83853149414062\n",
      "Epoch 62, Batch 13012, Loss: 184.59963989257812\n",
      "Epoch 62, Batch 13013, Loss: 180.7237091064453\n",
      "Epoch 62, Batch 13014, Loss: 163.86581420898438\n",
      "Epoch 62, Batch 13015, Loss: 171.71849060058594\n",
      "Epoch 62, Batch 13016, Loss: 176.37274169921875\n",
      "Epoch 62, Batch 13017, Loss: 178.61349487304688\n",
      "Epoch 62, Batch 13018, Loss: 183.9187774658203\n",
      "Epoch 62, Batch 13019, Loss: 175.74307250976562\n",
      "Epoch 62, Batch 13020, Loss: 170.33084106445312\n",
      "Epoch 62, Batch 13021, Loss: 159.83053588867188\n",
      "Epoch 62, Batch 13022, Loss: 172.75083923339844\n",
      "Epoch 62, Batch 13023, Loss: 179.86244201660156\n",
      "Epoch 62, Batch 13024, Loss: 173.75547790527344\n",
      "Epoch 62, Batch 13025, Loss: 161.5963134765625\n",
      "Epoch 62, Batch 13026, Loss: 169.70449829101562\n",
      "Epoch 62, Batch 13027, Loss: 157.19529724121094\n",
      "Epoch 62, Batch 13028, Loss: 177.09689331054688\n",
      "Epoch 62, Batch 13029, Loss: 160.5408477783203\n",
      "Epoch 62, Batch 13030, Loss: 174.64520263671875\n",
      "Epoch 62, Batch 13031, Loss: 183.59091186523438\n",
      "Epoch 62, Batch 13032, Loss: 167.82080078125\n",
      "Epoch 62, Batch 13033, Loss: 163.75672912597656\n",
      "Epoch 62, Batch 13034, Loss: 180.14260864257812\n",
      "Epoch 62, Batch 13035, Loss: 182.90785217285156\n",
      "Epoch 62, Batch 13036, Loss: 161.5738525390625\n",
      "Epoch 62, Batch 13037, Loss: 190.0769500732422\n",
      "Epoch 62, Batch 13038, Loss: 175.8061981201172\n",
      "Epoch 62, Batch 13039, Loss: 174.70010375976562\n",
      "Epoch 62, Batch 13040, Loss: 170.04519653320312\n",
      "Epoch 62, Batch 13041, Loss: 180.10043334960938\n",
      "Epoch 62, Batch 13042, Loss: 195.16311645507812\n",
      "Epoch 62, Batch 13043, Loss: 176.18197631835938\n",
      "Epoch 62, Batch 13044, Loss: 172.97879028320312\n",
      "Epoch 62, Batch 13045, Loss: 173.1574249267578\n",
      "Epoch 62, Batch 13046, Loss: 188.05360412597656\n",
      "Epoch 62, Batch 13047, Loss: 165.1344451904297\n",
      "Epoch 62, Batch 13048, Loss: 174.39352416992188\n",
      "Epoch 62, Batch 13049, Loss: 174.10679626464844\n",
      "Epoch 62, Batch 13050, Loss: 162.22515869140625\n",
      "Epoch 62, Batch 13051, Loss: 175.31173706054688\n",
      "Epoch 62, Batch 13052, Loss: 164.4230499267578\n",
      "Epoch 62, Batch 13053, Loss: 149.95101928710938\n",
      "Epoch 62, Batch 13054, Loss: 177.4220733642578\n",
      "Epoch 62, Batch 13055, Loss: 177.1517333984375\n",
      "Epoch 62, Batch 13056, Loss: 185.2160186767578\n",
      "Epoch 62, Batch 13057, Loss: 164.3853759765625\n",
      "Epoch 62, Batch 13058, Loss: 161.01995849609375\n",
      "Epoch 62, Batch 13059, Loss: 169.7857208251953\n",
      "Epoch 62, Batch 13060, Loss: 201.31546020507812\n",
      "Epoch 62, Batch 13061, Loss: 166.6700439453125\n",
      "Epoch 62, Batch 13062, Loss: 182.51483154296875\n",
      "Epoch 62, Batch 13063, Loss: 172.7001190185547\n",
      "Epoch 62, Batch 13064, Loss: 174.14405822753906\n",
      "Epoch 62, Batch 13065, Loss: 173.34593200683594\n",
      "Epoch 62, Batch 13066, Loss: 187.76185607910156\n",
      "Epoch 62, Batch 13067, Loss: 184.16867065429688\n",
      "Epoch 62, Batch 13068, Loss: 186.27682495117188\n",
      "Epoch 62, Batch 13069, Loss: 170.770263671875\n",
      "Epoch 62, Batch 13070, Loss: 166.31944274902344\n",
      "Epoch 62, Batch 13071, Loss: 168.66690063476562\n",
      "Epoch 62, Batch 13072, Loss: 189.40228271484375\n",
      "Epoch 62, Batch 13073, Loss: 212.03134155273438\n",
      "Epoch 62, Batch 13074, Loss: 182.1421356201172\n",
      "Epoch 62, Batch 13075, Loss: 172.1841583251953\n",
      "Epoch 62, Batch 13076, Loss: 178.76559448242188\n",
      "Epoch 62, Batch 13077, Loss: 158.09811401367188\n",
      "Epoch 62, Batch 13078, Loss: 170.53775024414062\n",
      "Epoch 62, Batch 13079, Loss: 171.6498565673828\n",
      "Epoch 62, Batch 13080, Loss: 158.77552795410156\n",
      "Epoch 62, Batch 13081, Loss: 161.81446838378906\n",
      "Epoch 62, Batch 13082, Loss: 149.36691284179688\n",
      "Epoch 62, Batch 13083, Loss: 182.4522247314453\n",
      "Epoch 62, Batch 13084, Loss: 176.6674346923828\n",
      "Epoch 62, Batch 13085, Loss: 174.62051391601562\n",
      "Epoch 62, Batch 13086, Loss: 176.10519409179688\n",
      "Epoch 62, Batch 13087, Loss: 166.06805419921875\n",
      "Epoch 62, Batch 13088, Loss: 168.28671264648438\n",
      "Epoch 62, Batch 13089, Loss: 179.33848571777344\n",
      "Epoch 62, Batch 13090, Loss: 162.3536376953125\n",
      "Epoch 62, Batch 13091, Loss: 174.58102416992188\n",
      "Epoch 62, Batch 13092, Loss: 173.27943420410156\n",
      "Epoch 62, Batch 13093, Loss: 178.194580078125\n",
      "Epoch 62, Batch 13094, Loss: 187.4328155517578\n",
      "Epoch 62, Batch 13095, Loss: 185.8944854736328\n",
      "Epoch 62, Batch 13096, Loss: 187.13833618164062\n",
      "Epoch 62, Batch 13097, Loss: 168.2301483154297\n",
      "Epoch 62, Batch 13098, Loss: 170.06692504882812\n",
      "Epoch 62, Batch 13099, Loss: 186.27053833007812\n",
      "Epoch 62, Batch 13100, Loss: 169.98593139648438\n",
      "Epoch 62, Batch 13101, Loss: 174.27859497070312\n",
      "Epoch 62, Batch 13102, Loss: 161.46884155273438\n",
      "Epoch 62, Batch 13103, Loss: 162.9043426513672\n",
      "Epoch 62, Batch 13104, Loss: 175.51980590820312\n",
      "Epoch 62, Batch 13105, Loss: 170.7808837890625\n",
      "Epoch 62, Batch 13106, Loss: 173.9185028076172\n",
      "Epoch 62, Batch 13107, Loss: 175.12753295898438\n",
      "Epoch 62, Batch 13108, Loss: 171.6719512939453\n",
      "Epoch 62, Batch 13109, Loss: 182.0742950439453\n",
      "Epoch 62, Batch 13110, Loss: 169.85794067382812\n",
      "Epoch 62, Batch 13111, Loss: 159.9700469970703\n",
      "Epoch 62, Batch 13112, Loss: 173.53672790527344\n",
      "Epoch 62, Batch 13113, Loss: 165.86373901367188\n",
      "Epoch 62, Batch 13114, Loss: 146.9192352294922\n",
      "Epoch 62, Batch 13115, Loss: 179.43923950195312\n",
      "Epoch 62, Batch 13116, Loss: 186.53538513183594\n",
      "Epoch 62, Batch 13117, Loss: 175.31967163085938\n",
      "Epoch 62, Batch 13118, Loss: 167.26553344726562\n",
      "Epoch 62, Batch 13119, Loss: 169.5887908935547\n",
      "Epoch 62, Batch 13120, Loss: 171.82200622558594\n",
      "Epoch 62, Batch 13121, Loss: 166.17941284179688\n",
      "Epoch 62, Batch 13122, Loss: 179.7918243408203\n",
      "Epoch 62, Batch 13123, Loss: 163.3944854736328\n",
      "Epoch 62, Batch 13124, Loss: 171.99819946289062\n",
      "Epoch 62, Batch 13125, Loss: 169.13954162597656\n",
      "Epoch 62, Batch 13126, Loss: 180.17430114746094\n",
      "Epoch 62, Batch 13127, Loss: 175.533203125\n",
      "Epoch 62, Batch 13128, Loss: 167.61053466796875\n",
      "Epoch 62, Batch 13129, Loss: 169.476318359375\n",
      "Epoch 62, Batch 13130, Loss: 165.61175537109375\n",
      "Epoch 62, Batch 13131, Loss: 175.37985229492188\n",
      "Epoch 62, Batch 13132, Loss: 179.14405822753906\n",
      "Epoch 62, Batch 13133, Loss: 181.87960815429688\n",
      "Epoch 62, Batch 13134, Loss: 188.73321533203125\n",
      "Epoch 62, Batch 13135, Loss: 178.2102508544922\n",
      "Epoch 62, Batch 13136, Loss: 170.7686004638672\n",
      "Epoch 62, Batch 13137, Loss: 182.3417510986328\n",
      "Epoch 62, Batch 13138, Loss: 161.55650329589844\n",
      "Epoch 62, Batch 13139, Loss: 159.60791015625\n",
      "Epoch 62, Batch 13140, Loss: 155.5621795654297\n",
      "Epoch 62, Batch 13141, Loss: 165.21783447265625\n",
      "Epoch 62, Batch 13142, Loss: 166.17617797851562\n",
      "Epoch 62, Batch 13143, Loss: 184.38723754882812\n",
      "Epoch 62, Batch 13144, Loss: 166.242431640625\n",
      "Epoch 62, Batch 13145, Loss: 180.08734130859375\n",
      "Epoch 62, Batch 13146, Loss: 171.3129425048828\n",
      "Epoch 62, Batch 13147, Loss: 174.6134796142578\n",
      "Epoch 62, Batch 13148, Loss: 166.28741455078125\n",
      "Epoch 62, Batch 13149, Loss: 175.37522888183594\n",
      "Epoch 62, Batch 13150, Loss: 165.37879943847656\n",
      "Epoch 62, Batch 13151, Loss: 188.0457763671875\n",
      "Epoch 62, Batch 13152, Loss: 171.5807647705078\n",
      "Epoch 62, Batch 13153, Loss: 180.9181365966797\n",
      "Epoch 62, Batch 13154, Loss: 172.93948364257812\n",
      "Epoch 62, Batch 13155, Loss: 171.46868896484375\n",
      "Epoch 62, Batch 13156, Loss: 166.32130432128906\n",
      "Epoch 62, Batch 13157, Loss: 168.66419982910156\n",
      "Epoch 62, Batch 13158, Loss: 168.30262756347656\n",
      "Epoch 62, Batch 13159, Loss: 175.05972290039062\n",
      "Epoch 62, Batch 13160, Loss: 183.55946350097656\n",
      "Epoch 62, Batch 13161, Loss: 168.06777954101562\n",
      "Epoch 62, Batch 13162, Loss: 174.32395935058594\n",
      "Epoch 62, Batch 13163, Loss: 164.64260864257812\n",
      "Epoch 62, Batch 13164, Loss: 189.30862426757812\n",
      "Epoch 62, Batch 13165, Loss: 176.435546875\n",
      "Epoch 62, Batch 13166, Loss: 166.0491943359375\n",
      "Epoch 62, Batch 13167, Loss: 178.04873657226562\n",
      "Epoch 62, Batch 13168, Loss: 173.8385467529297\n",
      "Epoch 62, Batch 13169, Loss: 171.84951782226562\n",
      "Epoch 62, Batch 13170, Loss: 159.11647033691406\n",
      "Epoch 62, Batch 13171, Loss: 169.97032165527344\n",
      "Epoch 62, Batch 13172, Loss: 165.69720458984375\n",
      "Epoch 62, Batch 13173, Loss: 182.7644805908203\n",
      "Epoch 62, Batch 13174, Loss: 166.6505126953125\n",
      "Epoch 62, Batch 13175, Loss: 178.16552734375\n",
      "Epoch 62, Batch 13176, Loss: 183.6508026123047\n",
      "Epoch 62, Batch 13177, Loss: 178.0295867919922\n",
      "Epoch 62, Batch 13178, Loss: 177.9620819091797\n",
      "Epoch 62, Batch 13179, Loss: 171.9792938232422\n",
      "Epoch 62, Batch 13180, Loss: 183.87867736816406\n",
      "Epoch 62, Batch 13181, Loss: 177.94683837890625\n",
      "Epoch 62, Batch 13182, Loss: 171.80052185058594\n",
      "Epoch 62, Batch 13183, Loss: 170.61508178710938\n",
      "Epoch 62, Batch 13184, Loss: 187.0596160888672\n",
      "Epoch 62, Batch 13185, Loss: 183.29896545410156\n",
      "Epoch 62, Batch 13186, Loss: 172.60281372070312\n",
      "Epoch 62, Batch 13187, Loss: 181.94371032714844\n",
      "Epoch 62, Batch 13188, Loss: 164.10462951660156\n",
      "Epoch 62, Batch 13189, Loss: 170.60992431640625\n",
      "Epoch 62, Batch 13190, Loss: 182.0789794921875\n",
      "Epoch 62, Batch 13191, Loss: 159.12521362304688\n",
      "Epoch 62, Batch 13192, Loss: 168.05047607421875\n",
      "Epoch 62, Batch 13193, Loss: 172.4761199951172\n",
      "Epoch 62, Batch 13194, Loss: 169.052001953125\n",
      "Epoch 62, Batch 13195, Loss: 172.9186553955078\n",
      "Epoch 62, Batch 13196, Loss: 154.9425506591797\n",
      "Epoch 62, Batch 13197, Loss: 160.30747985839844\n",
      "Epoch 62, Batch 13198, Loss: 168.719970703125\n",
      "Epoch 62, Batch 13199, Loss: 171.06982421875\n",
      "Epoch 62, Batch 13200, Loss: 175.11065673828125\n",
      "Epoch 62, Batch 13201, Loss: 175.58763122558594\n",
      "Epoch 62, Batch 13202, Loss: 182.93238830566406\n",
      "Epoch 62, Batch 13203, Loss: 171.7362823486328\n",
      "Epoch 62, Batch 13204, Loss: 166.98666381835938\n",
      "Epoch 62, Batch 13205, Loss: 176.8142852783203\n",
      "Epoch 62, Batch 13206, Loss: 173.73223876953125\n",
      "Epoch 62, Batch 13207, Loss: 156.89955139160156\n",
      "Epoch 62, Batch 13208, Loss: 171.45742797851562\n",
      "Epoch 62, Batch 13209, Loss: 190.80557250976562\n",
      "Epoch 62, Batch 13210, Loss: 170.0552215576172\n",
      "Epoch 62, Batch 13211, Loss: 168.1562042236328\n",
      "Epoch 62, Batch 13212, Loss: 165.63772583007812\n",
      "Epoch 62, Batch 13213, Loss: 154.783935546875\n",
      "Epoch 62, Batch 13214, Loss: 178.3069305419922\n",
      "Epoch 62, Batch 13215, Loss: 165.08370971679688\n",
      "Epoch 62, Batch 13216, Loss: 165.1714324951172\n",
      "Epoch 62, Batch 13217, Loss: 166.99310302734375\n",
      "Epoch 62, Batch 13218, Loss: 175.1513671875\n",
      "Epoch 62, Batch 13219, Loss: 187.48731994628906\n",
      "Epoch 62, Batch 13220, Loss: 170.26333618164062\n",
      "Epoch 62, Batch 13221, Loss: 184.00161743164062\n",
      "Epoch 62, Batch 13222, Loss: 174.31588745117188\n",
      "Epoch 62, Batch 13223, Loss: 169.42031860351562\n",
      "Epoch 62, Batch 13224, Loss: 174.99417114257812\n",
      "Epoch 62, Batch 13225, Loss: 172.06185913085938\n",
      "Epoch 62, Batch 13226, Loss: 176.85418701171875\n",
      "Epoch 62, Batch 13227, Loss: 156.8280487060547\n",
      "Epoch 62, Batch 13228, Loss: 182.71173095703125\n",
      "Epoch 62, Batch 13229, Loss: 153.45542907714844\n",
      "Epoch 62, Batch 13230, Loss: 179.681396484375\n",
      "Epoch 62, Batch 13231, Loss: 162.1948699951172\n",
      "Epoch 62, Batch 13232, Loss: 183.2454376220703\n",
      "Epoch 62, Batch 13233, Loss: 181.81407165527344\n",
      "Epoch 62, Batch 13234, Loss: 171.5562744140625\n",
      "Epoch 62, Batch 13235, Loss: 171.10238647460938\n",
      "Epoch 62, Batch 13236, Loss: 168.08633422851562\n",
      "Epoch 62, Batch 13237, Loss: 185.57431030273438\n",
      "Epoch 62, Batch 13238, Loss: 179.63912963867188\n",
      "Epoch 62, Batch 13239, Loss: 171.68035888671875\n",
      "Epoch 62, Batch 13240, Loss: 149.56625366210938\n",
      "Epoch 62, Batch 13241, Loss: 182.13462829589844\n",
      "Epoch 62, Batch 13242, Loss: 163.7720184326172\n",
      "Epoch 62, Batch 13243, Loss: 172.52197265625\n",
      "Epoch 62, Batch 13244, Loss: 173.8662109375\n",
      "Epoch 62, Batch 13245, Loss: 192.26016235351562\n",
      "Epoch 62, Batch 13246, Loss: 169.92999267578125\n",
      "Epoch 62, Batch 13247, Loss: 163.79405212402344\n",
      "Epoch 62, Batch 13248, Loss: 158.90367126464844\n",
      "Epoch 62, Batch 13249, Loss: 176.0973358154297\n",
      "Epoch 62, Batch 13250, Loss: 177.46160888671875\n",
      "Epoch 62, Batch 13251, Loss: 177.1805419921875\n",
      "Epoch 62, Batch 13252, Loss: 148.92657470703125\n",
      "Epoch 62, Batch 13253, Loss: 181.35130310058594\n",
      "Epoch 62, Batch 13254, Loss: 174.6293487548828\n",
      "Epoch 62, Batch 13255, Loss: 165.0843505859375\n",
      "Epoch 62, Batch 13256, Loss: 170.3941650390625\n",
      "Epoch 62, Batch 13257, Loss: 158.87619018554688\n",
      "Epoch 62, Batch 13258, Loss: 187.87289428710938\n",
      "Epoch 62, Batch 13259, Loss: 188.2337646484375\n",
      "Epoch 62, Batch 13260, Loss: 179.20086669921875\n",
      "Epoch 62, Batch 13261, Loss: 164.35850524902344\n",
      "Epoch 62, Batch 13262, Loss: 175.18038940429688\n",
      "Epoch 62, Batch 13263, Loss: 174.8025665283203\n",
      "Epoch 62, Batch 13264, Loss: 163.2653350830078\n",
      "Epoch 62, Batch 13265, Loss: 161.21603393554688\n",
      "Epoch 62, Batch 13266, Loss: 162.21408081054688\n",
      "Epoch 62, Batch 13267, Loss: 166.9910430908203\n",
      "Epoch 62, Batch 13268, Loss: 184.74530029296875\n",
      "Epoch 62, Batch 13269, Loss: 180.9138946533203\n",
      "Epoch 62, Batch 13270, Loss: 180.49896240234375\n",
      "Epoch 62, Batch 13271, Loss: 157.24119567871094\n",
      "Epoch 62, Batch 13272, Loss: 173.08265686035156\n",
      "Epoch 62, Batch 13273, Loss: 163.32008361816406\n",
      "Epoch 62, Batch 13274, Loss: 174.24630737304688\n",
      "Epoch 62, Batch 13275, Loss: 169.0790557861328\n",
      "Epoch 62, Batch 13276, Loss: 176.92007446289062\n",
      "Epoch 62, Batch 13277, Loss: 182.02020263671875\n",
      "Epoch 62, Batch 13278, Loss: 168.540771484375\n",
      "Epoch 62, Batch 13279, Loss: 167.28713989257812\n",
      "Epoch 62, Batch 13280, Loss: 181.51373291015625\n",
      "Epoch 62, Batch 13281, Loss: 165.61566162109375\n",
      "Epoch 62, Batch 13282, Loss: 192.76004028320312\n",
      "Epoch 62, Batch 13283, Loss: 175.39024353027344\n",
      "Epoch 62, Batch 13284, Loss: 163.0501708984375\n",
      "Epoch 62, Batch 13285, Loss: 182.58258056640625\n",
      "Epoch 62, Batch 13286, Loss: 191.14285278320312\n",
      "Epoch 62, Batch 13287, Loss: 194.89859008789062\n",
      "Epoch 62, Batch 13288, Loss: 158.41091918945312\n",
      "Epoch 62, Batch 13289, Loss: 189.9656524658203\n",
      "Epoch 62, Batch 13290, Loss: 178.48036193847656\n",
      "Epoch 62, Batch 13291, Loss: 163.98406982421875\n",
      "Epoch 62, Batch 13292, Loss: 176.8448486328125\n",
      "Epoch 62, Batch 13293, Loss: 182.0389862060547\n",
      "Epoch 62, Batch 13294, Loss: 166.3769989013672\n",
      "Epoch 62, Batch 13295, Loss: 172.34397888183594\n",
      "Epoch 62, Batch 13296, Loss: 154.0065460205078\n",
      "Epoch 62, Batch 13297, Loss: 162.8795623779297\n",
      "Epoch 62, Batch 13298, Loss: 189.33787536621094\n",
      "Epoch 62, Batch 13299, Loss: 178.20628356933594\n",
      "Epoch 62, Batch 13300, Loss: 163.80027770996094\n",
      "Epoch 62, Batch 13301, Loss: 188.384765625\n",
      "Epoch 62, Batch 13302, Loss: 181.25552368164062\n",
      "Epoch 62, Batch 13303, Loss: 159.77879333496094\n",
      "Epoch 62, Batch 13304, Loss: 167.9221649169922\n",
      "Epoch 62, Batch 13305, Loss: 183.13087463378906\n",
      "Epoch 62, Batch 13306, Loss: 157.03738403320312\n",
      "Epoch 62, Batch 13307, Loss: 177.74478149414062\n",
      "Epoch 62, Batch 13308, Loss: 165.35787963867188\n",
      "Epoch 62, Batch 13309, Loss: 168.52320861816406\n",
      "Epoch 62, Batch 13310, Loss: 183.33987426757812\n",
      "Epoch 62, Batch 13311, Loss: 184.42022705078125\n",
      "Epoch 62, Batch 13312, Loss: 166.65847778320312\n",
      "Epoch 62, Batch 13313, Loss: 165.1710662841797\n",
      "Epoch 62, Batch 13314, Loss: 177.26539611816406\n",
      "Epoch 62, Batch 13315, Loss: 169.5323028564453\n",
      "Epoch 62, Batch 13316, Loss: 176.05555725097656\n",
      "Epoch 62, Batch 13317, Loss: 180.1656494140625\n",
      "Epoch 62, Batch 13318, Loss: 155.91372680664062\n",
      "Epoch 62, Batch 13319, Loss: 169.57635498046875\n",
      "Epoch 62, Batch 13320, Loss: 163.93609619140625\n",
      "Epoch 62, Batch 13321, Loss: 205.3654022216797\n",
      "Epoch 62, Batch 13322, Loss: 168.34996032714844\n",
      "Epoch 62, Batch 13323, Loss: 167.81959533691406\n",
      "Epoch 62, Batch 13324, Loss: 169.1061248779297\n",
      "Epoch 62, Batch 13325, Loss: 158.8759765625\n",
      "Epoch 62, Batch 13326, Loss: 177.15576171875\n",
      "Epoch 62, Batch 13327, Loss: 179.6128692626953\n",
      "Epoch 62, Batch 13328, Loss: 164.3760528564453\n",
      "Epoch 62, Batch 13329, Loss: 161.89979553222656\n",
      "Epoch 62, Batch 13330, Loss: 167.4385528564453\n",
      "Epoch 62, Batch 13331, Loss: 187.3555145263672\n",
      "Epoch 62, Batch 13332, Loss: 167.88214111328125\n",
      "Epoch 62, Batch 13333, Loss: 190.3441925048828\n",
      "Epoch 62, Batch 13334, Loss: 170.1949920654297\n",
      "Epoch 62, Batch 13335, Loss: 170.6609649658203\n",
      "Epoch 62, Batch 13336, Loss: 191.85215759277344\n",
      "Epoch 62, Batch 13337, Loss: 179.03146362304688\n",
      "Epoch 62, Batch 13338, Loss: 171.4491424560547\n",
      "Epoch 62, Batch 13339, Loss: 178.44586181640625\n",
      "Epoch 62, Batch 13340, Loss: 171.91001892089844\n",
      "Epoch 62, Batch 13341, Loss: 172.42601013183594\n",
      "Epoch 62, Batch 13342, Loss: 159.7537841796875\n",
      "Epoch 62, Batch 13343, Loss: 179.73126220703125\n",
      "Epoch 62, Batch 13344, Loss: 159.24737548828125\n",
      "Epoch 62, Batch 13345, Loss: 177.10382080078125\n",
      "Epoch 62, Batch 13346, Loss: 175.2557373046875\n",
      "Epoch 62, Batch 13347, Loss: 184.06787109375\n",
      "Epoch 62, Batch 13348, Loss: 171.6861572265625\n",
      "Epoch 62, Batch 13349, Loss: 171.30908203125\n",
      "Epoch 62, Batch 13350, Loss: 188.94569396972656\n",
      "Epoch 62, Batch 13351, Loss: 181.94921875\n",
      "Epoch 62, Batch 13352, Loss: 177.811279296875\n",
      "Epoch 62, Batch 13353, Loss: 203.2763671875\n",
      "Epoch 62, Batch 13354, Loss: 165.30221557617188\n",
      "Epoch 62, Batch 13355, Loss: 189.62216186523438\n",
      "Epoch 62, Batch 13356, Loss: 186.31715393066406\n",
      "Epoch 62, Batch 13357, Loss: 168.47360229492188\n",
      "Epoch 62, Batch 13358, Loss: 188.08306884765625\n",
      "Epoch 62, Batch 13359, Loss: 167.8186798095703\n",
      "Epoch 62, Batch 13360, Loss: 174.8007354736328\n",
      "Epoch 62, Batch 13361, Loss: 162.9082794189453\n",
      "Epoch 62, Batch 13362, Loss: 190.64039611816406\n",
      "Epoch 62, Batch 13363, Loss: 197.53199768066406\n",
      "Epoch 62, Batch 13364, Loss: 173.23501586914062\n",
      "Epoch 62, Batch 13365, Loss: 184.2152099609375\n",
      "Epoch 62, Batch 13366, Loss: 161.0450439453125\n",
      "Epoch 62, Batch 13367, Loss: 163.7315673828125\n",
      "Epoch 62, Batch 13368, Loss: 166.7801513671875\n",
      "Epoch 62, Batch 13369, Loss: 174.90628051757812\n",
      "Epoch 62, Batch 13370, Loss: 166.9865264892578\n",
      "Epoch 62, Batch 13371, Loss: 171.01231384277344\n",
      "Epoch 62, Batch 13372, Loss: 183.86814880371094\n",
      "Epoch 62, Batch 13373, Loss: 176.4004669189453\n",
      "Epoch 62, Batch 13374, Loss: 188.5602569580078\n",
      "Epoch 62, Batch 13375, Loss: 169.13409423828125\n",
      "Epoch 62, Batch 13376, Loss: 175.0194091796875\n",
      "Epoch 62, Batch 13377, Loss: 164.20538330078125\n",
      "Epoch 62, Batch 13378, Loss: 196.0498809814453\n",
      "Epoch 62, Batch 13379, Loss: 177.6984100341797\n",
      "Epoch 62, Batch 13380, Loss: 169.8834686279297\n",
      "Epoch 62, Batch 13381, Loss: 175.9239959716797\n",
      "Epoch 62, Batch 13382, Loss: 172.1465301513672\n",
      "Epoch 62, Batch 13383, Loss: 179.71971130371094\n",
      "Epoch 62, Batch 13384, Loss: 165.9692840576172\n",
      "Epoch 62, Batch 13385, Loss: 170.52638244628906\n",
      "Epoch 62, Batch 13386, Loss: 166.05221557617188\n",
      "Epoch 62, Batch 13387, Loss: 161.21621704101562\n",
      "Epoch 62, Batch 13388, Loss: 176.77786254882812\n",
      "Epoch 62, Batch 13389, Loss: 183.620361328125\n",
      "Epoch 62, Batch 13390, Loss: 178.46119689941406\n",
      "Epoch 62, Batch 13391, Loss: 176.9813995361328\n",
      "Epoch 62, Batch 13392, Loss: 193.33984375\n",
      "Epoch 62, Batch 13393, Loss: 195.9945526123047\n",
      "Epoch 62, Batch 13394, Loss: 170.1378173828125\n",
      "Epoch 62, Batch 13395, Loss: 174.05035400390625\n",
      "Epoch 62, Batch 13396, Loss: 171.10296630859375\n",
      "Epoch 62, Batch 13397, Loss: 191.8706512451172\n",
      "Epoch 62, Batch 13398, Loss: 159.60609436035156\n",
      "Epoch 62, Batch 13399, Loss: 194.88539123535156\n",
      "Epoch 62, Batch 13400, Loss: 181.1038055419922\n",
      "Epoch 62, Batch 13401, Loss: 170.8218231201172\n",
      "Epoch 62, Batch 13402, Loss: 179.02700805664062\n",
      "Epoch 62, Batch 13403, Loss: 168.28868103027344\n",
      "Epoch 62, Batch 13404, Loss: 176.6835174560547\n",
      "Epoch 62, Batch 13405, Loss: 174.7413787841797\n",
      "Epoch 62, Batch 13406, Loss: 177.3125\n",
      "Epoch 62, Batch 13407, Loss: 172.9170379638672\n",
      "Epoch 62, Batch 13408, Loss: 186.35366821289062\n",
      "Epoch 62, Batch 13409, Loss: 171.73019409179688\n",
      "Epoch 62, Batch 13410, Loss: 180.73486328125\n",
      "Epoch 62, Batch 13411, Loss: 157.60597229003906\n",
      "Epoch 62, Batch 13412, Loss: 167.40359497070312\n",
      "Epoch 62, Batch 13413, Loss: 162.3550262451172\n",
      "Epoch 62, Batch 13414, Loss: 174.5871124267578\n",
      "Epoch 62, Batch 13415, Loss: 168.31353759765625\n",
      "Epoch 62, Batch 13416, Loss: 161.51010131835938\n",
      "Epoch 62, Batch 13417, Loss: 195.7299346923828\n",
      "Epoch 62, Batch 13418, Loss: 174.76524353027344\n",
      "Epoch 62, Batch 13419, Loss: 190.26817321777344\n",
      "Epoch 62, Batch 13420, Loss: 176.00181579589844\n",
      "Epoch 62, Batch 13421, Loss: 192.87472534179688\n",
      "Epoch 62, Batch 13422, Loss: 171.850341796875\n",
      "Epoch 62, Batch 13423, Loss: 170.8870086669922\n",
      "Epoch 62, Batch 13424, Loss: 172.26876831054688\n",
      "Epoch 62, Batch 13425, Loss: 178.54913330078125\n",
      "Epoch 62, Batch 13426, Loss: 182.2954559326172\n",
      "Epoch 62, Batch 13427, Loss: 168.94529724121094\n",
      "Epoch 62, Batch 13428, Loss: 180.5750274658203\n",
      "Epoch 62, Batch 13429, Loss: 161.7718963623047\n",
      "Epoch 62, Batch 13430, Loss: 191.44351196289062\n",
      "Epoch 62, Batch 13431, Loss: 195.14540100097656\n",
      "Epoch 62, Batch 13432, Loss: 167.8482208251953\n",
      "Epoch 62, Batch 13433, Loss: 168.88812255859375\n",
      "Epoch 62, Batch 13434, Loss: 174.6442413330078\n",
      "Epoch 62, Batch 13435, Loss: 177.2169952392578\n",
      "Epoch 62, Batch 13436, Loss: 175.0050811767578\n",
      "Epoch 62, Batch 13437, Loss: 186.48658752441406\n",
      "Epoch 62, Batch 13438, Loss: 168.7242431640625\n",
      "Epoch 62, Batch 13439, Loss: 168.81520080566406\n",
      "Epoch 62, Batch 13440, Loss: 174.26113891601562\n",
      "Epoch 62, Batch 13441, Loss: 177.51902770996094\n",
      "Epoch 62, Batch 13442, Loss: 159.56410217285156\n",
      "Epoch 62, Batch 13443, Loss: 167.12811279296875\n",
      "Epoch 62, Batch 13444, Loss: 178.49981689453125\n",
      "Epoch 62, Batch 13445, Loss: 164.97311401367188\n",
      "Epoch 62, Batch 13446, Loss: 172.36016845703125\n",
      "Epoch 62, Batch 13447, Loss: 176.25253295898438\n",
      "Epoch 62, Batch 13448, Loss: 187.7595672607422\n",
      "Epoch 62, Batch 13449, Loss: 163.69815063476562\n",
      "Epoch 62, Batch 13450, Loss: 170.97958374023438\n",
      "Epoch 62, Batch 13451, Loss: 186.87391662597656\n",
      "Epoch 62, Batch 13452, Loss: 162.70811462402344\n",
      "Epoch 62, Batch 13453, Loss: 169.76779174804688\n",
      "Epoch 62, Batch 13454, Loss: 167.87374877929688\n",
      "Epoch 62, Batch 13455, Loss: 180.85305786132812\n",
      "Epoch 62, Batch 13456, Loss: 175.75938415527344\n",
      "Epoch 62, Batch 13457, Loss: 161.73434448242188\n",
      "Epoch 62, Batch 13458, Loss: 153.05714416503906\n",
      "Epoch 62, Batch 13459, Loss: 171.86712646484375\n",
      "Epoch 62, Batch 13460, Loss: 169.354736328125\n",
      "Epoch 62, Batch 13461, Loss: 186.1842041015625\n",
      "Epoch 62, Batch 13462, Loss: 163.83338928222656\n",
      "Epoch 62, Batch 13463, Loss: 158.14859008789062\n",
      "Epoch 62, Batch 13464, Loss: 178.45718383789062\n",
      "Epoch 62, Batch 13465, Loss: 190.94236755371094\n",
      "Epoch 62, Batch 13466, Loss: 188.72927856445312\n",
      "Epoch 62, Batch 13467, Loss: 163.1603240966797\n",
      "Epoch 62, Batch 13468, Loss: 157.49037170410156\n",
      "Epoch 62, Batch 13469, Loss: 176.50372314453125\n",
      "Epoch 62, Batch 13470, Loss: 161.4364471435547\n",
      "Epoch 62, Batch 13471, Loss: 155.19786071777344\n",
      "Epoch 62, Batch 13472, Loss: 184.45741271972656\n",
      "Epoch 62, Batch 13473, Loss: 179.33541870117188\n",
      "Epoch 62, Batch 13474, Loss: 185.35348510742188\n",
      "Epoch 62, Batch 13475, Loss: 189.83602905273438\n",
      "Epoch 62, Batch 13476, Loss: 161.317626953125\n",
      "Epoch 62, Batch 13477, Loss: 188.69175720214844\n",
      "Epoch 62, Batch 13478, Loss: 168.30538940429688\n",
      "Epoch 62, Batch 13479, Loss: 171.68817138671875\n",
      "Epoch 62, Batch 13480, Loss: 168.7742462158203\n",
      "Epoch 62, Batch 13481, Loss: 200.0567169189453\n",
      "Epoch 62, Batch 13482, Loss: 163.3903350830078\n",
      "Epoch 62, Batch 13483, Loss: 163.7042236328125\n",
      "Epoch 62, Batch 13484, Loss: 173.94729614257812\n",
      "Epoch 62, Batch 13485, Loss: 189.27500915527344\n",
      "Epoch 62, Batch 13486, Loss: 166.19969177246094\n",
      "Epoch 62, Batch 13487, Loss: 165.17416381835938\n",
      "Epoch 62, Batch 13488, Loss: 185.85655212402344\n",
      "Epoch 62, Batch 13489, Loss: 176.06866455078125\n",
      "Epoch 62, Batch 13490, Loss: 169.56222534179688\n",
      "Epoch 62, Batch 13491, Loss: 166.04107666015625\n",
      "Epoch 62, Batch 13492, Loss: 170.67535400390625\n",
      "Epoch 62, Batch 13493, Loss: 174.31976318359375\n",
      "Epoch 62, Batch 13494, Loss: 176.2237091064453\n",
      "Epoch 62, Batch 13495, Loss: 160.0808868408203\n",
      "Epoch 62, Batch 13496, Loss: 164.14390563964844\n",
      "Epoch 62, Batch 13497, Loss: 176.2847137451172\n",
      "Epoch 62, Batch 13498, Loss: 167.60711669921875\n",
      "Epoch 62, Batch 13499, Loss: 175.0808563232422\n",
      "Epoch 62, Batch 13500, Loss: 154.5701141357422\n",
      "Epoch 62, Batch 13501, Loss: 187.3575439453125\n",
      "Epoch 62, Batch 13502, Loss: 177.83963012695312\n",
      "Epoch 62, Batch 13503, Loss: 159.79806518554688\n",
      "Epoch 62, Batch 13504, Loss: 189.65074157714844\n",
      "Epoch 62, Batch 13505, Loss: 167.39599609375\n",
      "Epoch 62, Batch 13506, Loss: 180.7104034423828\n",
      "Epoch 62, Batch 13507, Loss: 183.51527404785156\n",
      "Epoch 62, Batch 13508, Loss: 162.1848907470703\n",
      "Epoch 62, Batch 13509, Loss: 166.45977783203125\n",
      "Epoch 62, Batch 13510, Loss: 178.3031768798828\n",
      "Epoch 62, Batch 13511, Loss: 175.29957580566406\n",
      "Epoch 62, Batch 13512, Loss: 167.71250915527344\n",
      "Epoch 62, Batch 13513, Loss: 192.37586975097656\n",
      "Epoch 62, Batch 13514, Loss: 170.48109436035156\n",
      "Epoch 62, Batch 13515, Loss: 173.097412109375\n",
      "Epoch 62, Batch 13516, Loss: 166.72962951660156\n",
      "Epoch 62, Batch 13517, Loss: 175.7711639404297\n",
      "Epoch 62, Batch 13518, Loss: 182.02870178222656\n",
      "Epoch 62, Batch 13519, Loss: 170.25880432128906\n",
      "Epoch 62, Batch 13520, Loss: 190.11318969726562\n",
      "Epoch 62, Batch 13521, Loss: 171.4713134765625\n",
      "Epoch 62, Batch 13522, Loss: 173.39108276367188\n",
      "Epoch 62, Batch 13523, Loss: 180.87420654296875\n",
      "Epoch 62, Batch 13524, Loss: 174.64508056640625\n",
      "Epoch 62, Batch 13525, Loss: 155.0533447265625\n",
      "Epoch 62, Batch 13526, Loss: 187.26266479492188\n",
      "Epoch 62, Batch 13527, Loss: 176.35621643066406\n",
      "Epoch 62, Batch 13528, Loss: 165.37088012695312\n",
      "Epoch 62, Batch 13529, Loss: 146.2330780029297\n",
      "Epoch 62, Batch 13530, Loss: 175.50856018066406\n",
      "Epoch 62, Batch 13531, Loss: 170.96426391601562\n",
      "Epoch 62, Batch 13532, Loss: 162.36380004882812\n",
      "Epoch 62, Batch 13533, Loss: 177.9167938232422\n",
      "Epoch 62, Batch 13534, Loss: 167.20022583007812\n",
      "Epoch 62, Batch 13535, Loss: 174.49575805664062\n",
      "Epoch 62, Batch 13536, Loss: 174.0912628173828\n",
      "Epoch 62, Batch 13537, Loss: 170.88343811035156\n",
      "Epoch 62, Batch 13538, Loss: 196.0331573486328\n",
      "Epoch 62, Batch 13539, Loss: 175.16571044921875\n",
      "Epoch 62, Batch 13540, Loss: 171.6962890625\n",
      "Epoch 62, Batch 13541, Loss: 143.23770141601562\n",
      "Epoch 62, Batch 13542, Loss: 179.653076171875\n",
      "Epoch 62, Batch 13543, Loss: 173.15496826171875\n",
      "Epoch 62, Batch 13544, Loss: 161.47860717773438\n",
      "Epoch 62, Batch 13545, Loss: 180.8368682861328\n",
      "Epoch 62, Batch 13546, Loss: 155.16873168945312\n",
      "Epoch 62, Batch 13547, Loss: 183.6249237060547\n",
      "Epoch 62, Batch 13548, Loss: 168.89309692382812\n",
      "Epoch 62, Batch 13549, Loss: 181.05421447753906\n",
      "Epoch 62, Batch 13550, Loss: 175.53680419921875\n",
      "Epoch 62, Batch 13551, Loss: 181.4444580078125\n",
      "Epoch 62, Batch 13552, Loss: 190.79766845703125\n",
      "Epoch 62, Batch 13553, Loss: 187.23973083496094\n",
      "Epoch 62, Batch 13554, Loss: 158.5758056640625\n",
      "Epoch 62, Batch 13555, Loss: 177.31703186035156\n",
      "Epoch 62, Batch 13556, Loss: 163.9038543701172\n",
      "Epoch 62, Batch 13557, Loss: 161.3103790283203\n",
      "Epoch 62, Batch 13558, Loss: 172.35902404785156\n",
      "Epoch 62, Batch 13559, Loss: 173.0110626220703\n",
      "Epoch 62, Batch 13560, Loss: 183.07186889648438\n",
      "Epoch 62, Batch 13561, Loss: 160.12892150878906\n",
      "Epoch 62, Batch 13562, Loss: 164.9977264404297\n",
      "Epoch 62, Batch 13563, Loss: 171.5968017578125\n",
      "Epoch 62, Batch 13564, Loss: 165.6251983642578\n",
      "Epoch 62, Batch 13565, Loss: 161.9750518798828\n",
      "Epoch 62, Batch 13566, Loss: 179.80865478515625\n",
      "Epoch 62, Batch 13567, Loss: 179.86289978027344\n",
      "Epoch 62, Batch 13568, Loss: 179.91819763183594\n",
      "Epoch 62, Batch 13569, Loss: 184.32261657714844\n",
      "Epoch 62, Batch 13570, Loss: 175.1894073486328\n",
      "Epoch 62, Batch 13571, Loss: 182.2267608642578\n",
      "Epoch 62, Batch 13572, Loss: 170.1840057373047\n",
      "Epoch 62, Batch 13573, Loss: 181.7021026611328\n",
      "Epoch 62, Batch 13574, Loss: 173.30751037597656\n",
      "Epoch 62, Batch 13575, Loss: 175.8424072265625\n",
      "Epoch 62, Batch 13576, Loss: 175.13421630859375\n",
      "Epoch 62, Batch 13577, Loss: 173.4665985107422\n",
      "Epoch 62, Batch 13578, Loss: 159.4985809326172\n",
      "Epoch 62, Batch 13579, Loss: 158.55751037597656\n",
      "Epoch 62, Batch 13580, Loss: 170.76051330566406\n",
      "Epoch 62, Batch 13581, Loss: 160.35858154296875\n",
      "Epoch 62, Batch 13582, Loss: 177.64584350585938\n",
      "Epoch 62, Batch 13583, Loss: 183.76629638671875\n",
      "Epoch 62, Batch 13584, Loss: 160.177734375\n",
      "Epoch 62, Batch 13585, Loss: 185.1719512939453\n",
      "Epoch 62, Batch 13586, Loss: 165.14163208007812\n",
      "Epoch 62, Batch 13587, Loss: 158.4243621826172\n",
      "Epoch 62, Batch 13588, Loss: 156.661376953125\n",
      "Epoch 62, Batch 13589, Loss: 176.4013671875\n",
      "Epoch 62, Batch 13590, Loss: 197.28077697753906\n",
      "Epoch 62, Batch 13591, Loss: 194.1348876953125\n",
      "Epoch 62, Batch 13592, Loss: 187.53587341308594\n",
      "Epoch 62, Batch 13593, Loss: 199.83250427246094\n",
      "Epoch 62, Batch 13594, Loss: 177.1422119140625\n",
      "Epoch 62, Batch 13595, Loss: 177.0336151123047\n",
      "Epoch 62, Batch 13596, Loss: 183.7141571044922\n",
      "Epoch 62, Batch 13597, Loss: 166.8374481201172\n",
      "Epoch 62, Batch 13598, Loss: 181.60055541992188\n",
      "Epoch 62, Batch 13599, Loss: 175.77667236328125\n",
      "Epoch 62, Batch 13600, Loss: 168.7713623046875\n",
      "Epoch 62, Batch 13601, Loss: 176.40594482421875\n",
      "Epoch 62, Batch 13602, Loss: 147.8226318359375\n",
      "Epoch 62, Batch 13603, Loss: 160.28143310546875\n",
      "Epoch 62, Batch 13604, Loss: 179.65615844726562\n",
      "Epoch 62, Batch 13605, Loss: 178.1404266357422\n",
      "Epoch 62, Batch 13606, Loss: 168.19000244140625\n",
      "Epoch 62, Batch 13607, Loss: 157.33448791503906\n",
      "Epoch 62, Batch 13608, Loss: 186.50973510742188\n",
      "Epoch 62, Batch 13609, Loss: 168.67938232421875\n",
      "Epoch 62, Batch 13610, Loss: 165.3917236328125\n",
      "Epoch 62, Batch 13611, Loss: 164.41786193847656\n",
      "Epoch 62, Batch 13612, Loss: 169.99034118652344\n",
      "Epoch 62, Batch 13613, Loss: 162.20367431640625\n",
      "Epoch 62, Batch 13614, Loss: 184.66123962402344\n",
      "Epoch 62, Batch 13615, Loss: 178.4156951904297\n",
      "Epoch 62, Batch 13616, Loss: 167.1603240966797\n",
      "Epoch 62, Batch 13617, Loss: 174.92979431152344\n",
      "Epoch 62, Batch 13618, Loss: 175.09396362304688\n",
      "Epoch 62, Batch 13619, Loss: 180.08009338378906\n",
      "Epoch 62, Batch 13620, Loss: 180.87957763671875\n",
      "Epoch 62, Batch 13621, Loss: 168.64703369140625\n",
      "Epoch 62, Batch 13622, Loss: 182.36610412597656\n",
      "Epoch 62, Batch 13623, Loss: 164.48931884765625\n",
      "Epoch 62, Batch 13624, Loss: 178.8671112060547\n",
      "Epoch 62, Batch 13625, Loss: 181.54649353027344\n",
      "Epoch 62, Batch 13626, Loss: 179.075439453125\n",
      "Epoch 62, Batch 13627, Loss: 171.99632263183594\n",
      "Epoch 62, Batch 13628, Loss: 147.1600341796875\n",
      "Epoch 62, Batch 13629, Loss: 183.44651794433594\n",
      "Epoch 62, Batch 13630, Loss: 181.395751953125\n",
      "Epoch 62, Batch 13631, Loss: 164.91873168945312\n",
      "Epoch 62, Batch 13632, Loss: 174.34112548828125\n",
      "Epoch 62, Batch 13633, Loss: 173.1536102294922\n",
      "Epoch 62, Batch 13634, Loss: 174.81886291503906\n",
      "Epoch 62, Batch 13635, Loss: 186.8705596923828\n",
      "Epoch 62, Batch 13636, Loss: 171.05850219726562\n",
      "Epoch 62, Batch 13637, Loss: 152.94021606445312\n",
      "Epoch 62, Batch 13638, Loss: 169.5612335205078\n",
      "Epoch 62, Batch 13639, Loss: 171.10662841796875\n",
      "Epoch 62, Batch 13640, Loss: 160.12844848632812\n",
      "Epoch 62, Batch 13641, Loss: 158.86090087890625\n",
      "Epoch 62, Batch 13642, Loss: 171.42478942871094\n",
      "Epoch 62, Batch 13643, Loss: 164.47439575195312\n",
      "Epoch 62, Batch 13644, Loss: 178.50880432128906\n",
      "Epoch 62, Batch 13645, Loss: 170.4621124267578\n",
      "Epoch 62, Batch 13646, Loss: 172.43960571289062\n",
      "Epoch 62, Batch 13647, Loss: 170.76596069335938\n",
      "Epoch 62, Batch 13648, Loss: 177.93618774414062\n",
      "Epoch 62, Batch 13649, Loss: 177.90191650390625\n",
      "Epoch 62, Batch 13650, Loss: 179.6497344970703\n",
      "Epoch 62, Batch 13651, Loss: 170.2584686279297\n",
      "Epoch 62, Batch 13652, Loss: 167.7320556640625\n",
      "Epoch 62, Batch 13653, Loss: 171.21542358398438\n",
      "Epoch 62, Batch 13654, Loss: 182.99188232421875\n",
      "Epoch 62, Batch 13655, Loss: 166.8984375\n",
      "Epoch 62, Batch 13656, Loss: 157.32818603515625\n",
      "Epoch 62, Batch 13657, Loss: 173.65582275390625\n",
      "Epoch 62, Batch 13658, Loss: 170.9949951171875\n",
      "Epoch 62, Batch 13659, Loss: 192.62088012695312\n",
      "Epoch 62, Batch 13660, Loss: 163.02713012695312\n",
      "Epoch 62, Batch 13661, Loss: 173.55137634277344\n",
      "Epoch 62, Batch 13662, Loss: 190.43174743652344\n",
      "Epoch 62, Batch 13663, Loss: 177.35939025878906\n",
      "Epoch 62, Batch 13664, Loss: 163.31591796875\n",
      "Epoch 62, Batch 13665, Loss: 160.38063049316406\n",
      "Epoch 62, Batch 13666, Loss: 161.09864807128906\n",
      "Epoch 62, Batch 13667, Loss: 180.11412048339844\n",
      "Epoch 62, Batch 13668, Loss: 175.04457092285156\n",
      "Epoch 62, Batch 13669, Loss: 158.05039978027344\n",
      "Epoch 62, Batch 13670, Loss: 157.37632751464844\n",
      "Epoch 62, Batch 13671, Loss: 192.72738647460938\n",
      "Epoch 62, Batch 13672, Loss: 194.27845764160156\n",
      "Epoch 62, Batch 13673, Loss: 176.65960693359375\n",
      "Epoch 62, Batch 13674, Loss: 174.6348419189453\n",
      "Epoch 62, Batch 13675, Loss: 170.26206970214844\n",
      "Epoch 62, Batch 13676, Loss: 163.7128143310547\n",
      "Epoch 62, Batch 13677, Loss: 189.7073974609375\n",
      "Epoch 62, Batch 13678, Loss: 171.20089721679688\n",
      "Epoch 62, Batch 13679, Loss: 164.87112426757812\n",
      "Epoch 62, Batch 13680, Loss: 181.67083740234375\n",
      "Epoch 62, Batch 13681, Loss: 165.84213256835938\n",
      "Epoch 62, Batch 13682, Loss: 168.7996063232422\n",
      "Epoch 62, Batch 13683, Loss: 168.15835571289062\n",
      "Epoch 62, Batch 13684, Loss: 169.2237548828125\n",
      "Epoch 62, Batch 13685, Loss: 176.45677185058594\n",
      "Epoch 62, Batch 13686, Loss: 176.94544982910156\n",
      "Epoch 62, Batch 13687, Loss: 179.11253356933594\n",
      "Epoch 62, Batch 13688, Loss: 186.23582458496094\n",
      "Epoch 62, Batch 13689, Loss: 173.90257263183594\n",
      "Epoch 62, Batch 13690, Loss: 154.67330932617188\n",
      "Epoch 62, Batch 13691, Loss: 170.4088134765625\n",
      "Epoch 62, Batch 13692, Loss: 178.55459594726562\n",
      "Epoch 62, Batch 13693, Loss: 163.37059020996094\n",
      "Epoch 62, Batch 13694, Loss: 172.80224609375\n",
      "Epoch 62, Batch 13695, Loss: 164.64036560058594\n",
      "Epoch 62, Batch 13696, Loss: 183.34629821777344\n",
      "Epoch 62, Batch 13697, Loss: 154.65646362304688\n",
      "Epoch 62, Batch 13698, Loss: 184.07798767089844\n",
      "Epoch 62, Batch 13699, Loss: 182.65464782714844\n",
      "Epoch 62, Batch 13700, Loss: 168.89625549316406\n",
      "Epoch 62, Batch 13701, Loss: 176.44395446777344\n",
      "Epoch 62, Batch 13702, Loss: 170.9843292236328\n",
      "Epoch 62, Batch 13703, Loss: 187.96084594726562\n",
      "Epoch 62, Batch 13704, Loss: 167.30160522460938\n",
      "Epoch 62, Batch 13705, Loss: 154.51971435546875\n",
      "Epoch 62, Batch 13706, Loss: 163.17318725585938\n",
      "Epoch 62, Batch 13707, Loss: 163.93832397460938\n",
      "Epoch 62, Batch 13708, Loss: 156.103271484375\n",
      "Epoch 62, Batch 13709, Loss: 185.9877166748047\n",
      "Epoch 62, Batch 13710, Loss: 180.536865234375\n",
      "Epoch 62, Batch 13711, Loss: 173.11009216308594\n",
      "Epoch 62, Batch 13712, Loss: 163.55682373046875\n",
      "Epoch 62, Batch 13713, Loss: 183.67039489746094\n",
      "Epoch 62, Batch 13714, Loss: 175.00535583496094\n",
      "Epoch 62, Batch 13715, Loss: 171.28689575195312\n",
      "Epoch 62, Batch 13716, Loss: 184.8246612548828\n",
      "Epoch 62, Batch 13717, Loss: 169.17312622070312\n",
      "Epoch 62, Batch 13718, Loss: 162.7412567138672\n",
      "Epoch 62, Batch 13719, Loss: 193.10073852539062\n",
      "Epoch 62, Batch 13720, Loss: 179.92962646484375\n",
      "Epoch 62, Batch 13721, Loss: 189.9720458984375\n",
      "Epoch 62, Batch 13722, Loss: 157.44342041015625\n",
      "Epoch 62, Batch 13723, Loss: 181.43540954589844\n",
      "Epoch 62, Batch 13724, Loss: 175.49705505371094\n",
      "Epoch 62, Batch 13725, Loss: 171.14825439453125\n",
      "Epoch 62, Batch 13726, Loss: 169.82000732421875\n",
      "Epoch 62, Batch 13727, Loss: 161.85867309570312\n",
      "Epoch 62, Batch 13728, Loss: 177.62139892578125\n",
      "Epoch 62, Batch 13729, Loss: 176.57943725585938\n",
      "Epoch 62, Batch 13730, Loss: 177.97377014160156\n",
      "Epoch 62, Batch 13731, Loss: 172.48541259765625\n",
      "Epoch 62, Batch 13732, Loss: 162.05975341796875\n",
      "Epoch 62, Batch 13733, Loss: 166.18182373046875\n",
      "Epoch 62, Batch 13734, Loss: 172.13958740234375\n",
      "Epoch 62, Batch 13735, Loss: 148.818603515625\n",
      "Epoch 62, Batch 13736, Loss: 177.5022430419922\n",
      "Epoch 62, Batch 13737, Loss: 175.7256622314453\n",
      "Epoch 62, Batch 13738, Loss: 175.7942657470703\n",
      "Epoch 62, Batch 13739, Loss: 175.37535095214844\n",
      "Epoch 62, Batch 13740, Loss: 163.87356567382812\n",
      "Epoch 62, Batch 13741, Loss: 182.8823699951172\n",
      "Epoch 62, Batch 13742, Loss: 173.3052978515625\n",
      "Epoch 62, Batch 13743, Loss: 196.52503967285156\n",
      "Epoch 62, Batch 13744, Loss: 171.57421875\n",
      "Epoch 62, Batch 13745, Loss: 150.80001831054688\n",
      "Epoch 62, Batch 13746, Loss: 188.98471069335938\n",
      "Epoch 62, Batch 13747, Loss: 177.6380615234375\n",
      "Epoch 62, Batch 13748, Loss: 172.81460571289062\n",
      "Epoch 62, Batch 13749, Loss: 172.50743103027344\n",
      "Epoch 62, Batch 13750, Loss: 173.95553588867188\n",
      "Epoch 62, Batch 13751, Loss: 162.32208251953125\n",
      "Epoch 62, Batch 13752, Loss: 159.4906463623047\n",
      "Epoch 62, Batch 13753, Loss: 165.72715759277344\n",
      "Epoch 62, Batch 13754, Loss: 164.99095153808594\n",
      "Epoch 62, Batch 13755, Loss: 164.6296844482422\n",
      "Epoch 62, Batch 13756, Loss: 170.56283569335938\n",
      "Epoch 62, Batch 13757, Loss: 189.43331909179688\n",
      "Epoch 62, Batch 13758, Loss: 170.39634704589844\n",
      "Epoch 62, Batch 13759, Loss: 182.73489379882812\n",
      "Epoch 62, Batch 13760, Loss: 168.84185791015625\n",
      "Epoch 62, Batch 13761, Loss: 182.59019470214844\n",
      "Epoch 62, Batch 13762, Loss: 168.86019897460938\n",
      "Epoch 62, Batch 13763, Loss: 169.55844116210938\n",
      "Epoch 62, Batch 13764, Loss: 178.83555603027344\n",
      "Epoch 62, Batch 13765, Loss: 154.861572265625\n",
      "Epoch 62, Batch 13766, Loss: 168.5441436767578\n",
      "Epoch 62, Batch 13767, Loss: 169.51455688476562\n",
      "Epoch 62, Batch 13768, Loss: 165.3116455078125\n",
      "Epoch 62, Batch 13769, Loss: 159.00717163085938\n",
      "Epoch 62, Batch 13770, Loss: 164.80477905273438\n",
      "Epoch 62, Batch 13771, Loss: 165.36798095703125\n",
      "Epoch 62, Batch 13772, Loss: 170.30982971191406\n",
      "Epoch 62, Batch 13773, Loss: 176.7487335205078\n",
      "Epoch 62, Batch 13774, Loss: 161.5712890625\n",
      "Epoch 62, Batch 13775, Loss: 168.69100952148438\n",
      "Epoch 62, Batch 13776, Loss: 161.75401306152344\n",
      "Epoch 62, Batch 13777, Loss: 177.93182373046875\n",
      "Epoch 62, Batch 13778, Loss: 172.43679809570312\n",
      "Epoch 62, Batch 13779, Loss: 160.60391235351562\n",
      "Epoch 62, Batch 13780, Loss: 162.23402404785156\n",
      "Epoch 62, Batch 13781, Loss: 182.584228515625\n",
      "Epoch 62, Batch 13782, Loss: 184.56761169433594\n",
      "Epoch 62, Batch 13783, Loss: 162.33929443359375\n",
      "Epoch 62, Batch 13784, Loss: 171.94345092773438\n",
      "Epoch 62, Batch 13785, Loss: 159.5152587890625\n",
      "Epoch 62, Batch 13786, Loss: 186.9339599609375\n",
      "Epoch 62, Batch 13787, Loss: 178.00611877441406\n",
      "Epoch 62, Batch 13788, Loss: 170.80308532714844\n",
      "Epoch 62, Batch 13789, Loss: 168.7523193359375\n",
      "Epoch 62, Batch 13790, Loss: 169.7625732421875\n",
      "Epoch 62, Batch 13791, Loss: 164.1648406982422\n",
      "Epoch 62, Batch 13792, Loss: 174.43057250976562\n",
      "Epoch 62, Batch 13793, Loss: 183.30039978027344\n",
      "Epoch 62, Batch 13794, Loss: 174.7907257080078\n",
      "Epoch 62, Batch 13795, Loss: 161.4655303955078\n",
      "Epoch 62, Batch 13796, Loss: 188.07669067382812\n",
      "Epoch 62, Batch 13797, Loss: 190.53829956054688\n",
      "Epoch 62, Batch 13798, Loss: 178.52577209472656\n",
      "Epoch 62, Batch 13799, Loss: 171.60018920898438\n",
      "Epoch 62, Batch 13800, Loss: 181.11863708496094\n",
      "Epoch 62, Batch 13801, Loss: 174.25018310546875\n",
      "Epoch 62, Batch 13802, Loss: 168.00921630859375\n",
      "Epoch 62, Batch 13803, Loss: 169.37664794921875\n",
      "Epoch 62, Batch 13804, Loss: 177.15167236328125\n",
      "Epoch 62, Batch 13805, Loss: 167.28973388671875\n",
      "Epoch 62, Batch 13806, Loss: 158.63291931152344\n",
      "Epoch 62, Batch 13807, Loss: 163.51858520507812\n",
      "Epoch 62, Batch 13808, Loss: 169.35189819335938\n",
      "Epoch 62, Batch 13809, Loss: 178.28570556640625\n",
      "Epoch 62, Batch 13810, Loss: 167.16220092773438\n",
      "Epoch 62, Batch 13811, Loss: 168.3853759765625\n",
      "Epoch 62, Batch 13812, Loss: 178.12623596191406\n",
      "Epoch 62, Batch 13813, Loss: 167.4258270263672\n",
      "Epoch 62, Batch 13814, Loss: 165.1569061279297\n",
      "Epoch 62, Batch 13815, Loss: 171.92709350585938\n",
      "Epoch 62, Batch 13816, Loss: 174.38975524902344\n",
      "Epoch 62, Batch 13817, Loss: 172.4243621826172\n",
      "Epoch 62, Batch 13818, Loss: 196.26406860351562\n",
      "Epoch 62, Batch 13819, Loss: 168.4360809326172\n",
      "Epoch 62, Batch 13820, Loss: 166.0183868408203\n",
      "Epoch 62, Batch 13821, Loss: 167.44602966308594\n",
      "Epoch 62, Batch 13822, Loss: 161.04986572265625\n",
      "Epoch 62, Batch 13823, Loss: 185.0078125\n",
      "Epoch 62, Batch 13824, Loss: 167.00807189941406\n",
      "Epoch 62, Batch 13825, Loss: 176.9212188720703\n",
      "Epoch 62, Batch 13826, Loss: 170.54270935058594\n",
      "Epoch 62, Batch 13827, Loss: 165.25244140625\n",
      "Epoch 62, Batch 13828, Loss: 201.19732666015625\n",
      "Epoch 62, Batch 13829, Loss: 178.26918029785156\n",
      "Epoch 62, Batch 13830, Loss: 177.77183532714844\n",
      "Epoch 62, Batch 13831, Loss: 177.19822692871094\n",
      "Epoch 62, Batch 13832, Loss: 184.2798614501953\n",
      "Epoch 62, Batch 13833, Loss: 173.46949768066406\n",
      "Epoch 62, Batch 13834, Loss: 170.36935424804688\n",
      "Epoch 62, Batch 13835, Loss: 170.36988830566406\n",
      "Epoch 62, Batch 13836, Loss: 173.2938232421875\n",
      "Epoch 62, Batch 13837, Loss: 197.58168029785156\n",
      "Epoch 62, Batch 13838, Loss: 170.85487365722656\n",
      "Epoch 62, Batch 13839, Loss: 169.95184326171875\n",
      "Epoch 62, Batch 13840, Loss: 161.84628295898438\n",
      "Epoch 62, Batch 13841, Loss: 191.8353271484375\n",
      "Epoch 62, Batch 13842, Loss: 181.9981689453125\n",
      "Epoch 62, Batch 13843, Loss: 193.65621948242188\n",
      "Epoch 62, Batch 13844, Loss: 160.14088439941406\n",
      "Epoch 62, Batch 13845, Loss: 173.2677459716797\n",
      "Epoch 62, Batch 13846, Loss: 186.31488037109375\n",
      "Epoch 62, Batch 13847, Loss: 182.19923400878906\n",
      "Epoch 62, Batch 13848, Loss: 175.6177215576172\n",
      "Epoch 62, Batch 13849, Loss: 172.65731811523438\n",
      "Epoch 62, Batch 13850, Loss: 161.7843017578125\n",
      "Epoch 62, Batch 13851, Loss: 190.67910766601562\n",
      "Epoch 62, Batch 13852, Loss: 179.94642639160156\n",
      "Epoch 62, Batch 13853, Loss: 171.40914916992188\n",
      "Epoch 62, Batch 13854, Loss: 169.2722625732422\n",
      "Epoch 62, Batch 13855, Loss: 168.74147033691406\n",
      "Epoch 62, Batch 13856, Loss: 167.14942932128906\n",
      "Epoch 62, Batch 13857, Loss: 163.71485900878906\n",
      "Epoch 62, Batch 13858, Loss: 149.33763122558594\n",
      "Epoch 62, Batch 13859, Loss: 173.99293518066406\n",
      "Epoch 62, Batch 13860, Loss: 177.048828125\n",
      "Epoch 62, Batch 13861, Loss: 154.29534912109375\n",
      "Epoch 62, Batch 13862, Loss: 173.2295379638672\n",
      "Epoch 62, Batch 13863, Loss: 161.4476318359375\n",
      "Epoch 62, Batch 13864, Loss: 180.0847625732422\n",
      "Epoch 62, Batch 13865, Loss: 174.47158813476562\n",
      "Epoch 62, Batch 13866, Loss: 166.7375946044922\n",
      "Epoch 62, Batch 13867, Loss: 178.45123291015625\n",
      "Epoch 62, Batch 13868, Loss: 165.099853515625\n",
      "Epoch 62, Batch 13869, Loss: 183.31654357910156\n",
      "Epoch 62, Batch 13870, Loss: 167.4011993408203\n",
      "Epoch 62, Batch 13871, Loss: 183.8583526611328\n",
      "Epoch 62, Batch 13872, Loss: 171.6359100341797\n",
      "Epoch 62, Batch 13873, Loss: 173.4876708984375\n",
      "Epoch 62, Batch 13874, Loss: 170.2901611328125\n",
      "Epoch 62, Batch 13875, Loss: 166.37916564941406\n",
      "Epoch 62, Batch 13876, Loss: 164.9476776123047\n",
      "Epoch 62, Batch 13877, Loss: 159.43963623046875\n",
      "Epoch 62, Batch 13878, Loss: 153.8485565185547\n",
      "Epoch 62, Batch 13879, Loss: 177.21214294433594\n",
      "Epoch 62, Batch 13880, Loss: 170.44677734375\n",
      "Epoch 62, Batch 13881, Loss: 175.90762329101562\n",
      "Epoch 62, Batch 13882, Loss: 157.31954956054688\n",
      "Epoch 62, Batch 13883, Loss: 192.812255859375\n",
      "Epoch 62, Batch 13884, Loss: 188.3826141357422\n",
      "Epoch 62, Batch 13885, Loss: 164.3306427001953\n",
      "Epoch 62, Batch 13886, Loss: 171.71913146972656\n",
      "Epoch 62, Batch 13887, Loss: 169.30294799804688\n",
      "Epoch 62, Batch 13888, Loss: 178.89402770996094\n",
      "Epoch 62, Batch 13889, Loss: 155.32041931152344\n",
      "Epoch 62, Batch 13890, Loss: 175.1189727783203\n",
      "Epoch 62, Batch 13891, Loss: 173.4781036376953\n",
      "Epoch 62, Batch 13892, Loss: 170.7236785888672\n",
      "Epoch 62, Batch 13893, Loss: 156.98065185546875\n",
      "Epoch 62, Batch 13894, Loss: 172.88746643066406\n",
      "Epoch 62, Batch 13895, Loss: 159.9280548095703\n",
      "Epoch 62, Batch 13896, Loss: 173.46304321289062\n",
      "Epoch 62, Batch 13897, Loss: 194.9761505126953\n",
      "Epoch 62, Batch 13898, Loss: 176.53704833984375\n",
      "Epoch 62, Batch 13899, Loss: 157.13912963867188\n",
      "Epoch 62, Batch 13900, Loss: 171.24868774414062\n",
      "Epoch 62, Batch 13901, Loss: 169.95228576660156\n",
      "Epoch 62, Batch 13902, Loss: 171.91177368164062\n",
      "Epoch 62, Batch 13903, Loss: 169.62179565429688\n",
      "Epoch 62, Batch 13904, Loss: 172.49203491210938\n",
      "Epoch 62, Batch 13905, Loss: 175.4239959716797\n",
      "Epoch 62, Batch 13906, Loss: 176.92283630371094\n",
      "Epoch 62, Batch 13907, Loss: 167.28677368164062\n",
      "Epoch 62, Batch 13908, Loss: 174.69935607910156\n",
      "Epoch 62, Batch 13909, Loss: 152.87229919433594\n",
      "Epoch 62, Batch 13910, Loss: 173.5005340576172\n",
      "Epoch 62, Batch 13911, Loss: 169.98477172851562\n",
      "Epoch 62, Batch 13912, Loss: 172.74427795410156\n",
      "Epoch 62, Batch 13913, Loss: 178.14181518554688\n",
      "Epoch 62, Batch 13914, Loss: 165.32891845703125\n",
      "Epoch 62, Batch 13915, Loss: 164.92001342773438\n",
      "Epoch 62, Batch 13916, Loss: 173.998779296875\n",
      "Epoch 62, Batch 13917, Loss: 171.78353881835938\n",
      "Epoch 62, Batch 13918, Loss: 163.53640747070312\n",
      "Epoch 62, Batch 13919, Loss: 192.37615966796875\n",
      "Epoch 62, Batch 13920, Loss: 182.72491455078125\n",
      "Epoch 62, Batch 13921, Loss: 176.1074981689453\n",
      "Epoch 62, Batch 13922, Loss: 172.6969451904297\n",
      "Epoch 62, Batch 13923, Loss: 155.10064697265625\n",
      "Epoch 62, Batch 13924, Loss: 174.78396606445312\n",
      "Epoch 62, Batch 13925, Loss: 173.7082977294922\n",
      "Epoch 62, Batch 13926, Loss: 167.4786376953125\n",
      "Epoch 62, Batch 13927, Loss: 180.87245178222656\n",
      "Epoch 62, Batch 13928, Loss: 173.12184143066406\n",
      "Epoch 62, Batch 13929, Loss: 159.29322814941406\n",
      "Epoch 62, Batch 13930, Loss: 171.77682495117188\n",
      "Epoch 62, Batch 13931, Loss: 162.77407836914062\n",
      "Epoch 62, Batch 13932, Loss: 180.2261962890625\n",
      "Epoch 62, Batch 13933, Loss: 163.1223602294922\n",
      "Epoch 62, Batch 13934, Loss: 171.24658203125\n",
      "Epoch 62, Batch 13935, Loss: 182.33753967285156\n",
      "Epoch 62, Batch 13936, Loss: 166.32867431640625\n",
      "Epoch 62, Batch 13937, Loss: 180.952880859375\n",
      "Epoch 62, Batch 13938, Loss: 176.18997192382812\n",
      "Epoch 62, Batch 13939, Loss: 168.35215759277344\n",
      "Epoch 62, Batch 13940, Loss: 163.37445068359375\n",
      "Epoch 62, Batch 13941, Loss: 156.87063598632812\n",
      "Epoch 62, Batch 13942, Loss: 171.017578125\n",
      "Epoch 62, Batch 13943, Loss: 162.28211975097656\n",
      "Epoch 62, Batch 13944, Loss: 179.14794921875\n",
      "Epoch 62, Batch 13945, Loss: 170.31077575683594\n",
      "Epoch 62, Batch 13946, Loss: 186.2362518310547\n",
      "Epoch 62, Batch 13947, Loss: 163.83926391601562\n",
      "Epoch 62, Batch 13948, Loss: 178.55990600585938\n",
      "Epoch 62, Batch 13949, Loss: 170.40379333496094\n",
      "Epoch 62, Batch 13950, Loss: 191.2943878173828\n",
      "Epoch 62, Batch 13951, Loss: 158.97325134277344\n",
      "Epoch 62, Batch 13952, Loss: 182.45257568359375\n",
      "Epoch 62, Batch 13953, Loss: 179.68003845214844\n",
      "Epoch 62, Batch 13954, Loss: 186.40692138671875\n",
      "Epoch 62, Batch 13955, Loss: 177.42349243164062\n",
      "Epoch 62, Batch 13956, Loss: 182.0965576171875\n",
      "Epoch 62, Batch 13957, Loss: 187.6255645751953\n",
      "Epoch 62, Batch 13958, Loss: 179.44972229003906\n",
      "Epoch 62, Batch 13959, Loss: 169.31886291503906\n",
      "Epoch 62, Batch 13960, Loss: 165.66981506347656\n",
      "Epoch 62, Batch 13961, Loss: 172.5842742919922\n",
      "Epoch 62, Batch 13962, Loss: 172.30186462402344\n",
      "Epoch 62, Batch 13963, Loss: 180.40692138671875\n",
      "Epoch 62, Batch 13964, Loss: 177.56207275390625\n",
      "Epoch 62, Batch 13965, Loss: 169.02000427246094\n",
      "Epoch 62, Batch 13966, Loss: 165.39476013183594\n",
      "Epoch 62, Batch 13967, Loss: 165.05563354492188\n",
      "Epoch 62, Batch 13968, Loss: 167.5341339111328\n",
      "Epoch 62, Batch 13969, Loss: 172.25369262695312\n",
      "Epoch 62, Batch 13970, Loss: 175.68939208984375\n",
      "Epoch 62, Batch 13971, Loss: 173.14382934570312\n",
      "Epoch 62, Batch 13972, Loss: 192.82974243164062\n",
      "Epoch 62, Batch 13973, Loss: 162.43417358398438\n",
      "Epoch 62, Batch 13974, Loss: 162.31080627441406\n",
      "Epoch 62, Batch 13975, Loss: 177.26649475097656\n",
      "Epoch 62, Batch 13976, Loss: 169.56524658203125\n",
      "Epoch 62, Batch 13977, Loss: 174.89024353027344\n",
      "Epoch 62, Batch 13978, Loss: 182.19766235351562\n",
      "Epoch 62, Batch 13979, Loss: 177.07659912109375\n",
      "Epoch 62, Batch 13980, Loss: 181.9383087158203\n",
      "Epoch 62, Batch 13981, Loss: 182.93479919433594\n",
      "Epoch 62, Batch 13982, Loss: 169.21432495117188\n",
      "Epoch 62, Batch 13983, Loss: 166.8524627685547\n",
      "Epoch 62, Batch 13984, Loss: 157.78280639648438\n",
      "Epoch 62, Batch 13985, Loss: 174.03851318359375\n",
      "Epoch 62, Batch 13986, Loss: 176.91651916503906\n",
      "Epoch 62, Batch 13987, Loss: 173.85316467285156\n",
      "Epoch 62, Batch 13988, Loss: 170.67291259765625\n",
      "Epoch 62, Batch 13989, Loss: 175.2574920654297\n",
      "Epoch 62, Batch 13990, Loss: 173.30252075195312\n",
      "Epoch 62, Batch 13991, Loss: 170.76052856445312\n",
      "Epoch 62, Batch 13992, Loss: 175.49085998535156\n",
      "Epoch 62, Batch 13993, Loss: 183.58969116210938\n",
      "Epoch 62, Batch 13994, Loss: 163.34140014648438\n",
      "Epoch 62, Batch 13995, Loss: 168.0188446044922\n",
      "Epoch 62, Batch 13996, Loss: 169.74073791503906\n",
      "Epoch 62, Batch 13997, Loss: 172.5009002685547\n",
      "Epoch 62, Batch 13998, Loss: 174.15936279296875\n",
      "Epoch 62, Batch 13999, Loss: 168.7093048095703\n",
      "Epoch 62, Batch 14000, Loss: 168.50299072265625\n",
      "Epoch 62, Batch 14001, Loss: 165.61154174804688\n",
      "Epoch 62, Batch 14002, Loss: 171.9147491455078\n",
      "Epoch 62, Batch 14003, Loss: 171.8580322265625\n",
      "Epoch 62, Batch 14004, Loss: 174.10726928710938\n",
      "Epoch 62, Batch 14005, Loss: 175.5918731689453\n",
      "Epoch 62, Batch 14006, Loss: 185.9744110107422\n",
      "Epoch 62, Batch 14007, Loss: 174.55044555664062\n",
      "Epoch 62, Batch 14008, Loss: 166.71804809570312\n",
      "Epoch 62, Batch 14009, Loss: 176.13449096679688\n",
      "Epoch 62, Batch 14010, Loss: 172.89981079101562\n",
      "Epoch 62, Batch 14011, Loss: 167.9568634033203\n",
      "Epoch 62, Batch 14012, Loss: 180.05062866210938\n",
      "Epoch 62, Batch 14013, Loss: 184.27232360839844\n",
      "Epoch 62, Batch 14014, Loss: 176.4717254638672\n",
      "Epoch 62, Batch 14015, Loss: 178.62657165527344\n",
      "Epoch 62, Batch 14016, Loss: 182.216796875\n",
      "Epoch 62, Batch 14017, Loss: 163.4864044189453\n",
      "Epoch 62, Batch 14018, Loss: 185.95135498046875\n",
      "Epoch 62, Batch 14019, Loss: 166.29632568359375\n",
      "Epoch 62, Batch 14020, Loss: 178.6664276123047\n",
      "Epoch 62, Batch 14021, Loss: 159.58761596679688\n",
      "Epoch 62, Batch 14022, Loss: 182.86065673828125\n",
      "Epoch 62, Batch 14023, Loss: 185.31690979003906\n",
      "Epoch 62, Batch 14024, Loss: 171.9193115234375\n",
      "Epoch 62, Batch 14025, Loss: 161.055419921875\n",
      "Epoch 62, Batch 14026, Loss: 173.2236785888672\n",
      "Epoch 62, Batch 14027, Loss: 192.2357177734375\n",
      "Epoch 62, Batch 14028, Loss: 174.32127380371094\n",
      "Epoch 62, Batch 14029, Loss: 166.6773223876953\n",
      "Epoch 62, Batch 14030, Loss: 160.8601531982422\n",
      "Epoch 62, Batch 14031, Loss: 182.04209899902344\n",
      "Epoch 62, Batch 14032, Loss: 176.06582641601562\n",
      "Epoch 62, Batch 14033, Loss: 182.89048767089844\n",
      "Epoch 62, Batch 14034, Loss: 162.7065887451172\n",
      "Epoch 62, Batch 14035, Loss: 174.750244140625\n",
      "Epoch 62, Batch 14036, Loss: 176.2191925048828\n",
      "Epoch 62, Batch 14037, Loss: 179.41998291015625\n",
      "Epoch 62, Batch 14038, Loss: 181.47265625\n",
      "Epoch 62, Batch 14039, Loss: 171.76068115234375\n",
      "Epoch 62, Batch 14040, Loss: 164.84109497070312\n",
      "Epoch 62, Batch 14041, Loss: 154.34368896484375\n",
      "Epoch 62, Batch 14042, Loss: 155.2443389892578\n",
      "Epoch 62, Batch 14043, Loss: 178.24598693847656\n",
      "Epoch 62, Batch 14044, Loss: 169.0400848388672\n",
      "Epoch 62, Batch 14045, Loss: 165.1209716796875\n",
      "Epoch 62, Batch 14046, Loss: 156.69454956054688\n",
      "Epoch 62, Batch 14047, Loss: 168.85606384277344\n",
      "Epoch 62, Batch 14048, Loss: 184.33016967773438\n",
      "Epoch 62, Batch 14049, Loss: 172.9251251220703\n",
      "Epoch 62, Batch 14050, Loss: 183.54849243164062\n",
      "Epoch 62, Batch 14051, Loss: 185.96759033203125\n",
      "Epoch 62, Batch 14052, Loss: 187.62692260742188\n",
      "Epoch 62, Batch 14053, Loss: 181.9003448486328\n",
      "Epoch 62, Batch 14054, Loss: 159.50076293945312\n",
      "Epoch 62, Batch 14055, Loss: 178.76194763183594\n",
      "Epoch 62, Batch 14056, Loss: 189.53892517089844\n",
      "Epoch 62, Batch 14057, Loss: 188.4009246826172\n",
      "Epoch 62, Batch 14058, Loss: 182.44677734375\n",
      "Epoch 62, Batch 14059, Loss: 177.33956909179688\n",
      "Epoch 62, Batch 14060, Loss: 176.6251983642578\n",
      "Epoch 62, Batch 14061, Loss: 172.32737731933594\n",
      "Epoch 62, Batch 14062, Loss: 160.6059112548828\n",
      "Epoch 62, Batch 14063, Loss: 169.20037841796875\n",
      "Epoch 62, Batch 14064, Loss: 174.14610290527344\n",
      "Epoch 62, Batch 14065, Loss: 160.2529296875\n",
      "Epoch 62, Batch 14066, Loss: 159.39662170410156\n",
      "Epoch 62, Batch 14067, Loss: 170.11697387695312\n",
      "Epoch 62, Batch 14068, Loss: 176.96063232421875\n",
      "Epoch 62, Batch 14069, Loss: 165.8517303466797\n",
      "Epoch 62, Batch 14070, Loss: 162.34779357910156\n",
      "Epoch 62, Batch 14071, Loss: 174.76866149902344\n",
      "Epoch 62, Batch 14072, Loss: 169.30160522460938\n",
      "Epoch 62, Batch 14073, Loss: 182.7749786376953\n",
      "Epoch 62, Batch 14074, Loss: 180.65219116210938\n",
      "Epoch 62, Batch 14075, Loss: 181.3422393798828\n",
      "Epoch 62, Batch 14076, Loss: 166.97177124023438\n",
      "Epoch 62, Batch 14077, Loss: 165.53961181640625\n",
      "Epoch 62, Batch 14078, Loss: 189.38345336914062\n",
      "Epoch 62, Batch 14079, Loss: 179.48117065429688\n",
      "Epoch 62, Batch 14080, Loss: 180.4447479248047\n",
      "Epoch 62, Batch 14081, Loss: 183.3937530517578\n",
      "Epoch 62, Batch 14082, Loss: 180.31185913085938\n",
      "Epoch 62, Batch 14083, Loss: 184.78709411621094\n",
      "Epoch 62, Batch 14084, Loss: 160.36009216308594\n",
      "Epoch 62, Batch 14085, Loss: 170.9888153076172\n",
      "Epoch 62, Batch 14086, Loss: 180.78842163085938\n",
      "Epoch 62, Batch 14087, Loss: 156.50633239746094\n",
      "Epoch 62, Batch 14088, Loss: 176.21018981933594\n",
      "Epoch 62, Batch 14089, Loss: 180.5015106201172\n",
      "Epoch 62, Batch 14090, Loss: 176.69692993164062\n",
      "Epoch 62, Batch 14091, Loss: 160.85772705078125\n",
      "Epoch 62, Batch 14092, Loss: 173.38278198242188\n",
      "Epoch 62, Batch 14093, Loss: 166.00746154785156\n",
      "Epoch 62, Batch 14094, Loss: 158.49000549316406\n",
      "Epoch 62, Batch 14095, Loss: 168.1559600830078\n",
      "Epoch 62, Batch 14096, Loss: 177.08518981933594\n",
      "Epoch 62, Batch 14097, Loss: 163.91964721679688\n",
      "Epoch 62, Batch 14098, Loss: 170.80593872070312\n",
      "Epoch 62, Batch 14099, Loss: 176.81646728515625\n",
      "Epoch 62, Batch 14100, Loss: 173.32688903808594\n",
      "Epoch 62, Batch 14101, Loss: 170.21746826171875\n",
      "Epoch 62, Batch 14102, Loss: 164.39111328125\n",
      "Epoch 62, Batch 14103, Loss: 189.23431396484375\n",
      "Epoch 62, Batch 14104, Loss: 167.9503173828125\n",
      "Epoch 62, Batch 14105, Loss: 165.87054443359375\n",
      "Epoch 62, Batch 14106, Loss: 165.2307891845703\n",
      "Epoch 62, Batch 14107, Loss: 149.45794677734375\n",
      "Epoch 62, Batch 14108, Loss: 175.21270751953125\n",
      "Epoch 62, Batch 14109, Loss: 188.0045623779297\n",
      "Epoch 62, Batch 14110, Loss: 180.31588745117188\n",
      "Epoch 62, Batch 14111, Loss: 161.79664611816406\n",
      "Epoch 62, Batch 14112, Loss: 164.81484985351562\n",
      "Epoch 62, Batch 14113, Loss: 176.08827209472656\n",
      "Epoch 62, Batch 14114, Loss: 168.67532348632812\n",
      "Epoch 62, Batch 14115, Loss: 183.9957275390625\n",
      "Epoch 62, Batch 14116, Loss: 166.03643798828125\n",
      "Epoch 62, Batch 14117, Loss: 172.34658813476562\n",
      "Epoch 62, Batch 14118, Loss: 184.53773498535156\n",
      "Epoch 62, Batch 14119, Loss: 160.95986938476562\n",
      "Epoch 62, Batch 14120, Loss: 171.21694946289062\n",
      "Epoch 62, Batch 14121, Loss: 190.26438903808594\n",
      "Epoch 62, Batch 14122, Loss: 169.8860626220703\n",
      "Epoch 62, Batch 14123, Loss: 172.02645874023438\n",
      "Epoch 62, Batch 14124, Loss: 199.9079132080078\n",
      "Epoch 62, Batch 14125, Loss: 174.09298706054688\n",
      "Epoch 62, Batch 14126, Loss: 171.81106567382812\n",
      "Epoch 62, Batch 14127, Loss: 184.8681640625\n",
      "Epoch 62, Batch 14128, Loss: 163.0621337890625\n",
      "Epoch 62, Batch 14129, Loss: 179.32086181640625\n",
      "Epoch 62, Batch 14130, Loss: 170.4712371826172\n",
      "Epoch 62, Batch 14131, Loss: 175.3683624267578\n",
      "Epoch 62, Batch 14132, Loss: 173.4115753173828\n",
      "Epoch 62, Batch 14133, Loss: 167.34909057617188\n",
      "Epoch 62, Batch 14134, Loss: 175.71261596679688\n",
      "Epoch 62, Batch 14135, Loss: 186.79226684570312\n",
      "Epoch 62, Batch 14136, Loss: 185.52392578125\n",
      "Epoch 62, Batch 14137, Loss: 194.57870483398438\n",
      "Epoch 62, Batch 14138, Loss: 182.28350830078125\n",
      "Epoch 62, Batch 14139, Loss: 189.12217712402344\n",
      "Epoch 62, Batch 14140, Loss: 167.5544891357422\n",
      "Epoch 62, Batch 14141, Loss: 175.07769775390625\n",
      "Epoch 62, Batch 14142, Loss: 168.25674438476562\n",
      "Epoch 62, Batch 14143, Loss: 156.6085205078125\n",
      "Epoch 62, Batch 14144, Loss: 182.04083251953125\n",
      "Epoch 62, Batch 14145, Loss: 172.79171752929688\n",
      "Epoch 62, Batch 14146, Loss: 174.37232971191406\n",
      "Epoch 62, Batch 14147, Loss: 169.94912719726562\n",
      "Epoch 62, Batch 14148, Loss: 173.6704864501953\n",
      "Epoch 62, Batch 14149, Loss: 160.052978515625\n",
      "Epoch 62, Batch 14150, Loss: 179.21607971191406\n",
      "Epoch 62, Batch 14151, Loss: 170.57008361816406\n",
      "Epoch 62, Batch 14152, Loss: 156.4287567138672\n",
      "Epoch 62, Batch 14153, Loss: 178.281494140625\n",
      "Epoch 62, Batch 14154, Loss: 173.22238159179688\n",
      "Epoch 62, Batch 14155, Loss: 165.20980834960938\n",
      "Epoch 62, Batch 14156, Loss: 169.37205505371094\n",
      "Epoch 62, Batch 14157, Loss: 179.17092895507812\n",
      "Epoch 62, Batch 14158, Loss: 178.40805053710938\n",
      "Epoch 62, Batch 14159, Loss: 166.6018829345703\n",
      "Epoch 62, Batch 14160, Loss: 162.15939331054688\n",
      "Epoch 62, Batch 14161, Loss: 175.8320770263672\n",
      "Epoch 62, Batch 14162, Loss: 181.7322235107422\n",
      "Epoch 62, Batch 14163, Loss: 178.27096557617188\n",
      "Epoch 62, Batch 14164, Loss: 167.6409912109375\n",
      "Epoch 62, Batch 14165, Loss: 195.05865478515625\n",
      "Epoch 62, Batch 14166, Loss: 176.34292602539062\n",
      "Epoch 62, Batch 14167, Loss: 152.93289184570312\n",
      "Epoch 62, Batch 14168, Loss: 174.96878051757812\n",
      "Epoch 62, Batch 14169, Loss: 198.53196716308594\n",
      "Epoch 62, Batch 14170, Loss: 158.02627563476562\n",
      "Epoch 62, Batch 14171, Loss: 164.78445434570312\n",
      "Epoch 62, Batch 14172, Loss: 178.94912719726562\n",
      "Epoch 62, Batch 14173, Loss: 165.26400756835938\n",
      "Epoch 62, Batch 14174, Loss: 180.8757781982422\n",
      "Epoch 62, Batch 14175, Loss: 163.2453155517578\n",
      "Epoch 62, Batch 14176, Loss: 166.97335815429688\n",
      "Epoch 62, Batch 14177, Loss: 178.70274353027344\n",
      "Epoch 62, Batch 14178, Loss: 175.2400360107422\n",
      "Epoch 62, Batch 14179, Loss: 177.79933166503906\n",
      "Epoch 62, Batch 14180, Loss: 162.843017578125\n",
      "Epoch 62, Batch 14181, Loss: 174.67820739746094\n",
      "Epoch 62, Batch 14182, Loss: 155.46804809570312\n",
      "Epoch 62, Batch 14183, Loss: 158.23629760742188\n",
      "Epoch 62, Batch 14184, Loss: 178.86422729492188\n",
      "Epoch 62, Batch 14185, Loss: 168.53233337402344\n",
      "Epoch 62, Batch 14186, Loss: 174.05015563964844\n",
      "Epoch 62, Batch 14187, Loss: 173.815673828125\n",
      "Epoch 62, Batch 14188, Loss: 174.8768768310547\n",
      "Epoch 62, Batch 14189, Loss: 168.94076538085938\n",
      "Epoch 62, Batch 14190, Loss: 182.1643524169922\n",
      "Epoch 62, Batch 14191, Loss: 163.69842529296875\n",
      "Epoch 62, Batch 14192, Loss: 180.7648162841797\n",
      "Epoch 62, Batch 14193, Loss: 182.98765563964844\n",
      "Epoch 62, Batch 14194, Loss: 178.08372497558594\n",
      "Epoch 62, Batch 14195, Loss: 182.3741912841797\n",
      "Epoch 62, Batch 14196, Loss: 163.8325958251953\n",
      "Epoch 62, Batch 14197, Loss: 161.84202575683594\n",
      "Epoch 62, Batch 14198, Loss: 168.9612274169922\n",
      "Epoch 62, Batch 14199, Loss: 164.20257568359375\n",
      "Epoch 62, Batch 14200, Loss: 188.44302368164062\n",
      "Epoch 62, Batch 14201, Loss: 162.23106384277344\n",
      "Epoch 62, Batch 14202, Loss: 184.1238555908203\n",
      "Epoch 62, Batch 14203, Loss: 165.3901824951172\n",
      "Epoch 62, Batch 14204, Loss: 164.8219757080078\n",
      "Epoch 62, Batch 14205, Loss: 175.9352569580078\n",
      "Epoch 62, Batch 14206, Loss: 174.0268096923828\n",
      "Epoch 62, Batch 14207, Loss: 173.58966064453125\n",
      "Epoch 62, Batch 14208, Loss: 178.57749938964844\n",
      "Epoch 62, Batch 14209, Loss: 185.75616455078125\n",
      "Epoch 62, Batch 14210, Loss: 193.39259338378906\n",
      "Epoch 62, Batch 14211, Loss: 166.54779052734375\n",
      "Epoch 62, Batch 14212, Loss: 143.75865173339844\n",
      "Epoch 62, Batch 14213, Loss: 170.5426483154297\n",
      "Epoch 62, Batch 14214, Loss: 168.87545776367188\n",
      "Epoch 62, Batch 14215, Loss: 177.38558959960938\n",
      "Epoch 62, Batch 14216, Loss: 181.42889404296875\n",
      "Epoch 62, Batch 14217, Loss: 162.29226684570312\n",
      "Epoch 62, Batch 14218, Loss: 178.179931640625\n",
      "Epoch 62, Batch 14219, Loss: 174.64608764648438\n",
      "Epoch 62, Batch 14220, Loss: 170.55735778808594\n",
      "Epoch 62, Batch 14221, Loss: 172.159423828125\n",
      "Epoch 62, Batch 14222, Loss: 176.7476043701172\n",
      "Epoch 62, Batch 14223, Loss: 182.48147583007812\n",
      "Epoch 62, Batch 14224, Loss: 172.96731567382812\n",
      "Epoch 62, Batch 14225, Loss: 170.67929077148438\n",
      "Epoch 62, Batch 14226, Loss: 193.90145874023438\n",
      "Epoch 62, Batch 14227, Loss: 173.08633422851562\n",
      "Epoch 62, Batch 14228, Loss: 182.19900512695312\n",
      "Epoch 62, Batch 14229, Loss: 174.3181610107422\n",
      "Epoch 62, Batch 14230, Loss: 172.63633728027344\n",
      "Epoch 62, Batch 14231, Loss: 167.87115478515625\n",
      "Epoch 62, Batch 14232, Loss: 175.27764892578125\n",
      "Epoch 62, Batch 14233, Loss: 163.95095825195312\n",
      "Epoch 62, Batch 14234, Loss: 194.18890380859375\n",
      "Epoch 62, Batch 14235, Loss: 169.81552124023438\n",
      "Epoch 62, Batch 14236, Loss: 186.8129119873047\n",
      "Epoch 62, Batch 14237, Loss: 184.39320373535156\n",
      "Epoch 62, Batch 14238, Loss: 162.82139587402344\n",
      "Epoch 62, Batch 14239, Loss: 167.4178924560547\n",
      "Epoch 62, Batch 14240, Loss: 189.7286376953125\n",
      "Epoch 62, Batch 14241, Loss: 184.8282928466797\n",
      "Epoch 62, Batch 14242, Loss: 170.6057891845703\n",
      "Epoch 62, Batch 14243, Loss: 184.85821533203125\n",
      "Epoch 62, Batch 14244, Loss: 158.38560485839844\n",
      "Epoch 62, Batch 14245, Loss: 169.25244140625\n",
      "Epoch 62, Batch 14246, Loss: 168.12460327148438\n",
      "Epoch 62, Batch 14247, Loss: 175.60159301757812\n",
      "Epoch 62, Batch 14248, Loss: 169.42874145507812\n",
      "Epoch 62, Batch 14249, Loss: 167.83140563964844\n",
      "Epoch 62, Batch 14250, Loss: 174.77801513671875\n",
      "Epoch 62, Batch 14251, Loss: 178.6829071044922\n",
      "Epoch 62, Batch 14252, Loss: 172.9703826904297\n",
      "Epoch 62, Batch 14253, Loss: 174.43008422851562\n",
      "Epoch 62, Batch 14254, Loss: 167.4853973388672\n",
      "Epoch 62, Batch 14255, Loss: 172.75416564941406\n",
      "Epoch 62, Batch 14256, Loss: 179.52987670898438\n",
      "Epoch 62, Batch 14257, Loss: 174.35243225097656\n",
      "Epoch 62, Batch 14258, Loss: 169.4836883544922\n",
      "Epoch 62, Batch 14259, Loss: 181.77110290527344\n",
      "Epoch 62, Batch 14260, Loss: 154.6548309326172\n",
      "Epoch 62, Batch 14261, Loss: 188.53280639648438\n",
      "Epoch 62, Batch 14262, Loss: 166.637939453125\n",
      "Epoch 62, Batch 14263, Loss: 177.49319458007812\n",
      "Epoch 62, Batch 14264, Loss: 182.7299346923828\n",
      "Epoch 62, Batch 14265, Loss: 148.70265197753906\n",
      "Epoch 62, Batch 14266, Loss: 165.33944702148438\n",
      "Epoch 62, Batch 14267, Loss: 171.461181640625\n",
      "Epoch 62, Batch 14268, Loss: 187.78335571289062\n",
      "Epoch 62, Batch 14269, Loss: 206.86056518554688\n",
      "Epoch 62, Batch 14270, Loss: 172.41415405273438\n",
      "Epoch 62, Batch 14271, Loss: 177.1080780029297\n",
      "Epoch 62, Batch 14272, Loss: 170.6465301513672\n",
      "Epoch 62, Batch 14273, Loss: 166.1219940185547\n",
      "Epoch 62, Batch 14274, Loss: 161.87014770507812\n",
      "Epoch 62, Batch 14275, Loss: 161.32737731933594\n",
      "Epoch 62, Batch 14276, Loss: 171.28378295898438\n",
      "Epoch 62, Batch 14277, Loss: 178.42633056640625\n",
      "Epoch 62, Batch 14278, Loss: 166.4890899658203\n",
      "Epoch 62, Batch 14279, Loss: 171.69142150878906\n",
      "Epoch 62, Batch 14280, Loss: 173.1879119873047\n",
      "Epoch 62, Batch 14281, Loss: 161.19541931152344\n",
      "Epoch 62, Batch 14282, Loss: 174.5664825439453\n",
      "Epoch 62, Batch 14283, Loss: 177.53038024902344\n",
      "Epoch 62, Batch 14284, Loss: 170.64479064941406\n",
      "Epoch 62, Batch 14285, Loss: 158.68431091308594\n",
      "Epoch 62, Batch 14286, Loss: 177.83914184570312\n",
      "Epoch 62, Batch 14287, Loss: 181.10801696777344\n",
      "Epoch 62, Batch 14288, Loss: 183.19305419921875\n",
      "Epoch 62, Batch 14289, Loss: 151.33575439453125\n",
      "Epoch 62, Batch 14290, Loss: 185.3391571044922\n",
      "Epoch 62, Batch 14291, Loss: 184.27105712890625\n",
      "Epoch 62, Batch 14292, Loss: 173.92796325683594\n",
      "Epoch 62, Batch 14293, Loss: 193.1472930908203\n",
      "Epoch 62, Batch 14294, Loss: 171.73593139648438\n",
      "Epoch 62, Batch 14295, Loss: 171.7708740234375\n",
      "Epoch 62, Batch 14296, Loss: 170.84083557128906\n",
      "Epoch 62, Batch 14297, Loss: 180.89454650878906\n",
      "Epoch 62, Batch 14298, Loss: 171.2068634033203\n",
      "Epoch 62, Batch 14299, Loss: 176.4130401611328\n",
      "Epoch 62, Batch 14300, Loss: 178.2026824951172\n",
      "Epoch 62, Batch 14301, Loss: 163.67999267578125\n",
      "Epoch 62, Batch 14302, Loss: 166.0635223388672\n",
      "Epoch 62, Batch 14303, Loss: 183.3538818359375\n",
      "Epoch 62, Batch 14304, Loss: 183.010498046875\n",
      "Epoch 62, Batch 14305, Loss: 173.91671752929688\n",
      "Epoch 62, Batch 14306, Loss: 210.64109802246094\n",
      "Epoch 62, Batch 14307, Loss: 187.8615264892578\n",
      "Epoch 62, Batch 14308, Loss: 166.56515502929688\n",
      "Epoch 62, Batch 14309, Loss: 167.1099090576172\n",
      "Epoch 62, Batch 14310, Loss: 177.9754180908203\n",
      "Epoch 62, Batch 14311, Loss: 176.611083984375\n",
      "Epoch 62, Batch 14312, Loss: 168.31942749023438\n",
      "Epoch 62, Batch 14313, Loss: 181.2382049560547\n",
      "Epoch 62, Batch 14314, Loss: 171.73924255371094\n",
      "Epoch 62, Batch 14315, Loss: 147.73373413085938\n",
      "Epoch 62, Batch 14316, Loss: 156.54440307617188\n",
      "Epoch 62, Batch 14317, Loss: 185.27178955078125\n",
      "Epoch 62, Batch 14318, Loss: 169.27548217773438\n",
      "Epoch 62, Batch 14319, Loss: 171.4288787841797\n",
      "Epoch 62, Batch 14320, Loss: 178.63116455078125\n",
      "Epoch 62, Batch 14321, Loss: 152.22222900390625\n",
      "Epoch 62, Batch 14322, Loss: 158.0289764404297\n",
      "Epoch 62, Batch 14323, Loss: 182.1407012939453\n",
      "Epoch 62, Batch 14324, Loss: 178.73919677734375\n",
      "Epoch 62, Batch 14325, Loss: 184.9734344482422\n",
      "Epoch 62, Batch 14326, Loss: 158.66627502441406\n",
      "Epoch 62, Batch 14327, Loss: 184.3860321044922\n",
      "Epoch 62, Batch 14328, Loss: 174.08355712890625\n",
      "Epoch 62, Batch 14329, Loss: 167.8788604736328\n",
      "Epoch 62, Batch 14330, Loss: 156.2906494140625\n",
      "Epoch 62, Batch 14331, Loss: 177.4969024658203\n",
      "Epoch 62, Batch 14332, Loss: 192.99200439453125\n",
      "Epoch 62, Batch 14333, Loss: 154.78651428222656\n",
      "Epoch 62, Batch 14334, Loss: 164.2531280517578\n",
      "Epoch 62, Batch 14335, Loss: 165.04710388183594\n",
      "Epoch 62, Batch 14336, Loss: 163.49859619140625\n",
      "Epoch 62, Batch 14337, Loss: 163.26986694335938\n",
      "Epoch 62, Batch 14338, Loss: 175.13787841796875\n",
      "Epoch 62, Batch 14339, Loss: 195.29653930664062\n",
      "Epoch 62, Batch 14340, Loss: 180.6254425048828\n",
      "Epoch 62, Batch 14341, Loss: 178.80247497558594\n",
      "Epoch 62, Batch 14342, Loss: 184.62478637695312\n",
      "Epoch 62, Batch 14343, Loss: 179.84933471679688\n",
      "Epoch 62, Batch 14344, Loss: 164.66673278808594\n",
      "Epoch 62, Batch 14345, Loss: 181.65237426757812\n",
      "Epoch 62, Batch 14346, Loss: 179.34375\n",
      "Epoch 62, Batch 14347, Loss: 173.71632385253906\n",
      "Epoch 62, Batch 14348, Loss: 171.5745086669922\n",
      "Epoch 62, Batch 14349, Loss: 178.37025451660156\n",
      "Epoch 62, Batch 14350, Loss: 170.66932678222656\n",
      "Epoch 62, Batch 14351, Loss: 160.78639221191406\n",
      "Epoch 62, Batch 14352, Loss: 168.57345581054688\n",
      "Epoch 62, Batch 14353, Loss: 171.9523468017578\n",
      "Epoch 62, Batch 14354, Loss: 177.32272338867188\n",
      "Epoch 62, Batch 14355, Loss: 184.11398315429688\n",
      "Epoch 62, Batch 14356, Loss: 185.62684631347656\n",
      "Epoch 62, Batch 14357, Loss: 170.0921173095703\n",
      "Epoch 62, Batch 14358, Loss: 178.44618225097656\n",
      "Epoch 62, Batch 14359, Loss: 168.388427734375\n",
      "Epoch 62, Batch 14360, Loss: 177.33384704589844\n",
      "Epoch 62, Batch 14361, Loss: 168.81639099121094\n",
      "Epoch 62, Batch 14362, Loss: 182.6958465576172\n",
      "Epoch 62, Batch 14363, Loss: 170.519775390625\n",
      "Epoch 62, Batch 14364, Loss: 184.57028198242188\n",
      "Epoch 62, Batch 14365, Loss: 185.0319366455078\n",
      "Epoch 62, Batch 14366, Loss: 176.00900268554688\n",
      "Epoch 62, Batch 14367, Loss: 183.34849548339844\n",
      "Epoch 62, Batch 14368, Loss: 169.1167449951172\n",
      "Epoch 62, Batch 14369, Loss: 180.67544555664062\n",
      "Epoch 62, Batch 14370, Loss: 167.689208984375\n",
      "Epoch 62, Batch 14371, Loss: 167.9339599609375\n",
      "Epoch 62, Batch 14372, Loss: 172.29464721679688\n",
      "Epoch 62, Batch 14373, Loss: 190.26551818847656\n",
      "Epoch 62, Batch 14374, Loss: 191.14154052734375\n",
      "Epoch 62, Batch 14375, Loss: 198.71604919433594\n",
      "Epoch 62, Batch 14376, Loss: 178.10311889648438\n",
      "Epoch 62, Batch 14377, Loss: 171.2320556640625\n",
      "Epoch 62, Batch 14378, Loss: 161.158203125\n",
      "Epoch 62, Batch 14379, Loss: 182.06988525390625\n",
      "Epoch 62, Batch 14380, Loss: 178.37408447265625\n",
      "Epoch 62, Batch 14381, Loss: 168.6352081298828\n",
      "Epoch 62, Batch 14382, Loss: 189.70294189453125\n",
      "Epoch 62, Batch 14383, Loss: 163.1371307373047\n",
      "Epoch 62, Batch 14384, Loss: 177.9602813720703\n",
      "Epoch 62, Batch 14385, Loss: 177.1476593017578\n",
      "Epoch 62, Batch 14386, Loss: 197.3202362060547\n",
      "Epoch 62, Batch 14387, Loss: 197.95896911621094\n",
      "Epoch 62, Batch 14388, Loss: 164.4733428955078\n",
      "Epoch 62, Batch 14389, Loss: 166.3800506591797\n",
      "Epoch 62, Batch 14390, Loss: 172.81674194335938\n",
      "Epoch 62, Batch 14391, Loss: 194.02786254882812\n",
      "Epoch 62, Batch 14392, Loss: 172.74131774902344\n",
      "Epoch 62, Batch 14393, Loss: 176.45596313476562\n",
      "Epoch 62, Batch 14394, Loss: 178.06353759765625\n",
      "Epoch 62, Batch 14395, Loss: 169.70083618164062\n",
      "Epoch 62, Batch 14396, Loss: 179.347412109375\n",
      "Epoch 62, Batch 14397, Loss: 187.98153686523438\n",
      "Epoch 62, Batch 14398, Loss: 182.01956176757812\n",
      "Epoch 62, Batch 14399, Loss: 159.08692932128906\n",
      "Epoch 62, Batch 14400, Loss: 150.77308654785156\n",
      "Epoch 62, Batch 14401, Loss: 179.87777709960938\n",
      "Epoch 62, Batch 14402, Loss: 185.52256774902344\n",
      "Epoch 62, Batch 14403, Loss: 170.2938232421875\n",
      "Epoch 62, Batch 14404, Loss: 166.8517303466797\n",
      "Epoch 62, Batch 14405, Loss: 165.03399658203125\n",
      "Epoch 62, Batch 14406, Loss: 170.33424377441406\n",
      "Epoch 62, Batch 14407, Loss: 195.7645721435547\n",
      "Epoch 62, Batch 14408, Loss: 166.57044982910156\n",
      "Epoch 62, Batch 14409, Loss: 168.89776611328125\n",
      "Epoch 62, Batch 14410, Loss: 164.2504425048828\n",
      "Epoch 62, Batch 14411, Loss: 165.88528442382812\n",
      "Epoch 62, Batch 14412, Loss: 164.60691833496094\n",
      "Epoch 62, Batch 14413, Loss: 177.3647918701172\n",
      "Epoch 62, Batch 14414, Loss: 170.66278076171875\n",
      "Epoch 62, Batch 14415, Loss: 161.94696044921875\n",
      "Epoch 62, Batch 14416, Loss: 163.79193115234375\n",
      "Epoch 62, Batch 14417, Loss: 174.56321716308594\n",
      "Epoch 62, Batch 14418, Loss: 165.63568115234375\n",
      "Epoch 62, Batch 14419, Loss: 155.19630432128906\n",
      "Epoch 62, Batch 14420, Loss: 179.22665405273438\n",
      "Epoch 62, Batch 14421, Loss: 182.14794921875\n",
      "Epoch 62, Batch 14422, Loss: 171.04981994628906\n",
      "Epoch 62, Batch 14423, Loss: 183.86448669433594\n",
      "Epoch 62, Batch 14424, Loss: 163.33447265625\n",
      "Epoch 62, Batch 14425, Loss: 163.39772033691406\n",
      "Epoch 62, Batch 14426, Loss: 188.78775024414062\n",
      "Epoch 62, Batch 14427, Loss: 166.33717346191406\n",
      "Epoch 62, Batch 14428, Loss: 187.3513641357422\n",
      "Epoch 62, Batch 14429, Loss: 184.96112060546875\n",
      "Epoch 62, Batch 14430, Loss: 168.43516540527344\n",
      "Epoch 62, Batch 14431, Loss: 188.7140350341797\n",
      "Epoch 62, Batch 14432, Loss: 167.29542541503906\n",
      "Epoch 62, Batch 14433, Loss: 189.079345703125\n",
      "Epoch 62, Batch 14434, Loss: 175.6063690185547\n",
      "Epoch 62, Batch 14435, Loss: 170.19100952148438\n",
      "Epoch 62, Batch 14436, Loss: 168.5684814453125\n",
      "Epoch 62, Batch 14437, Loss: 174.231689453125\n",
      "Epoch 62, Batch 14438, Loss: 166.38958740234375\n",
      "Epoch 62, Batch 14439, Loss: 169.06634521484375\n",
      "Epoch 62, Batch 14440, Loss: 164.18331909179688\n",
      "Epoch 62, Batch 14441, Loss: 156.50523376464844\n",
      "Epoch 62, Batch 14442, Loss: 179.09864807128906\n",
      "Epoch 62, Batch 14443, Loss: 178.4160614013672\n",
      "Epoch 62, Batch 14444, Loss: 160.5621795654297\n",
      "Epoch 62, Batch 14445, Loss: 172.20907592773438\n",
      "Epoch 62, Batch 14446, Loss: 174.989013671875\n",
      "Epoch 62, Batch 14447, Loss: 171.48280334472656\n",
      "Epoch 62, Batch 14448, Loss: 171.61309814453125\n",
      "Epoch 62, Batch 14449, Loss: 182.62940979003906\n",
      "Epoch 62, Batch 14450, Loss: 181.5499725341797\n",
      "Epoch 62, Batch 14451, Loss: 188.5318145751953\n",
      "Epoch 62, Batch 14452, Loss: 169.77333068847656\n",
      "Epoch 62, Batch 14453, Loss: 175.66940307617188\n",
      "Epoch 62, Batch 14454, Loss: 170.00863647460938\n",
      "Epoch 62, Batch 14455, Loss: 175.52432250976562\n",
      "Epoch 62, Batch 14456, Loss: 161.14881896972656\n",
      "Epoch 62, Batch 14457, Loss: 174.7401123046875\n",
      "Epoch 62, Batch 14458, Loss: 186.05958557128906\n",
      "Epoch 62, Batch 14459, Loss: 170.6514129638672\n",
      "Epoch 62, Batch 14460, Loss: 175.3163604736328\n",
      "Epoch 62, Batch 14461, Loss: 163.46607971191406\n",
      "Epoch 62, Batch 14462, Loss: 169.90274047851562\n",
      "Epoch 62, Batch 14463, Loss: 181.5147247314453\n",
      "Epoch 62, Batch 14464, Loss: 192.82168579101562\n",
      "Epoch 62, Batch 14465, Loss: 180.1356201171875\n",
      "Epoch 62, Batch 14466, Loss: 184.230712890625\n",
      "Epoch 62, Batch 14467, Loss: 172.19908142089844\n",
      "Epoch 62, Batch 14468, Loss: 159.9415283203125\n",
      "Epoch 62, Batch 14469, Loss: 179.27914428710938\n",
      "Epoch 62, Batch 14470, Loss: 166.02459716796875\n",
      "Epoch 62, Batch 14471, Loss: 178.11676025390625\n",
      "Epoch 62, Batch 14472, Loss: 167.8870849609375\n",
      "Epoch 62, Batch 14473, Loss: 182.9074249267578\n",
      "Epoch 62, Batch 14474, Loss: 181.27162170410156\n",
      "Epoch 62, Batch 14475, Loss: 187.99534606933594\n",
      "Epoch 62, Batch 14476, Loss: 178.2532501220703\n",
      "Epoch 62, Batch 14477, Loss: 170.80335998535156\n",
      "Epoch 62, Batch 14478, Loss: 172.03250122070312\n",
      "Epoch 62, Batch 14479, Loss: 174.38397216796875\n",
      "Epoch 62, Batch 14480, Loss: 174.490966796875\n",
      "Epoch 62, Batch 14481, Loss: 167.56832885742188\n",
      "Epoch 62, Batch 14482, Loss: 165.1494598388672\n",
      "Epoch 62, Batch 14483, Loss: 163.86029052734375\n",
      "Epoch 62, Batch 14484, Loss: 159.9638214111328\n",
      "Epoch 62, Batch 14485, Loss: 196.07656860351562\n",
      "Epoch 62, Batch 14486, Loss: 167.35003662109375\n",
      "Epoch 62, Batch 14487, Loss: 173.52171325683594\n",
      "Epoch 62, Batch 14488, Loss: 170.57867431640625\n",
      "Epoch 62, Batch 14489, Loss: 174.96734619140625\n",
      "Epoch 62, Batch 14490, Loss: 158.93002319335938\n",
      "Epoch 62, Batch 14491, Loss: 168.14390563964844\n",
      "Epoch 62, Batch 14492, Loss: 172.55043029785156\n",
      "Epoch 62, Batch 14493, Loss: 171.57562255859375\n",
      "Epoch 62, Batch 14494, Loss: 163.6255645751953\n",
      "Epoch 62, Batch 14495, Loss: 166.9063720703125\n",
      "Epoch 62, Batch 14496, Loss: 179.80331420898438\n",
      "Epoch 62, Batch 14497, Loss: 165.53399658203125\n",
      "Epoch 62, Batch 14498, Loss: 169.4383087158203\n",
      "Epoch 62, Batch 14499, Loss: 160.4988250732422\n",
      "Epoch 62, Batch 14500, Loss: 204.2252960205078\n",
      "Epoch 62, Batch 14501, Loss: 168.29359436035156\n",
      "Epoch 62, Batch 14502, Loss: 166.71372985839844\n",
      "Epoch 62, Batch 14503, Loss: 184.14950561523438\n",
      "Epoch 62, Batch 14504, Loss: 165.41656494140625\n",
      "Epoch 62, Batch 14505, Loss: 181.52500915527344\n",
      "Epoch 62, Batch 14506, Loss: 184.11251831054688\n",
      "Epoch 62, Batch 14507, Loss: 173.9585723876953\n",
      "Epoch 62, Batch 14508, Loss: 171.2764892578125\n",
      "Epoch 62, Batch 14509, Loss: 177.5410614013672\n",
      "Epoch 62, Batch 14510, Loss: 196.6894073486328\n",
      "Epoch 62, Batch 14511, Loss: 165.23768615722656\n",
      "Epoch 62, Batch 14512, Loss: 163.1442108154297\n",
      "Epoch 62, Batch 14513, Loss: 168.1232452392578\n",
      "Epoch 62, Batch 14514, Loss: 174.7010498046875\n",
      "Epoch 62, Batch 14515, Loss: 158.1891326904297\n",
      "Epoch 62, Batch 14516, Loss: 160.81668090820312\n",
      "Epoch 62, Batch 14517, Loss: 165.0549774169922\n",
      "Epoch 62, Batch 14518, Loss: 168.54701232910156\n",
      "Epoch 62, Batch 14519, Loss: 180.0314178466797\n",
      "Epoch 62, Batch 14520, Loss: 173.51976013183594\n",
      "Epoch 62, Batch 14521, Loss: 149.22711181640625\n",
      "Epoch 62, Batch 14522, Loss: 191.73988342285156\n",
      "Epoch 62, Batch 14523, Loss: 160.39154052734375\n",
      "Epoch 62, Batch 14524, Loss: 173.90431213378906\n",
      "Epoch 62, Batch 14525, Loss: 166.2352294921875\n",
      "Epoch 62, Batch 14526, Loss: 166.24171447753906\n",
      "Epoch 62, Batch 14527, Loss: 173.98300170898438\n",
      "Epoch 62, Batch 14528, Loss: 156.824462890625\n",
      "Epoch 62, Batch 14529, Loss: 166.6690673828125\n",
      "Epoch 62, Batch 14530, Loss: 173.0918426513672\n",
      "Epoch 62, Batch 14531, Loss: 184.1593475341797\n",
      "Epoch 62, Batch 14532, Loss: 181.81849670410156\n",
      "Epoch 62, Batch 14533, Loss: 183.93405151367188\n",
      "Epoch 62, Batch 14534, Loss: 188.45262145996094\n",
      "Epoch 62, Batch 14535, Loss: 176.50479125976562\n",
      "Epoch 62, Batch 14536, Loss: 188.76611328125\n",
      "Epoch 62, Batch 14537, Loss: 157.4782257080078\n",
      "Epoch 62, Batch 14538, Loss: 177.05966186523438\n",
      "Epoch 62, Batch 14539, Loss: 165.97903442382812\n",
      "Epoch 62, Batch 14540, Loss: 177.86170959472656\n",
      "Epoch 62, Batch 14541, Loss: 182.1171875\n",
      "Epoch 62, Batch 14542, Loss: 158.70619201660156\n",
      "Epoch 62, Batch 14543, Loss: 185.61277770996094\n",
      "Epoch 62, Batch 14544, Loss: 155.8284454345703\n",
      "Epoch 62, Batch 14545, Loss: 188.82081604003906\n",
      "Epoch 62, Batch 14546, Loss: 160.47149658203125\n",
      "Epoch 62, Batch 14547, Loss: 172.57774353027344\n",
      "Epoch 62, Batch 14548, Loss: 169.4882354736328\n",
      "Epoch 62, Batch 14549, Loss: 180.0909881591797\n",
      "Epoch 62, Batch 14550, Loss: 162.90484619140625\n",
      "Epoch 62, Batch 14551, Loss: 178.8789520263672\n",
      "Epoch 62, Batch 14552, Loss: 177.83538818359375\n",
      "Epoch 62, Batch 14553, Loss: 171.563232421875\n",
      "Epoch 62, Batch 14554, Loss: 181.39901733398438\n",
      "Epoch 62, Batch 14555, Loss: 171.1966094970703\n",
      "Epoch 62, Batch 14556, Loss: 173.82876586914062\n",
      "Epoch 62, Batch 14557, Loss: 191.38900756835938\n",
      "Epoch 62, Batch 14558, Loss: 169.54432678222656\n",
      "Epoch 62, Batch 14559, Loss: 171.34324645996094\n",
      "Epoch 62, Batch 14560, Loss: 168.087646484375\n",
      "Epoch 62, Batch 14561, Loss: 179.29428100585938\n",
      "Epoch 62, Batch 14562, Loss: 174.65570068359375\n",
      "Epoch 62, Batch 14563, Loss: 189.8762664794922\n",
      "Epoch 62, Batch 14564, Loss: 185.6964569091797\n",
      "Epoch 62, Batch 14565, Loss: 181.26153564453125\n",
      "Epoch 62, Batch 14566, Loss: 170.04180908203125\n",
      "Epoch 62, Batch 14567, Loss: 175.2750701904297\n",
      "Epoch 62, Batch 14568, Loss: 173.88995361328125\n",
      "Epoch 62, Batch 14569, Loss: 172.33963012695312\n",
      "Epoch 62, Batch 14570, Loss: 174.15316772460938\n",
      "Epoch 62, Batch 14571, Loss: 187.88694763183594\n",
      "Epoch 62, Batch 14572, Loss: 175.61663818359375\n",
      "Epoch 62, Batch 14573, Loss: 178.96253967285156\n",
      "Epoch 62, Batch 14574, Loss: 174.40187072753906\n",
      "Epoch 62, Batch 14575, Loss: 173.13748168945312\n",
      "Epoch 62, Batch 14576, Loss: 172.62374877929688\n",
      "Epoch 62, Batch 14577, Loss: 167.64096069335938\n",
      "Epoch 62, Batch 14578, Loss: 172.43016052246094\n",
      "Epoch 62, Batch 14579, Loss: 175.41934204101562\n",
      "Epoch 62, Batch 14580, Loss: 177.57015991210938\n",
      "Epoch 62, Batch 14581, Loss: 178.99818420410156\n",
      "Epoch 62, Batch 14582, Loss: 196.37574768066406\n",
      "Epoch 62, Batch 14583, Loss: 178.95233154296875\n",
      "Epoch 62, Batch 14584, Loss: 171.09951782226562\n",
      "Epoch 62, Batch 14585, Loss: 174.54794311523438\n",
      "Epoch 62, Batch 14586, Loss: 163.0574493408203\n",
      "Epoch 62, Batch 14587, Loss: 167.25257873535156\n",
      "Epoch 62, Batch 14588, Loss: 169.6195526123047\n",
      "Epoch 62, Batch 14589, Loss: 168.74935913085938\n",
      "Epoch 62, Batch 14590, Loss: 169.09425354003906\n",
      "Epoch 62, Batch 14591, Loss: 172.97238159179688\n",
      "Epoch 62, Batch 14592, Loss: 171.36679077148438\n",
      "Epoch 62, Batch 14593, Loss: 179.68701171875\n",
      "Epoch 62, Batch 14594, Loss: 183.43809509277344\n",
      "Epoch 62, Batch 14595, Loss: 186.67257690429688\n",
      "Epoch 62, Batch 14596, Loss: 186.8892364501953\n",
      "Epoch 62, Batch 14597, Loss: 183.6099853515625\n",
      "Epoch 62, Batch 14598, Loss: 170.9601593017578\n",
      "Epoch 62, Batch 14599, Loss: 175.97702026367188\n",
      "Epoch 62, Batch 14600, Loss: 173.114013671875\n",
      "Epoch 62, Batch 14601, Loss: 166.74746704101562\n",
      "Epoch 62, Batch 14602, Loss: 174.3753204345703\n",
      "Epoch 62, Batch 14603, Loss: 165.08236694335938\n",
      "Epoch 62, Batch 14604, Loss: 152.5604248046875\n",
      "Epoch 62, Batch 14605, Loss: 192.74237060546875\n",
      "Epoch 62, Batch 14606, Loss: 176.271240234375\n",
      "Epoch 62, Batch 14607, Loss: 181.39163208007812\n",
      "Epoch 62, Batch 14608, Loss: 182.75274658203125\n",
      "Epoch 62, Batch 14609, Loss: 162.9623260498047\n",
      "Epoch 62, Batch 14610, Loss: 182.14735412597656\n",
      "Epoch 62, Batch 14611, Loss: 166.81655883789062\n",
      "Epoch 62, Batch 14612, Loss: 192.28024291992188\n",
      "Epoch 62, Batch 14613, Loss: 168.2627410888672\n",
      "Epoch 62, Batch 14614, Loss: 171.05233764648438\n",
      "Epoch 62, Batch 14615, Loss: 182.81619262695312\n",
      "Epoch 62, Batch 14616, Loss: 202.61361694335938\n",
      "Epoch 62, Batch 14617, Loss: 191.0685577392578\n",
      "Epoch 62, Batch 14618, Loss: 186.82687377929688\n",
      "Epoch 62, Batch 14619, Loss: 159.10581970214844\n",
      "Epoch 62, Batch 14620, Loss: 175.41152954101562\n",
      "Epoch 62, Batch 14621, Loss: 199.9404754638672\n",
      "Epoch 62, Batch 14622, Loss: 165.20938110351562\n",
      "Epoch 62, Batch 14623, Loss: 167.569091796875\n",
      "Epoch 62, Batch 14624, Loss: 147.77999877929688\n",
      "Epoch 62, Batch 14625, Loss: 176.4669647216797\n",
      "Epoch 62, Batch 14626, Loss: 182.47044372558594\n",
      "Epoch 62, Batch 14627, Loss: 184.38592529296875\n",
      "Epoch 62, Batch 14628, Loss: 182.94639587402344\n",
      "Epoch 62, Batch 14629, Loss: 173.26797485351562\n",
      "Epoch 62, Batch 14630, Loss: 160.80352783203125\n",
      "Epoch 62, Batch 14631, Loss: 171.1769561767578\n",
      "Epoch 62, Batch 14632, Loss: 169.46173095703125\n",
      "Epoch 62, Batch 14633, Loss: 172.19390869140625\n",
      "Epoch 62, Batch 14634, Loss: 176.25384521484375\n",
      "Epoch 62, Batch 14635, Loss: 177.13665771484375\n",
      "Epoch 62, Batch 14636, Loss: 167.2410430908203\n",
      "Epoch 62, Batch 14637, Loss: 165.68316650390625\n",
      "Epoch 62, Batch 14638, Loss: 171.6275634765625\n",
      "Epoch 62, Batch 14639, Loss: 179.7154541015625\n",
      "Epoch 62, Batch 14640, Loss: 168.56723022460938\n",
      "Epoch 62, Batch 14641, Loss: 171.722412109375\n",
      "Epoch 62, Batch 14642, Loss: 165.5985870361328\n",
      "Epoch 62, Batch 14643, Loss: 174.6149444580078\n",
      "Epoch 62, Batch 14644, Loss: 178.1953582763672\n",
      "Epoch 62, Batch 14645, Loss: 165.8584442138672\n",
      "Epoch 62, Batch 14646, Loss: 178.40040588378906\n",
      "Epoch 62, Batch 14647, Loss: 193.0004425048828\n",
      "Epoch 62, Batch 14648, Loss: 165.3511199951172\n",
      "Epoch 62, Batch 14649, Loss: 168.36659240722656\n",
      "Epoch 62, Batch 14650, Loss: 178.76715087890625\n",
      "Epoch 62, Batch 14651, Loss: 184.44793701171875\n",
      "Epoch 62, Batch 14652, Loss: 175.38780212402344\n",
      "Epoch 62, Batch 14653, Loss: 188.5779266357422\n",
      "Epoch 62, Batch 14654, Loss: 181.361328125\n",
      "Epoch 62, Batch 14655, Loss: 170.9031219482422\n",
      "Epoch 62, Batch 14656, Loss: 182.68685913085938\n",
      "Epoch 62, Batch 14657, Loss: 174.68666076660156\n",
      "Epoch 62, Batch 14658, Loss: 159.9225311279297\n",
      "Epoch 62, Batch 14659, Loss: 172.66012573242188\n",
      "Epoch 62, Batch 14660, Loss: 164.1446990966797\n",
      "Epoch 62, Batch 14661, Loss: 176.36749267578125\n",
      "Epoch 62, Batch 14662, Loss: 164.12100219726562\n",
      "Epoch 62, Batch 14663, Loss: 163.38970947265625\n",
      "Epoch 62, Batch 14664, Loss: 161.42115783691406\n",
      "Epoch 62, Batch 14665, Loss: 175.3336639404297\n",
      "Epoch 62, Batch 14666, Loss: 173.9915313720703\n",
      "Epoch 62, Batch 14667, Loss: 179.49000549316406\n",
      "Epoch 62, Batch 14668, Loss: 189.44920349121094\n",
      "Epoch 62, Batch 14669, Loss: 185.51437377929688\n",
      "Epoch 62, Batch 14670, Loss: 159.52926635742188\n",
      "Epoch 62, Batch 14671, Loss: 164.3194580078125\n",
      "Epoch 62, Batch 14672, Loss: 168.96109008789062\n",
      "Epoch 62, Batch 14673, Loss: 173.7989501953125\n",
      "Epoch 62, Batch 14674, Loss: 163.84161376953125\n",
      "Epoch 62, Batch 14675, Loss: 190.0546875\n",
      "Epoch 62, Batch 14676, Loss: 179.8976593017578\n",
      "Epoch 62, Batch 14677, Loss: 167.22222900390625\n",
      "Epoch 62, Batch 14678, Loss: 173.5753173828125\n",
      "Epoch 62, Batch 14679, Loss: 189.75643920898438\n",
      "Epoch 62, Batch 14680, Loss: 180.97219848632812\n",
      "Epoch 62, Batch 14681, Loss: 155.27999877929688\n",
      "Epoch 62, Batch 14682, Loss: 174.42608642578125\n",
      "Epoch 62, Batch 14683, Loss: 157.912109375\n",
      "Epoch 62, Batch 14684, Loss: 174.82334899902344\n",
      "Epoch 62, Batch 14685, Loss: 164.4164581298828\n",
      "Epoch 62, Batch 14686, Loss: 179.6014862060547\n",
      "Epoch 62, Batch 14687, Loss: 176.140625\n",
      "Epoch 62, Batch 14688, Loss: 185.23255920410156\n",
      "Epoch 62, Batch 14689, Loss: 172.23602294921875\n",
      "Epoch 62, Batch 14690, Loss: 175.86172485351562\n",
      "Epoch 62, Batch 14691, Loss: 157.31040954589844\n",
      "Epoch 62, Batch 14692, Loss: 193.73655700683594\n",
      "Epoch 62, Batch 14693, Loss: 153.3412322998047\n",
      "Epoch 62, Batch 14694, Loss: 171.1537628173828\n",
      "Epoch 62, Batch 14695, Loss: 162.4857635498047\n",
      "Epoch 62, Batch 14696, Loss: 160.85128784179688\n",
      "Epoch 62, Batch 14697, Loss: 153.51513671875\n",
      "Epoch 62, Batch 14698, Loss: 171.40902709960938\n",
      "Epoch 62, Batch 14699, Loss: 169.07305908203125\n",
      "Epoch 62, Batch 14700, Loss: 176.21011352539062\n",
      "Epoch 62, Batch 14701, Loss: 168.66818237304688\n",
      "Epoch 62, Batch 14702, Loss: 186.44534301757812\n",
      "Epoch 62, Batch 14703, Loss: 174.47012329101562\n",
      "Epoch 62, Batch 14704, Loss: 167.6157989501953\n",
      "Epoch 62, Batch 14705, Loss: 171.7693328857422\n",
      "Epoch 62, Batch 14706, Loss: 165.75015258789062\n",
      "Epoch 62, Batch 14707, Loss: 168.90408325195312\n",
      "Epoch 62, Batch 14708, Loss: 181.4102783203125\n",
      "Epoch 62, Batch 14709, Loss: 178.90011596679688\n",
      "Epoch 62, Batch 14710, Loss: 170.04258728027344\n",
      "Epoch 62, Batch 14711, Loss: 190.1292724609375\n",
      "Epoch 62, Batch 14712, Loss: 171.60142517089844\n",
      "Epoch 62, Batch 14713, Loss: 166.6749267578125\n",
      "Epoch 62, Batch 14714, Loss: 164.3325653076172\n",
      "Epoch 62, Batch 14715, Loss: 155.27392578125\n",
      "Epoch 62, Batch 14716, Loss: 165.1411590576172\n",
      "Epoch 62, Batch 14717, Loss: 167.98690795898438\n",
      "Epoch 62, Batch 14718, Loss: 153.30908203125\n",
      "Epoch 62, Batch 14719, Loss: 181.70361328125\n",
      "Epoch 62, Batch 14720, Loss: 166.52447509765625\n",
      "Epoch 62, Batch 14721, Loss: 193.78057861328125\n",
      "Epoch 62, Batch 14722, Loss: 159.1982879638672\n",
      "Epoch 62, Batch 14723, Loss: 172.67669677734375\n",
      "Epoch 62, Batch 14724, Loss: 186.56602478027344\n",
      "Epoch 62, Batch 14725, Loss: 161.405517578125\n",
      "Epoch 62, Batch 14726, Loss: 177.75930786132812\n",
      "Epoch 62, Batch 14727, Loss: 188.5742950439453\n",
      "Epoch 62, Batch 14728, Loss: 186.28269958496094\n",
      "Epoch 62, Batch 14729, Loss: 172.74998474121094\n",
      "Epoch 62, Batch 14730, Loss: 167.8662109375\n",
      "Epoch 62, Batch 14731, Loss: 187.29554748535156\n",
      "Epoch 62, Batch 14732, Loss: 169.53216552734375\n",
      "Epoch 62, Batch 14733, Loss: 189.78732299804688\n",
      "Epoch 62, Batch 14734, Loss: 178.38926696777344\n",
      "Epoch 62, Batch 14735, Loss: 173.11093139648438\n",
      "Epoch 62, Batch 14736, Loss: 163.92919921875\n",
      "Epoch 62, Batch 14737, Loss: 175.20611572265625\n",
      "Epoch 62, Batch 14738, Loss: 165.47280883789062\n",
      "Epoch 62, Batch 14739, Loss: 171.39613342285156\n",
      "Epoch 62, Batch 14740, Loss: 182.13893127441406\n",
      "Epoch 62, Batch 14741, Loss: 187.5876922607422\n",
      "Epoch 62, Batch 14742, Loss: 167.4594268798828\n",
      "Epoch 62, Batch 14743, Loss: 193.2824249267578\n",
      "Epoch 62, Batch 14744, Loss: 159.61927795410156\n",
      "Epoch 62, Batch 14745, Loss: 188.919189453125\n",
      "Epoch 62, Batch 14746, Loss: 168.9728546142578\n",
      "Epoch 62, Batch 14747, Loss: 188.4552459716797\n",
      "Epoch 62, Batch 14748, Loss: 169.92288208007812\n",
      "Epoch 62, Batch 14749, Loss: 188.04905700683594\n",
      "Epoch 62, Batch 14750, Loss: 175.4878692626953\n",
      "Epoch 62, Batch 14751, Loss: 177.267333984375\n",
      "Epoch 62, Batch 14752, Loss: 182.26780700683594\n",
      "Epoch 62, Batch 14753, Loss: 169.52740478515625\n",
      "Epoch 62, Batch 14754, Loss: 173.39828491210938\n",
      "Epoch 62, Batch 14755, Loss: 180.337158203125\n",
      "Epoch 62, Batch 14756, Loss: 174.15966796875\n",
      "Epoch 62, Batch 14757, Loss: 176.84498596191406\n",
      "Epoch 62, Batch 14758, Loss: 161.94822692871094\n",
      "Epoch 62, Batch 14759, Loss: 175.47470092773438\n",
      "Epoch 62, Batch 14760, Loss: 166.67518615722656\n",
      "Epoch 62, Batch 14761, Loss: 177.2452392578125\n",
      "Epoch 62, Batch 14762, Loss: 167.52377319335938\n",
      "Epoch 62, Batch 14763, Loss: 180.4522705078125\n",
      "Epoch 62, Batch 14764, Loss: 185.38954162597656\n",
      "Epoch 62, Batch 14765, Loss: 184.56370544433594\n",
      "Epoch 62, Batch 14766, Loss: 183.82228088378906\n",
      "Epoch 62, Batch 14767, Loss: 177.3609619140625\n",
      "Epoch 62, Batch 14768, Loss: 192.1949920654297\n",
      "Epoch 62, Batch 14769, Loss: 181.56976318359375\n",
      "Epoch 62, Batch 14770, Loss: 170.01927185058594\n",
      "Epoch 62, Batch 14771, Loss: 166.26222229003906\n",
      "Epoch 62, Batch 14772, Loss: 177.3905487060547\n",
      "Epoch 62, Batch 14773, Loss: 184.43833923339844\n",
      "Epoch 62, Batch 14774, Loss: 150.55078125\n",
      "Epoch 62, Batch 14775, Loss: 177.09571838378906\n",
      "Epoch 62, Batch 14776, Loss: 170.25302124023438\n",
      "Epoch 62, Batch 14777, Loss: 174.4623565673828\n",
      "Epoch 62, Batch 14778, Loss: 187.19061279296875\n",
      "Epoch 62, Batch 14779, Loss: 191.14706420898438\n",
      "Epoch 62, Batch 14780, Loss: 171.65936279296875\n",
      "Epoch 62, Batch 14781, Loss: 161.7292022705078\n",
      "Epoch 62, Batch 14782, Loss: 173.30613708496094\n",
      "Epoch 62, Batch 14783, Loss: 164.80673217773438\n",
      "Epoch 62, Batch 14784, Loss: 168.4831085205078\n",
      "Epoch 62, Batch 14785, Loss: 170.6419677734375\n",
      "Epoch 62, Batch 14786, Loss: 175.4358673095703\n",
      "Epoch 62, Batch 14787, Loss: 168.9781036376953\n",
      "Epoch 62, Batch 14788, Loss: 192.65750122070312\n",
      "Epoch 62, Batch 14789, Loss: 169.62403869628906\n",
      "Epoch 62, Batch 14790, Loss: 183.1207275390625\n",
      "Epoch 62, Batch 14791, Loss: 177.1392364501953\n",
      "Epoch 62, Batch 14792, Loss: 169.36827087402344\n",
      "Epoch 62, Batch 14793, Loss: 185.03892517089844\n",
      "Epoch 62, Batch 14794, Loss: 152.16183471679688\n",
      "Epoch 62, Batch 14795, Loss: 161.8224639892578\n",
      "Epoch 62, Batch 14796, Loss: 173.06005859375\n",
      "Epoch 62, Batch 14797, Loss: 185.79476928710938\n",
      "Epoch 62, Batch 14798, Loss: 171.63926696777344\n",
      "Epoch 62, Batch 14799, Loss: 164.7430419921875\n",
      "Epoch 62, Batch 14800, Loss: 181.94554138183594\n",
      "Epoch 62, Batch 14801, Loss: 177.20034790039062\n",
      "Epoch 62, Batch 14802, Loss: 158.86795043945312\n",
      "Epoch 62, Batch 14803, Loss: 179.54074096679688\n",
      "Epoch 62, Batch 14804, Loss: 168.08155822753906\n",
      "Epoch 62, Batch 14805, Loss: 159.96238708496094\n",
      "Epoch 62, Batch 14806, Loss: 176.27011108398438\n",
      "Epoch 62, Batch 14807, Loss: 166.89840698242188\n",
      "Epoch 62, Batch 14808, Loss: 162.4243927001953\n",
      "Epoch 62, Batch 14809, Loss: 181.11582946777344\n",
      "Epoch 62, Batch 14810, Loss: 166.0142364501953\n",
      "Epoch 62, Batch 14811, Loss: 151.0844268798828\n",
      "Epoch 62, Batch 14812, Loss: 162.12872314453125\n",
      "Epoch 62, Batch 14813, Loss: 167.5459442138672\n",
      "Epoch 62, Batch 14814, Loss: 164.75497436523438\n",
      "Epoch 62, Batch 14815, Loss: 172.67706298828125\n",
      "Epoch 62, Batch 14816, Loss: 177.60072326660156\n",
      "Epoch 62, Batch 14817, Loss: 186.21676635742188\n",
      "Epoch 62, Batch 14818, Loss: 189.42041015625\n",
      "Epoch 62, Batch 14819, Loss: 178.4397735595703\n",
      "Epoch 62, Batch 14820, Loss: 160.875\n",
      "Epoch 62, Batch 14821, Loss: 163.6348876953125\n",
      "Epoch 62, Batch 14822, Loss: 163.2758026123047\n",
      "Epoch 62, Batch 14823, Loss: 162.00442504882812\n",
      "Epoch 62, Batch 14824, Loss: 170.20791625976562\n",
      "Epoch 62, Batch 14825, Loss: 174.73033142089844\n",
      "Epoch 62, Batch 14826, Loss: 173.64859008789062\n",
      "Epoch 62, Batch 14827, Loss: 178.01699829101562\n",
      "Epoch 62, Batch 14828, Loss: 163.60206604003906\n",
      "Epoch 62, Batch 14829, Loss: 169.90042114257812\n",
      "Epoch 62, Batch 14830, Loss: 181.2000732421875\n",
      "Epoch 62, Batch 14831, Loss: 158.6275634765625\n",
      "Epoch 62, Batch 14832, Loss: 173.90768432617188\n",
      "Epoch 62, Batch 14833, Loss: 196.20486450195312\n",
      "Epoch 62, Batch 14834, Loss: 182.50460815429688\n",
      "Epoch 62, Batch 14835, Loss: 171.12913513183594\n",
      "Epoch 62, Batch 14836, Loss: 187.07980346679688\n",
      "Epoch 62, Batch 14837, Loss: 186.78494262695312\n",
      "Epoch 62, Batch 14838, Loss: 177.4629364013672\n",
      "Epoch 62, Batch 14839, Loss: 175.4520721435547\n",
      "Epoch 62, Batch 14840, Loss: 169.45809936523438\n",
      "Epoch 62, Batch 14841, Loss: 165.68624877929688\n",
      "Epoch 62, Batch 14842, Loss: 170.1543426513672\n",
      "Epoch 62, Batch 14843, Loss: 174.6775360107422\n",
      "Epoch 62, Batch 14844, Loss: 194.56085205078125\n",
      "Epoch 62, Batch 14845, Loss: 161.99351501464844\n",
      "Epoch 62, Batch 14846, Loss: 159.50286865234375\n",
      "Epoch 62, Batch 14847, Loss: 188.699951171875\n",
      "Epoch 62, Batch 14848, Loss: 170.5463409423828\n",
      "Epoch 62, Batch 14849, Loss: 168.3506317138672\n",
      "Epoch 62, Batch 14850, Loss: 173.6928253173828\n",
      "Epoch 62, Batch 14851, Loss: 158.58583068847656\n",
      "Epoch 62, Batch 14852, Loss: 169.0166015625\n",
      "Epoch 62, Batch 14853, Loss: 176.1279296875\n",
      "Epoch 62, Batch 14854, Loss: 165.2349853515625\n",
      "Epoch 62, Batch 14855, Loss: 183.66342163085938\n",
      "Epoch 62, Batch 14856, Loss: 187.74920654296875\n",
      "Epoch 62, Batch 14857, Loss: 185.76409912109375\n",
      "Epoch 62, Batch 14858, Loss: 171.457275390625\n",
      "Epoch 62, Batch 14859, Loss: 179.65115356445312\n",
      "Epoch 62, Batch 14860, Loss: 156.0771026611328\n",
      "Epoch 62, Batch 14861, Loss: 172.41134643554688\n",
      "Epoch 62, Batch 14862, Loss: 175.91921997070312\n",
      "Epoch 62, Batch 14863, Loss: 181.94781494140625\n",
      "Epoch 62, Batch 14864, Loss: 175.17750549316406\n",
      "Epoch 62, Batch 14865, Loss: 168.84939575195312\n",
      "Epoch 62, Batch 14866, Loss: 156.96505737304688\n",
      "Epoch 62, Batch 14867, Loss: 149.41424560546875\n",
      "Epoch 62, Batch 14868, Loss: 176.23753356933594\n",
      "Epoch 62, Batch 14869, Loss: 176.73123168945312\n",
      "Epoch 62, Batch 14870, Loss: 181.57005310058594\n",
      "Epoch 62, Batch 14871, Loss: 199.35614013671875\n",
      "Epoch 62, Batch 14872, Loss: 179.17929077148438\n",
      "Epoch 62, Batch 14873, Loss: 155.5438232421875\n",
      "Epoch 62, Batch 14874, Loss: 169.26528930664062\n",
      "Epoch 62, Batch 14875, Loss: 165.49769592285156\n",
      "Epoch 62, Batch 14876, Loss: 162.03199768066406\n",
      "Epoch 62, Batch 14877, Loss: 180.23190307617188\n",
      "Epoch 62, Batch 14878, Loss: 169.94271850585938\n",
      "Epoch 62, Batch 14879, Loss: 170.7895965576172\n",
      "Epoch 62, Batch 14880, Loss: 162.93756103515625\n",
      "Epoch 62, Batch 14881, Loss: 167.09805297851562\n",
      "Epoch 62, Batch 14882, Loss: 181.06785583496094\n",
      "Epoch 62, Batch 14883, Loss: 162.3015899658203\n",
      "Epoch 62, Batch 14884, Loss: 179.30450439453125\n",
      "Epoch 62, Batch 14885, Loss: 179.873291015625\n",
      "Epoch 62, Batch 14886, Loss: 171.57598876953125\n",
      "Epoch 62, Batch 14887, Loss: 182.00807189941406\n",
      "Epoch 62, Batch 14888, Loss: 169.4568328857422\n",
      "Epoch 62, Batch 14889, Loss: 160.64175415039062\n",
      "Epoch 62, Batch 14890, Loss: 178.63919067382812\n",
      "Epoch 62, Batch 14891, Loss: 159.6393585205078\n",
      "Epoch 62, Batch 14892, Loss: 167.23326110839844\n",
      "Epoch 62, Batch 14893, Loss: 176.46932983398438\n",
      "Epoch 62, Batch 14894, Loss: 172.00735473632812\n",
      "Epoch 62, Batch 14895, Loss: 163.31402587890625\n",
      "Epoch 62, Batch 14896, Loss: 169.63462829589844\n",
      "Epoch 62, Batch 14897, Loss: 182.55810546875\n",
      "Epoch 62, Batch 14898, Loss: 162.3874969482422\n",
      "Epoch 62, Batch 14899, Loss: 166.3383026123047\n",
      "Epoch 62, Batch 14900, Loss: 166.26165771484375\n",
      "Epoch 62, Batch 14901, Loss: 182.1624755859375\n",
      "Epoch 62, Batch 14902, Loss: 185.74819946289062\n",
      "Epoch 62, Batch 14903, Loss: 167.91038513183594\n",
      "Epoch 62, Batch 14904, Loss: 173.62860107421875\n",
      "Epoch 62, Batch 14905, Loss: 177.58778381347656\n",
      "Epoch 62, Batch 14906, Loss: 182.93614196777344\n",
      "Epoch 62, Batch 14907, Loss: 177.71507263183594\n",
      "Epoch 62, Batch 14908, Loss: 175.025146484375\n",
      "Epoch 62, Batch 14909, Loss: 186.01869201660156\n",
      "Epoch 62, Batch 14910, Loss: 179.23583984375\n",
      "Epoch 62, Batch 14911, Loss: 171.79820251464844\n",
      "Epoch 62, Batch 14912, Loss: 167.3142547607422\n",
      "Epoch 62, Batch 14913, Loss: 169.56924438476562\n",
      "Epoch 62, Batch 14914, Loss: 171.72279357910156\n",
      "Epoch 62, Batch 14915, Loss: 164.69940185546875\n",
      "Epoch 62, Batch 14916, Loss: 170.5211944580078\n",
      "Epoch 62, Batch 14917, Loss: 176.41818237304688\n",
      "Epoch 62, Batch 14918, Loss: 197.65980529785156\n",
      "Epoch 62, Batch 14919, Loss: 186.53762817382812\n",
      "Epoch 62, Batch 14920, Loss: 174.57131958007812\n",
      "Epoch 62, Batch 14921, Loss: 169.95501708984375\n",
      "Epoch 62, Batch 14922, Loss: 168.67852783203125\n",
      "Epoch 62, Batch 14923, Loss: 188.28578186035156\n",
      "Epoch 62, Batch 14924, Loss: 159.21937561035156\n",
      "Epoch 62, Batch 14925, Loss: 177.17437744140625\n",
      "Epoch 62, Batch 14926, Loss: 153.4667510986328\n",
      "Epoch 62, Batch 14927, Loss: 169.27561950683594\n",
      "Epoch 62, Batch 14928, Loss: 178.04757690429688\n",
      "Epoch 62, Batch 14929, Loss: 168.29391479492188\n",
      "Epoch 62, Batch 14930, Loss: 163.69326782226562\n",
      "Epoch 62, Batch 14931, Loss: 185.4868621826172\n",
      "Epoch 62, Batch 14932, Loss: 170.5599365234375\n",
      "Epoch 62, Batch 14933, Loss: 154.3895263671875\n",
      "Epoch 62, Batch 14934, Loss: 187.020751953125\n",
      "Epoch 62, Batch 14935, Loss: 154.42724609375\n",
      "Epoch 62, Batch 14936, Loss: 150.3140106201172\n",
      "Epoch 62, Batch 14937, Loss: 171.4678192138672\n",
      "Epoch 62, Batch 14938, Loss: 165.12091064453125\n",
      "Epoch 62, Batch 14939, Loss: 163.2646484375\n",
      "Epoch 62, Batch 14940, Loss: 173.9013671875\n",
      "Epoch 62, Batch 14941, Loss: 183.42991638183594\n",
      "Epoch 62, Batch 14942, Loss: 160.10186767578125\n",
      "Epoch 62, Batch 14943, Loss: 180.88174438476562\n",
      "Epoch 62, Batch 14944, Loss: 184.1195068359375\n",
      "Epoch 62, Batch 14945, Loss: 179.1394500732422\n",
      "Epoch 62, Batch 14946, Loss: 159.21633911132812\n",
      "Epoch 62, Batch 14947, Loss: 185.5981903076172\n",
      "Epoch 62, Batch 14948, Loss: 172.45025634765625\n",
      "Epoch 62, Batch 14949, Loss: 162.8375244140625\n",
      "Epoch 62, Batch 14950, Loss: 177.2174072265625\n",
      "Epoch 62, Batch 14951, Loss: 169.32069396972656\n",
      "Epoch 62, Batch 14952, Loss: 188.2378692626953\n",
      "Epoch 62, Batch 14953, Loss: 178.0162353515625\n",
      "Epoch 62, Batch 14954, Loss: 169.94383239746094\n",
      "Epoch 62, Batch 14955, Loss: 163.33645629882812\n",
      "Epoch 62, Batch 14956, Loss: 150.9545135498047\n",
      "Epoch 62, Batch 14957, Loss: 170.1511688232422\n",
      "Epoch 62, Batch 14958, Loss: 182.46481323242188\n",
      "Epoch 62, Batch 14959, Loss: 184.37628173828125\n",
      "Epoch 62, Batch 14960, Loss: 163.069580078125\n",
      "Epoch 62, Batch 14961, Loss: 175.43455505371094\n",
      "Epoch 62, Batch 14962, Loss: 164.5127716064453\n",
      "Epoch 62, Batch 14963, Loss: 175.15380859375\n",
      "Epoch 62, Batch 14964, Loss: 186.28057861328125\n",
      "Epoch 62, Batch 14965, Loss: 174.9452667236328\n",
      "Epoch 62, Batch 14966, Loss: 174.67919921875\n",
      "Epoch 62, Batch 14967, Loss: 187.965087890625\n",
      "Epoch 62, Batch 14968, Loss: 178.08355712890625\n",
      "Epoch 62, Batch 14969, Loss: 171.51536560058594\n",
      "Epoch 62, Batch 14970, Loss: 168.6025848388672\n",
      "Epoch 62, Batch 14971, Loss: 191.90870666503906\n",
      "Epoch 62, Batch 14972, Loss: 157.07423400878906\n",
      "Epoch 62, Batch 14973, Loss: 166.63552856445312\n",
      "Epoch 62, Batch 14974, Loss: 179.80902099609375\n",
      "Epoch 62, Batch 14975, Loss: 170.42247009277344\n",
      "Epoch 62, Batch 14976, Loss: 163.64816284179688\n",
      "Epoch 62, Batch 14977, Loss: 170.80360412597656\n",
      "Epoch 62, Batch 14978, Loss: 176.66407775878906\n",
      "Epoch 62, Batch 14979, Loss: 173.8794403076172\n",
      "Epoch 62, Batch 14980, Loss: 168.15382385253906\n",
      "Epoch 62, Batch 14981, Loss: 172.83255004882812\n",
      "Epoch 62, Batch 14982, Loss: 185.03919982910156\n",
      "Epoch 62, Batch 14983, Loss: 178.15098571777344\n",
      "Epoch 62, Batch 14984, Loss: 180.0424041748047\n",
      "Epoch 62, Batch 14985, Loss: 186.34674072265625\n",
      "Epoch 62, Batch 14986, Loss: 162.03053283691406\n",
      "Epoch 62, Batch 14987, Loss: 176.2374725341797\n",
      "Epoch 62, Batch 14988, Loss: 172.01153564453125\n",
      "Epoch 62, Batch 14989, Loss: 174.0351104736328\n",
      "Epoch 62, Batch 14990, Loss: 189.42898559570312\n",
      "Epoch 62, Batch 14991, Loss: 170.60890197753906\n",
      "Epoch 62, Batch 14992, Loss: 171.80331420898438\n",
      "Epoch 62, Batch 14993, Loss: 168.3773956298828\n",
      "Epoch 62, Batch 14994, Loss: 159.65872192382812\n",
      "Epoch 62, Batch 14995, Loss: 192.71856689453125\n",
      "Epoch 62, Batch 14996, Loss: 187.26739501953125\n",
      "Epoch 62, Batch 14997, Loss: 187.7640380859375\n",
      "Epoch 62, Batch 14998, Loss: 177.07229614257812\n",
      "Epoch 62, Batch 14999, Loss: 168.71632385253906\n",
      "Epoch 62, Batch 15000, Loss: 160.86830139160156\n",
      "Epoch 62, Batch 15001, Loss: 156.84153747558594\n",
      "Epoch 62, Batch 15002, Loss: 171.93455505371094\n",
      "Epoch 62, Batch 15003, Loss: 172.29966735839844\n",
      "Epoch 62, Batch 15004, Loss: 172.6905517578125\n",
      "Epoch 62, Batch 15005, Loss: 180.05348205566406\n",
      "Epoch 62, Batch 15006, Loss: 167.4702911376953\n",
      "Epoch 62, Batch 15007, Loss: 184.781005859375\n",
      "Epoch 62, Batch 15008, Loss: 172.2838134765625\n",
      "Epoch 62, Batch 15009, Loss: 165.79534912109375\n",
      "Epoch 62, Batch 15010, Loss: 180.53321838378906\n",
      "Epoch 62, Batch 15011, Loss: 153.52613830566406\n",
      "Epoch 62, Batch 15012, Loss: 164.0152130126953\n",
      "Epoch 62, Batch 15013, Loss: 185.658935546875\n",
      "Epoch 62, Batch 15014, Loss: 180.03054809570312\n",
      "Epoch 62, Batch 15015, Loss: 172.15382385253906\n",
      "Epoch 62, Batch 15016, Loss: 162.1400604248047\n",
      "Epoch 62, Batch 15017, Loss: 178.6996612548828\n",
      "Epoch 62, Batch 15018, Loss: 156.9838409423828\n",
      "Epoch 62, Batch 15019, Loss: 162.6952667236328\n",
      "Epoch 62, Batch 15020, Loss: 179.0844268798828\n",
      "Epoch 62, Batch 15021, Loss: 171.27005004882812\n",
      "Epoch 62, Batch 15022, Loss: 181.58094787597656\n",
      "Epoch 62, Batch 15023, Loss: 169.489990234375\n",
      "Epoch 62, Batch 15024, Loss: 164.54904174804688\n",
      "Epoch 62, Batch 15025, Loss: 160.62271118164062\n",
      "Epoch 62, Batch 15026, Loss: 166.00686645507812\n",
      "Epoch 62, Batch 15027, Loss: 166.583251953125\n",
      "Epoch 62, Batch 15028, Loss: 174.7742156982422\n",
      "Epoch 62, Batch 15029, Loss: 171.72605895996094\n",
      "Epoch 62, Batch 15030, Loss: 179.45834350585938\n",
      "Epoch 62, Batch 15031, Loss: 164.12396240234375\n",
      "Epoch 62, Batch 15032, Loss: 187.63497924804688\n",
      "Epoch 62, Batch 15033, Loss: 167.97482299804688\n",
      "Epoch 62, Batch 15034, Loss: 181.54843139648438\n",
      "Epoch 62, Batch 15035, Loss: 183.7660369873047\n",
      "Epoch 62, Batch 15036, Loss: 172.38990783691406\n",
      "Epoch 62, Batch 15037, Loss: 170.55221557617188\n",
      "Epoch 62, Batch 15038, Loss: 164.41259765625\n",
      "Epoch 62, Batch 15039, Loss: 173.32313537597656\n",
      "Epoch 62, Batch 15040, Loss: 185.9149932861328\n",
      "Epoch 62, Batch 15041, Loss: 181.72386169433594\n",
      "Epoch 62, Batch 15042, Loss: 157.8583221435547\n",
      "Epoch 62, Batch 15043, Loss: 155.1916961669922\n",
      "Epoch 62, Batch 15044, Loss: 179.99200439453125\n",
      "Epoch 62, Batch 15045, Loss: 177.20994567871094\n",
      "Epoch 62, Batch 15046, Loss: 178.2258758544922\n",
      "Epoch 62, Batch 15047, Loss: 185.27471923828125\n",
      "Epoch 62, Batch 15048, Loss: 171.1056365966797\n",
      "Epoch 62, Batch 15049, Loss: 160.88143920898438\n",
      "Epoch 62, Batch 15050, Loss: 177.07949829101562\n",
      "Epoch 62, Batch 15051, Loss: 157.00994873046875\n",
      "Epoch 62, Batch 15052, Loss: 180.24969482421875\n",
      "Epoch 62, Batch 15053, Loss: 178.8588104248047\n",
      "Epoch 62, Batch 15054, Loss: 174.53211975097656\n",
      "Epoch 62, Batch 15055, Loss: 180.5841827392578\n",
      "Epoch 62, Batch 15056, Loss: 166.3388214111328\n",
      "Epoch 62, Batch 15057, Loss: 174.8342742919922\n",
      "Epoch 62, Batch 15058, Loss: 165.68931579589844\n",
      "Epoch 62, Batch 15059, Loss: 169.87759399414062\n",
      "Epoch 62, Batch 15060, Loss: 167.11270141601562\n",
      "Epoch 62, Batch 15061, Loss: 182.24221801757812\n",
      "Epoch 62, Batch 15062, Loss: 183.01715087890625\n",
      "Epoch 62, Batch 15063, Loss: 179.93556213378906\n",
      "Epoch 62, Batch 15064, Loss: 176.58436584472656\n",
      "Epoch 62, Batch 15065, Loss: 175.61976623535156\n",
      "Epoch 62, Batch 15066, Loss: 182.53001403808594\n",
      "Epoch 62, Batch 15067, Loss: 167.33692932128906\n",
      "Epoch 62, Batch 15068, Loss: 180.7151641845703\n",
      "Epoch 62, Batch 15069, Loss: 184.10797119140625\n",
      "Epoch 62, Batch 15070, Loss: 177.76058959960938\n",
      "Epoch 62, Batch 15071, Loss: 166.83680725097656\n",
      "Epoch 62, Batch 15072, Loss: 167.32142639160156\n",
      "Epoch 62, Batch 15073, Loss: 174.01919555664062\n",
      "Epoch 62, Batch 15074, Loss: 138.3853759765625\n",
      "Epoch 62, Batch 15075, Loss: 168.66891479492188\n",
      "Epoch 62, Batch 15076, Loss: 176.9644775390625\n",
      "Epoch 62, Batch 15077, Loss: 190.67884826660156\n",
      "Epoch 62, Batch 15078, Loss: 172.91517639160156\n",
      "Epoch 62, Batch 15079, Loss: 151.0886688232422\n",
      "Epoch 62, Batch 15080, Loss: 182.9435272216797\n",
      "Epoch 62, Batch 15081, Loss: 165.0465850830078\n",
      "Epoch 62, Batch 15082, Loss: 161.22679138183594\n",
      "Epoch 62, Batch 15083, Loss: 163.8594512939453\n",
      "Epoch 62, Batch 15084, Loss: 175.90182495117188\n",
      "Epoch 62, Batch 15085, Loss: 170.2708282470703\n",
      "Epoch 62, Batch 15086, Loss: 163.38833618164062\n",
      "Epoch 62, Batch 15087, Loss: 174.71653747558594\n",
      "Epoch 62, Batch 15088, Loss: 170.2922821044922\n",
      "Epoch 62, Batch 15089, Loss: 164.0191650390625\n",
      "Epoch 62, Batch 15090, Loss: 177.49124145507812\n",
      "Epoch 62, Batch 15091, Loss: 177.76388549804688\n",
      "Epoch 62, Batch 15092, Loss: 168.75343322753906\n",
      "Epoch 62, Batch 15093, Loss: 178.9261474609375\n",
      "Epoch 62, Batch 15094, Loss: 169.54249572753906\n",
      "Epoch 62, Batch 15095, Loss: 171.6748809814453\n",
      "Epoch 62, Batch 15096, Loss: 174.2520751953125\n",
      "Epoch 62, Batch 15097, Loss: 155.856689453125\n",
      "Epoch 62, Batch 15098, Loss: 179.62628173828125\n",
      "Epoch 62, Batch 15099, Loss: 176.12356567382812\n",
      "Epoch 62, Batch 15100, Loss: 169.06484985351562\n",
      "Epoch 62, Batch 15101, Loss: 177.9398193359375\n",
      "Epoch 62, Batch 15102, Loss: 159.53729248046875\n",
      "Epoch 62, Batch 15103, Loss: 163.05178833007812\n",
      "Epoch 62, Batch 15104, Loss: 170.6461944580078\n",
      "Epoch 62, Batch 15105, Loss: 157.17520141601562\n",
      "Epoch 62, Batch 15106, Loss: 189.0009307861328\n",
      "Epoch 62, Batch 15107, Loss: 175.5083770751953\n",
      "Epoch 62, Batch 15108, Loss: 164.52377319335938\n",
      "Epoch 62, Batch 15109, Loss: 165.20111083984375\n",
      "Epoch 62, Batch 15110, Loss: 153.1027069091797\n",
      "Epoch 62, Batch 15111, Loss: 187.06802368164062\n",
      "Epoch 62, Batch 15112, Loss: 174.67462158203125\n",
      "Epoch 62, Batch 15113, Loss: 173.9191131591797\n",
      "Epoch 62, Batch 15114, Loss: 189.6695556640625\n",
      "Epoch 62, Batch 15115, Loss: 165.9534912109375\n",
      "Epoch 62, Batch 15116, Loss: 189.8311309814453\n",
      "Epoch 62, Batch 15117, Loss: 178.47389221191406\n",
      "Epoch 62, Batch 15118, Loss: 170.56436157226562\n",
      "Epoch 62, Batch 15119, Loss: 184.80438232421875\n",
      "Epoch 62, Batch 15120, Loss: 171.12469482421875\n",
      "Epoch 62, Batch 15121, Loss: 186.9373321533203\n",
      "Epoch 62, Batch 15122, Loss: 184.48548889160156\n",
      "Epoch 62, Batch 15123, Loss: 177.15139770507812\n",
      "Epoch 62, Batch 15124, Loss: 175.4016571044922\n",
      "Epoch 62, Batch 15125, Loss: 183.12142944335938\n",
      "Epoch 62, Batch 15126, Loss: 181.8192138671875\n",
      "Epoch 62, Batch 15127, Loss: 191.70655822753906\n",
      "Epoch 62, Batch 15128, Loss: 167.37637329101562\n",
      "Epoch 62, Batch 15129, Loss: 176.47254943847656\n",
      "Epoch 62, Batch 15130, Loss: 182.9418487548828\n",
      "Epoch 62, Batch 15131, Loss: 187.9642333984375\n",
      "Epoch 62, Batch 15132, Loss: 166.95286560058594\n",
      "Epoch 62, Batch 15133, Loss: 180.3349609375\n",
      "Epoch 62, Batch 15134, Loss: 150.80580139160156\n",
      "Epoch 62, Batch 15135, Loss: 173.5261993408203\n",
      "Epoch 62, Batch 15136, Loss: 180.06463623046875\n",
      "Epoch 62, Batch 15137, Loss: 176.8070068359375\n",
      "Epoch 62, Batch 15138, Loss: 164.7001953125\n",
      "Epoch 62, Batch 15139, Loss: 159.04452514648438\n",
      "Epoch 62, Batch 15140, Loss: 171.10427856445312\n",
      "Epoch 62, Batch 15141, Loss: 180.6077880859375\n",
      "Epoch 62, Batch 15142, Loss: 176.3998565673828\n",
      "Epoch 62, Batch 15143, Loss: 172.5382537841797\n",
      "Epoch 62, Batch 15144, Loss: 168.39414978027344\n",
      "Epoch 62, Batch 15145, Loss: 181.607177734375\n",
      "Epoch 62, Batch 15146, Loss: 164.12806701660156\n",
      "Epoch 62, Batch 15147, Loss: 170.04266357421875\n",
      "Epoch 62, Batch 15148, Loss: 154.78636169433594\n",
      "Epoch 62, Batch 15149, Loss: 181.36769104003906\n",
      "Epoch 62, Batch 15150, Loss: 170.58984375\n",
      "Epoch 62, Batch 15151, Loss: 174.82110595703125\n",
      "Epoch 62, Batch 15152, Loss: 163.09129333496094\n",
      "Epoch 62, Batch 15153, Loss: 175.44186401367188\n",
      "Epoch 62, Batch 15154, Loss: 170.85520935058594\n",
      "Epoch 62, Batch 15155, Loss: 178.33558654785156\n",
      "Epoch 62, Batch 15156, Loss: 204.84165954589844\n",
      "Epoch 62, Batch 15157, Loss: 193.1492156982422\n",
      "Epoch 62, Batch 15158, Loss: 194.82046508789062\n",
      "Epoch 62, Batch 15159, Loss: 172.0534210205078\n",
      "Epoch 62, Batch 15160, Loss: 163.3968048095703\n",
      "Epoch 62, Batch 15161, Loss: 159.0804443359375\n",
      "Epoch 62, Batch 15162, Loss: 177.65802001953125\n",
      "Epoch 62, Batch 15163, Loss: 150.654296875\n",
      "Epoch 62, Batch 15164, Loss: 172.85076904296875\n",
      "Epoch 62, Batch 15165, Loss: 173.31414794921875\n",
      "Epoch 62, Batch 15166, Loss: 182.15769958496094\n",
      "Epoch 62, Batch 15167, Loss: 181.69061279296875\n",
      "Epoch 62, Batch 15168, Loss: 174.91244506835938\n",
      "Epoch 62, Batch 15169, Loss: 173.83909606933594\n",
      "Epoch 62, Batch 15170, Loss: 176.89772033691406\n",
      "Epoch 62, Batch 15171, Loss: 185.56410217285156\n",
      "Epoch 62, Batch 15172, Loss: 166.87623596191406\n",
      "Epoch 62, Batch 15173, Loss: 178.1192169189453\n",
      "Epoch 62, Batch 15174, Loss: 179.61355590820312\n",
      "Epoch 62, Batch 15175, Loss: 176.93577575683594\n",
      "Epoch 62, Batch 15176, Loss: 164.41436767578125\n",
      "Epoch 62, Batch 15177, Loss: 172.9213104248047\n",
      "Epoch 62, Batch 15178, Loss: 166.14938354492188\n",
      "Epoch 62, Batch 15179, Loss: 180.71812438964844\n",
      "Epoch 62, Batch 15180, Loss: 180.5435028076172\n",
      "Epoch 62, Batch 15181, Loss: 163.7192840576172\n",
      "Epoch 62, Batch 15182, Loss: 165.40231323242188\n",
      "Epoch 62, Batch 15183, Loss: 171.6759796142578\n",
      "Epoch 62, Batch 15184, Loss: 175.5015411376953\n",
      "Epoch 62, Batch 15185, Loss: 176.3660125732422\n",
      "Epoch 62, Batch 15186, Loss: 178.5464324951172\n",
      "Epoch 62, Batch 15187, Loss: 158.08544921875\n",
      "Epoch 62, Batch 15188, Loss: 183.91416931152344\n",
      "Epoch 62, Batch 15189, Loss: 176.41873168945312\n",
      "Epoch 62, Batch 15190, Loss: 173.4802703857422\n",
      "Epoch 62, Batch 15191, Loss: 174.0149688720703\n",
      "Epoch 62, Batch 15192, Loss: 167.3275146484375\n",
      "Epoch 62, Batch 15193, Loss: 171.73924255371094\n",
      "Epoch 62, Batch 15194, Loss: 171.14707946777344\n",
      "Epoch 62, Batch 15195, Loss: 175.38621520996094\n",
      "Epoch 62, Batch 15196, Loss: 171.9288330078125\n",
      "Epoch 62, Batch 15197, Loss: 186.636962890625\n",
      "Epoch 62, Batch 15198, Loss: 185.89117431640625\n",
      "Epoch 62, Batch 15199, Loss: 159.77207946777344\n",
      "Epoch 62, Batch 15200, Loss: 165.64547729492188\n",
      "Epoch 62, Batch 15201, Loss: 179.15878295898438\n",
      "Epoch 62, Batch 15202, Loss: 160.9423370361328\n",
      "Epoch 62, Batch 15203, Loss: 180.056396484375\n",
      "Epoch 62, Batch 15204, Loss: 182.63427734375\n",
      "Epoch 62, Batch 15205, Loss: 168.4585418701172\n",
      "Epoch 62, Batch 15206, Loss: 166.77780151367188\n",
      "Epoch 62, Batch 15207, Loss: 172.10287475585938\n",
      "Epoch 62, Batch 15208, Loss: 174.8138885498047\n",
      "Epoch 62, Batch 15209, Loss: 184.84178161621094\n",
      "Epoch 62, Batch 15210, Loss: 151.65585327148438\n",
      "Epoch 62, Batch 15211, Loss: 183.26284790039062\n",
      "Epoch 62, Batch 15212, Loss: 154.64205932617188\n",
      "Epoch 62, Batch 15213, Loss: 200.9779815673828\n",
      "Epoch 62, Batch 15214, Loss: 178.44815063476562\n",
      "Epoch 62, Batch 15215, Loss: 179.32192993164062\n",
      "Epoch 62, Batch 15216, Loss: 169.88560485839844\n",
      "Epoch 62, Batch 15217, Loss: 156.90164184570312\n",
      "Epoch 62, Batch 15218, Loss: 180.7108612060547\n",
      "Epoch 62, Batch 15219, Loss: 188.99545288085938\n",
      "Epoch 62, Batch 15220, Loss: 180.73089599609375\n",
      "Epoch 62, Batch 15221, Loss: 178.6610870361328\n",
      "Epoch 62, Batch 15222, Loss: 167.08914184570312\n",
      "Epoch 62, Batch 15223, Loss: 186.9175262451172\n",
      "Epoch 62, Batch 15224, Loss: 189.35714721679688\n",
      "Epoch 62, Batch 15225, Loss: 175.38522338867188\n",
      "Epoch 62, Batch 15226, Loss: 175.10594177246094\n",
      "Epoch 62, Batch 15227, Loss: 178.77845764160156\n",
      "Epoch 62, Batch 15228, Loss: 173.89700317382812\n",
      "Epoch 62, Batch 15229, Loss: 186.25360107421875\n",
      "Epoch 62, Batch 15230, Loss: 188.56704711914062\n",
      "Epoch 62, Batch 15231, Loss: 186.7538299560547\n",
      "Epoch 62, Batch 15232, Loss: 159.2662353515625\n",
      "Epoch 62, Batch 15233, Loss: 168.1029815673828\n",
      "Epoch 62, Batch 15234, Loss: 152.58438110351562\n",
      "Epoch 62, Batch 15235, Loss: 170.5294647216797\n",
      "Epoch 62, Batch 15236, Loss: 163.81326293945312\n",
      "Epoch 62, Batch 15237, Loss: 194.31973266601562\n",
      "Epoch 62, Batch 15238, Loss: 186.40379333496094\n",
      "Epoch 62, Batch 15239, Loss: 165.95323181152344\n",
      "Epoch 62, Batch 15240, Loss: 166.37998962402344\n",
      "Epoch 62, Batch 15241, Loss: 180.6475067138672\n",
      "Epoch 62, Batch 15242, Loss: 166.5584259033203\n",
      "Epoch 62, Batch 15243, Loss: 177.22247314453125\n",
      "Epoch 62, Batch 15244, Loss: 159.12503051757812\n",
      "Epoch 62, Batch 15245, Loss: 188.8208465576172\n",
      "Epoch 62, Batch 15246, Loss: 161.517333984375\n",
      "Epoch 62, Batch 15247, Loss: 164.18934631347656\n",
      "Epoch 62, Batch 15248, Loss: 165.028076171875\n",
      "Epoch 62, Batch 15249, Loss: 172.71185302734375\n",
      "Epoch 62, Batch 15250, Loss: 190.0928192138672\n",
      "Epoch 62, Batch 15251, Loss: 162.42959594726562\n",
      "Epoch 62, Batch 15252, Loss: 171.6000518798828\n",
      "Epoch 62, Batch 15253, Loss: 167.95570373535156\n",
      "Epoch 62, Batch 15254, Loss: 173.83914184570312\n",
      "Epoch 62, Batch 15255, Loss: 190.26388549804688\n",
      "Epoch 62, Batch 15256, Loss: 171.78973388671875\n",
      "Epoch 62, Batch 15257, Loss: 186.44210815429688\n",
      "Epoch 62, Batch 15258, Loss: 149.73191833496094\n",
      "Epoch 62, Batch 15259, Loss: 182.0233154296875\n",
      "Epoch 62, Batch 15260, Loss: 166.93646240234375\n",
      "Epoch 62, Batch 15261, Loss: 161.99269104003906\n",
      "Epoch 62, Batch 15262, Loss: 164.36676025390625\n",
      "Epoch 62, Batch 15263, Loss: 169.31915283203125\n",
      "Epoch 62, Batch 15264, Loss: 187.35926818847656\n",
      "Epoch 62, Batch 15265, Loss: 175.61822509765625\n",
      "Epoch 62, Batch 15266, Loss: 175.4910430908203\n",
      "Epoch 62, Batch 15267, Loss: 174.87428283691406\n",
      "Epoch 62, Batch 15268, Loss: 199.62303161621094\n",
      "Epoch 62, Batch 15269, Loss: 186.1245880126953\n",
      "Epoch 62, Batch 15270, Loss: 171.7108154296875\n",
      "Epoch 62, Batch 15271, Loss: 184.07369995117188\n",
      "Epoch 62, Batch 15272, Loss: 193.7554473876953\n",
      "Epoch 62, Batch 15273, Loss: 173.73580932617188\n",
      "Epoch 62, Batch 15274, Loss: 181.47500610351562\n",
      "Epoch 62, Batch 15275, Loss: 169.20263671875\n",
      "Epoch 62, Batch 15276, Loss: 172.44100952148438\n",
      "Epoch 62, Batch 15277, Loss: 162.12716674804688\n",
      "Epoch 62, Batch 15278, Loss: 168.08584594726562\n",
      "Epoch 62, Batch 15279, Loss: 189.28819274902344\n",
      "Epoch 62, Batch 15280, Loss: 163.22267150878906\n",
      "Epoch 62, Batch 15281, Loss: 167.3203582763672\n",
      "Epoch 62, Batch 15282, Loss: 172.83883666992188\n",
      "Epoch 62, Batch 15283, Loss: 186.4653778076172\n",
      "Epoch 62, Batch 15284, Loss: 185.0540008544922\n",
      "Epoch 62, Batch 15285, Loss: 177.6138916015625\n",
      "Epoch 62, Batch 15286, Loss: 168.1721954345703\n",
      "Epoch 62, Batch 15287, Loss: 167.4949493408203\n",
      "Epoch 62, Batch 15288, Loss: 174.2450408935547\n",
      "Epoch 62, Batch 15289, Loss: 169.97964477539062\n",
      "Epoch 62, Batch 15290, Loss: 171.545654296875\n",
      "Epoch 62, Batch 15291, Loss: 167.544189453125\n",
      "Epoch 62, Batch 15292, Loss: 183.0151824951172\n",
      "Epoch 62, Batch 15293, Loss: 170.62754821777344\n",
      "Epoch 62, Batch 15294, Loss: 169.01284790039062\n",
      "Epoch 62, Batch 15295, Loss: 175.5714874267578\n",
      "Epoch 62, Batch 15296, Loss: 171.4952850341797\n",
      "Epoch 62, Batch 15297, Loss: 163.53282165527344\n",
      "Epoch 62, Batch 15298, Loss: 171.4499053955078\n",
      "Epoch 62, Batch 15299, Loss: 175.34323120117188\n",
      "Epoch 62, Batch 15300, Loss: 181.5927734375\n",
      "Epoch 62, Batch 15301, Loss: 184.8174285888672\n",
      "Epoch 62, Batch 15302, Loss: 175.57618713378906\n",
      "Epoch 62, Batch 15303, Loss: 171.72662353515625\n",
      "Epoch 62, Batch 15304, Loss: 173.58053588867188\n",
      "Epoch 62, Batch 15305, Loss: 160.69834899902344\n",
      "Epoch 62, Batch 15306, Loss: 169.37911987304688\n",
      "Epoch 62, Batch 15307, Loss: 179.21817016601562\n",
      "Epoch 62, Batch 15308, Loss: 165.9903106689453\n",
      "Epoch 62, Batch 15309, Loss: 158.57742309570312\n",
      "Epoch 62, Batch 15310, Loss: 177.79058837890625\n",
      "Epoch 62, Batch 15311, Loss: 178.3905487060547\n",
      "Epoch 62, Batch 15312, Loss: 167.405517578125\n",
      "Epoch 62, Batch 15313, Loss: 186.76034545898438\n",
      "Epoch 62, Batch 15314, Loss: 177.18746948242188\n",
      "Epoch 62, Batch 15315, Loss: 185.25433349609375\n",
      "Epoch 62, Batch 15316, Loss: 165.33551025390625\n",
      "Epoch 62, Batch 15317, Loss: 184.36241149902344\n",
      "Epoch 62, Batch 15318, Loss: 173.96475219726562\n",
      "Epoch 62, Batch 15319, Loss: 173.1879425048828\n",
      "Epoch 62, Batch 15320, Loss: 169.04995727539062\n",
      "Epoch 62, Batch 15321, Loss: 174.28292846679688\n",
      "Epoch 62, Batch 15322, Loss: 185.0270233154297\n",
      "Epoch 62, Batch 15323, Loss: 180.99497985839844\n",
      "Epoch 62, Batch 15324, Loss: 182.24234008789062\n",
      "Epoch 62, Batch 15325, Loss: 175.8478240966797\n",
      "Epoch 62, Batch 15326, Loss: 151.26084899902344\n",
      "Epoch 62, Batch 15327, Loss: 166.8513946533203\n",
      "Epoch 62, Batch 15328, Loss: 169.51055908203125\n",
      "Epoch 62, Batch 15329, Loss: 175.92506408691406\n",
      "Epoch 62, Batch 15330, Loss: 169.3966522216797\n",
      "Epoch 62, Batch 15331, Loss: 160.40859985351562\n",
      "Epoch 62, Batch 15332, Loss: 171.6309356689453\n",
      "Epoch 62, Batch 15333, Loss: 173.95584106445312\n",
      "Epoch 62, Batch 15334, Loss: 183.54371643066406\n",
      "Epoch 62, Batch 15335, Loss: 171.45751953125\n",
      "Epoch 62, Batch 15336, Loss: 180.71755981445312\n",
      "Epoch 62, Batch 15337, Loss: 165.6013946533203\n",
      "Epoch 62, Batch 15338, Loss: 181.88279724121094\n",
      "Epoch 62, Batch 15339, Loss: 185.8100128173828\n",
      "Epoch 62, Batch 15340, Loss: 175.2688751220703\n",
      "Epoch 62, Batch 15341, Loss: 185.86204528808594\n",
      "Epoch 62, Batch 15342, Loss: 173.8134307861328\n",
      "Epoch 62, Batch 15343, Loss: 163.2137908935547\n",
      "Epoch 62, Batch 15344, Loss: 182.0640411376953\n",
      "Epoch 62, Batch 15345, Loss: 163.87808227539062\n",
      "Epoch 62, Batch 15346, Loss: 164.5047149658203\n",
      "Epoch 62, Batch 15347, Loss: 165.03138732910156\n",
      "Epoch 62, Batch 15348, Loss: 195.31576538085938\n",
      "Epoch 62, Batch 15349, Loss: 175.77259826660156\n",
      "Epoch 62, Batch 15350, Loss: 161.0831756591797\n",
      "Epoch 62, Batch 15351, Loss: 177.7978057861328\n",
      "Epoch 62, Batch 15352, Loss: 167.49990844726562\n",
      "Epoch 62, Batch 15353, Loss: 180.78469848632812\n",
      "Epoch 62, Batch 15354, Loss: 177.55694580078125\n",
      "Epoch 62, Batch 15355, Loss: 170.43324279785156\n",
      "Epoch 62, Batch 15356, Loss: 191.9562225341797\n",
      "Epoch 62, Batch 15357, Loss: 185.53359985351562\n",
      "Epoch 62, Batch 15358, Loss: 154.69940185546875\n",
      "Epoch 62, Batch 15359, Loss: 168.83282470703125\n",
      "Epoch 62, Batch 15360, Loss: 173.72201538085938\n",
      "Epoch 62, Batch 15361, Loss: 169.9488525390625\n",
      "Epoch 62, Batch 15362, Loss: 174.63169860839844\n",
      "Epoch 62, Batch 15363, Loss: 163.4389190673828\n",
      "Epoch 62, Batch 15364, Loss: 156.95457458496094\n",
      "Epoch 62, Batch 15365, Loss: 174.1639862060547\n",
      "Epoch 62, Batch 15366, Loss: 176.86512756347656\n",
      "Epoch 62, Batch 15367, Loss: 188.95108032226562\n",
      "Epoch 62, Batch 15368, Loss: 169.81326293945312\n",
      "Epoch 62, Batch 15369, Loss: 157.7646026611328\n",
      "Epoch 62, Batch 15370, Loss: 175.15377807617188\n",
      "Epoch 62, Batch 15371, Loss: 156.15567016601562\n",
      "Epoch 62, Batch 15372, Loss: 176.37777709960938\n",
      "Epoch 62, Batch 15373, Loss: 178.35635375976562\n",
      "Epoch 62, Batch 15374, Loss: 167.64907836914062\n",
      "Epoch 62, Batch 15375, Loss: 177.59088134765625\n",
      "Epoch 62, Batch 15376, Loss: 179.09384155273438\n",
      "Epoch 62, Batch 15377, Loss: 175.7692413330078\n",
      "Epoch 62, Batch 15378, Loss: 157.32093811035156\n",
      "Epoch 62, Batch 15379, Loss: 182.57952880859375\n",
      "Epoch 62, Batch 15380, Loss: 173.33360290527344\n",
      "Epoch 62, Batch 15381, Loss: 167.44204711914062\n",
      "Epoch 62, Batch 15382, Loss: 178.19598388671875\n",
      "Epoch 62, Batch 15383, Loss: 174.55152893066406\n",
      "Epoch 62, Batch 15384, Loss: 178.14060974121094\n",
      "Epoch 62, Batch 15385, Loss: 167.83070373535156\n",
      "Epoch 62, Batch 15386, Loss: 190.0983428955078\n",
      "Epoch 62, Batch 15387, Loss: 159.81195068359375\n",
      "Epoch 62, Batch 15388, Loss: 160.69033813476562\n",
      "Epoch 62, Batch 15389, Loss: 179.77432250976562\n",
      "Epoch 62, Batch 15390, Loss: 171.49896240234375\n",
      "Epoch 62, Batch 15391, Loss: 178.32977294921875\n",
      "Epoch 62, Batch 15392, Loss: 184.20498657226562\n",
      "Epoch 62, Batch 15393, Loss: 184.0780487060547\n",
      "Epoch 62, Batch 15394, Loss: 168.5986785888672\n",
      "Epoch 62, Batch 15395, Loss: 162.74557495117188\n",
      "Epoch 62, Batch 15396, Loss: 175.97750854492188\n",
      "Epoch 62, Batch 15397, Loss: 167.57791137695312\n",
      "Epoch 62, Batch 15398, Loss: 178.57138061523438\n",
      "Epoch 62, Batch 15399, Loss: 173.8030242919922\n",
      "Epoch 62, Batch 15400, Loss: 185.02613830566406\n",
      "Epoch 62, Batch 15401, Loss: 175.44418334960938\n",
      "Epoch 62, Batch 15402, Loss: 169.34181213378906\n",
      "Epoch 62, Batch 15403, Loss: 179.0966033935547\n",
      "Epoch 62, Batch 15404, Loss: 180.18292236328125\n",
      "Epoch 62, Batch 15405, Loss: 193.5491180419922\n",
      "Epoch 62, Batch 15406, Loss: 184.83340454101562\n",
      "Epoch 62, Batch 15407, Loss: 175.08570861816406\n",
      "Epoch 62, Batch 15408, Loss: 182.76866149902344\n",
      "Epoch 62, Batch 15409, Loss: 177.23451232910156\n",
      "Epoch 62, Batch 15410, Loss: 175.193359375\n",
      "Epoch 62, Batch 15411, Loss: 161.36305236816406\n",
      "Epoch 62, Batch 15412, Loss: 178.655029296875\n",
      "Epoch 62, Batch 15413, Loss: 165.9694061279297\n",
      "Epoch 62, Batch 15414, Loss: 161.92189025878906\n",
      "Epoch 62, Batch 15415, Loss: 190.8001251220703\n",
      "Epoch 62, Batch 15416, Loss: 176.92466735839844\n",
      "Epoch 62, Batch 15417, Loss: 165.346923828125\n",
      "Epoch 62, Batch 15418, Loss: 154.6088409423828\n",
      "Epoch 62, Batch 15419, Loss: 173.81149291992188\n",
      "Epoch 62, Batch 15420, Loss: 177.51492309570312\n",
      "Epoch 62, Batch 15421, Loss: 165.3182373046875\n",
      "Epoch 62, Batch 15422, Loss: 174.1556396484375\n",
      "Epoch 62, Batch 15423, Loss: 185.31434631347656\n",
      "Epoch 62, Batch 15424, Loss: 164.75511169433594\n",
      "Epoch 62, Batch 15425, Loss: 176.0922393798828\n",
      "Epoch 62, Batch 15426, Loss: 180.3056640625\n",
      "Epoch 62, Batch 15427, Loss: 185.25071716308594\n",
      "Epoch 62, Batch 15428, Loss: 144.20465087890625\n",
      "Epoch 62, Batch 15429, Loss: 164.10076904296875\n",
      "Epoch 62, Batch 15430, Loss: 182.96096801757812\n",
      "Epoch 62, Batch 15431, Loss: 174.35498046875\n",
      "Epoch 62, Batch 15432, Loss: 173.7904052734375\n",
      "Epoch 62, Batch 15433, Loss: 170.06295776367188\n",
      "Epoch 62, Batch 15434, Loss: 167.88128662109375\n",
      "Epoch 62, Batch 15435, Loss: 190.57064819335938\n",
      "Epoch 62, Batch 15436, Loss: 165.93740844726562\n",
      "Epoch 62, Batch 15437, Loss: 181.15756225585938\n",
      "Epoch 62, Batch 15438, Loss: 176.79879760742188\n",
      "Epoch 62, Batch 15439, Loss: 190.15982055664062\n",
      "Epoch 62, Batch 15440, Loss: 177.6802978515625\n",
      "Epoch 62, Batch 15441, Loss: 166.55613708496094\n",
      "Epoch 62, Batch 15442, Loss: 168.91937255859375\n",
      "Epoch 62, Batch 15443, Loss: 192.41546630859375\n",
      "Epoch 62, Batch 15444, Loss: 181.3273162841797\n",
      "Epoch 62, Batch 15445, Loss: 167.32142639160156\n",
      "Epoch 62, Batch 15446, Loss: 171.5007781982422\n",
      "Epoch 62, Batch 15447, Loss: 185.1413116455078\n",
      "Epoch 62, Batch 15448, Loss: 180.1089630126953\n",
      "Epoch 62, Batch 15449, Loss: 181.05133056640625\n",
      "Epoch 62, Batch 15450, Loss: 178.99095153808594\n",
      "Epoch 62, Batch 15451, Loss: 174.2439727783203\n",
      "Epoch 62, Batch 15452, Loss: 173.49545288085938\n",
      "Epoch 62, Batch 15453, Loss: 184.89718627929688\n",
      "Epoch 62, Batch 15454, Loss: 198.35169982910156\n",
      "Epoch 62, Batch 15455, Loss: 192.55166625976562\n",
      "Epoch 62, Batch 15456, Loss: 185.82545471191406\n",
      "Epoch 62, Batch 15457, Loss: 169.28945922851562\n",
      "Epoch 62, Batch 15458, Loss: 181.32472229003906\n",
      "Epoch 62, Batch 15459, Loss: 195.28839111328125\n",
      "Epoch 62, Batch 15460, Loss: 187.22010803222656\n",
      "Epoch 62, Batch 15461, Loss: 178.0632781982422\n",
      "Epoch 62, Batch 15462, Loss: 169.66270446777344\n",
      "Epoch 62, Batch 15463, Loss: 167.56361389160156\n",
      "Epoch 62, Batch 15464, Loss: 181.2480926513672\n",
      "Epoch 62, Batch 15465, Loss: 184.9107208251953\n",
      "Epoch 62, Batch 15466, Loss: 195.33717346191406\n",
      "Epoch 62, Batch 15467, Loss: 170.1962432861328\n",
      "Epoch 62, Batch 15468, Loss: 167.81521606445312\n",
      "Epoch 62, Batch 15469, Loss: 170.1113739013672\n",
      "Epoch 62, Batch 15470, Loss: 178.25863647460938\n",
      "Epoch 62, Batch 15471, Loss: 168.99830627441406\n",
      "Epoch 62, Batch 15472, Loss: 186.24073791503906\n",
      "Epoch 62, Batch 15473, Loss: 170.9152374267578\n",
      "Epoch 62, Batch 15474, Loss: 172.06568908691406\n",
      "Epoch 62, Batch 15475, Loss: 180.09239196777344\n",
      "Epoch 62, Batch 15476, Loss: 183.56935119628906\n",
      "Epoch 62, Batch 15477, Loss: 175.02755737304688\n",
      "Epoch 62, Batch 15478, Loss: 186.04144287109375\n",
      "Epoch 62, Batch 15479, Loss: 180.36575317382812\n",
      "Epoch 62, Batch 15480, Loss: 158.5445556640625\n",
      "Epoch 62, Batch 15481, Loss: 167.46266174316406\n",
      "Epoch 62, Batch 15482, Loss: 166.1295623779297\n",
      "Epoch 62, Batch 15483, Loss: 174.3803253173828\n",
      "Epoch 62, Batch 15484, Loss: 151.3378143310547\n",
      "Epoch 62, Batch 15485, Loss: 183.457275390625\n",
      "Epoch 62, Batch 15486, Loss: 165.22970581054688\n",
      "Epoch 62, Batch 15487, Loss: 166.15426635742188\n",
      "Epoch 62, Batch 15488, Loss: 167.3502197265625\n",
      "Epoch 62, Batch 15489, Loss: 180.56646728515625\n",
      "Epoch 62, Batch 15490, Loss: 182.73663330078125\n",
      "Epoch 62, Batch 15491, Loss: 166.5851593017578\n",
      "Epoch 62, Batch 15492, Loss: 171.9065704345703\n",
      "Epoch 62, Batch 15493, Loss: 175.20948791503906\n",
      "Epoch 62, Batch 15494, Loss: 177.45016479492188\n",
      "Epoch 62, Batch 15495, Loss: 168.64772033691406\n",
      "Epoch 62, Batch 15496, Loss: 189.71871948242188\n",
      "Epoch 62, Batch 15497, Loss: 166.6884002685547\n",
      "Epoch 62, Batch 15498, Loss: 177.98828125\n",
      "Epoch 62, Batch 15499, Loss: 168.12518310546875\n",
      "Epoch 62, Batch 15500, Loss: 193.7035369873047\n",
      "Epoch 62, Batch 15501, Loss: 184.64471435546875\n",
      "Epoch 62, Batch 15502, Loss: 179.3232879638672\n",
      "Epoch 62, Batch 15503, Loss: 168.49139404296875\n",
      "Epoch 62, Batch 15504, Loss: 173.53860473632812\n",
      "Epoch 62, Batch 15505, Loss: 178.17784118652344\n",
      "Epoch 62, Batch 15506, Loss: 172.9642333984375\n",
      "Epoch 62, Batch 15507, Loss: 175.21278381347656\n",
      "Epoch 62, Batch 15508, Loss: 192.38424682617188\n",
      "Epoch 62, Batch 15509, Loss: 169.99920654296875\n",
      "Epoch 62, Batch 15510, Loss: 167.99462890625\n",
      "Epoch 62, Batch 15511, Loss: 175.58258056640625\n",
      "Epoch 62, Batch 15512, Loss: 174.26512145996094\n",
      "Epoch 62, Batch 15513, Loss: 171.89105224609375\n",
      "Epoch 62, Batch 15514, Loss: 168.93153381347656\n",
      "Epoch 62, Batch 15515, Loss: 174.0778045654297\n",
      "Epoch 62, Batch 15516, Loss: 184.30824279785156\n",
      "Epoch 62, Batch 15517, Loss: 174.8219757080078\n",
      "Epoch 62, Batch 15518, Loss: 164.6399383544922\n",
      "Epoch 62, Batch 15519, Loss: 185.2348175048828\n",
      "Epoch 62, Batch 15520, Loss: 182.45700073242188\n",
      "Epoch 62, Batch 15521, Loss: 171.1544952392578\n",
      "Epoch 62, Batch 15522, Loss: 175.1143035888672\n",
      "Epoch 62, Batch 15523, Loss: 165.71282958984375\n",
      "Epoch 62, Batch 15524, Loss: 177.81436157226562\n",
      "Epoch 62, Batch 15525, Loss: 195.3355255126953\n",
      "Epoch 62, Batch 15526, Loss: 172.02032470703125\n",
      "Epoch 62, Batch 15527, Loss: 174.2244415283203\n",
      "Epoch 62, Batch 15528, Loss: 184.1975555419922\n",
      "Epoch 62, Batch 15529, Loss: 161.0124053955078\n",
      "Epoch 62, Batch 15530, Loss: 179.13841247558594\n",
      "Epoch 62, Batch 15531, Loss: 170.2447509765625\n",
      "Epoch 62, Batch 15532, Loss: 168.79885864257812\n",
      "Epoch 62, Batch 15533, Loss: 181.88702392578125\n",
      "Epoch 62, Batch 15534, Loss: 177.45281982421875\n",
      "Epoch 62, Batch 15535, Loss: 181.2566375732422\n",
      "Epoch 62, Batch 15536, Loss: 165.73719787597656\n",
      "Epoch 62, Batch 15537, Loss: 176.464111328125\n",
      "Epoch 62, Batch 15538, Loss: 169.68727111816406\n",
      "Epoch 62, Batch 15539, Loss: 172.2135467529297\n",
      "Epoch 62, Batch 15540, Loss: 178.7480926513672\n",
      "Epoch 62, Batch 15541, Loss: 174.28045654296875\n",
      "Epoch 62, Batch 15542, Loss: 190.0712432861328\n",
      "Epoch 62, Batch 15543, Loss: 162.84132385253906\n",
      "Epoch 62, Batch 15544, Loss: 164.5418701171875\n",
      "Epoch 62, Batch 15545, Loss: 184.971923828125\n",
      "Epoch 62, Batch 15546, Loss: 170.6830596923828\n",
      "Epoch 62, Batch 15547, Loss: 167.80747985839844\n",
      "Epoch 62, Batch 15548, Loss: 156.1544647216797\n",
      "Epoch 62, Batch 15549, Loss: 172.02015686035156\n",
      "Epoch 62, Batch 15550, Loss: 172.90176391601562\n",
      "Epoch 62, Batch 15551, Loss: 165.27255249023438\n",
      "Epoch 62, Batch 15552, Loss: 178.18450927734375\n",
      "Epoch 62, Batch 15553, Loss: 160.33587646484375\n",
      "Epoch 62, Batch 15554, Loss: 174.30819702148438\n",
      "Epoch 62, Batch 15555, Loss: 165.6100311279297\n",
      "Epoch 62, Batch 15556, Loss: 169.4638671875\n",
      "Epoch 62, Batch 15557, Loss: 167.05926513671875\n",
      "Epoch 62, Batch 15558, Loss: 190.8958740234375\n",
      "Epoch 62, Batch 15559, Loss: 192.7276611328125\n",
      "Epoch 62, Batch 15560, Loss: 189.87986755371094\n",
      "Epoch 62, Batch 15561, Loss: 187.88783264160156\n",
      "Epoch 62, Batch 15562, Loss: 169.1513214111328\n",
      "Epoch 62, Batch 15563, Loss: 172.50733947753906\n",
      "Epoch 62, Batch 15564, Loss: 189.30738830566406\n",
      "Epoch 62, Batch 15565, Loss: 175.1229705810547\n",
      "Epoch 62, Batch 15566, Loss: 168.9342041015625\n",
      "Epoch 62, Batch 15567, Loss: 180.63861083984375\n",
      "Epoch 62, Batch 15568, Loss: 146.7371063232422\n",
      "Epoch 62, Batch 15569, Loss: 166.08349609375\n",
      "Epoch 62, Batch 15570, Loss: 169.61805725097656\n",
      "Epoch 62, Batch 15571, Loss: 188.66404724121094\n",
      "Epoch 62, Batch 15572, Loss: 183.364501953125\n",
      "Epoch 62, Batch 15573, Loss: 181.9394073486328\n",
      "Epoch 62, Batch 15574, Loss: 165.5642852783203\n",
      "Epoch 62, Batch 15575, Loss: 175.61058044433594\n",
      "Epoch 62, Batch 15576, Loss: 213.81552124023438\n",
      "Epoch 62, Batch 15577, Loss: 173.7400360107422\n",
      "Epoch 62, Batch 15578, Loss: 187.66065979003906\n",
      "Epoch 62, Batch 15579, Loss: 164.76229858398438\n",
      "Epoch 62, Batch 15580, Loss: 170.8421173095703\n",
      "Epoch 62, Batch 15581, Loss: 168.31454467773438\n",
      "Epoch 62, Batch 15582, Loss: 169.09739685058594\n",
      "Epoch 62, Batch 15583, Loss: 172.79769897460938\n",
      "Epoch 62, Batch 15584, Loss: 201.0851287841797\n",
      "Epoch 62, Batch 15585, Loss: 175.2461700439453\n",
      "Epoch 62, Batch 15586, Loss: 188.9731903076172\n",
      "Epoch 62, Batch 15587, Loss: 177.2494659423828\n",
      "Epoch 62, Batch 15588, Loss: 172.5743865966797\n",
      "Epoch 62, Batch 15589, Loss: 187.7263641357422\n",
      "Epoch 62, Batch 15590, Loss: 161.1230010986328\n",
      "Epoch 62, Batch 15591, Loss: 167.97332763671875\n",
      "Epoch 62, Batch 15592, Loss: 191.59323120117188\n",
      "Epoch 62, Batch 15593, Loss: 186.53018188476562\n",
      "Epoch 62, Batch 15594, Loss: 182.2532958984375\n",
      "Epoch 62, Batch 15595, Loss: 163.20579528808594\n",
      "Epoch 62, Batch 15596, Loss: 151.47552490234375\n",
      "Epoch 62, Batch 15597, Loss: 164.88966369628906\n",
      "Epoch 62, Batch 15598, Loss: 165.74624633789062\n",
      "Epoch 62, Batch 15599, Loss: 183.31495666503906\n",
      "Epoch 62, Batch 15600, Loss: 184.67596435546875\n",
      "Epoch 62, Batch 15601, Loss: 172.46327209472656\n",
      "Epoch 62, Batch 15602, Loss: 175.3502655029297\n",
      "Epoch 62, Batch 15603, Loss: 184.1336212158203\n",
      "Epoch 62, Batch 15604, Loss: 172.1268768310547\n",
      "Epoch 62, Batch 15605, Loss: 181.60269165039062\n",
      "Epoch 62, Batch 15606, Loss: 175.99349975585938\n",
      "Epoch 62, Batch 15607, Loss: 173.2313232421875\n",
      "Epoch 62, Batch 15608, Loss: 155.79385375976562\n",
      "Epoch 62, Batch 15609, Loss: 169.9834442138672\n",
      "Epoch 62, Batch 15610, Loss: 167.7858428955078\n",
      "Epoch 62, Batch 15611, Loss: 174.10665893554688\n",
      "Epoch 62, Batch 15612, Loss: 156.3075408935547\n",
      "Epoch 62, Batch 15613, Loss: 182.58197021484375\n",
      "Epoch 62, Batch 15614, Loss: 176.06007385253906\n",
      "Epoch 62, Batch 15615, Loss: 173.12860107421875\n",
      "Epoch 62, Batch 15616, Loss: 185.009765625\n",
      "Epoch 62, Batch 15617, Loss: 167.8435821533203\n",
      "Epoch 62, Batch 15618, Loss: 169.40496826171875\n",
      "Epoch 62, Batch 15619, Loss: 159.7056427001953\n",
      "Epoch 62, Batch 15620, Loss: 171.58441162109375\n",
      "Epoch 62, Batch 15621, Loss: 174.8277130126953\n",
      "Epoch 62, Batch 15622, Loss: 168.5892333984375\n",
      "Epoch 62, Batch 15623, Loss: 179.6983184814453\n",
      "Epoch 62, Batch 15624, Loss: 191.9017791748047\n",
      "Epoch 62, Batch 15625, Loss: 171.3367462158203\n",
      "Epoch 62, Batch 15626, Loss: 168.77552795410156\n",
      "Epoch 62, Batch 15627, Loss: 173.0005340576172\n",
      "Epoch 62, Batch 15628, Loss: 167.23619079589844\n",
      "Epoch 62, Batch 15629, Loss: 168.60128784179688\n",
      "Epoch 62, Batch 15630, Loss: 173.98414611816406\n",
      "Epoch 62, Batch 15631, Loss: 163.70159912109375\n",
      "Epoch 62, Batch 15632, Loss: 177.87759399414062\n",
      "Epoch 62, Batch 15633, Loss: 170.9330291748047\n",
      "Epoch 62, Batch 15634, Loss: 176.87942504882812\n",
      "Epoch 62, Batch 15635, Loss: 171.1343994140625\n",
      "Epoch 62, Batch 15636, Loss: 180.03553771972656\n",
      "Epoch 62, Batch 15637, Loss: 176.8595428466797\n",
      "Epoch 62, Batch 15638, Loss: 173.75135803222656\n",
      "Epoch 62, Batch 15639, Loss: 179.1165008544922\n",
      "Epoch 62, Batch 15640, Loss: 150.91940307617188\n",
      "Epoch 62, Batch 15641, Loss: 170.70071411132812\n",
      "Epoch 62, Batch 15642, Loss: 175.61404418945312\n",
      "Epoch 62, Batch 15643, Loss: 156.74172973632812\n",
      "Epoch 62, Batch 15644, Loss: 181.6656036376953\n",
      "Epoch 62, Batch 15645, Loss: 168.19796752929688\n",
      "Epoch 62, Batch 15646, Loss: 174.93179321289062\n",
      "Epoch 62, Batch 15647, Loss: 184.0622100830078\n",
      "Epoch 62, Batch 15648, Loss: 176.0330352783203\n",
      "Epoch 62, Batch 15649, Loss: 175.34278869628906\n",
      "Epoch 62, Batch 15650, Loss: 174.89517211914062\n",
      "Epoch 62, Batch 15651, Loss: 175.90382385253906\n",
      "Epoch 62, Batch 15652, Loss: 189.38768005371094\n",
      "Epoch 62, Batch 15653, Loss: 165.15960693359375\n",
      "Epoch 62, Batch 15654, Loss: 154.0725860595703\n",
      "Epoch 62, Batch 15655, Loss: 166.14430236816406\n",
      "Epoch 62, Batch 15656, Loss: 178.13876342773438\n",
      "Epoch 62, Batch 15657, Loss: 172.7784881591797\n",
      "Epoch 62, Batch 15658, Loss: 178.76243591308594\n",
      "Epoch 62, Batch 15659, Loss: 168.45382690429688\n",
      "Epoch 62, Batch 15660, Loss: 169.74688720703125\n",
      "Epoch 62, Batch 15661, Loss: 177.3340301513672\n",
      "Epoch 62, Batch 15662, Loss: 170.0177001953125\n",
      "Epoch 62, Batch 15663, Loss: 183.3576202392578\n",
      "Epoch 62, Batch 15664, Loss: 177.98326110839844\n",
      "Epoch 62, Batch 15665, Loss: 170.8289031982422\n",
      "Epoch 62, Batch 15666, Loss: 173.44143676757812\n",
      "Epoch 62, Batch 15667, Loss: 186.1990966796875\n",
      "Epoch 62, Batch 15668, Loss: 163.69606018066406\n",
      "Epoch 62, Batch 15669, Loss: 172.0123748779297\n",
      "Epoch 62, Batch 15670, Loss: 191.7300567626953\n",
      "Epoch 62, Batch 15671, Loss: 161.90936279296875\n",
      "Epoch 62, Batch 15672, Loss: 157.97300720214844\n",
      "Epoch 62, Batch 15673, Loss: 171.84991455078125\n",
      "Epoch 62, Batch 15674, Loss: 170.12538146972656\n",
      "Epoch 62, Batch 15675, Loss: 176.31689453125\n",
      "Epoch 62, Batch 15676, Loss: 164.7865753173828\n",
      "Epoch 62, Batch 15677, Loss: 173.83016967773438\n",
      "Epoch 62, Batch 15678, Loss: 170.4672393798828\n",
      "Epoch 62, Batch 15679, Loss: 180.12826538085938\n",
      "Epoch 62, Batch 15680, Loss: 160.5218048095703\n",
      "Epoch 62, Batch 15681, Loss: 159.9066925048828\n",
      "Epoch 62, Batch 15682, Loss: 181.4988250732422\n",
      "Epoch 62, Batch 15683, Loss: 178.43307495117188\n",
      "Epoch 62, Batch 15684, Loss: 172.67764282226562\n",
      "Epoch 62, Batch 15685, Loss: 157.46620178222656\n",
      "Epoch 62, Batch 15686, Loss: 177.82192993164062\n",
      "Epoch 62, Batch 15687, Loss: 159.2255401611328\n",
      "Epoch 62, Batch 15688, Loss: 177.79248046875\n",
      "Epoch 62, Batch 15689, Loss: 180.02464294433594\n",
      "Epoch 62, Batch 15690, Loss: 179.83522033691406\n",
      "Epoch 62, Batch 15691, Loss: 173.62965393066406\n",
      "Epoch 62, Batch 15692, Loss: 152.4886474609375\n",
      "Epoch 62, Batch 15693, Loss: 187.412841796875\n",
      "Epoch 62, Batch 15694, Loss: 159.39808654785156\n",
      "Epoch 62, Batch 15695, Loss: 159.26953125\n",
      "Epoch 62, Batch 15696, Loss: 173.78468322753906\n",
      "Epoch 62, Batch 15697, Loss: 164.3443145751953\n",
      "Epoch 62, Batch 15698, Loss: 175.62496948242188\n",
      "Epoch 62, Batch 15699, Loss: 179.9752655029297\n",
      "Epoch 62, Batch 15700, Loss: 156.2391357421875\n",
      "Epoch 62, Batch 15701, Loss: 172.4188232421875\n",
      "Epoch 62, Batch 15702, Loss: 186.884765625\n",
      "Epoch 62, Batch 15703, Loss: 172.2296142578125\n",
      "Epoch 62, Batch 15704, Loss: 175.049560546875\n",
      "Epoch 62, Batch 15705, Loss: 182.8162384033203\n",
      "Epoch 62, Batch 15706, Loss: 153.64788818359375\n",
      "Epoch 62, Batch 15707, Loss: 155.2436981201172\n",
      "Epoch 62, Batch 15708, Loss: 174.89126586914062\n",
      "Epoch 62, Batch 15709, Loss: 176.9625701904297\n",
      "Epoch 62, Batch 15710, Loss: 172.81639099121094\n",
      "Epoch 62, Batch 15711, Loss: 172.12155151367188\n",
      "Epoch 62, Batch 15712, Loss: 169.9416961669922\n",
      "Epoch 62, Batch 15713, Loss: 162.1138458251953\n",
      "Epoch 62, Batch 15714, Loss: 168.0635223388672\n",
      "Epoch 62, Batch 15715, Loss: 171.4841766357422\n",
      "Epoch 62, Batch 15716, Loss: 181.3559112548828\n",
      "Epoch 62, Batch 15717, Loss: 186.1426544189453\n",
      "Epoch 62, Batch 15718, Loss: 170.85169982910156\n",
      "Epoch 62, Batch 15719, Loss: 180.70834350585938\n",
      "Epoch 62, Batch 15720, Loss: 176.57186889648438\n",
      "Epoch 62, Batch 15721, Loss: 186.62657165527344\n",
      "Epoch 62, Batch 15722, Loss: 177.93936157226562\n",
      "Epoch 62, Batch 15723, Loss: 180.35887145996094\n",
      "Epoch 62, Batch 15724, Loss: 161.36294555664062\n",
      "Epoch 62, Batch 15725, Loss: 175.16139221191406\n",
      "Epoch 62, Batch 15726, Loss: 177.41937255859375\n",
      "Epoch 62, Batch 15727, Loss: 175.61048889160156\n",
      "Epoch 62, Batch 15728, Loss: 160.90309143066406\n",
      "Epoch 62, Batch 15729, Loss: 179.26490783691406\n",
      "Epoch 62, Batch 15730, Loss: 170.3618621826172\n",
      "Epoch 62, Batch 15731, Loss: 169.0740509033203\n",
      "Epoch 62, Batch 15732, Loss: 183.56558227539062\n",
      "Epoch 62, Batch 15733, Loss: 181.85142517089844\n",
      "Epoch 62, Batch 15734, Loss: 181.57432556152344\n",
      "Epoch 62, Batch 15735, Loss: 165.68116760253906\n",
      "Epoch 62, Batch 15736, Loss: 170.85336303710938\n",
      "Epoch 62, Batch 15737, Loss: 192.07398986816406\n",
      "Epoch 62, Batch 15738, Loss: 168.1714324951172\n",
      "Epoch 62, Batch 15739, Loss: 165.17437744140625\n",
      "Epoch 62, Batch 15740, Loss: 170.0440216064453\n",
      "Epoch 62, Batch 15741, Loss: 168.81765747070312\n",
      "Epoch 62, Batch 15742, Loss: 164.23744201660156\n",
      "Epoch 62, Batch 15743, Loss: 166.81385803222656\n",
      "Epoch 62, Batch 15744, Loss: 176.15065002441406\n",
      "Epoch 62, Batch 15745, Loss: 188.30372619628906\n",
      "Epoch 62, Batch 15746, Loss: 183.10653686523438\n",
      "Epoch 62, Batch 15747, Loss: 160.5211944580078\n",
      "Epoch 62, Batch 15748, Loss: 174.5248565673828\n",
      "Epoch 62, Batch 15749, Loss: 172.42893981933594\n",
      "Epoch 62, Batch 15750, Loss: 181.52392578125\n",
      "Epoch 62, Batch 15751, Loss: 174.98016357421875\n",
      "Epoch 62, Batch 15752, Loss: 173.33316040039062\n",
      "Epoch 62, Batch 15753, Loss: 168.49899291992188\n",
      "Epoch 62, Batch 15754, Loss: 182.2588348388672\n",
      "Epoch 62, Batch 15755, Loss: 189.19891357421875\n",
      "Epoch 62, Batch 15756, Loss: 191.10472106933594\n",
      "Epoch 62, Batch 15757, Loss: 169.91168212890625\n",
      "Epoch 62, Batch 15758, Loss: 173.3293914794922\n",
      "Epoch 62, Batch 15759, Loss: 170.62538146972656\n",
      "Epoch 62, Batch 15760, Loss: 176.3153076171875\n",
      "Epoch 62, Batch 15761, Loss: 165.26498413085938\n",
      "Epoch 62, Batch 15762, Loss: 163.06044006347656\n",
      "Epoch 62, Batch 15763, Loss: 168.1351776123047\n",
      "Epoch 62, Batch 15764, Loss: 182.4744873046875\n",
      "Epoch 62, Batch 15765, Loss: 191.09793090820312\n",
      "Epoch 62, Batch 15766, Loss: 172.22584533691406\n",
      "Epoch 62, Batch 15767, Loss: 171.70193481445312\n",
      "Epoch 62, Batch 15768, Loss: 166.6158905029297\n",
      "Epoch 62, Batch 15769, Loss: 182.14797973632812\n",
      "Epoch 62, Batch 15770, Loss: 181.9295196533203\n",
      "Epoch 62, Batch 15771, Loss: 158.1146240234375\n",
      "Epoch 62, Batch 15772, Loss: 175.93753051757812\n",
      "Epoch 62, Batch 15773, Loss: 163.65963745117188\n",
      "Epoch 62, Batch 15774, Loss: 169.4520721435547\n",
      "Epoch 62, Batch 15775, Loss: 184.08474731445312\n",
      "Epoch 62, Batch 15776, Loss: 163.4326629638672\n",
      "Epoch 62, Batch 15777, Loss: 180.83644104003906\n",
      "Epoch 62, Batch 15778, Loss: 184.7543487548828\n",
      "Epoch 62, Batch 15779, Loss: 177.9368438720703\n",
      "Epoch 62, Batch 15780, Loss: 162.04222106933594\n",
      "Epoch 62, Batch 15781, Loss: 177.29312133789062\n",
      "Epoch 62, Batch 15782, Loss: 183.50941467285156\n",
      "Epoch 62, Batch 15783, Loss: 167.12806701660156\n",
      "Epoch 62, Batch 15784, Loss: 169.93667602539062\n",
      "Epoch 62, Batch 15785, Loss: 165.5680389404297\n",
      "Epoch 62, Batch 15786, Loss: 158.40908813476562\n",
      "Epoch 62, Batch 15787, Loss: 163.46315002441406\n",
      "Epoch 62, Batch 15788, Loss: 164.63577270507812\n",
      "Epoch 62, Batch 15789, Loss: 184.5049591064453\n",
      "Epoch 62, Batch 15790, Loss: 170.00364685058594\n",
      "Epoch 62, Batch 15791, Loss: 164.68292236328125\n",
      "Epoch 62, Batch 15792, Loss: 186.92019653320312\n",
      "Epoch 62, Batch 15793, Loss: 171.02279663085938\n",
      "Epoch 62, Batch 15794, Loss: 169.632568359375\n",
      "Epoch 62, Batch 15795, Loss: 170.2527313232422\n",
      "Epoch 62, Batch 15796, Loss: 175.8415985107422\n",
      "Epoch 62, Batch 15797, Loss: 169.91334533691406\n",
      "Epoch 62, Batch 15798, Loss: 182.9779052734375\n",
      "Epoch 62, Batch 15799, Loss: 171.7912139892578\n",
      "Epoch 62, Batch 15800, Loss: 175.80160522460938\n",
      "Epoch 62, Batch 15801, Loss: 173.1266632080078\n",
      "Epoch 62, Batch 15802, Loss: 171.63011169433594\n",
      "Epoch 62, Batch 15803, Loss: 163.81524658203125\n",
      "Epoch 62, Batch 15804, Loss: 167.89010620117188\n",
      "Epoch 62, Batch 15805, Loss: 182.21006774902344\n",
      "Epoch 62, Batch 15806, Loss: 177.5281219482422\n",
      "Epoch 62, Batch 15807, Loss: 161.5963134765625\n",
      "Epoch 62, Batch 15808, Loss: 179.97100830078125\n",
      "Epoch 62, Batch 15809, Loss: 169.88807678222656\n",
      "Epoch 62, Batch 15810, Loss: 170.79698181152344\n",
      "Epoch 62, Batch 15811, Loss: 181.18765258789062\n",
      "Epoch 62, Batch 15812, Loss: 189.1595001220703\n",
      "Epoch 62, Batch 15813, Loss: 165.77703857421875\n",
      "Epoch 62, Batch 15814, Loss: 173.50482177734375\n",
      "Epoch 62, Batch 15815, Loss: 165.91773986816406\n",
      "Epoch 62, Batch 15816, Loss: 172.14759826660156\n",
      "Epoch 62, Batch 15817, Loss: 176.32814025878906\n",
      "Epoch 62, Batch 15818, Loss: 178.08584594726562\n",
      "Epoch 62, Batch 15819, Loss: 167.30442810058594\n",
      "Epoch 62, Batch 15820, Loss: 152.9279327392578\n",
      "Epoch 62, Batch 15821, Loss: 161.261474609375\n",
      "Epoch 62, Batch 15822, Loss: 186.15573120117188\n",
      "Epoch 62, Batch 15823, Loss: 172.21832275390625\n",
      "Epoch 62, Batch 15824, Loss: 174.85252380371094\n",
      "Epoch 62, Batch 15825, Loss: 156.98631286621094\n",
      "Epoch 62, Batch 15826, Loss: 184.55221557617188\n",
      "Epoch 62, Batch 15827, Loss: 179.85946655273438\n",
      "Epoch 62, Batch 15828, Loss: 181.29800415039062\n",
      "Epoch 62, Batch 15829, Loss: 185.80055236816406\n",
      "Epoch 62, Batch 15830, Loss: 169.64456176757812\n",
      "Epoch 62, Batch 15831, Loss: 184.21534729003906\n",
      "Epoch 62, Batch 15832, Loss: 174.4818572998047\n",
      "Epoch 62, Batch 15833, Loss: 179.72247314453125\n",
      "Epoch 62, Batch 15834, Loss: 160.7710418701172\n",
      "Epoch 62, Batch 15835, Loss: 179.98468017578125\n",
      "Epoch 62, Batch 15836, Loss: 182.36895751953125\n",
      "Epoch 62, Batch 15837, Loss: 167.06846618652344\n",
      "Epoch 62, Batch 15838, Loss: 153.02755737304688\n",
      "Epoch 62, Batch 15839, Loss: 171.77650451660156\n",
      "Epoch 62, Batch 15840, Loss: 152.6868896484375\n",
      "Epoch 62, Batch 15841, Loss: 174.0082244873047\n",
      "Epoch 62, Batch 15842, Loss: 173.99745178222656\n",
      "Epoch 62, Batch 15843, Loss: 178.34165954589844\n",
      "Epoch 62, Batch 15844, Loss: 167.0517578125\n",
      "Epoch 62, Batch 15845, Loss: 176.6569061279297\n",
      "Epoch 62, Batch 15846, Loss: 148.817138671875\n",
      "Epoch 62, Batch 15847, Loss: 180.1835479736328\n",
      "Epoch 62, Batch 15848, Loss: 184.8336639404297\n",
      "Epoch 62, Batch 15849, Loss: 174.29104614257812\n",
      "Epoch 62, Batch 15850, Loss: 167.6865692138672\n",
      "Epoch 62, Batch 15851, Loss: 159.63340759277344\n",
      "Epoch 62, Batch 15852, Loss: 170.75137329101562\n",
      "Epoch 62, Batch 15853, Loss: 186.6583251953125\n",
      "Epoch 62, Batch 15854, Loss: 160.83233642578125\n",
      "Epoch 62, Batch 15855, Loss: 163.10667419433594\n",
      "Epoch 62, Batch 15856, Loss: 166.4832000732422\n",
      "Epoch 62, Batch 15857, Loss: 177.14596557617188\n",
      "Epoch 62, Batch 15858, Loss: 177.52908325195312\n",
      "Epoch 62, Batch 15859, Loss: 162.11549377441406\n",
      "Epoch 62, Batch 15860, Loss: 171.34877014160156\n",
      "Epoch 62, Batch 15861, Loss: 162.1200714111328\n",
      "Epoch 62, Batch 15862, Loss: 174.40818786621094\n",
      "Epoch 62, Batch 15863, Loss: 173.05130004882812\n",
      "Epoch 62, Batch 15864, Loss: 163.59683227539062\n",
      "Epoch 62, Batch 15865, Loss: 178.46539306640625\n",
      "Epoch 62, Batch 15866, Loss: 166.07901000976562\n",
      "Epoch 62, Batch 15867, Loss: 163.5418243408203\n",
      "Epoch 62, Batch 15868, Loss: 173.27133178710938\n",
      "Epoch 62, Batch 15869, Loss: 165.2387237548828\n",
      "Epoch 62, Batch 15870, Loss: 181.35182189941406\n",
      "Epoch 62, Batch 15871, Loss: 179.34735107421875\n",
      "Epoch 62, Batch 15872, Loss: 173.84071350097656\n",
      "Epoch 62, Batch 15873, Loss: 167.200439453125\n",
      "Epoch 62, Batch 15874, Loss: 156.93264770507812\n",
      "Epoch 62, Batch 15875, Loss: 188.06170654296875\n",
      "Epoch 62, Batch 15876, Loss: 171.1439208984375\n",
      "Epoch 62, Batch 15877, Loss: 172.52313232421875\n",
      "Epoch 62, Batch 15878, Loss: 178.02066040039062\n",
      "Epoch 62, Batch 15879, Loss: 167.83238220214844\n",
      "Epoch 62, Batch 15880, Loss: 163.91358947753906\n",
      "Epoch 62, Batch 15881, Loss: 176.4465789794922\n",
      "Epoch 62, Batch 15882, Loss: 176.3508758544922\n",
      "Epoch 62, Batch 15883, Loss: 166.45721435546875\n",
      "Epoch 62, Batch 15884, Loss: 161.9633026123047\n",
      "Epoch 62, Batch 15885, Loss: 171.79629516601562\n",
      "Epoch 62, Batch 15886, Loss: 173.2261199951172\n",
      "Epoch 62, Batch 15887, Loss: 170.336181640625\n",
      "Epoch 62, Batch 15888, Loss: 190.95724487304688\n",
      "Epoch 62, Batch 15889, Loss: 189.35226440429688\n",
      "Epoch 62, Batch 15890, Loss: 175.95736694335938\n",
      "Epoch 62, Batch 15891, Loss: 164.6141357421875\n",
      "Epoch 62, Batch 15892, Loss: 172.0257568359375\n",
      "Epoch 62, Batch 15893, Loss: 161.07888793945312\n",
      "Epoch 62, Batch 15894, Loss: 169.53790283203125\n",
      "Epoch 62, Batch 15895, Loss: 162.22230529785156\n",
      "Epoch 62, Batch 15896, Loss: 165.3438720703125\n",
      "Epoch 62, Batch 15897, Loss: 164.25711059570312\n",
      "Epoch 62, Batch 15898, Loss: 165.73635864257812\n",
      "Epoch 62, Batch 15899, Loss: 178.31869506835938\n",
      "Epoch 62, Batch 15900, Loss: 164.57470703125\n",
      "Epoch 62, Batch 15901, Loss: 166.98770141601562\n",
      "Epoch 62, Batch 15902, Loss: 193.2528076171875\n",
      "Epoch 62, Batch 15903, Loss: 194.38442993164062\n",
      "Epoch 62, Batch 15904, Loss: 164.4540557861328\n",
      "Epoch 62, Batch 15905, Loss: 176.11056518554688\n",
      "Epoch 62, Batch 15906, Loss: 169.60690307617188\n",
      "Epoch 62, Batch 15907, Loss: 166.99070739746094\n",
      "Epoch 62, Batch 15908, Loss: 179.22775268554688\n",
      "Epoch 62, Batch 15909, Loss: 190.64710998535156\n",
      "Epoch 62, Batch 15910, Loss: 184.47727966308594\n",
      "Epoch 62, Batch 15911, Loss: 165.0054168701172\n",
      "Epoch 62, Batch 15912, Loss: 173.613525390625\n",
      "Epoch 62, Batch 15913, Loss: 181.54615783691406\n",
      "Epoch 62, Batch 15914, Loss: 179.65591430664062\n",
      "Epoch 62, Batch 15915, Loss: 170.90216064453125\n",
      "Epoch 62, Batch 15916, Loss: 161.7995147705078\n",
      "Epoch 62, Batch 15917, Loss: 179.79110717773438\n",
      "Epoch 62, Batch 15918, Loss: 184.0878143310547\n",
      "Epoch 62, Batch 15919, Loss: 174.5304412841797\n",
      "Epoch 62, Batch 15920, Loss: 164.06105041503906\n",
      "Epoch 62, Batch 15921, Loss: 170.35581970214844\n",
      "Epoch 62, Batch 15922, Loss: 189.05526733398438\n",
      "Epoch 62, Batch 15923, Loss: 168.39678955078125\n",
      "Epoch 62, Batch 15924, Loss: 162.38307189941406\n",
      "Epoch 62, Batch 15925, Loss: 201.0787353515625\n",
      "Epoch 62, Batch 15926, Loss: 170.66433715820312\n",
      "Epoch 62, Batch 15927, Loss: 173.6851348876953\n",
      "Epoch 62, Batch 15928, Loss: 165.27093505859375\n",
      "Epoch 62, Batch 15929, Loss: 174.5011749267578\n",
      "Epoch 62, Batch 15930, Loss: 187.0368194580078\n",
      "Epoch 62, Batch 15931, Loss: 183.184814453125\n",
      "Epoch 62, Batch 15932, Loss: 183.96664428710938\n",
      "Epoch 62, Batch 15933, Loss: 167.73475646972656\n",
      "Epoch 62, Batch 15934, Loss: 151.56185913085938\n",
      "Epoch 62, Batch 15935, Loss: 180.9810333251953\n",
      "Epoch 62, Batch 15936, Loss: 188.34425354003906\n",
      "Epoch 62, Batch 15937, Loss: 180.9313507080078\n",
      "Epoch 62, Batch 15938, Loss: 176.15249633789062\n",
      "Epoch 62, Batch 15939, Loss: 169.2699737548828\n",
      "Epoch 62, Batch 15940, Loss: 185.9393310546875\n",
      "Epoch 62, Batch 15941, Loss: 180.3059844970703\n",
      "Epoch 62, Batch 15942, Loss: 180.59344482421875\n",
      "Epoch 62, Batch 15943, Loss: 166.0248565673828\n",
      "Epoch 62, Batch 15944, Loss: 173.65264892578125\n",
      "Epoch 62, Batch 15945, Loss: 177.73301696777344\n",
      "Epoch 62, Batch 15946, Loss: 182.13912963867188\n",
      "Epoch 62, Batch 15947, Loss: 175.62904357910156\n",
      "Epoch 62, Batch 15948, Loss: 163.3943634033203\n",
      "Epoch 62, Batch 15949, Loss: 170.62716674804688\n",
      "Epoch 62, Batch 15950, Loss: 176.5227508544922\n",
      "Epoch 62, Batch 15951, Loss: 176.58018493652344\n",
      "Epoch 62, Batch 15952, Loss: 172.2422332763672\n",
      "Epoch 62, Batch 15953, Loss: 147.07435607910156\n",
      "Epoch 62, Batch 15954, Loss: 160.23556518554688\n",
      "Epoch 62, Batch 15955, Loss: 183.42962646484375\n",
      "Epoch 62, Batch 15956, Loss: 180.78781127929688\n",
      "Epoch 62, Batch 15957, Loss: 189.27517700195312\n",
      "Epoch 62, Batch 15958, Loss: 175.0603485107422\n",
      "Epoch 62, Batch 15959, Loss: 169.77435302734375\n",
      "Epoch 62, Batch 15960, Loss: 176.7520294189453\n",
      "Epoch 62, Batch 15961, Loss: 177.31068420410156\n",
      "Epoch 62, Batch 15962, Loss: 169.8013458251953\n",
      "Epoch 62, Batch 15963, Loss: 169.08633422851562\n",
      "Epoch 62, Batch 15964, Loss: 194.00306701660156\n",
      "Epoch 62, Batch 15965, Loss: 168.98854064941406\n",
      "Epoch 62, Batch 15966, Loss: 174.78338623046875\n",
      "Epoch 62, Batch 15967, Loss: 172.8339385986328\n",
      "Epoch 62, Batch 15968, Loss: 183.02069091796875\n",
      "Epoch 62, Batch 15969, Loss: 170.50701904296875\n",
      "Epoch 62, Batch 15970, Loss: 177.99134826660156\n",
      "Epoch 62, Batch 15971, Loss: 167.72669982910156\n",
      "Epoch 62, Batch 15972, Loss: 161.1895751953125\n",
      "Epoch 62, Batch 15973, Loss: 173.865478515625\n",
      "Epoch 62, Batch 15974, Loss: 167.46194458007812\n",
      "Epoch 62, Batch 15975, Loss: 171.04026794433594\n",
      "Epoch 62, Batch 15976, Loss: 170.20852661132812\n",
      "Epoch 62, Batch 15977, Loss: 166.30308532714844\n",
      "Epoch 62, Batch 15978, Loss: 170.80145263671875\n",
      "Epoch 62, Batch 15979, Loss: 171.53695678710938\n",
      "Epoch 62, Batch 15980, Loss: 169.9436492919922\n",
      "Epoch 62, Batch 15981, Loss: 174.11766052246094\n",
      "Epoch 62, Batch 15982, Loss: 159.35447692871094\n",
      "Epoch 62, Batch 15983, Loss: 188.2598114013672\n",
      "Epoch 62, Batch 15984, Loss: 198.6170196533203\n",
      "Epoch 62, Batch 15985, Loss: 180.44351196289062\n",
      "Epoch 62, Batch 15986, Loss: 166.29867553710938\n",
      "Epoch 62, Batch 15987, Loss: 192.4456024169922\n",
      "Epoch 62, Batch 15988, Loss: 168.20909118652344\n",
      "Epoch 62, Batch 15989, Loss: 165.7572784423828\n",
      "Epoch 62, Batch 15990, Loss: 186.01373291015625\n",
      "Epoch 62, Batch 15991, Loss: 161.36781311035156\n",
      "Epoch 62, Batch 15992, Loss: 179.5753936767578\n",
      "Epoch 62, Batch 15993, Loss: 166.65606689453125\n",
      "Epoch 62, Batch 15994, Loss: 168.09585571289062\n",
      "Epoch 62, Batch 15995, Loss: 168.3341522216797\n",
      "Epoch 62, Batch 15996, Loss: 168.08473205566406\n",
      "Epoch 62, Batch 15997, Loss: 157.5319061279297\n",
      "Epoch 62, Batch 15998, Loss: 180.26451110839844\n",
      "Epoch 62, Batch 15999, Loss: 184.2820587158203\n",
      "Epoch 62, Batch 16000, Loss: 164.43304443359375\n",
      "Epoch 62, Batch 16001, Loss: 169.60687255859375\n",
      "Epoch 62, Batch 16002, Loss: 184.3313446044922\n",
      "Epoch 62, Batch 16003, Loss: 182.70521545410156\n",
      "Epoch 62, Batch 16004, Loss: 179.985107421875\n",
      "Epoch 62, Batch 16005, Loss: 170.5101776123047\n",
      "Epoch 62, Batch 16006, Loss: 169.47128295898438\n",
      "Epoch 62, Batch 16007, Loss: 189.7580108642578\n",
      "Epoch 62, Batch 16008, Loss: 183.32196044921875\n",
      "Epoch 62, Batch 16009, Loss: 177.4268798828125\n",
      "Epoch 62, Batch 16010, Loss: 152.4004364013672\n",
      "Epoch 62, Batch 16011, Loss: 168.98236083984375\n",
      "Epoch 62, Batch 16012, Loss: 189.47451782226562\n",
      "Epoch 62, Batch 16013, Loss: 178.76004028320312\n",
      "Epoch 62, Batch 16014, Loss: 175.82684326171875\n",
      "Epoch 62, Batch 16015, Loss: 205.43060302734375\n",
      "Epoch 62, Batch 16016, Loss: 187.7732696533203\n",
      "Epoch 62, Batch 16017, Loss: 167.11924743652344\n",
      "Epoch 62, Batch 16018, Loss: 179.48731994628906\n",
      "Epoch 62, Batch 16019, Loss: 194.59449768066406\n",
      "Epoch 62, Batch 16020, Loss: 164.84988403320312\n",
      "Epoch 62, Batch 16021, Loss: 174.0850830078125\n",
      "Epoch 62, Batch 16022, Loss: 166.83740234375\n",
      "Epoch 62, Batch 16023, Loss: 181.33602905273438\n",
      "Epoch 62, Batch 16024, Loss: 190.368896484375\n",
      "Epoch 62, Batch 16025, Loss: 178.6422576904297\n",
      "Epoch 62, Batch 16026, Loss: 178.6051483154297\n",
      "Epoch 62, Batch 16027, Loss: 178.5004119873047\n",
      "Epoch 62, Batch 16028, Loss: 169.39366149902344\n",
      "Epoch 62, Batch 16029, Loss: 187.3791961669922\n",
      "Epoch 62, Batch 16030, Loss: 162.7052764892578\n",
      "Epoch 62, Batch 16031, Loss: 170.4234619140625\n",
      "Epoch 62, Batch 16032, Loss: 151.46658325195312\n",
      "Epoch 62, Batch 16033, Loss: 183.69873046875\n",
      "Epoch 62, Batch 16034, Loss: 170.63748168945312\n",
      "Epoch 62, Batch 16035, Loss: 172.50743103027344\n",
      "Epoch 62, Batch 16036, Loss: 184.77066040039062\n",
      "Epoch 62, Batch 16037, Loss: 194.38177490234375\n",
      "Epoch 62, Batch 16038, Loss: 173.43359375\n",
      "Epoch 62, Batch 16039, Loss: 171.2854461669922\n",
      "Epoch 62, Batch 16040, Loss: 172.44729614257812\n",
      "Epoch 62, Batch 16041, Loss: 174.5522918701172\n",
      "Epoch 62, Batch 16042, Loss: 166.99440002441406\n",
      "Epoch 62, Batch 16043, Loss: 176.83567810058594\n",
      "Epoch 62, Batch 16044, Loss: 182.44357299804688\n",
      "Epoch 62, Batch 16045, Loss: 164.5902557373047\n",
      "Epoch 62, Batch 16046, Loss: 176.14393615722656\n",
      "Epoch 62, Batch 16047, Loss: 183.4328155517578\n",
      "Epoch 62, Batch 16048, Loss: 183.66058349609375\n",
      "Epoch 62, Batch 16049, Loss: 167.99945068359375\n",
      "Epoch 62, Batch 16050, Loss: 176.989013671875\n",
      "Epoch 62, Batch 16051, Loss: 168.35951232910156\n",
      "Epoch 62, Batch 16052, Loss: 169.357666015625\n",
      "Epoch 62, Batch 16053, Loss: 201.8404083251953\n",
      "Epoch 62, Batch 16054, Loss: 179.20091247558594\n",
      "Epoch 62, Batch 16055, Loss: 178.31761169433594\n",
      "Epoch 62, Batch 16056, Loss: 177.42950439453125\n",
      "Epoch 62, Batch 16057, Loss: 177.29869079589844\n",
      "Epoch 62, Batch 16058, Loss: 164.56138610839844\n",
      "Epoch 62, Batch 16059, Loss: 172.0892791748047\n",
      "Epoch 62, Batch 16060, Loss: 188.74270629882812\n",
      "Epoch 62, Batch 16061, Loss: 195.1706085205078\n",
      "Epoch 62, Batch 16062, Loss: 174.9209747314453\n",
      "Epoch 62, Batch 16063, Loss: 168.23326110839844\n",
      "Epoch 62, Batch 16064, Loss: 162.06204223632812\n",
      "Epoch 62, Batch 16065, Loss: 167.06016540527344\n",
      "Epoch 62, Batch 16066, Loss: 180.0560302734375\n",
      "Epoch 62, Batch 16067, Loss: 191.892333984375\n",
      "Epoch 62, Batch 16068, Loss: 179.22320556640625\n",
      "Epoch 62, Batch 16069, Loss: 154.25387573242188\n",
      "Epoch 62, Batch 16070, Loss: 181.86822509765625\n",
      "Epoch 62, Batch 16071, Loss: 176.2601318359375\n",
      "Epoch 62, Batch 16072, Loss: 172.94113159179688\n",
      "Epoch 62, Batch 16073, Loss: 162.59490966796875\n",
      "Epoch 62, Batch 16074, Loss: 176.53843688964844\n",
      "Epoch 62, Batch 16075, Loss: 177.0561065673828\n",
      "Epoch 62, Batch 16076, Loss: 160.89151000976562\n",
      "Epoch 62, Batch 16077, Loss: 183.1175537109375\n",
      "Epoch 62, Batch 16078, Loss: 179.34576416015625\n",
      "Epoch 62, Batch 16079, Loss: 167.2882080078125\n",
      "Epoch 62, Batch 16080, Loss: 178.24839782714844\n",
      "Epoch 62, Batch 16081, Loss: 170.548828125\n",
      "Epoch 62, Batch 16082, Loss: 195.22047424316406\n",
      "Epoch 62, Batch 16083, Loss: 178.06201171875\n",
      "Epoch 62, Batch 16084, Loss: 157.12490844726562\n",
      "Epoch 62, Batch 16085, Loss: 169.4844207763672\n",
      "Epoch 62, Batch 16086, Loss: 186.7706298828125\n",
      "Epoch 62, Batch 16087, Loss: 165.42974853515625\n",
      "Epoch 62, Batch 16088, Loss: 185.46913146972656\n",
      "Epoch 62, Batch 16089, Loss: 179.76065063476562\n",
      "Epoch 62, Batch 16090, Loss: 180.83981323242188\n",
      "Epoch 62, Batch 16091, Loss: 168.72796630859375\n",
      "Epoch 62, Batch 16092, Loss: 162.7117919921875\n",
      "Epoch 62, Batch 16093, Loss: 169.82586669921875\n",
      "Epoch 62, Batch 16094, Loss: 158.16168212890625\n",
      "Epoch 62, Batch 16095, Loss: 179.15306091308594\n",
      "Epoch 62, Batch 16096, Loss: 172.18995666503906\n",
      "Epoch 62, Batch 16097, Loss: 179.89794921875\n",
      "Epoch 62, Batch 16098, Loss: 190.25982666015625\n",
      "Epoch 62, Batch 16099, Loss: 195.61032104492188\n",
      "Epoch 62, Batch 16100, Loss: 192.06216430664062\n",
      "Epoch 62, Batch 16101, Loss: 170.43495178222656\n",
      "Epoch 62, Batch 16102, Loss: 181.97647094726562\n",
      "Epoch 62, Batch 16103, Loss: 181.05442810058594\n",
      "Epoch 62, Batch 16104, Loss: 185.8634796142578\n",
      "Epoch 62, Batch 16105, Loss: 162.52230834960938\n",
      "Epoch 62, Batch 16106, Loss: 165.2555389404297\n",
      "Epoch 62, Batch 16107, Loss: 161.7060546875\n",
      "Epoch 62, Batch 16108, Loss: 176.40951538085938\n",
      "Epoch 62, Batch 16109, Loss: 173.2823944091797\n",
      "Epoch 62, Batch 16110, Loss: 180.22158813476562\n",
      "Epoch 62, Batch 16111, Loss: 167.83058166503906\n",
      "Epoch 62, Batch 16112, Loss: 167.6856689453125\n",
      "Epoch 62, Batch 16113, Loss: 173.5668487548828\n",
      "Epoch 62, Batch 16114, Loss: 188.83432006835938\n",
      "Epoch 62, Batch 16115, Loss: 180.73594665527344\n",
      "Epoch 62, Batch 16116, Loss: 168.26100158691406\n",
      "Epoch 62, Batch 16117, Loss: 166.887939453125\n",
      "Epoch 62, Batch 16118, Loss: 182.51611328125\n",
      "Epoch 62, Batch 16119, Loss: 184.6729736328125\n",
      "Epoch 62, Batch 16120, Loss: 177.3607940673828\n",
      "Epoch 62, Batch 16121, Loss: 185.18312072753906\n",
      "Epoch 62, Batch 16122, Loss: 174.29554748535156\n",
      "Epoch 62, Batch 16123, Loss: 167.97601318359375\n",
      "Epoch 62, Batch 16124, Loss: 188.80496215820312\n",
      "Epoch 62, Batch 16125, Loss: 151.1475067138672\n",
      "Epoch 62, Batch 16126, Loss: 168.8587646484375\n",
      "Epoch 62, Batch 16127, Loss: 168.45506286621094\n",
      "Epoch 62, Batch 16128, Loss: 184.35272216796875\n",
      "Epoch 62, Batch 16129, Loss: 174.0363006591797\n",
      "Epoch 62, Batch 16130, Loss: 207.12393188476562\n",
      "Epoch 62, Batch 16131, Loss: 153.59243774414062\n",
      "Epoch 62, Batch 16132, Loss: 158.4762725830078\n",
      "Epoch 62, Batch 16133, Loss: 172.79750061035156\n",
      "Epoch 62, Batch 16134, Loss: 168.606689453125\n",
      "Epoch 62, Batch 16135, Loss: 166.9970703125\n",
      "Epoch 62, Batch 16136, Loss: 192.63345336914062\n",
      "Epoch 62, Batch 16137, Loss: 177.9716339111328\n",
      "Epoch 62, Batch 16138, Loss: 173.85052490234375\n",
      "Epoch 62, Batch 16139, Loss: 169.3474884033203\n",
      "Epoch 62, Batch 16140, Loss: 181.57005310058594\n",
      "Epoch 62, Batch 16141, Loss: 189.14041137695312\n",
      "Epoch 62, Batch 16142, Loss: 161.760986328125\n",
      "Epoch 62, Batch 16143, Loss: 177.0742950439453\n",
      "Epoch 62, Batch 16144, Loss: 180.52304077148438\n",
      "Epoch 62, Batch 16145, Loss: 175.7891082763672\n",
      "Epoch 62, Batch 16146, Loss: 167.42906188964844\n",
      "Epoch 62, Batch 16147, Loss: 175.45367431640625\n",
      "Epoch 62, Batch 16148, Loss: 166.05641174316406\n",
      "Epoch 62, Batch 16149, Loss: 189.0885772705078\n",
      "Epoch 62, Batch 16150, Loss: 180.83738708496094\n",
      "Epoch 62, Batch 16151, Loss: 179.38037109375\n",
      "Epoch 62, Batch 16152, Loss: 172.90499877929688\n",
      "Epoch 62, Batch 16153, Loss: 174.95118713378906\n",
      "Epoch 62, Batch 16154, Loss: 163.6201934814453\n",
      "Epoch 62, Batch 16155, Loss: 164.3589324951172\n",
      "Epoch 62, Batch 16156, Loss: 172.63558959960938\n",
      "Epoch 62, Batch 16157, Loss: 174.61265563964844\n",
      "Epoch 62, Batch 16158, Loss: 172.43618774414062\n",
      "Epoch 62, Batch 16159, Loss: 166.79408264160156\n",
      "Epoch 62, Batch 16160, Loss: 167.25498962402344\n",
      "Epoch 62, Batch 16161, Loss: 163.728271484375\n",
      "Epoch 62, Batch 16162, Loss: 178.89414978027344\n",
      "Epoch 62, Batch 16163, Loss: 182.97679138183594\n",
      "Epoch 62, Batch 16164, Loss: 181.9359588623047\n",
      "Epoch 62, Batch 16165, Loss: 172.35707092285156\n",
      "Epoch 62, Batch 16166, Loss: 164.96485900878906\n",
      "Epoch 62, Batch 16167, Loss: 170.56568908691406\n",
      "Epoch 62, Batch 16168, Loss: 159.4730987548828\n",
      "Epoch 62, Batch 16169, Loss: 160.6106414794922\n",
      "Epoch 62, Batch 16170, Loss: 198.19606018066406\n",
      "Epoch 62, Batch 16171, Loss: 157.9909210205078\n",
      "Epoch 62, Batch 16172, Loss: 162.7168731689453\n",
      "Epoch 62, Batch 16173, Loss: 166.266357421875\n",
      "Epoch 62, Batch 16174, Loss: 158.63026428222656\n",
      "Epoch 62, Batch 16175, Loss: 182.4006805419922\n",
      "Epoch 62, Batch 16176, Loss: 162.73190307617188\n",
      "Epoch 62, Batch 16177, Loss: 156.53550720214844\n",
      "Epoch 62, Batch 16178, Loss: 169.18161010742188\n",
      "Epoch 62, Batch 16179, Loss: 147.69680786132812\n",
      "Epoch 62, Batch 16180, Loss: 178.4695587158203\n",
      "Epoch 62, Batch 16181, Loss: 162.17510986328125\n",
      "Epoch 62, Batch 16182, Loss: 152.9594268798828\n",
      "Epoch 62, Batch 16183, Loss: 172.08538818359375\n",
      "Epoch 62, Batch 16184, Loss: 182.3916778564453\n",
      "Epoch 62, Batch 16185, Loss: 177.66122436523438\n",
      "Epoch 62, Batch 16186, Loss: 173.22897338867188\n",
      "Epoch 62, Batch 16187, Loss: 175.56906127929688\n",
      "Epoch 62, Batch 16188, Loss: 173.03306579589844\n",
      "Epoch 62, Batch 16189, Loss: 165.22296142578125\n",
      "Epoch 62, Batch 16190, Loss: 182.11514282226562\n",
      "Epoch 62, Batch 16191, Loss: 163.9949493408203\n",
      "Epoch 62, Batch 16192, Loss: 181.37713623046875\n",
      "Epoch 62, Batch 16193, Loss: 176.652099609375\n",
      "Epoch 62, Batch 16194, Loss: 175.2888641357422\n",
      "Epoch 62, Batch 16195, Loss: 180.19947814941406\n",
      "Epoch 62, Batch 16196, Loss: 166.00047302246094\n",
      "Epoch 62, Batch 16197, Loss: 183.94232177734375\n",
      "Epoch 62, Batch 16198, Loss: 164.0726318359375\n",
      "Epoch 62, Batch 16199, Loss: 170.119140625\n",
      "Epoch 62, Batch 16200, Loss: 190.87660217285156\n",
      "Epoch 62, Batch 16201, Loss: 165.4617919921875\n",
      "Epoch 62, Batch 16202, Loss: 169.5457000732422\n",
      "Epoch 62, Batch 16203, Loss: 164.5\n",
      "Epoch 62, Batch 16204, Loss: 172.7512664794922\n",
      "Epoch 62, Batch 16205, Loss: 166.94142150878906\n",
      "Epoch 62, Batch 16206, Loss: 172.52247619628906\n",
      "Epoch 62, Batch 16207, Loss: 157.4208526611328\n",
      "Epoch 62, Batch 16208, Loss: 186.2714385986328\n",
      "Epoch 62, Batch 16209, Loss: 170.07766723632812\n",
      "Epoch 62, Batch 16210, Loss: 179.00717163085938\n",
      "Epoch 62, Batch 16211, Loss: 173.0439453125\n",
      "Epoch 62, Batch 16212, Loss: 183.16183471679688\n",
      "Epoch 62, Batch 16213, Loss: 177.2726287841797\n",
      "Epoch 62, Batch 16214, Loss: 172.27684020996094\n",
      "Epoch 62, Batch 16215, Loss: 178.25869750976562\n",
      "Epoch 62, Batch 16216, Loss: 188.57223510742188\n",
      "Epoch 62, Batch 16217, Loss: 174.90147399902344\n",
      "Epoch 62, Batch 16218, Loss: 178.26109313964844\n",
      "Epoch 62, Batch 16219, Loss: 175.15127563476562\n",
      "Epoch 62, Batch 16220, Loss: 184.40350341796875\n",
      "Epoch 62, Batch 16221, Loss: 166.67254638671875\n",
      "Epoch 62, Batch 16222, Loss: 182.94581604003906\n",
      "Epoch 62, Batch 16223, Loss: 174.6024932861328\n",
      "Epoch 62, Batch 16224, Loss: 201.16824340820312\n",
      "Epoch 62, Batch 16225, Loss: 188.26698303222656\n",
      "Epoch 62, Batch 16226, Loss: 174.0392608642578\n",
      "Epoch 62, Batch 16227, Loss: 178.58055114746094\n",
      "Epoch 62, Batch 16228, Loss: 166.78585815429688\n",
      "Epoch 62, Batch 16229, Loss: 180.94309997558594\n",
      "Epoch 62, Batch 16230, Loss: 188.0030059814453\n",
      "Epoch 62, Batch 16231, Loss: 180.17726135253906\n",
      "Epoch 62, Batch 16232, Loss: 174.33981323242188\n",
      "Epoch 62, Batch 16233, Loss: 193.16310119628906\n",
      "Epoch 62, Batch 16234, Loss: 185.95277404785156\n",
      "Epoch 62, Batch 16235, Loss: 157.9622802734375\n",
      "Epoch 62, Batch 16236, Loss: 177.91983032226562\n",
      "Epoch 62, Batch 16237, Loss: 183.9000701904297\n",
      "Epoch 62, Batch 16238, Loss: 166.75730895996094\n",
      "Epoch 62, Batch 16239, Loss: 175.16062927246094\n",
      "Epoch 62, Batch 16240, Loss: 174.3352813720703\n",
      "Epoch 62, Batch 16241, Loss: 175.77346801757812\n",
      "Epoch 62, Batch 16242, Loss: 178.46044921875\n",
      "Epoch 62, Batch 16243, Loss: 187.47154235839844\n",
      "Epoch 62, Batch 16244, Loss: 171.86097717285156\n",
      "Epoch 62, Batch 16245, Loss: 194.62100219726562\n",
      "Epoch 62, Batch 16246, Loss: 162.89956665039062\n",
      "Epoch 62, Batch 16247, Loss: 186.3627166748047\n",
      "Epoch 62, Batch 16248, Loss: 169.45791625976562\n",
      "Epoch 62, Batch 16249, Loss: 174.8727264404297\n",
      "Epoch 62, Batch 16250, Loss: 184.37001037597656\n",
      "Epoch 62, Batch 16251, Loss: 169.39112854003906\n",
      "Epoch 62, Batch 16252, Loss: 162.78482055664062\n",
      "Epoch 62, Batch 16253, Loss: 187.5609588623047\n",
      "Epoch 62, Batch 16254, Loss: 185.85089111328125\n",
      "Epoch 62, Batch 16255, Loss: 187.6236572265625\n",
      "Epoch 62, Batch 16256, Loss: 190.6584014892578\n",
      "Epoch 62, Batch 16257, Loss: 180.89805603027344\n",
      "Epoch 62, Batch 16258, Loss: 197.0291290283203\n",
      "Epoch 62, Batch 16259, Loss: 166.71827697753906\n",
      "Epoch 62, Batch 16260, Loss: 167.9484100341797\n",
      "Epoch 62, Batch 16261, Loss: 178.71609497070312\n",
      "Epoch 62, Batch 16262, Loss: 173.58729553222656\n",
      "Epoch 62, Batch 16263, Loss: 182.6136474609375\n",
      "Epoch 62, Batch 16264, Loss: 181.6784210205078\n",
      "Epoch 62, Batch 16265, Loss: 172.67495727539062\n",
      "Epoch 62, Batch 16266, Loss: 178.60757446289062\n",
      "Epoch 62, Batch 16267, Loss: 177.0421142578125\n",
      "Epoch 62, Batch 16268, Loss: 168.34486389160156\n",
      "Epoch 62, Batch 16269, Loss: 167.74172973632812\n",
      "Epoch 62, Batch 16270, Loss: 172.6016082763672\n",
      "Epoch 62, Batch 16271, Loss: 177.73312377929688\n",
      "Epoch 62, Batch 16272, Loss: 184.50634765625\n",
      "Epoch 62, Batch 16273, Loss: 171.61558532714844\n",
      "Epoch 62, Batch 16274, Loss: 160.5928192138672\n",
      "Epoch 62, Batch 16275, Loss: 173.61984252929688\n",
      "Epoch 62, Batch 16276, Loss: 172.8715057373047\n",
      "Epoch 62, Batch 16277, Loss: 158.62608337402344\n",
      "Epoch 62, Batch 16278, Loss: 178.23141479492188\n",
      "Epoch 62, Batch 16279, Loss: 181.77960205078125\n",
      "Epoch 62, Batch 16280, Loss: 180.37757873535156\n",
      "Epoch 62, Batch 16281, Loss: 175.85853576660156\n",
      "Epoch 62, Batch 16282, Loss: 181.3347930908203\n",
      "Epoch 62, Batch 16283, Loss: 183.5689697265625\n",
      "Epoch 62, Batch 16284, Loss: 182.54295349121094\n",
      "Epoch 62, Batch 16285, Loss: 181.0347900390625\n",
      "Epoch 62, Batch 16286, Loss: 166.5548095703125\n",
      "Epoch 62, Batch 16287, Loss: 183.74252319335938\n",
      "Epoch 62, Batch 16288, Loss: 170.80770874023438\n",
      "Epoch 62, Batch 16289, Loss: 174.4987335205078\n",
      "Epoch 62, Batch 16290, Loss: 196.3885955810547\n",
      "Epoch 62, Batch 16291, Loss: 173.48504638671875\n",
      "Epoch 62, Batch 16292, Loss: 174.5470428466797\n",
      "Epoch 62, Batch 16293, Loss: 165.50563049316406\n",
      "Epoch 62, Batch 16294, Loss: 177.55751037597656\n",
      "Epoch 62, Batch 16295, Loss: 169.96896362304688\n",
      "Epoch 62, Batch 16296, Loss: 171.0190887451172\n",
      "Epoch 62, Batch 16297, Loss: 187.17076110839844\n",
      "Epoch 62, Batch 16298, Loss: 165.6825714111328\n",
      "Epoch 62, Batch 16299, Loss: 194.75595092773438\n",
      "Epoch 62, Batch 16300, Loss: 174.92724609375\n",
      "Epoch 62, Batch 16301, Loss: 169.47926330566406\n",
      "Epoch 62, Batch 16302, Loss: 180.52056884765625\n",
      "Epoch 62, Batch 16303, Loss: 177.83790588378906\n",
      "Epoch 62, Batch 16304, Loss: 168.26373291015625\n",
      "Epoch 62, Batch 16305, Loss: 169.98104858398438\n",
      "Epoch 62, Batch 16306, Loss: 175.92324829101562\n",
      "Epoch 62, Batch 16307, Loss: 173.3981170654297\n",
      "Epoch 62, Batch 16308, Loss: 188.8603515625\n",
      "Epoch 62, Batch 16309, Loss: 162.50851440429688\n",
      "Epoch 62, Batch 16310, Loss: 178.7831268310547\n",
      "Epoch 62, Batch 16311, Loss: 169.8248291015625\n",
      "Epoch 62, Batch 16312, Loss: 166.7786102294922\n",
      "Epoch 62, Batch 16313, Loss: 197.62841796875\n",
      "Epoch 62, Batch 16314, Loss: 175.1356201171875\n",
      "Epoch 62, Batch 16315, Loss: 176.35720825195312\n",
      "Epoch 62, Batch 16316, Loss: 188.17950439453125\n",
      "Epoch 62, Batch 16317, Loss: 160.58493041992188\n",
      "Epoch 62, Batch 16318, Loss: 193.74740600585938\n",
      "Epoch 62, Batch 16319, Loss: 180.1106719970703\n",
      "Epoch 62, Batch 16320, Loss: 154.90895080566406\n",
      "Epoch 62, Batch 16321, Loss: 177.83273315429688\n",
      "Epoch 62, Batch 16322, Loss: 185.07611083984375\n",
      "Epoch 62, Batch 16323, Loss: 155.53091430664062\n",
      "Epoch 62, Batch 16324, Loss: 172.15281677246094\n",
      "Epoch 62, Batch 16325, Loss: 159.78961181640625\n",
      "Epoch 62, Batch 16326, Loss: 188.75897216796875\n",
      "Epoch 62, Batch 16327, Loss: 170.29664611816406\n",
      "Epoch 62, Batch 16328, Loss: 168.83303833007812\n",
      "Epoch 62, Batch 16329, Loss: 177.08352661132812\n",
      "Epoch 62, Batch 16330, Loss: 163.61834716796875\n",
      "Epoch 62, Batch 16331, Loss: 169.35476684570312\n",
      "Epoch 62, Batch 16332, Loss: 165.57925415039062\n",
      "Epoch 62, Batch 16333, Loss: 169.9353790283203\n",
      "Epoch 62, Batch 16334, Loss: 180.15170288085938\n",
      "Epoch 62, Batch 16335, Loss: 170.25613403320312\n",
      "Epoch 62, Batch 16336, Loss: 177.04945373535156\n",
      "Epoch 62, Batch 16337, Loss: 167.42225646972656\n",
      "Epoch 62, Batch 16338, Loss: 169.5940704345703\n",
      "Epoch 62, Batch 16339, Loss: 178.28091430664062\n",
      "Epoch 62, Batch 16340, Loss: 166.5084686279297\n",
      "Epoch 62, Batch 16341, Loss: 162.32635498046875\n",
      "Epoch 62, Batch 16342, Loss: 167.065673828125\n",
      "Epoch 62, Batch 16343, Loss: 175.6149444580078\n",
      "Epoch 62, Batch 16344, Loss: 179.7514190673828\n",
      "Epoch 62, Batch 16345, Loss: 158.93521118164062\n",
      "Epoch 62, Batch 16346, Loss: 152.1171112060547\n",
      "Epoch 62, Batch 16347, Loss: 183.02198791503906\n",
      "Epoch 62, Batch 16348, Loss: 181.7194366455078\n",
      "Epoch 62, Batch 16349, Loss: 159.3375701904297\n",
      "Epoch 62, Batch 16350, Loss: 173.88845825195312\n",
      "Epoch 62, Batch 16351, Loss: 166.3720245361328\n",
      "Epoch 62, Batch 16352, Loss: 186.29989624023438\n",
      "Epoch 62, Batch 16353, Loss: 182.0414581298828\n",
      "Epoch 62, Batch 16354, Loss: 166.24940490722656\n",
      "Epoch 62, Batch 16355, Loss: 184.83432006835938\n",
      "Epoch 62, Batch 16356, Loss: 181.63929748535156\n",
      "Epoch 62, Batch 16357, Loss: 168.85516357421875\n",
      "Epoch 62, Batch 16358, Loss: 168.26614379882812\n",
      "Epoch 62, Batch 16359, Loss: 174.3030548095703\n",
      "Epoch 62, Batch 16360, Loss: 170.83303833007812\n",
      "Epoch 62, Batch 16361, Loss: 193.36521911621094\n",
      "Epoch 62, Batch 16362, Loss: 182.20108032226562\n",
      "Epoch 62, Batch 16363, Loss: 173.14178466796875\n",
      "Epoch 62, Batch 16364, Loss: 162.9000701904297\n",
      "Epoch 62, Batch 16365, Loss: 171.82334899902344\n",
      "Epoch 62, Batch 16366, Loss: 169.85317993164062\n",
      "Epoch 62, Batch 16367, Loss: 163.0775909423828\n",
      "Epoch 62, Batch 16368, Loss: 169.5642852783203\n",
      "Epoch 62, Batch 16369, Loss: 176.2298126220703\n",
      "Epoch 62, Batch 16370, Loss: 170.23191833496094\n",
      "Epoch 62, Batch 16371, Loss: 164.95263671875\n",
      "Epoch 62, Batch 16372, Loss: 184.9941864013672\n",
      "Epoch 62, Batch 16373, Loss: 160.81866455078125\n",
      "Epoch 62, Batch 16374, Loss: 167.88870239257812\n",
      "Epoch 62, Batch 16375, Loss: 174.90206909179688\n",
      "Epoch 62, Batch 16376, Loss: 184.228759765625\n",
      "Epoch 62, Batch 16377, Loss: 170.970458984375\n",
      "Epoch 62, Batch 16378, Loss: 192.36056518554688\n",
      "Epoch 62, Batch 16379, Loss: 187.0955352783203\n",
      "Epoch 62, Batch 16380, Loss: 167.98419189453125\n",
      "Epoch 62, Batch 16381, Loss: 176.70721435546875\n",
      "Epoch 62, Batch 16382, Loss: 168.97433471679688\n",
      "Epoch 62, Batch 16383, Loss: 165.0728759765625\n",
      "Epoch 62, Batch 16384, Loss: 162.38595581054688\n",
      "Epoch 62, Batch 16385, Loss: 190.7898406982422\n",
      "Epoch 62, Batch 16386, Loss: 185.62496948242188\n",
      "Epoch 62, Batch 16387, Loss: 183.8425750732422\n",
      "Epoch 62, Batch 16388, Loss: 167.7509002685547\n",
      "Epoch 62, Batch 16389, Loss: 195.2779541015625\n",
      "Epoch 62, Batch 16390, Loss: 173.9530487060547\n",
      "Epoch 62, Batch 16391, Loss: 171.85357666015625\n",
      "Epoch 62, Batch 16392, Loss: 177.741943359375\n",
      "Epoch 62, Batch 16393, Loss: 168.79086303710938\n",
      "Epoch 62, Batch 16394, Loss: 159.9857940673828\n",
      "Epoch 62, Batch 16395, Loss: 176.17124938964844\n",
      "Epoch 62, Batch 16396, Loss: 171.46609497070312\n",
      "Epoch 62, Batch 16397, Loss: 165.63343811035156\n",
      "Epoch 62, Batch 16398, Loss: 173.49636840820312\n",
      "Epoch 62, Batch 16399, Loss: 164.26126098632812\n",
      "Epoch 62, Batch 16400, Loss: 180.2437286376953\n",
      "Epoch 62, Batch 16401, Loss: 155.03334045410156\n",
      "Epoch 62, Batch 16402, Loss: 173.9762725830078\n",
      "Epoch 62, Batch 16403, Loss: 152.11888122558594\n",
      "Epoch 62, Batch 16404, Loss: 179.93072509765625\n",
      "Epoch 62, Batch 16405, Loss: 154.97503662109375\n",
      "Epoch 62, Batch 16406, Loss: 165.47213745117188\n",
      "Epoch 62, Batch 16407, Loss: 166.0532684326172\n",
      "Epoch 62, Batch 16408, Loss: 174.43272399902344\n",
      "Epoch 62, Batch 16409, Loss: 181.6400604248047\n",
      "Epoch 62, Batch 16410, Loss: 181.69198608398438\n",
      "Epoch 62, Batch 16411, Loss: 172.64743041992188\n",
      "Epoch 62, Batch 16412, Loss: 174.59068298339844\n",
      "Epoch 62, Batch 16413, Loss: 173.6441192626953\n",
      "Epoch 62, Batch 16414, Loss: 165.12843322753906\n",
      "Epoch 62, Batch 16415, Loss: 178.30320739746094\n",
      "Epoch 62, Batch 16416, Loss: 165.84202575683594\n",
      "Epoch 62, Batch 16417, Loss: 158.34640502929688\n",
      "Epoch 62, Batch 16418, Loss: 162.9619140625\n",
      "Epoch 62, Batch 16419, Loss: 187.88682556152344\n",
      "Epoch 62, Batch 16420, Loss: 167.3370819091797\n",
      "Epoch 62, Batch 16421, Loss: 170.82943725585938\n",
      "Epoch 62, Batch 16422, Loss: 155.93048095703125\n",
      "Epoch 62, Batch 16423, Loss: 183.1970672607422\n",
      "Epoch 62, Batch 16424, Loss: 176.3048553466797\n",
      "Epoch 62, Batch 16425, Loss: 189.3449249267578\n",
      "Epoch 62, Batch 16426, Loss: 174.23989868164062\n",
      "Epoch 62, Batch 16427, Loss: 173.69166564941406\n",
      "Epoch 62, Batch 16428, Loss: 166.9757080078125\n",
      "Epoch 62, Batch 16429, Loss: 169.8519744873047\n",
      "Epoch 62, Batch 16430, Loss: 196.62698364257812\n",
      "Epoch 62, Batch 16431, Loss: 167.98330688476562\n",
      "Epoch 62, Batch 16432, Loss: 179.1538543701172\n",
      "Epoch 62, Batch 16433, Loss: 164.6892547607422\n",
      "Epoch 62, Batch 16434, Loss: 172.45498657226562\n",
      "Epoch 62, Batch 16435, Loss: 175.99757385253906\n",
      "Epoch 62, Batch 16436, Loss: 164.6882781982422\n",
      "Epoch 62, Batch 16437, Loss: 157.33596801757812\n",
      "Epoch 62, Batch 16438, Loss: 177.4076385498047\n",
      "Epoch 62, Batch 16439, Loss: 182.9254608154297\n",
      "Epoch 62, Batch 16440, Loss: 153.4215850830078\n",
      "Epoch 62, Batch 16441, Loss: 177.1812744140625\n",
      "Epoch 62, Batch 16442, Loss: 168.82786560058594\n",
      "Epoch 62, Batch 16443, Loss: 185.97415161132812\n",
      "Epoch 62, Batch 16444, Loss: 168.4871368408203\n",
      "Epoch 62, Batch 16445, Loss: 167.72869873046875\n",
      "Epoch 62, Batch 16446, Loss: 177.00616455078125\n",
      "Epoch 62, Batch 16447, Loss: 160.1411895751953\n",
      "Epoch 62, Batch 16448, Loss: 175.50656127929688\n",
      "Epoch 62, Batch 16449, Loss: 179.22552490234375\n",
      "Epoch 62, Batch 16450, Loss: 177.42518615722656\n",
      "Epoch 62, Batch 16451, Loss: 163.15489196777344\n",
      "Epoch 62, Batch 16452, Loss: 179.97494506835938\n",
      "Epoch 62, Batch 16453, Loss: 148.16534423828125\n",
      "Epoch 62, Batch 16454, Loss: 173.56597900390625\n",
      "Epoch 62, Batch 16455, Loss: 168.3417510986328\n",
      "Epoch 62, Batch 16456, Loss: 175.58384704589844\n",
      "Epoch 62, Batch 16457, Loss: 159.44505310058594\n",
      "Epoch 62, Batch 16458, Loss: 190.55223083496094\n",
      "Epoch 62, Batch 16459, Loss: 185.33460998535156\n",
      "Epoch 62, Batch 16460, Loss: 169.42947387695312\n",
      "Epoch 62, Batch 16461, Loss: 170.795654296875\n",
      "Epoch 62, Batch 16462, Loss: 185.425048828125\n",
      "Epoch 62, Batch 16463, Loss: 176.702880859375\n",
      "Epoch 62, Batch 16464, Loss: 171.89254760742188\n",
      "Epoch 62, Batch 16465, Loss: 186.06256103515625\n",
      "Epoch 62, Batch 16466, Loss: 164.06431579589844\n",
      "Epoch 62, Batch 16467, Loss: 175.50877380371094\n",
      "Epoch 62, Batch 16468, Loss: 164.98216247558594\n",
      "Epoch 62, Batch 16469, Loss: 180.60720825195312\n",
      "Epoch 62, Batch 16470, Loss: 169.2400360107422\n",
      "Epoch 62, Batch 16471, Loss: 173.8977813720703\n",
      "Epoch 62, Batch 16472, Loss: 165.31552124023438\n",
      "Epoch 62, Batch 16473, Loss: 204.11973571777344\n",
      "Epoch 62, Batch 16474, Loss: 191.90798950195312\n",
      "Epoch 62, Batch 16475, Loss: 163.5124053955078\n",
      "Epoch 62, Batch 16476, Loss: 164.12869262695312\n",
      "Epoch 62, Batch 16477, Loss: 181.97254943847656\n",
      "Epoch 62, Batch 16478, Loss: 193.1909942626953\n",
      "Epoch 62, Batch 16479, Loss: 183.7027130126953\n",
      "Epoch 62, Batch 16480, Loss: 189.4700164794922\n",
      "Epoch 62, Batch 16481, Loss: 188.970703125\n",
      "Epoch 62, Batch 16482, Loss: 171.75489807128906\n",
      "Epoch 62, Batch 16483, Loss: 190.1253204345703\n",
      "Epoch 62, Batch 16484, Loss: 178.07049560546875\n",
      "Epoch 62, Batch 16485, Loss: 179.9669952392578\n",
      "Epoch 62, Batch 16486, Loss: 176.1675567626953\n",
      "Epoch 62, Batch 16487, Loss: 168.94361877441406\n",
      "Epoch 62, Batch 16488, Loss: 158.7384490966797\n",
      "Epoch 62, Batch 16489, Loss: 181.58636474609375\n",
      "Epoch 62, Batch 16490, Loss: 158.08348083496094\n",
      "Epoch 62, Batch 16491, Loss: 169.1814422607422\n",
      "Epoch 62, Batch 16492, Loss: 156.7615509033203\n",
      "Epoch 62, Batch 16493, Loss: 188.4085235595703\n",
      "Epoch 62, Batch 16494, Loss: 158.9232635498047\n",
      "Epoch 62, Batch 16495, Loss: 169.06314086914062\n",
      "Epoch 62, Batch 16496, Loss: 173.49488830566406\n",
      "Epoch 62, Batch 16497, Loss: 170.75411987304688\n",
      "Epoch 62, Batch 16498, Loss: 166.46128845214844\n",
      "Epoch 62, Batch 16499, Loss: 166.40646362304688\n",
      "Epoch 62, Batch 16500, Loss: 168.51950073242188\n",
      "Epoch 62, Batch 16501, Loss: 174.3187255859375\n",
      "Epoch 62, Batch 16502, Loss: 186.39024353027344\n",
      "Epoch 62, Batch 16503, Loss: 170.8248748779297\n",
      "Epoch 62, Batch 16504, Loss: 185.00376892089844\n",
      "Epoch 62, Batch 16505, Loss: 175.6688232421875\n",
      "Epoch 62, Batch 16506, Loss: 165.4649658203125\n",
      "Epoch 62, Batch 16507, Loss: 167.95216369628906\n",
      "Epoch 62, Batch 16508, Loss: 168.93190002441406\n",
      "Epoch 62, Batch 16509, Loss: 172.95147705078125\n",
      "Epoch 62, Batch 16510, Loss: 168.62564086914062\n",
      "Epoch 62, Batch 16511, Loss: 179.4227294921875\n",
      "Epoch 62, Batch 16512, Loss: 158.25405883789062\n",
      "Epoch 62, Batch 16513, Loss: 164.3529052734375\n",
      "Epoch 62, Batch 16514, Loss: 165.82980346679688\n",
      "Epoch 62, Batch 16515, Loss: 164.73980712890625\n",
      "Epoch 62, Batch 16516, Loss: 172.67971801757812\n",
      "Epoch 62, Batch 16517, Loss: 178.0351104736328\n",
      "Epoch 62, Batch 16518, Loss: 185.72413635253906\n",
      "Epoch 62, Batch 16519, Loss: 181.9344024658203\n",
      "Epoch 62, Batch 16520, Loss: 164.74041748046875\n",
      "Epoch 62, Batch 16521, Loss: 163.45001220703125\n",
      "Epoch 62, Batch 16522, Loss: 172.07388305664062\n",
      "Epoch 62, Batch 16523, Loss: 190.9512939453125\n",
      "Epoch 62, Batch 16524, Loss: 180.84213256835938\n",
      "Epoch 62, Batch 16525, Loss: 181.81951904296875\n",
      "Epoch 62, Batch 16526, Loss: 165.4151611328125\n",
      "Epoch 62, Batch 16527, Loss: 167.35009765625\n",
      "Epoch 62, Batch 16528, Loss: 186.75222778320312\n",
      "Epoch 62, Batch 16529, Loss: 177.90464782714844\n",
      "Epoch 62, Batch 16530, Loss: 175.75843811035156\n",
      "Epoch 62, Batch 16531, Loss: 185.77325439453125\n",
      "Epoch 62, Batch 16532, Loss: 170.010009765625\n",
      "Epoch 62, Batch 16533, Loss: 186.5323028564453\n",
      "Epoch 62, Batch 16534, Loss: 169.14608764648438\n",
      "Epoch 62, Batch 16535, Loss: 162.21109008789062\n",
      "Epoch 62, Batch 16536, Loss: 178.9774932861328\n",
      "Epoch 62, Batch 16537, Loss: 174.61985778808594\n",
      "Epoch 62, Batch 16538, Loss: 178.9463653564453\n",
      "Epoch 62, Batch 16539, Loss: 182.92727661132812\n",
      "Epoch 62, Batch 16540, Loss: 162.99517822265625\n",
      "Epoch 62, Batch 16541, Loss: 192.40570068359375\n",
      "Epoch 62, Batch 16542, Loss: 172.92884826660156\n",
      "Epoch 62, Batch 16543, Loss: 175.00204467773438\n",
      "Epoch 62, Batch 16544, Loss: 187.1881561279297\n",
      "Epoch 62, Batch 16545, Loss: 173.3562469482422\n",
      "Epoch 62, Batch 16546, Loss: 180.08726501464844\n",
      "Epoch 62, Batch 16547, Loss: 180.68353271484375\n",
      "Epoch 62, Batch 16548, Loss: 189.6561737060547\n",
      "Epoch 62, Batch 16549, Loss: 170.53094482421875\n",
      "Epoch 62, Batch 16550, Loss: 161.61407470703125\n",
      "Epoch 62, Batch 16551, Loss: 176.1693115234375\n",
      "Epoch 62, Batch 16552, Loss: 165.20098876953125\n",
      "Epoch 62, Batch 16553, Loss: 178.6468048095703\n",
      "Epoch 62, Batch 16554, Loss: 184.31179809570312\n",
      "Epoch 62, Batch 16555, Loss: 183.81788635253906\n",
      "Epoch 62, Batch 16556, Loss: 171.81671142578125\n",
      "Epoch 62, Batch 16557, Loss: 179.25111389160156\n",
      "Epoch 62, Batch 16558, Loss: 171.93531799316406\n",
      "Epoch 62, Batch 16559, Loss: 160.36688232421875\n",
      "Epoch 62, Batch 16560, Loss: 177.3405303955078\n",
      "Epoch 62, Batch 16561, Loss: 190.80908203125\n",
      "Epoch 62, Batch 16562, Loss: 169.51287841796875\n",
      "Epoch 62, Batch 16563, Loss: 154.14276123046875\n",
      "Epoch 62, Batch 16564, Loss: 163.54347229003906\n",
      "Epoch 62, Batch 16565, Loss: 188.1276092529297\n",
      "Epoch 62, Batch 16566, Loss: 165.85311889648438\n",
      "Epoch 62, Batch 16567, Loss: 171.93150329589844\n",
      "Epoch 62, Batch 16568, Loss: 179.77525329589844\n",
      "Epoch 62, Batch 16569, Loss: 184.204833984375\n",
      "Epoch 62, Batch 16570, Loss: 173.42706298828125\n",
      "Epoch 62, Batch 16571, Loss: 170.46400451660156\n",
      "Epoch 62, Batch 16572, Loss: 178.21676635742188\n",
      "Epoch 62, Batch 16573, Loss: 160.04302978515625\n",
      "Epoch 62, Batch 16574, Loss: 179.10498046875\n",
      "Epoch 62, Batch 16575, Loss: 158.94032287597656\n",
      "Epoch 62, Batch 16576, Loss: 173.814697265625\n",
      "Epoch 62, Batch 16577, Loss: 174.76290893554688\n",
      "Epoch 62, Batch 16578, Loss: 159.72213745117188\n",
      "Epoch 62, Batch 16579, Loss: 164.027099609375\n",
      "Epoch 62, Batch 16580, Loss: 168.1539306640625\n",
      "Epoch 62, Batch 16581, Loss: 169.12493896484375\n",
      "Epoch 62, Batch 16582, Loss: 176.49256896972656\n",
      "Epoch 62, Batch 16583, Loss: 148.57891845703125\n",
      "Epoch 62, Batch 16584, Loss: 195.371826171875\n",
      "Epoch 62, Batch 16585, Loss: 189.18460083007812\n",
      "Epoch 62, Batch 16586, Loss: 172.12356567382812\n",
      "Epoch 62, Batch 16587, Loss: 167.5178985595703\n",
      "Epoch 62, Batch 16588, Loss: 172.2325439453125\n",
      "Epoch 62, Batch 16589, Loss: 164.58172607421875\n",
      "Epoch 62, Batch 16590, Loss: 168.68431091308594\n",
      "Epoch 62, Batch 16591, Loss: 167.27517700195312\n",
      "Epoch 62, Batch 16592, Loss: 189.0945281982422\n",
      "Epoch 62, Batch 16593, Loss: 177.5039520263672\n",
      "Epoch 62, Batch 16594, Loss: 164.064453125\n",
      "Epoch 62, Batch 16595, Loss: 179.9426727294922\n",
      "Epoch 62, Batch 16596, Loss: 178.13218688964844\n",
      "Epoch 62, Batch 16597, Loss: 167.80184936523438\n",
      "Epoch 62, Batch 16598, Loss: 169.68914794921875\n",
      "Epoch 62, Batch 16599, Loss: 170.9040985107422\n",
      "Epoch 62, Batch 16600, Loss: 186.46620178222656\n",
      "Epoch 62, Batch 16601, Loss: 174.6309814453125\n",
      "Epoch 62, Batch 16602, Loss: 192.7796173095703\n",
      "Epoch 62, Batch 16603, Loss: 184.60218811035156\n",
      "Epoch 62, Batch 16604, Loss: 165.3676300048828\n",
      "Epoch 62, Batch 16605, Loss: 168.3527069091797\n",
      "Epoch 62, Batch 16606, Loss: 191.4647674560547\n",
      "Epoch 62, Batch 16607, Loss: 162.6448516845703\n",
      "Epoch 62, Batch 16608, Loss: 195.0861053466797\n",
      "Epoch 62, Batch 16609, Loss: 188.8606719970703\n",
      "Epoch 62, Batch 16610, Loss: 181.9129638671875\n",
      "Epoch 62, Batch 16611, Loss: 175.1991424560547\n",
      "Epoch 62, Batch 16612, Loss: 171.12124633789062\n",
      "Epoch 62, Batch 16613, Loss: 160.76568603515625\n",
      "Epoch 62, Batch 16614, Loss: 184.80911254882812\n",
      "Epoch 62, Batch 16615, Loss: 179.7686309814453\n",
      "Epoch 62, Batch 16616, Loss: 186.8433837890625\n",
      "Epoch 62, Batch 16617, Loss: 166.20407104492188\n",
      "Epoch 62, Batch 16618, Loss: 171.32591247558594\n",
      "Epoch 62, Batch 16619, Loss: 183.5755615234375\n",
      "Epoch 62, Batch 16620, Loss: 168.2190704345703\n",
      "Epoch 62, Batch 16621, Loss: 183.63058471679688\n",
      "Epoch 62, Batch 16622, Loss: 164.98997497558594\n",
      "Epoch 62, Batch 16623, Loss: 174.68124389648438\n",
      "Epoch 62, Batch 16624, Loss: 191.06253051757812\n",
      "Epoch 62, Batch 16625, Loss: 164.93316650390625\n",
      "Epoch 62, Batch 16626, Loss: 182.4320068359375\n",
      "Epoch 62, Batch 16627, Loss: 159.53131103515625\n",
      "Epoch 62, Batch 16628, Loss: 175.0111541748047\n",
      "Epoch 62, Batch 16629, Loss: 175.85076904296875\n",
      "Epoch 62, Batch 16630, Loss: 169.58151245117188\n",
      "Epoch 62, Batch 16631, Loss: 174.5952606201172\n",
      "Epoch 62, Batch 16632, Loss: 152.26235961914062\n",
      "Epoch 62, Batch 16633, Loss: 178.0565643310547\n",
      "Epoch 62, Batch 16634, Loss: 170.03077697753906\n",
      "Epoch 62, Batch 16635, Loss: 179.03260803222656\n",
      "Epoch 62, Batch 16636, Loss: 164.25770568847656\n",
      "Epoch 62, Batch 16637, Loss: 167.61058044433594\n",
      "Epoch 62, Batch 16638, Loss: 180.04080200195312\n",
      "Epoch 62, Batch 16639, Loss: 182.2865753173828\n",
      "Epoch 62, Batch 16640, Loss: 169.75979614257812\n",
      "Epoch 62, Batch 16641, Loss: 180.86146545410156\n",
      "Epoch 62, Batch 16642, Loss: 190.3346405029297\n",
      "Epoch 62, Batch 16643, Loss: 182.7730712890625\n",
      "Epoch 62, Batch 16644, Loss: 163.54481506347656\n",
      "Epoch 62, Batch 16645, Loss: 173.93399047851562\n",
      "Epoch 62, Batch 16646, Loss: 176.75299072265625\n",
      "Epoch 62, Batch 16647, Loss: 162.68634033203125\n",
      "Epoch 62, Batch 16648, Loss: 164.17379760742188\n",
      "Epoch 62, Batch 16649, Loss: 182.3718719482422\n",
      "Epoch 62, Batch 16650, Loss: 172.81796264648438\n",
      "Epoch 62, Batch 16651, Loss: 177.43460083007812\n",
      "Epoch 62, Batch 16652, Loss: 168.78353881835938\n",
      "Epoch 62, Batch 16653, Loss: 165.6251220703125\n",
      "Epoch 62, Batch 16654, Loss: 191.12539672851562\n",
      "Epoch 62, Batch 16655, Loss: 174.23800659179688\n",
      "Epoch 62, Batch 16656, Loss: 183.836669921875\n",
      "Epoch 62, Batch 16657, Loss: 184.5742950439453\n",
      "Epoch 62, Batch 16658, Loss: 188.2857208251953\n",
      "Epoch 62, Batch 16659, Loss: 168.37059020996094\n",
      "Epoch 62, Batch 16660, Loss: 184.4392852783203\n",
      "Epoch 62, Batch 16661, Loss: 175.5308074951172\n",
      "Epoch 62, Batch 16662, Loss: 171.94493103027344\n",
      "Epoch 62, Batch 16663, Loss: 162.2383575439453\n",
      "Epoch 62, Batch 16664, Loss: 171.0561065673828\n",
      "Epoch 62, Batch 16665, Loss: 179.642578125\n",
      "Epoch 62, Batch 16666, Loss: 169.18226623535156\n",
      "Epoch 62, Batch 16667, Loss: 162.30880737304688\n",
      "Epoch 62, Batch 16668, Loss: 165.8900909423828\n",
      "Epoch 62, Batch 16669, Loss: 167.20384216308594\n",
      "Epoch 62, Batch 16670, Loss: 173.935546875\n",
      "Epoch 62, Batch 16671, Loss: 179.8773193359375\n",
      "Epoch 62, Batch 16672, Loss: 180.57479858398438\n",
      "Epoch 62, Batch 16673, Loss: 172.64132690429688\n",
      "Epoch 62, Batch 16674, Loss: 181.0624542236328\n",
      "Epoch 62, Batch 16675, Loss: 157.1085968017578\n",
      "Epoch 62, Batch 16676, Loss: 172.1371612548828\n",
      "Epoch 62, Batch 16677, Loss: 166.3343505859375\n",
      "Epoch 62, Batch 16678, Loss: 157.030029296875\n",
      "Epoch 62, Batch 16679, Loss: 171.07266235351562\n",
      "Epoch 62, Batch 16680, Loss: 167.81927490234375\n",
      "Epoch 62, Batch 16681, Loss: 167.40576171875\n",
      "Epoch 62, Batch 16682, Loss: 175.41981506347656\n",
      "Epoch 62, Batch 16683, Loss: 167.60430908203125\n",
      "Epoch 62, Batch 16684, Loss: 172.22088623046875\n",
      "Epoch 62, Batch 16685, Loss: 171.95108032226562\n",
      "Epoch 62, Batch 16686, Loss: 174.48435974121094\n",
      "Epoch 62, Batch 16687, Loss: 171.62168884277344\n",
      "Epoch 62, Batch 16688, Loss: 183.3096466064453\n",
      "Epoch 62, Batch 16689, Loss: 163.81358337402344\n",
      "Epoch 62, Batch 16690, Loss: 175.44061279296875\n",
      "Epoch 62, Batch 16691, Loss: 193.30381774902344\n",
      "Epoch 62, Batch 16692, Loss: 157.0015411376953\n",
      "Epoch 62, Batch 16693, Loss: 170.1750946044922\n",
      "Epoch 62, Batch 16694, Loss: 173.4539337158203\n",
      "Epoch 62, Batch 16695, Loss: 179.70779418945312\n",
      "Epoch 62, Batch 16696, Loss: 165.14044189453125\n",
      "Epoch 62, Batch 16697, Loss: 171.8115234375\n",
      "Epoch 62, Batch 16698, Loss: 169.1144256591797\n",
      "Epoch 62, Batch 16699, Loss: 166.86676025390625\n",
      "Epoch 62, Batch 16700, Loss: 156.40225219726562\n",
      "Epoch 62, Batch 16701, Loss: 183.42771911621094\n",
      "Epoch 62, Batch 16702, Loss: 194.0634307861328\n",
      "Epoch 62, Batch 16703, Loss: 173.4307403564453\n",
      "Epoch 62, Batch 16704, Loss: 181.96746826171875\n",
      "Epoch 62, Batch 16705, Loss: 183.80274963378906\n",
      "Epoch 62, Batch 16706, Loss: 172.0579071044922\n",
      "Epoch 62, Batch 16707, Loss: 171.47679138183594\n",
      "Epoch 62, Batch 16708, Loss: 185.80369567871094\n",
      "Epoch 62, Batch 16709, Loss: 182.0505828857422\n",
      "Epoch 62, Batch 16710, Loss: 191.00357055664062\n",
      "Epoch 62, Batch 16711, Loss: 162.345947265625\n",
      "Epoch 62, Batch 16712, Loss: 163.5969696044922\n",
      "Epoch 62, Batch 16713, Loss: 169.77041625976562\n",
      "Epoch 62, Batch 16714, Loss: 173.00265502929688\n",
      "Epoch 62, Batch 16715, Loss: 171.51296997070312\n",
      "Epoch 62, Batch 16716, Loss: 166.90994262695312\n",
      "Epoch 62, Batch 16717, Loss: 172.43267822265625\n",
      "Epoch 62, Batch 16718, Loss: 179.6111297607422\n",
      "Epoch 62, Batch 16719, Loss: 183.54605102539062\n",
      "Epoch 62, Batch 16720, Loss: 161.45156860351562\n",
      "Epoch 62, Batch 16721, Loss: 167.08358764648438\n",
      "Epoch 62, Batch 16722, Loss: 198.7928009033203\n",
      "Epoch 62, Batch 16723, Loss: 181.74169921875\n",
      "Epoch 62, Batch 16724, Loss: 159.19403076171875\n",
      "Epoch 62, Batch 16725, Loss: 165.47991943359375\n",
      "Epoch 62, Batch 16726, Loss: 165.22491455078125\n",
      "Epoch 62, Batch 16727, Loss: 179.32781982421875\n",
      "Epoch 62, Batch 16728, Loss: 174.6249542236328\n",
      "Epoch 62, Batch 16729, Loss: 166.8373565673828\n",
      "Epoch 62, Batch 16730, Loss: 175.89466857910156\n",
      "Epoch 62, Batch 16731, Loss: 177.76881408691406\n",
      "Epoch 62, Batch 16732, Loss: 162.3560028076172\n",
      "Epoch 62, Batch 16733, Loss: 171.98350524902344\n",
      "Epoch 62, Batch 16734, Loss: 182.69677734375\n",
      "Epoch 62, Batch 16735, Loss: 173.3568572998047\n",
      "Epoch 62, Batch 16736, Loss: 180.16697692871094\n",
      "Epoch 62, Batch 16737, Loss: 168.59230041503906\n",
      "Epoch 62, Batch 16738, Loss: 184.9527130126953\n",
      "Epoch 62, Batch 16739, Loss: 185.8097381591797\n",
      "Epoch 62, Batch 16740, Loss: 181.5319366455078\n",
      "Epoch 62, Batch 16741, Loss: 153.3450469970703\n",
      "Epoch 62, Batch 16742, Loss: 174.94424438476562\n",
      "Epoch 62, Batch 16743, Loss: 176.96559143066406\n",
      "Epoch 62, Batch 16744, Loss: 182.45230102539062\n",
      "Epoch 62, Batch 16745, Loss: 168.0998992919922\n",
      "Epoch 62, Batch 16746, Loss: 165.9954071044922\n",
      "Epoch 62, Batch 16747, Loss: 149.94847106933594\n",
      "Epoch 62, Batch 16748, Loss: 198.7617645263672\n",
      "Epoch 62, Batch 16749, Loss: 177.67323303222656\n",
      "Epoch 62, Batch 16750, Loss: 185.23927307128906\n",
      "Epoch 62, Batch 16751, Loss: 169.16549682617188\n",
      "Epoch 62, Batch 16752, Loss: 161.50308227539062\n",
      "Epoch 62, Batch 16753, Loss: 164.91094970703125\n",
      "Epoch 62, Batch 16754, Loss: 157.0460968017578\n",
      "Epoch 62, Batch 16755, Loss: 165.8995819091797\n",
      "Epoch 62, Batch 16756, Loss: 174.13966369628906\n",
      "Epoch 62, Batch 16757, Loss: 168.98175048828125\n",
      "Epoch 62, Batch 16758, Loss: 198.62525939941406\n",
      "Epoch 62, Batch 16759, Loss: 182.5780029296875\n",
      "Epoch 62, Batch 16760, Loss: 182.22950744628906\n",
      "Epoch 62, Batch 16761, Loss: 181.94015502929688\n",
      "Epoch 62, Batch 16762, Loss: 181.58340454101562\n",
      "Epoch 62, Batch 16763, Loss: 183.11099243164062\n",
      "Epoch 62, Batch 16764, Loss: 180.6263885498047\n",
      "Epoch 62, Batch 16765, Loss: 185.13084411621094\n",
      "Epoch 62, Batch 16766, Loss: 179.70486450195312\n",
      "Epoch 62, Batch 16767, Loss: 166.3521270751953\n",
      "Epoch 62, Batch 16768, Loss: 165.8631134033203\n",
      "Epoch 62, Batch 16769, Loss: 175.15467834472656\n",
      "Epoch 62, Batch 16770, Loss: 160.7515411376953\n",
      "Epoch 62, Batch 16771, Loss: 166.1929931640625\n",
      "Epoch 62, Batch 16772, Loss: 184.1722412109375\n",
      "Epoch 62, Batch 16773, Loss: 177.97532653808594\n",
      "Epoch 62, Batch 16774, Loss: 178.4508514404297\n",
      "Epoch 62, Batch 16775, Loss: 168.1486358642578\n",
      "Epoch 62, Batch 16776, Loss: 166.7574005126953\n",
      "Epoch 62, Batch 16777, Loss: 169.935791015625\n",
      "Epoch 62, Batch 16778, Loss: 170.5806121826172\n",
      "Epoch 62, Batch 16779, Loss: 188.12777709960938\n",
      "Epoch 62, Batch 16780, Loss: 176.0190887451172\n",
      "Epoch 62, Batch 16781, Loss: 182.38658142089844\n",
      "Epoch 62, Batch 16782, Loss: 172.35731506347656\n",
      "Epoch 62, Batch 16783, Loss: 180.0263671875\n",
      "Epoch 62, Batch 16784, Loss: 167.66455078125\n",
      "Epoch 62, Batch 16785, Loss: 181.9930419921875\n",
      "Epoch 62, Batch 16786, Loss: 184.73410034179688\n",
      "Epoch 62, Batch 16787, Loss: 171.05677795410156\n",
      "Epoch 62, Batch 16788, Loss: 178.27809143066406\n",
      "Epoch 62, Batch 16789, Loss: 170.51254272460938\n",
      "Epoch 62, Batch 16790, Loss: 169.54779052734375\n",
      "Epoch 62, Batch 16791, Loss: 171.7172088623047\n",
      "Epoch 62, Batch 16792, Loss: 182.91896057128906\n",
      "Epoch 62, Batch 16793, Loss: 182.25819396972656\n",
      "Epoch 62, Batch 16794, Loss: 179.91824340820312\n",
      "Epoch 62, Batch 16795, Loss: 163.44467163085938\n",
      "Epoch 62, Batch 16796, Loss: 180.46974182128906\n",
      "Epoch 62, Batch 16797, Loss: 162.41770935058594\n",
      "Epoch 62, Batch 16798, Loss: 156.68882751464844\n",
      "Epoch 62, Batch 16799, Loss: 166.90289306640625\n",
      "Epoch 62, Batch 16800, Loss: 175.2567138671875\n",
      "Epoch 62, Batch 16801, Loss: 177.55917358398438\n",
      "Epoch 62, Batch 16802, Loss: 156.2024383544922\n",
      "Epoch 62, Batch 16803, Loss: 161.43411254882812\n",
      "Epoch 62, Batch 16804, Loss: 169.884521484375\n",
      "Epoch 62, Batch 16805, Loss: 165.23841857910156\n",
      "Epoch 62, Batch 16806, Loss: 161.6790008544922\n",
      "Epoch 62, Batch 16807, Loss: 172.3001708984375\n",
      "Epoch 62, Batch 16808, Loss: 172.72056579589844\n",
      "Epoch 62, Batch 16809, Loss: 165.60330200195312\n",
      "Epoch 62, Batch 16810, Loss: 158.0047149658203\n",
      "Epoch 62, Batch 16811, Loss: 178.92120361328125\n",
      "Epoch 62, Batch 16812, Loss: 172.81092834472656\n",
      "Epoch 62, Batch 16813, Loss: 168.85369873046875\n",
      "Epoch 62, Batch 16814, Loss: 175.2748565673828\n",
      "Epoch 62, Batch 16815, Loss: 171.866455078125\n",
      "Epoch 62, Batch 16816, Loss: 188.66098022460938\n",
      "Epoch 62, Batch 16817, Loss: 167.94725036621094\n",
      "Epoch 62, Batch 16818, Loss: 173.2921905517578\n",
      "Epoch 62, Batch 16819, Loss: 171.767822265625\n",
      "Epoch 62, Batch 16820, Loss: 163.77101135253906\n",
      "Epoch 62, Batch 16821, Loss: 168.31301879882812\n",
      "Epoch 62, Batch 16822, Loss: 183.11773681640625\n",
      "Epoch 62, Batch 16823, Loss: 165.2498779296875\n",
      "Epoch 62, Batch 16824, Loss: 186.57281494140625\n",
      "Epoch 62, Batch 16825, Loss: 167.62010192871094\n",
      "Epoch 62, Batch 16826, Loss: 169.53211975097656\n",
      "Epoch 62, Batch 16827, Loss: 173.70599365234375\n",
      "Epoch 62, Batch 16828, Loss: 167.66163635253906\n",
      "Epoch 62, Batch 16829, Loss: 165.39817810058594\n",
      "Epoch 62, Batch 16830, Loss: 176.23828125\n",
      "Epoch 62, Batch 16831, Loss: 173.7838134765625\n",
      "Epoch 62, Batch 16832, Loss: 169.19021606445312\n",
      "Epoch 62, Batch 16833, Loss: 164.65939331054688\n",
      "Epoch 62, Batch 16834, Loss: 172.94801330566406\n",
      "Epoch 62, Batch 16835, Loss: 179.4042205810547\n",
      "Epoch 62, Batch 16836, Loss: 168.32879638671875\n",
      "Epoch 62, Batch 16837, Loss: 191.16819763183594\n",
      "Epoch 62, Batch 16838, Loss: 163.43626403808594\n",
      "Epoch 62, Batch 16839, Loss: 170.04727172851562\n",
      "Epoch 62, Batch 16840, Loss: 163.23170471191406\n",
      "Epoch 62, Batch 16841, Loss: 158.80230712890625\n",
      "Epoch 62, Batch 16842, Loss: 175.65110778808594\n",
      "Epoch 62, Batch 16843, Loss: 180.97796630859375\n",
      "Epoch 62, Batch 16844, Loss: 169.57339477539062\n",
      "Epoch 62, Batch 16845, Loss: 173.7949981689453\n",
      "Epoch 62, Batch 16846, Loss: 159.816162109375\n",
      "Epoch 62, Batch 16847, Loss: 175.51022338867188\n",
      "Epoch 62, Batch 16848, Loss: 182.95758056640625\n",
      "Epoch 62, Batch 16849, Loss: 182.36972045898438\n",
      "Epoch 62, Batch 16850, Loss: 177.43057250976562\n",
      "Epoch 62, Batch 16851, Loss: 165.2451171875\n",
      "Epoch 62, Batch 16852, Loss: 169.65994262695312\n",
      "Epoch 62, Batch 16853, Loss: 178.01768493652344\n",
      "Epoch 62, Batch 16854, Loss: 175.91567993164062\n",
      "Epoch 62, Batch 16855, Loss: 178.9156951904297\n",
      "Epoch 62, Batch 16856, Loss: 177.6228485107422\n",
      "Epoch 62, Batch 16857, Loss: 171.77005004882812\n",
      "Epoch 62, Batch 16858, Loss: 180.97787475585938\n",
      "Epoch 62, Batch 16859, Loss: 162.4824981689453\n",
      "Epoch 62, Batch 16860, Loss: 179.9023895263672\n",
      "Epoch 62, Batch 16861, Loss: 174.60496520996094\n",
      "Epoch 62, Batch 16862, Loss: 179.47909545898438\n",
      "Epoch 62, Batch 16863, Loss: 176.38595581054688\n",
      "Epoch 62, Batch 16864, Loss: 173.17198181152344\n",
      "Epoch 62, Batch 16865, Loss: 171.92413330078125\n",
      "Epoch 62, Batch 16866, Loss: 165.14212036132812\n",
      "Epoch 62, Batch 16867, Loss: 168.990478515625\n",
      "Epoch 62, Batch 16868, Loss: 170.299560546875\n",
      "Epoch 62, Batch 16869, Loss: 173.2989959716797\n",
      "Epoch 62, Batch 16870, Loss: 153.98426818847656\n",
      "Epoch 62, Batch 16871, Loss: 180.8691864013672\n",
      "Epoch 62, Batch 16872, Loss: 166.84249877929688\n",
      "Epoch 62, Batch 16873, Loss: 177.49960327148438\n",
      "Epoch 62, Batch 16874, Loss: 176.059814453125\n",
      "Epoch 62, Batch 16875, Loss: 194.6831817626953\n",
      "Epoch 62, Batch 16876, Loss: 183.0568084716797\n",
      "Epoch 62, Batch 16877, Loss: 171.51133728027344\n",
      "Epoch 62, Batch 16878, Loss: 157.52622985839844\n",
      "Epoch 62, Batch 16879, Loss: 172.35641479492188\n",
      "Epoch 62, Batch 16880, Loss: 155.55795288085938\n",
      "Epoch 62, Batch 16881, Loss: 162.27789306640625\n",
      "Epoch 62, Batch 16882, Loss: 175.6097869873047\n",
      "Epoch 62, Batch 16883, Loss: 182.9358367919922\n",
      "Epoch 62, Batch 16884, Loss: 173.93319702148438\n",
      "Epoch 62, Batch 16885, Loss: 174.95343017578125\n",
      "Epoch 62, Batch 16886, Loss: 170.1343536376953\n",
      "Epoch 62, Batch 16887, Loss: 165.850341796875\n",
      "Epoch 62, Batch 16888, Loss: 160.60333251953125\n",
      "Epoch 62, Batch 16889, Loss: 175.28907775878906\n",
      "Epoch 62, Batch 16890, Loss: 192.1194305419922\n",
      "Epoch 62, Batch 16891, Loss: 179.4181671142578\n",
      "Epoch 62, Batch 16892, Loss: 179.968505859375\n",
      "Epoch 62, Batch 16893, Loss: 156.57638549804688\n",
      "Epoch 62, Batch 16894, Loss: 158.47117614746094\n",
      "Epoch 62, Batch 16895, Loss: 173.1567840576172\n",
      "Epoch 62, Batch 16896, Loss: 176.2932586669922\n",
      "Epoch 62, Batch 16897, Loss: 164.7799072265625\n",
      "Epoch 62, Batch 16898, Loss: 173.83633422851562\n",
      "Epoch 62, Batch 16899, Loss: 168.7373046875\n",
      "Epoch 62, Batch 16900, Loss: 179.5289764404297\n",
      "Epoch 62, Batch 16901, Loss: 191.0303955078125\n",
      "Epoch 62, Batch 16902, Loss: 174.62684631347656\n",
      "Epoch 62, Batch 16903, Loss: 169.70960998535156\n",
      "Epoch 62, Batch 16904, Loss: 164.1500701904297\n",
      "Epoch 62, Batch 16905, Loss: 182.71392822265625\n",
      "Epoch 62, Batch 16906, Loss: 171.52783203125\n",
      "Epoch 62, Batch 16907, Loss: 180.78125\n",
      "Epoch 62, Batch 16908, Loss: 163.1039276123047\n",
      "Epoch 62, Batch 16909, Loss: 185.4955291748047\n",
      "Epoch 62, Batch 16910, Loss: 172.7681427001953\n",
      "Epoch 62, Batch 16911, Loss: 167.02963256835938\n",
      "Epoch 62, Batch 16912, Loss: 178.50601196289062\n",
      "Epoch 62, Batch 16913, Loss: 178.9317169189453\n",
      "Epoch 62, Batch 16914, Loss: 159.875732421875\n",
      "Epoch 62, Batch 16915, Loss: 156.3517608642578\n",
      "Epoch 62, Batch 16916, Loss: 173.5392608642578\n",
      "Epoch 62, Batch 16917, Loss: 167.74839782714844\n",
      "Epoch 62, Batch 16918, Loss: 176.42771911621094\n",
      "Epoch 62, Batch 16919, Loss: 159.20428466796875\n",
      "Epoch 62, Batch 16920, Loss: 170.72329711914062\n",
      "Epoch 62, Batch 16921, Loss: 166.7485809326172\n",
      "Epoch 62, Batch 16922, Loss: 175.98086547851562\n",
      "Epoch 62, Batch 16923, Loss: 173.09751892089844\n",
      "Epoch 62, Batch 16924, Loss: 158.7509307861328\n",
      "Epoch 62, Batch 16925, Loss: 164.22361755371094\n",
      "Epoch 62, Batch 16926, Loss: 178.10150146484375\n",
      "Epoch 62, Batch 16927, Loss: 163.3137969970703\n",
      "Epoch 62, Batch 16928, Loss: 176.79397583007812\n",
      "Epoch 62, Batch 16929, Loss: 184.15711975097656\n",
      "Epoch 62, Batch 16930, Loss: 165.06930541992188\n",
      "Epoch 62, Batch 16931, Loss: 162.79885864257812\n",
      "Epoch 62, Batch 16932, Loss: 202.59226989746094\n",
      "Epoch 62, Batch 16933, Loss: 162.27349853515625\n",
      "Epoch 62, Batch 16934, Loss: 176.2966766357422\n",
      "Epoch 62, Batch 16935, Loss: 179.0729522705078\n",
      "Epoch 62, Batch 16936, Loss: 166.38331604003906\n",
      "Epoch 62, Batch 16937, Loss: 174.60569763183594\n",
      "Epoch 62, Batch 16938, Loss: 188.11801147460938\n",
      "Epoch 62, Batch 16939, Loss: 156.8749237060547\n",
      "Epoch 62, Batch 16940, Loss: 161.7328338623047\n",
      "Epoch 62, Batch 16941, Loss: 168.07200622558594\n",
      "Epoch 62, Batch 16942, Loss: 166.96099853515625\n",
      "Epoch 62, Batch 16943, Loss: 160.9966278076172\n",
      "Epoch 62, Batch 16944, Loss: 164.25979614257812\n",
      "Epoch 62, Batch 16945, Loss: 166.1419677734375\n",
      "Epoch 62, Batch 16946, Loss: 169.4129638671875\n",
      "Epoch 62, Batch 16947, Loss: 175.32044982910156\n",
      "Epoch 62, Batch 16948, Loss: 171.43553161621094\n",
      "Epoch 62, Batch 16949, Loss: 184.9340362548828\n",
      "Epoch 62, Batch 16950, Loss: 160.81948852539062\n",
      "Epoch 62, Batch 16951, Loss: 184.1065673828125\n",
      "Epoch 62, Batch 16952, Loss: 178.53414916992188\n",
      "Epoch 62, Batch 16953, Loss: 170.54849243164062\n",
      "Epoch 62, Batch 16954, Loss: 178.67489624023438\n",
      "Epoch 62, Batch 16955, Loss: 169.80628967285156\n",
      "Epoch 62, Batch 16956, Loss: 172.25099182128906\n",
      "Epoch 62, Batch 16957, Loss: 172.8664093017578\n",
      "Epoch 62, Batch 16958, Loss: 195.83164978027344\n",
      "Epoch 62, Batch 16959, Loss: 178.63404846191406\n",
      "Epoch 62, Batch 16960, Loss: 188.8002471923828\n",
      "Epoch 62, Batch 16961, Loss: 159.6464385986328\n",
      "Epoch 62, Batch 16962, Loss: 151.1397247314453\n",
      "Epoch 62, Batch 16963, Loss: 181.29779052734375\n",
      "Epoch 62, Batch 16964, Loss: 168.7208709716797\n",
      "Epoch 62, Batch 16965, Loss: 169.89920043945312\n",
      "Epoch 62, Batch 16966, Loss: 171.43316650390625\n",
      "Epoch 62, Batch 16967, Loss: 179.71090698242188\n",
      "Epoch 62, Batch 16968, Loss: 175.93975830078125\n",
      "Epoch 62, Batch 16969, Loss: 174.36325073242188\n",
      "Epoch 62, Batch 16970, Loss: 183.968017578125\n",
      "Epoch 62, Batch 16971, Loss: 170.7062225341797\n",
      "Epoch 62, Batch 16972, Loss: 178.05264282226562\n",
      "Epoch 62, Batch 16973, Loss: 180.285888671875\n",
      "Epoch 62, Batch 16974, Loss: 191.71377563476562\n",
      "Epoch 62, Batch 16975, Loss: 157.4004364013672\n",
      "Epoch 62, Batch 16976, Loss: 186.2128448486328\n",
      "Epoch 62, Batch 16977, Loss: 191.87440490722656\n",
      "Epoch 62, Batch 16978, Loss: 166.16392517089844\n",
      "Epoch 62, Batch 16979, Loss: 190.1728515625\n",
      "Epoch 62, Batch 16980, Loss: 186.02786254882812\n",
      "Epoch 62, Batch 16981, Loss: 163.2766876220703\n",
      "Epoch 62, Batch 16982, Loss: 191.2094268798828\n",
      "Epoch 62, Batch 16983, Loss: 168.2109375\n",
      "Epoch 62, Batch 16984, Loss: 171.5467987060547\n",
      "Epoch 62, Batch 16985, Loss: 172.74554443359375\n",
      "Epoch 62, Batch 16986, Loss: 182.70388793945312\n",
      "Epoch 62, Batch 16987, Loss: 165.7271728515625\n",
      "Epoch 62, Batch 16988, Loss: 167.81260681152344\n",
      "Epoch 62, Batch 16989, Loss: 154.52508544921875\n",
      "Epoch 62, Batch 16990, Loss: 172.4369354248047\n",
      "Epoch 62, Batch 16991, Loss: 179.9293212890625\n",
      "Epoch 62, Batch 16992, Loss: 182.86280822753906\n",
      "Epoch 62, Batch 16993, Loss: 185.13482666015625\n",
      "Epoch 62, Batch 16994, Loss: 178.76734924316406\n",
      "Epoch 62, Batch 16995, Loss: 184.79002380371094\n",
      "Epoch 62, Batch 16996, Loss: 179.06141662597656\n",
      "Epoch 62, Batch 16997, Loss: 157.68353271484375\n",
      "Epoch 62, Batch 16998, Loss: 177.99122619628906\n",
      "Epoch 62, Batch 16999, Loss: 176.17787170410156\n",
      "Epoch 62, Batch 17000, Loss: 166.19143676757812\n",
      "Epoch 62, Batch 17001, Loss: 178.96051025390625\n",
      "Epoch 62, Batch 17002, Loss: 177.99000549316406\n",
      "Epoch 62, Batch 17003, Loss: 175.86386108398438\n",
      "Epoch 62, Batch 17004, Loss: 172.60006713867188\n",
      "Epoch 62, Batch 17005, Loss: 167.93600463867188\n",
      "Epoch 62, Batch 17006, Loss: 169.76055908203125\n",
      "Epoch 62, Batch 17007, Loss: 178.10302734375\n",
      "Epoch 62, Batch 17008, Loss: 183.8128204345703\n",
      "Epoch 62, Batch 17009, Loss: 169.34005737304688\n",
      "Epoch 62, Batch 17010, Loss: 167.2957305908203\n",
      "Epoch 62, Batch 17011, Loss: 168.89854431152344\n",
      "Epoch 62, Batch 17012, Loss: 167.0465850830078\n",
      "Epoch 62, Batch 17013, Loss: 179.7041015625\n",
      "Epoch 62, Batch 17014, Loss: 179.71412658691406\n",
      "Epoch 62, Batch 17015, Loss: 168.5909423828125\n",
      "Epoch 62, Batch 17016, Loss: 154.47894287109375\n",
      "Epoch 62, Batch 17017, Loss: 182.51663208007812\n",
      "Epoch 62, Batch 17018, Loss: 193.7196044921875\n",
      "Epoch 62, Batch 17019, Loss: 164.3570098876953\n",
      "Epoch 62, Batch 17020, Loss: 194.438232421875\n",
      "Epoch 62, Batch 17021, Loss: 177.37413024902344\n",
      "Epoch 62, Batch 17022, Loss: 164.641845703125\n",
      "Epoch 62, Batch 17023, Loss: 164.1461639404297\n",
      "Epoch 62, Batch 17024, Loss: 165.22885131835938\n",
      "Epoch 62, Batch 17025, Loss: 177.4725799560547\n",
      "Epoch 62, Batch 17026, Loss: 167.0746612548828\n",
      "Epoch 62, Batch 17027, Loss: 160.04483032226562\n",
      "Epoch 62, Batch 17028, Loss: 156.3313446044922\n",
      "Epoch 62, Batch 17029, Loss: 166.81591796875\n",
      "Epoch 62, Batch 17030, Loss: 175.98947143554688\n",
      "Epoch 62, Batch 17031, Loss: 198.37620544433594\n",
      "Epoch 62, Batch 17032, Loss: 160.53941345214844\n",
      "Epoch 62, Batch 17033, Loss: 190.15921020507812\n",
      "Epoch 62, Batch 17034, Loss: 157.99378967285156\n",
      "Epoch 62, Batch 17035, Loss: 187.0679168701172\n",
      "Epoch 62, Batch 17036, Loss: 185.8157501220703\n",
      "Epoch 62, Batch 17037, Loss: 181.7058563232422\n",
      "Epoch 62, Batch 17038, Loss: 169.81927490234375\n",
      "Epoch 62, Batch 17039, Loss: 174.63597106933594\n",
      "Epoch 62, Batch 17040, Loss: 175.84715270996094\n",
      "Epoch 62, Batch 17041, Loss: 178.8739013671875\n",
      "Epoch 62, Batch 17042, Loss: 166.79258728027344\n",
      "Epoch 62, Batch 17043, Loss: 174.42869567871094\n",
      "Epoch 62, Batch 17044, Loss: 162.67640686035156\n",
      "Epoch 62, Batch 17045, Loss: 174.27366638183594\n",
      "Epoch 62, Batch 17046, Loss: 175.88246154785156\n",
      "Epoch 62, Batch 17047, Loss: 160.5824432373047\n",
      "Epoch 62, Batch 17048, Loss: 160.33726501464844\n",
      "Epoch 62, Batch 17049, Loss: 170.52725219726562\n",
      "Epoch 62, Batch 17050, Loss: 164.3590087890625\n",
      "Epoch 62, Batch 17051, Loss: 186.93643188476562\n",
      "Epoch 62, Batch 17052, Loss: 178.06385803222656\n",
      "Epoch 62, Batch 17053, Loss: 167.38555908203125\n",
      "Epoch 62, Batch 17054, Loss: 176.24644470214844\n",
      "Epoch 62, Batch 17055, Loss: 160.7299346923828\n",
      "Epoch 62, Batch 17056, Loss: 179.16664123535156\n",
      "Epoch 62, Batch 17057, Loss: 154.61688232421875\n",
      "Epoch 62, Batch 17058, Loss: 160.30618286132812\n",
      "Epoch 62, Batch 17059, Loss: 182.84939575195312\n",
      "Epoch 62, Batch 17060, Loss: 167.73875427246094\n",
      "Epoch 62, Batch 17061, Loss: 201.9470672607422\n",
      "Epoch 62, Batch 17062, Loss: 165.85440063476562\n",
      "Epoch 62, Batch 17063, Loss: 166.57882690429688\n",
      "Epoch 62, Batch 17064, Loss: 178.16380310058594\n",
      "Epoch 62, Batch 17065, Loss: 176.63784790039062\n",
      "Epoch 62, Batch 17066, Loss: 171.29615783691406\n",
      "Epoch 62, Batch 17067, Loss: 162.66305541992188\n",
      "Epoch 62, Batch 17068, Loss: 182.1610870361328\n",
      "Epoch 62, Batch 17069, Loss: 165.03750610351562\n",
      "Epoch 62, Batch 17070, Loss: 163.05638122558594\n",
      "Epoch 62, Batch 17071, Loss: 178.66380310058594\n",
      "Epoch 62, Batch 17072, Loss: 167.36111450195312\n",
      "Epoch 62, Batch 17073, Loss: 171.92303466796875\n",
      "Epoch 62, Batch 17074, Loss: 176.00238037109375\n",
      "Epoch 62, Batch 17075, Loss: 153.3404541015625\n",
      "Epoch 62, Batch 17076, Loss: 164.7617950439453\n",
      "Epoch 62, Batch 17077, Loss: 172.92453002929688\n",
      "Epoch 62, Batch 17078, Loss: 176.0512237548828\n",
      "Epoch 62, Batch 17079, Loss: 175.7754669189453\n",
      "Epoch 62, Batch 17080, Loss: 179.28541564941406\n",
      "Epoch 62, Batch 17081, Loss: 178.09181213378906\n",
      "Epoch 62, Batch 17082, Loss: 159.84214782714844\n",
      "Epoch 62, Batch 17083, Loss: 179.1046600341797\n",
      "Epoch 62, Batch 17084, Loss: 165.9652557373047\n",
      "Epoch 62, Batch 17085, Loss: 190.42007446289062\n",
      "Epoch 62, Batch 17086, Loss: 171.3502197265625\n",
      "Epoch 62, Batch 17087, Loss: 163.4176788330078\n",
      "Epoch 62, Batch 17088, Loss: 167.6107635498047\n",
      "Epoch 62, Batch 17089, Loss: 173.94882202148438\n",
      "Epoch 62, Batch 17090, Loss: 164.88829040527344\n",
      "Epoch 62, Batch 17091, Loss: 159.5962677001953\n",
      "Epoch 62, Batch 17092, Loss: 164.94732666015625\n",
      "Epoch 62, Batch 17093, Loss: 170.55850219726562\n",
      "Epoch 62, Batch 17094, Loss: 171.77874755859375\n",
      "Epoch 62, Batch 17095, Loss: 161.30877685546875\n",
      "Epoch 62, Batch 17096, Loss: 179.86260986328125\n",
      "Epoch 62, Batch 17097, Loss: 183.46832275390625\n",
      "Epoch 62, Batch 17098, Loss: 169.3092041015625\n",
      "Epoch 62, Batch 17099, Loss: 179.19403076171875\n",
      "Epoch 62, Batch 17100, Loss: 202.04483032226562\n",
      "Epoch 62, Batch 17101, Loss: 157.66876220703125\n",
      "Epoch 62, Batch 17102, Loss: 160.80372619628906\n",
      "Epoch 62, Batch 17103, Loss: 174.9197998046875\n",
      "Epoch 62, Batch 17104, Loss: 182.117919921875\n",
      "Epoch 62, Batch 17105, Loss: 164.39976501464844\n",
      "Epoch 62, Batch 17106, Loss: 170.51124572753906\n",
      "Epoch 62, Batch 17107, Loss: 163.1887664794922\n",
      "Epoch 62, Batch 17108, Loss: 188.18785095214844\n",
      "Epoch 62, Batch 17109, Loss: 164.47718811035156\n",
      "Epoch 62, Batch 17110, Loss: 167.42947387695312\n",
      "Epoch 62, Batch 17111, Loss: 167.2607879638672\n",
      "Epoch 62, Batch 17112, Loss: 160.0491943359375\n",
      "Epoch 62, Batch 17113, Loss: 169.17544555664062\n",
      "Epoch 62, Batch 17114, Loss: 161.64495849609375\n",
      "Epoch 62, Batch 17115, Loss: 172.35870361328125\n",
      "Epoch 62, Batch 17116, Loss: 164.50502014160156\n",
      "Epoch 62, Batch 17117, Loss: 165.7779998779297\n",
      "Epoch 62, Batch 17118, Loss: 173.8584442138672\n",
      "Epoch 62, Batch 17119, Loss: 176.60903930664062\n",
      "Epoch 62, Batch 17120, Loss: 170.6012725830078\n",
      "Epoch 62, Batch 17121, Loss: 172.2612762451172\n",
      "Epoch 62, Batch 17122, Loss: 168.79966735839844\n",
      "Epoch 62, Batch 17123, Loss: 167.15708923339844\n",
      "Epoch 62, Batch 17124, Loss: 179.38961791992188\n",
      "Epoch 62, Batch 17125, Loss: 174.18963623046875\n",
      "Epoch 62, Batch 17126, Loss: 176.06640625\n",
      "Epoch 62, Batch 17127, Loss: 175.1868896484375\n",
      "Epoch 62, Batch 17128, Loss: 198.08181762695312\n",
      "Epoch 62, Batch 17129, Loss: 173.49331665039062\n",
      "Epoch 62, Batch 17130, Loss: 165.80323791503906\n",
      "Epoch 62, Batch 17131, Loss: 170.70680236816406\n",
      "Epoch 62, Batch 17132, Loss: 174.0276336669922\n",
      "Epoch 62, Batch 17133, Loss: 169.5559844970703\n",
      "Epoch 62, Batch 17134, Loss: 174.32322692871094\n",
      "Epoch 62, Batch 17135, Loss: 162.9530792236328\n",
      "Epoch 62, Batch 17136, Loss: 180.48048400878906\n",
      "Epoch 62, Batch 17137, Loss: 172.50852966308594\n",
      "Epoch 62, Batch 17138, Loss: 157.3719024658203\n",
      "Epoch 62, Batch 17139, Loss: 166.71844482421875\n",
      "Epoch 62, Batch 17140, Loss: 166.21604919433594\n",
      "Epoch 62, Batch 17141, Loss: 178.99020385742188\n",
      "Epoch 62, Batch 17142, Loss: 178.21414184570312\n",
      "Epoch 62, Batch 17143, Loss: 187.43292236328125\n",
      "Epoch 62, Batch 17144, Loss: 181.6155242919922\n",
      "Epoch 62, Batch 17145, Loss: 169.00143432617188\n",
      "Epoch 62, Batch 17146, Loss: 181.29150390625\n",
      "Epoch 62, Batch 17147, Loss: 157.50885009765625\n",
      "Epoch 62, Batch 17148, Loss: 187.98268127441406\n",
      "Epoch 62, Batch 17149, Loss: 177.6443328857422\n",
      "Epoch 62, Batch 17150, Loss: 175.65432739257812\n",
      "Epoch 62, Batch 17151, Loss: 177.5702362060547\n",
      "Epoch 62, Batch 17152, Loss: 194.26699829101562\n",
      "Epoch 62, Batch 17153, Loss: 170.32191467285156\n",
      "Epoch 62, Batch 17154, Loss: 183.08055114746094\n",
      "Epoch 62, Batch 17155, Loss: 164.30572509765625\n",
      "Epoch 62, Batch 17156, Loss: 165.15176391601562\n",
      "Epoch 62, Batch 17157, Loss: 192.5419158935547\n",
      "Epoch 62, Batch 17158, Loss: 168.30978393554688\n",
      "Epoch 62, Batch 17159, Loss: 183.55908203125\n",
      "Epoch 62, Batch 17160, Loss: 155.69882202148438\n",
      "Epoch 62, Batch 17161, Loss: 165.25546264648438\n",
      "Epoch 62, Batch 17162, Loss: 188.54689025878906\n",
      "Epoch 62, Batch 17163, Loss: 188.97537231445312\n",
      "Epoch 62, Batch 17164, Loss: 164.54942321777344\n",
      "Epoch 62, Batch 17165, Loss: 164.2725372314453\n",
      "Epoch 62, Batch 17166, Loss: 185.833984375\n",
      "Epoch 62, Batch 17167, Loss: 168.89283752441406\n",
      "Epoch 62, Batch 17168, Loss: 166.6188201904297\n",
      "Epoch 62, Batch 17169, Loss: 179.88446044921875\n",
      "Epoch 62, Batch 17170, Loss: 170.41983032226562\n",
      "Epoch 62, Batch 17171, Loss: 162.7950897216797\n",
      "Epoch 62, Batch 17172, Loss: 176.6291961669922\n",
      "Epoch 62, Batch 17173, Loss: 157.23060607910156\n",
      "Epoch 62, Batch 17174, Loss: 166.4232177734375\n",
      "Epoch 62, Batch 17175, Loss: 172.32406616210938\n",
      "Epoch 62, Batch 17176, Loss: 173.87240600585938\n",
      "Epoch 62, Batch 17177, Loss: 172.9480743408203\n",
      "Epoch 62, Batch 17178, Loss: 176.87396240234375\n",
      "Epoch 62, Batch 17179, Loss: 164.2368927001953\n",
      "Epoch 62, Batch 17180, Loss: 174.6173553466797\n",
      "Epoch 62, Batch 17181, Loss: 183.44735717773438\n",
      "Epoch 62, Batch 17182, Loss: 159.7588348388672\n",
      "Epoch 62, Batch 17183, Loss: 162.2284698486328\n",
      "Epoch 62, Batch 17184, Loss: 176.8064422607422\n",
      "Epoch 62, Batch 17185, Loss: 170.1463165283203\n",
      "Epoch 62, Batch 17186, Loss: 174.4161376953125\n",
      "Epoch 62, Batch 17187, Loss: 170.71652221679688\n",
      "Epoch 62, Batch 17188, Loss: 176.6634979248047\n",
      "Epoch 62, Batch 17189, Loss: 183.8502197265625\n",
      "Epoch 62, Batch 17190, Loss: 170.18722534179688\n",
      "Epoch 62, Batch 17191, Loss: 198.93821716308594\n",
      "Epoch 62, Batch 17192, Loss: 167.8841552734375\n",
      "Epoch 62, Batch 17193, Loss: 174.3374481201172\n",
      "Epoch 62, Batch 17194, Loss: 169.5283660888672\n",
      "Epoch 62, Batch 17195, Loss: 180.36549377441406\n",
      "Epoch 62, Batch 17196, Loss: 170.3408966064453\n",
      "Epoch 62, Batch 17197, Loss: 188.60653686523438\n",
      "Epoch 62, Batch 17198, Loss: 184.2089385986328\n",
      "Epoch 62, Batch 17199, Loss: 181.40623474121094\n",
      "Epoch 62, Batch 17200, Loss: 158.63372802734375\n",
      "Epoch 62, Batch 17201, Loss: 178.71205139160156\n",
      "Epoch 62, Batch 17202, Loss: 183.70570373535156\n",
      "Epoch 62, Batch 17203, Loss: 175.78878784179688\n",
      "Epoch 62, Batch 17204, Loss: 166.20628356933594\n",
      "Epoch 62, Batch 17205, Loss: 169.23020935058594\n",
      "Epoch 62, Batch 17206, Loss: 176.85853576660156\n",
      "Epoch 62, Batch 17207, Loss: 182.9420623779297\n",
      "Epoch 62, Batch 17208, Loss: 166.7062530517578\n",
      "Epoch 62, Batch 17209, Loss: 182.0492706298828\n",
      "Epoch 62, Batch 17210, Loss: 178.76853942871094\n",
      "Epoch 62, Batch 17211, Loss: 177.61685180664062\n",
      "Epoch 62, Batch 17212, Loss: 165.9462127685547\n",
      "Epoch 62, Batch 17213, Loss: 178.34169006347656\n",
      "Epoch 62, Batch 17214, Loss: 163.31515502929688\n",
      "Epoch 62, Batch 17215, Loss: 166.0985107421875\n",
      "Epoch 62, Batch 17216, Loss: 149.89187622070312\n",
      "Epoch 62, Batch 17217, Loss: 184.63270568847656\n",
      "Epoch 62, Batch 17218, Loss: 175.76828002929688\n",
      "Epoch 62, Batch 17219, Loss: 183.30084228515625\n",
      "Epoch 62, Batch 17220, Loss: 183.17526245117188\n",
      "Epoch 62, Batch 17221, Loss: 167.90985107421875\n",
      "Epoch 62, Batch 17222, Loss: 164.3466796875\n",
      "Epoch 62, Batch 17223, Loss: 147.35760498046875\n",
      "Epoch 62, Batch 17224, Loss: 162.17762756347656\n",
      "Epoch 62, Batch 17225, Loss: 156.5261993408203\n",
      "Epoch 62, Batch 17226, Loss: 178.66305541992188\n",
      "Epoch 62, Batch 17227, Loss: 180.61709594726562\n",
      "Epoch 62, Batch 17228, Loss: 165.87359619140625\n",
      "Epoch 62, Batch 17229, Loss: 175.38621520996094\n",
      "Epoch 62, Batch 17230, Loss: 172.11721801757812\n",
      "Epoch 62, Batch 17231, Loss: 182.37359619140625\n",
      "Epoch 62, Batch 17232, Loss: 169.49996948242188\n",
      "Epoch 62, Batch 17233, Loss: 182.26510620117188\n",
      "Epoch 62, Batch 17234, Loss: 166.03439331054688\n",
      "Epoch 62, Batch 17235, Loss: 176.8673095703125\n",
      "Epoch 62, Batch 17236, Loss: 184.42300415039062\n",
      "Epoch 62, Batch 17237, Loss: 176.60227966308594\n",
      "Epoch 62, Batch 17238, Loss: 160.84259033203125\n",
      "Epoch 62, Batch 17239, Loss: 158.541015625\n",
      "Epoch 62, Batch 17240, Loss: 173.4910430908203\n",
      "Epoch 62, Batch 17241, Loss: 195.6934814453125\n",
      "Epoch 62, Batch 17242, Loss: 173.8932342529297\n",
      "Epoch 62, Batch 17243, Loss: 167.26905822753906\n",
      "Epoch 62, Batch 17244, Loss: 190.4003448486328\n",
      "Epoch 62, Batch 17245, Loss: 178.68264770507812\n",
      "Epoch 62, Batch 17246, Loss: 160.16552734375\n",
      "Epoch 62, Batch 17247, Loss: 159.05068969726562\n",
      "Epoch 62, Batch 17248, Loss: 181.62635803222656\n",
      "Epoch 62, Batch 17249, Loss: 156.75271606445312\n",
      "Epoch 62, Batch 17250, Loss: 171.23765563964844\n",
      "Epoch 62, Batch 17251, Loss: 165.7586212158203\n",
      "Epoch 62, Batch 17252, Loss: 179.03265380859375\n",
      "Epoch 62, Batch 17253, Loss: 165.26953125\n",
      "Epoch 62, Batch 17254, Loss: 158.27272033691406\n",
      "Epoch 62, Batch 17255, Loss: 183.79742431640625\n",
      "Epoch 62, Batch 17256, Loss: 167.20150756835938\n",
      "Epoch 62, Batch 17257, Loss: 171.67759704589844\n",
      "Epoch 62, Batch 17258, Loss: 174.28842163085938\n",
      "Epoch 62, Batch 17259, Loss: 173.9396514892578\n",
      "Epoch 62, Batch 17260, Loss: 173.47866821289062\n",
      "Epoch 62, Batch 17261, Loss: 175.1654052734375\n",
      "Epoch 62, Batch 17262, Loss: 158.8562774658203\n",
      "Epoch 62, Batch 17263, Loss: 154.12327575683594\n",
      "Epoch 62, Batch 17264, Loss: 159.0170135498047\n",
      "Epoch 62, Batch 17265, Loss: 178.8607635498047\n",
      "Epoch 62, Batch 17266, Loss: 174.94329833984375\n",
      "Epoch 62, Batch 17267, Loss: 184.93310546875\n",
      "Epoch 62, Batch 17268, Loss: 173.86724853515625\n",
      "Epoch 62, Batch 17269, Loss: 171.7705078125\n",
      "Epoch 62, Batch 17270, Loss: 187.492919921875\n",
      "Epoch 62, Batch 17271, Loss: 173.9839324951172\n",
      "Epoch 62, Batch 17272, Loss: 168.937744140625\n",
      "Epoch 62, Batch 17273, Loss: 175.12252807617188\n",
      "Epoch 62, Batch 17274, Loss: 167.12094116210938\n",
      "Epoch 62, Batch 17275, Loss: 180.95005798339844\n",
      "Epoch 62, Batch 17276, Loss: 175.2122344970703\n",
      "Epoch 62, Batch 17277, Loss: 173.2161407470703\n",
      "Epoch 62, Batch 17278, Loss: 187.59771728515625\n",
      "Epoch 62, Batch 17279, Loss: 170.1363525390625\n",
      "Epoch 62, Batch 17280, Loss: 155.9432373046875\n",
      "Epoch 62, Batch 17281, Loss: 189.3319549560547\n",
      "Epoch 62, Batch 17282, Loss: 182.79612731933594\n",
      "Epoch 62, Batch 17283, Loss: 163.66786193847656\n",
      "Epoch 62, Batch 17284, Loss: 160.02081298828125\n",
      "Epoch 62, Batch 17285, Loss: 176.82936096191406\n",
      "Epoch 62, Batch 17286, Loss: 182.39236450195312\n",
      "Epoch 62, Batch 17287, Loss: 174.7225799560547\n",
      "Epoch 62, Batch 17288, Loss: 155.41725158691406\n",
      "Epoch 62, Batch 17289, Loss: 187.31802368164062\n",
      "Epoch 62, Batch 17290, Loss: 163.1319122314453\n",
      "Epoch 62, Batch 17291, Loss: 160.72030639648438\n",
      "Epoch 62, Batch 17292, Loss: 167.97845458984375\n",
      "Epoch 62, Batch 17293, Loss: 181.44989013671875\n",
      "Epoch 62, Batch 17294, Loss: 181.5855712890625\n",
      "Epoch 62, Batch 17295, Loss: 174.6984405517578\n",
      "Epoch 62, Batch 17296, Loss: 171.8024139404297\n",
      "Epoch 62, Batch 17297, Loss: 169.36062622070312\n",
      "Epoch 62, Batch 17298, Loss: 176.66285705566406\n",
      "Epoch 62, Batch 17299, Loss: 174.0086669921875\n",
      "Epoch 62, Batch 17300, Loss: 176.70797729492188\n",
      "Epoch 62, Batch 17301, Loss: 155.0845184326172\n",
      "Epoch 62, Batch 17302, Loss: 172.2591552734375\n",
      "Epoch 62, Batch 17303, Loss: 167.4386444091797\n",
      "Epoch 62, Batch 17304, Loss: 167.16448974609375\n",
      "Epoch 62, Batch 17305, Loss: 171.15896606445312\n",
      "Epoch 62, Batch 17306, Loss: 179.1373748779297\n",
      "Epoch 62, Batch 17307, Loss: 179.7084197998047\n",
      "Epoch 62, Batch 17308, Loss: 154.5741729736328\n",
      "Epoch 62, Batch 17309, Loss: 175.12132263183594\n",
      "Epoch 62, Batch 17310, Loss: 161.06890869140625\n",
      "Epoch 62, Batch 17311, Loss: 181.9347381591797\n",
      "Epoch 62, Batch 17312, Loss: 156.99058532714844\n",
      "Epoch 62, Batch 17313, Loss: 175.2437286376953\n",
      "Epoch 62, Batch 17314, Loss: 177.39364624023438\n",
      "Epoch 62, Batch 17315, Loss: 172.93362426757812\n",
      "Epoch 62, Batch 17316, Loss: 180.0080108642578\n",
      "Epoch 62, Batch 17317, Loss: 181.00958251953125\n",
      "Epoch 62, Batch 17318, Loss: 179.6610565185547\n",
      "Epoch 62, Batch 17319, Loss: 169.70458984375\n",
      "Epoch 62, Batch 17320, Loss: 158.27491760253906\n",
      "Epoch 62, Batch 17321, Loss: 166.74342346191406\n",
      "Epoch 62, Batch 17322, Loss: 155.27508544921875\n",
      "Epoch 62, Batch 17323, Loss: 175.32347106933594\n",
      "Epoch 62, Batch 17324, Loss: 175.35203552246094\n",
      "Epoch 62, Batch 17325, Loss: 164.6743927001953\n",
      "Epoch 62, Batch 17326, Loss: 172.4132080078125\n",
      "Epoch 62, Batch 17327, Loss: 177.17929077148438\n",
      "Epoch 62, Batch 17328, Loss: 173.2855682373047\n",
      "Epoch 62, Batch 17329, Loss: 178.36709594726562\n",
      "Epoch 62, Batch 17330, Loss: 189.7518310546875\n",
      "Epoch 62, Batch 17331, Loss: 170.4605255126953\n",
      "Epoch 62, Batch 17332, Loss: 174.8754119873047\n",
      "Epoch 62, Batch 17333, Loss: 170.29490661621094\n",
      "Epoch 62, Batch 17334, Loss: 169.43861389160156\n",
      "Epoch 62, Batch 17335, Loss: 184.8102264404297\n",
      "Epoch 62, Batch 17336, Loss: 169.2751922607422\n",
      "Epoch 62, Batch 17337, Loss: 175.03089904785156\n",
      "Epoch 62, Batch 17338, Loss: 165.26080322265625\n",
      "Epoch 62, Batch 17339, Loss: 174.48886108398438\n",
      "Epoch 62, Batch 17340, Loss: 176.49720764160156\n",
      "Epoch 62, Batch 17341, Loss: 179.760009765625\n",
      "Epoch 62, Batch 17342, Loss: 164.9134521484375\n",
      "Epoch 62, Batch 17343, Loss: 174.6112518310547\n",
      "Epoch 62, Batch 17344, Loss: 174.93295288085938\n",
      "Epoch 62, Batch 17345, Loss: 164.27371215820312\n",
      "Epoch 62, Batch 17346, Loss: 156.5187530517578\n",
      "Epoch 62, Batch 17347, Loss: 165.45643615722656\n",
      "Epoch 62, Batch 17348, Loss: 170.20962524414062\n",
      "Epoch 62, Batch 17349, Loss: 191.39468383789062\n",
      "Epoch 62, Batch 17350, Loss: 159.16421508789062\n",
      "Epoch 62, Batch 17351, Loss: 182.44131469726562\n",
      "Epoch 62, Batch 17352, Loss: 164.6177978515625\n",
      "Epoch 62, Batch 17353, Loss: 184.04345703125\n",
      "Epoch 62, Batch 17354, Loss: 161.75531005859375\n",
      "Epoch 62, Batch 17355, Loss: 206.70640563964844\n",
      "Epoch 62, Batch 17356, Loss: 185.36293029785156\n",
      "Epoch 62, Batch 17357, Loss: 163.20462036132812\n",
      "Epoch 62, Batch 17358, Loss: 186.33900451660156\n",
      "Epoch 62, Batch 17359, Loss: 174.1249542236328\n",
      "Epoch 62, Batch 17360, Loss: 174.7744140625\n",
      "Epoch 62, Batch 17361, Loss: 171.0528564453125\n",
      "Epoch 62, Batch 17362, Loss: 158.36380004882812\n",
      "Epoch 62, Batch 17363, Loss: 171.95115661621094\n",
      "Epoch 62, Batch 17364, Loss: 173.42063903808594\n",
      "Epoch 62, Batch 17365, Loss: 184.88046264648438\n",
      "Epoch 62, Batch 17366, Loss: 166.3685760498047\n",
      "Epoch 62, Batch 17367, Loss: 180.46710205078125\n",
      "Epoch 62, Batch 17368, Loss: 174.64515686035156\n",
      "Epoch 62, Batch 17369, Loss: 176.9631805419922\n",
      "Epoch 62, Batch 17370, Loss: 158.33351135253906\n",
      "Epoch 62, Batch 17371, Loss: 191.09713745117188\n",
      "Epoch 62, Batch 17372, Loss: 180.3212432861328\n",
      "Epoch 62, Batch 17373, Loss: 179.1185760498047\n",
      "Epoch 62, Batch 17374, Loss: 176.51602172851562\n",
      "Epoch 62, Batch 17375, Loss: 168.41311645507812\n",
      "Epoch 62, Batch 17376, Loss: 182.8798370361328\n",
      "Epoch 62, Batch 17377, Loss: 177.16949462890625\n",
      "Epoch 62, Batch 17378, Loss: 151.9426727294922\n",
      "Epoch 62, Batch 17379, Loss: 181.03170776367188\n",
      "Epoch 62, Batch 17380, Loss: 182.4117431640625\n",
      "Epoch 62, Batch 17381, Loss: 153.73696899414062\n",
      "Epoch 62, Batch 17382, Loss: 165.23306274414062\n",
      "Epoch 62, Batch 17383, Loss: 173.5130615234375\n",
      "Epoch 62, Batch 17384, Loss: 179.7230224609375\n",
      "Epoch 62, Batch 17385, Loss: 172.7584686279297\n",
      "Epoch 62, Batch 17386, Loss: 166.46697998046875\n",
      "Epoch 62, Batch 17387, Loss: 162.54034423828125\n",
      "Epoch 62, Batch 17388, Loss: 168.09254455566406\n",
      "Epoch 62, Batch 17389, Loss: 160.14024353027344\n",
      "Epoch 62, Batch 17390, Loss: 167.1936492919922\n",
      "Epoch 62, Batch 17391, Loss: 166.18386840820312\n",
      "Epoch 62, Batch 17392, Loss: 172.1809844970703\n",
      "Epoch 62, Batch 17393, Loss: 165.15115356445312\n",
      "Epoch 62, Batch 17394, Loss: 177.59716796875\n",
      "Epoch 62, Batch 17395, Loss: 177.80117797851562\n",
      "Epoch 62, Batch 17396, Loss: 167.28968811035156\n",
      "Epoch 62, Batch 17397, Loss: 182.64974975585938\n",
      "Epoch 62, Batch 17398, Loss: 174.71499633789062\n",
      "Epoch 62, Batch 17399, Loss: 178.46786499023438\n",
      "Epoch 62, Batch 17400, Loss: 179.10105895996094\n",
      "Epoch 62, Batch 17401, Loss: 191.67149353027344\n",
      "Epoch 62, Batch 17402, Loss: 167.0075225830078\n",
      "Epoch 62, Batch 17403, Loss: 184.10848999023438\n",
      "Epoch 62, Batch 17404, Loss: 166.14317321777344\n",
      "Epoch 62, Batch 17405, Loss: 180.06106567382812\n",
      "Epoch 62, Batch 17406, Loss: 162.4502410888672\n",
      "Epoch 62, Batch 17407, Loss: 165.53565979003906\n",
      "Epoch 62, Batch 17408, Loss: 165.54046630859375\n",
      "Epoch 62, Batch 17409, Loss: 176.12571716308594\n",
      "Epoch 62, Batch 17410, Loss: 175.51097106933594\n",
      "Epoch 62, Batch 17411, Loss: 168.2052764892578\n",
      "Epoch 62, Batch 17412, Loss: 181.71527099609375\n",
      "Epoch 62, Batch 17413, Loss: 170.27809143066406\n",
      "Epoch 62, Batch 17414, Loss: 164.54373168945312\n",
      "Epoch 62, Batch 17415, Loss: 169.54156494140625\n",
      "Epoch 62, Batch 17416, Loss: 176.0190887451172\n",
      "Epoch 62, Batch 17417, Loss: 163.088134765625\n",
      "Epoch 62, Batch 17418, Loss: 172.45504760742188\n",
      "Epoch 62, Batch 17419, Loss: 166.880615234375\n",
      "Epoch 62, Batch 17420, Loss: 156.56773376464844\n",
      "Epoch 62, Batch 17421, Loss: 174.8602294921875\n",
      "Epoch 62, Batch 17422, Loss: 171.0732879638672\n",
      "Epoch 62, Batch 17423, Loss: 170.39369201660156\n",
      "Epoch 62, Batch 17424, Loss: 176.89332580566406\n",
      "Epoch 62, Batch 17425, Loss: 169.130859375\n",
      "Epoch 62, Batch 17426, Loss: 187.93618774414062\n",
      "Epoch 62, Batch 17427, Loss: 160.86740112304688\n",
      "Epoch 62, Batch 17428, Loss: 170.90875244140625\n",
      "Epoch 62, Batch 17429, Loss: 171.63137817382812\n",
      "Epoch 62, Batch 17430, Loss: 165.72500610351562\n",
      "Epoch 62, Batch 17431, Loss: 183.34014892578125\n",
      "Epoch 62, Batch 17432, Loss: 176.48153686523438\n",
      "Epoch 62, Batch 17433, Loss: 179.01748657226562\n",
      "Epoch 62, Batch 17434, Loss: 175.9348602294922\n",
      "Epoch 62, Batch 17435, Loss: 166.84112548828125\n",
      "Epoch 62, Batch 17436, Loss: 161.66253662109375\n",
      "Epoch 62, Batch 17437, Loss: 176.5470733642578\n",
      "Epoch 62, Batch 17438, Loss: 163.4361572265625\n",
      "Epoch 62, Batch 17439, Loss: 181.93463134765625\n",
      "Epoch 62, Batch 17440, Loss: 183.01528930664062\n",
      "Epoch 62, Batch 17441, Loss: 167.9003143310547\n",
      "Epoch 62, Batch 17442, Loss: 168.05859375\n",
      "Epoch 62, Batch 17443, Loss: 148.06451416015625\n",
      "Epoch 62, Batch 17444, Loss: 175.05014038085938\n",
      "Epoch 62, Batch 17445, Loss: 163.5769805908203\n",
      "Epoch 62, Batch 17446, Loss: 174.6051788330078\n",
      "Epoch 62, Batch 17447, Loss: 186.7508087158203\n",
      "Epoch 62, Batch 17448, Loss: 172.1515350341797\n",
      "Epoch 62, Batch 17449, Loss: 178.6158905029297\n",
      "Epoch 62, Batch 17450, Loss: 168.8732452392578\n",
      "Epoch 62, Batch 17451, Loss: 182.20669555664062\n",
      "Epoch 62, Batch 17452, Loss: 184.01365661621094\n",
      "Epoch 62, Batch 17453, Loss: 175.55441284179688\n",
      "Epoch 62, Batch 17454, Loss: 163.0568084716797\n",
      "Epoch 62, Batch 17455, Loss: 177.29957580566406\n",
      "Epoch 62, Batch 17456, Loss: 165.19863891601562\n",
      "Epoch 62, Batch 17457, Loss: 167.14605712890625\n",
      "Epoch 62, Batch 17458, Loss: 168.5558624267578\n",
      "Epoch 62, Batch 17459, Loss: 162.36019897460938\n",
      "Epoch 62, Batch 17460, Loss: 154.73208618164062\n",
      "Epoch 62, Batch 17461, Loss: 177.11077880859375\n",
      "Epoch 62, Batch 17462, Loss: 182.536376953125\n",
      "Epoch 62, Batch 17463, Loss: 182.49339294433594\n",
      "Epoch 62, Batch 17464, Loss: 165.8240203857422\n",
      "Epoch 62, Batch 17465, Loss: 178.34878540039062\n",
      "Epoch 62, Batch 17466, Loss: 170.59234619140625\n",
      "Epoch 62, Batch 17467, Loss: 181.04176330566406\n",
      "Epoch 62, Batch 17468, Loss: 160.6996307373047\n",
      "Epoch 62, Batch 17469, Loss: 163.52992248535156\n",
      "Epoch 62, Batch 17470, Loss: 167.24652099609375\n",
      "Epoch 62, Batch 17471, Loss: 158.72872924804688\n",
      "Epoch 62, Batch 17472, Loss: 176.77676391601562\n",
      "Epoch 62, Batch 17473, Loss: 167.70220947265625\n",
      "Epoch 62, Batch 17474, Loss: 166.56431579589844\n",
      "Epoch 62, Batch 17475, Loss: 159.3401336669922\n",
      "Epoch 62, Batch 17476, Loss: 183.78717041015625\n",
      "Epoch 62, Batch 17477, Loss: 176.4965057373047\n",
      "Epoch 62, Batch 17478, Loss: 186.94639587402344\n",
      "Epoch 62, Batch 17479, Loss: 187.7844696044922\n",
      "Epoch 62, Batch 17480, Loss: 175.7939453125\n",
      "Epoch 62, Batch 17481, Loss: 182.01583862304688\n",
      "Epoch 62, Batch 17482, Loss: 185.4112091064453\n",
      "Epoch 62, Batch 17483, Loss: 182.2996826171875\n",
      "Epoch 62, Batch 17484, Loss: 169.02186584472656\n",
      "Epoch 62, Batch 17485, Loss: 173.09591674804688\n",
      "Epoch 62, Batch 17486, Loss: 170.7858428955078\n",
      "Epoch 62, Batch 17487, Loss: 165.35435485839844\n",
      "Epoch 62, Batch 17488, Loss: 158.44427490234375\n",
      "Epoch 62, Batch 17489, Loss: 169.8967742919922\n",
      "Epoch 62, Batch 17490, Loss: 170.17608642578125\n",
      "Epoch 62, Batch 17491, Loss: 172.13644409179688\n",
      "Epoch 62, Batch 17492, Loss: 175.97010803222656\n",
      "Epoch 62, Batch 17493, Loss: 170.4396209716797\n",
      "Epoch 62, Batch 17494, Loss: 170.0144500732422\n",
      "Epoch 62, Batch 17495, Loss: 174.93797302246094\n",
      "Epoch 62, Batch 17496, Loss: 179.46603393554688\n",
      "Epoch 62, Batch 17497, Loss: 171.3541259765625\n",
      "Epoch 62, Batch 17498, Loss: 170.4447021484375\n",
      "Epoch 62, Batch 17499, Loss: 170.4143829345703\n",
      "Epoch 62, Batch 17500, Loss: 179.53009033203125\n",
      "Epoch 62, Batch 17501, Loss: 160.73208618164062\n",
      "Epoch 62, Batch 17502, Loss: 160.39732360839844\n",
      "Epoch 62, Batch 17503, Loss: 197.7578887939453\n",
      "Epoch 62, Batch 17504, Loss: 169.96466064453125\n",
      "Epoch 62, Batch 17505, Loss: 176.24862670898438\n",
      "Epoch 62, Batch 17506, Loss: 187.67587280273438\n",
      "Epoch 62, Batch 17507, Loss: 165.20469665527344\n",
      "Epoch 62, Batch 17508, Loss: 169.3448486328125\n",
      "Epoch 62, Batch 17509, Loss: 167.7392578125\n",
      "Epoch 62, Batch 17510, Loss: 170.22845458984375\n",
      "Epoch 62, Batch 17511, Loss: 177.1553955078125\n",
      "Epoch 62, Batch 17512, Loss: 175.6848907470703\n",
      "Epoch 62, Batch 17513, Loss: 161.98919677734375\n",
      "Epoch 62, Batch 17514, Loss: 188.36842346191406\n",
      "Epoch 62, Batch 17515, Loss: 172.25204467773438\n",
      "Epoch 62, Batch 17516, Loss: 180.77955627441406\n",
      "Epoch 62, Batch 17517, Loss: 181.87814331054688\n",
      "Epoch 62, Batch 17518, Loss: 153.28196716308594\n",
      "Epoch 62, Batch 17519, Loss: 175.074951171875\n",
      "Epoch 62, Batch 17520, Loss: 167.6615753173828\n",
      "Epoch 62, Batch 17521, Loss: 181.7936248779297\n",
      "Epoch 62, Batch 17522, Loss: 169.0220947265625\n",
      "Epoch 62, Batch 17523, Loss: 168.6338348388672\n",
      "Epoch 62, Batch 17524, Loss: 166.71170043945312\n",
      "Epoch 62, Batch 17525, Loss: 175.0253448486328\n",
      "Epoch 62, Batch 17526, Loss: 189.2019500732422\n",
      "Epoch 62, Batch 17527, Loss: 164.20436096191406\n",
      "Epoch 62, Batch 17528, Loss: 163.1134796142578\n",
      "Epoch 62, Batch 17529, Loss: 178.86956787109375\n",
      "Epoch 62, Batch 17530, Loss: 177.48374938964844\n",
      "Epoch 62, Batch 17531, Loss: 167.09664916992188\n",
      "Epoch 62, Batch 17532, Loss: 145.45675659179688\n",
      "Epoch 62, Batch 17533, Loss: 183.99993896484375\n",
      "Epoch 62, Batch 17534, Loss: 164.26893615722656\n",
      "Epoch 62, Batch 17535, Loss: 183.7672576904297\n",
      "Epoch 62, Batch 17536, Loss: 190.95828247070312\n",
      "Epoch 62, Batch 17537, Loss: 179.1851806640625\n",
      "Epoch 62, Batch 17538, Loss: 172.3271942138672\n",
      "Epoch 62, Batch 17539, Loss: 173.05162048339844\n",
      "Epoch 62, Batch 17540, Loss: 185.3565216064453\n",
      "Epoch 62, Batch 17541, Loss: 184.7177734375\n",
      "Epoch 62, Batch 17542, Loss: 190.1138916015625\n",
      "Epoch 62, Batch 17543, Loss: 180.39111328125\n",
      "Epoch 62, Batch 17544, Loss: 160.72808837890625\n",
      "Epoch 62, Batch 17545, Loss: 170.4718017578125\n",
      "Epoch 62, Batch 17546, Loss: 167.81045532226562\n",
      "Epoch 62, Batch 17547, Loss: 177.15806579589844\n",
      "Epoch 62, Batch 17548, Loss: 182.0472412109375\n",
      "Epoch 62, Batch 17549, Loss: 162.55780029296875\n",
      "Epoch 62, Batch 17550, Loss: 170.56004333496094\n",
      "Epoch 62, Batch 17551, Loss: 187.23974609375\n",
      "Epoch 62, Batch 17552, Loss: 168.98976135253906\n",
      "Epoch 62, Batch 17553, Loss: 170.22836303710938\n",
      "Epoch 62, Batch 17554, Loss: 174.53427124023438\n",
      "Epoch 62, Batch 17555, Loss: 175.404296875\n",
      "Epoch 62, Batch 17556, Loss: 156.7569580078125\n",
      "Epoch 62, Batch 17557, Loss: 167.1733856201172\n",
      "Epoch 62, Batch 17558, Loss: 167.3804168701172\n",
      "Epoch 62, Batch 17559, Loss: 162.68516540527344\n",
      "Epoch 62, Batch 17560, Loss: 166.4958038330078\n",
      "Epoch 62, Batch 17561, Loss: 173.70530700683594\n",
      "Epoch 62, Batch 17562, Loss: 165.53712463378906\n",
      "Epoch 62, Batch 17563, Loss: 176.1471405029297\n",
      "Epoch 62, Batch 17564, Loss: 169.7209930419922\n",
      "Epoch 62, Batch 17565, Loss: 167.7971649169922\n",
      "Epoch 62, Batch 17566, Loss: 181.04379272460938\n",
      "Epoch 62, Batch 17567, Loss: 167.66680908203125\n",
      "Epoch 62, Batch 17568, Loss: 177.7012176513672\n",
      "Epoch 62, Batch 17569, Loss: 173.46620178222656\n",
      "Epoch 62, Batch 17570, Loss: 178.64442443847656\n",
      "Epoch 62, Batch 17571, Loss: 164.1298065185547\n",
      "Epoch 62, Batch 17572, Loss: 166.99708557128906\n",
      "Epoch 62, Batch 17573, Loss: 186.76499938964844\n",
      "Epoch 62, Batch 17574, Loss: 168.0461883544922\n",
      "Epoch 62, Batch 17575, Loss: 161.8561553955078\n",
      "Epoch 62, Batch 17576, Loss: 174.75450134277344\n",
      "Epoch 62, Batch 17577, Loss: 166.5391082763672\n",
      "Epoch 62, Batch 17578, Loss: 177.8251953125\n",
      "Epoch 62, Batch 17579, Loss: 194.9967803955078\n",
      "Epoch 62, Batch 17580, Loss: 177.46693420410156\n",
      "Epoch 62, Batch 17581, Loss: 153.22564697265625\n",
      "Epoch 62, Batch 17582, Loss: 176.2783660888672\n",
      "Epoch 62, Batch 17583, Loss: 155.5655059814453\n",
      "Epoch 62, Batch 17584, Loss: 166.30174255371094\n",
      "Epoch 62, Batch 17585, Loss: 186.61900329589844\n",
      "Epoch 62, Batch 17586, Loss: 169.74615478515625\n",
      "Epoch 62, Batch 17587, Loss: 154.0375213623047\n",
      "Epoch 62, Batch 17588, Loss: 163.1769561767578\n",
      "Epoch 62, Batch 17589, Loss: 177.51490783691406\n",
      "Epoch 62, Batch 17590, Loss: 181.38848876953125\n",
      "Epoch 62, Batch 17591, Loss: 170.04281616210938\n",
      "Epoch 62, Batch 17592, Loss: 171.24806213378906\n",
      "Epoch 62, Batch 17593, Loss: 157.48394775390625\n",
      "Epoch 62, Batch 17594, Loss: 195.48165893554688\n",
      "Epoch 62, Batch 17595, Loss: 166.63035583496094\n",
      "Epoch 62, Batch 17596, Loss: 166.9060821533203\n",
      "Epoch 62, Batch 17597, Loss: 163.963623046875\n",
      "Epoch 62, Batch 17598, Loss: 168.57078552246094\n",
      "Epoch 62, Batch 17599, Loss: 155.1400909423828\n",
      "Epoch 62, Batch 17600, Loss: 159.1021728515625\n",
      "Epoch 62, Batch 17601, Loss: 169.69223022460938\n",
      "Epoch 62, Batch 17602, Loss: 167.3146209716797\n",
      "Epoch 62, Batch 17603, Loss: 177.88429260253906\n",
      "Epoch 62, Batch 17604, Loss: 187.42730712890625\n",
      "Epoch 62, Batch 17605, Loss: 176.82958984375\n",
      "Epoch 62, Batch 17606, Loss: 164.68359375\n",
      "Epoch 62, Batch 17607, Loss: 172.11373901367188\n",
      "Epoch 62, Batch 17608, Loss: 151.48046875\n",
      "Epoch 62, Batch 17609, Loss: 180.332763671875\n",
      "Epoch 62, Batch 17610, Loss: 175.0946807861328\n",
      "Epoch 62, Batch 17611, Loss: 175.09732055664062\n",
      "Epoch 62, Batch 17612, Loss: 160.9324188232422\n",
      "Epoch 62, Batch 17613, Loss: 182.1781768798828\n",
      "Epoch 62, Batch 17614, Loss: 165.0513153076172\n",
      "Epoch 62, Batch 17615, Loss: 175.17233276367188\n",
      "Epoch 62, Batch 17616, Loss: 174.953125\n",
      "Epoch 62, Batch 17617, Loss: 182.66419982910156\n",
      "Epoch 62, Batch 17618, Loss: 180.1540985107422\n",
      "Epoch 62, Batch 17619, Loss: 186.7360382080078\n",
      "Epoch 62, Batch 17620, Loss: 178.94415283203125\n",
      "Epoch 62, Batch 17621, Loss: 184.28912353515625\n",
      "Epoch 62, Batch 17622, Loss: 183.4160614013672\n",
      "Epoch 62, Batch 17623, Loss: 165.12710571289062\n",
      "Epoch 62, Batch 17624, Loss: 160.82725524902344\n",
      "Epoch 62, Batch 17625, Loss: 174.44696044921875\n",
      "Epoch 62, Batch 17626, Loss: 183.35409545898438\n",
      "Epoch 62, Batch 17627, Loss: 166.4604949951172\n",
      "Epoch 62, Batch 17628, Loss: 175.8482208251953\n",
      "Epoch 62, Batch 17629, Loss: 185.52232360839844\n",
      "Epoch 62, Batch 17630, Loss: 177.99673461914062\n",
      "Epoch 62, Batch 17631, Loss: 161.96287536621094\n",
      "Epoch 62, Batch 17632, Loss: 167.459228515625\n",
      "Epoch 62, Batch 17633, Loss: 171.5402069091797\n",
      "Epoch 62, Batch 17634, Loss: 166.29026794433594\n",
      "Epoch 62, Batch 17635, Loss: 192.6479034423828\n",
      "Epoch 62, Batch 17636, Loss: 182.8935089111328\n",
      "Epoch 62, Batch 17637, Loss: 173.44662475585938\n",
      "Epoch 62, Batch 17638, Loss: 170.3126678466797\n",
      "Epoch 62, Batch 17639, Loss: 163.85324096679688\n",
      "Epoch 62, Batch 17640, Loss: 161.29022216796875\n",
      "Epoch 62, Batch 17641, Loss: 174.38368225097656\n",
      "Epoch 62, Batch 17642, Loss: 185.870849609375\n",
      "Epoch 62, Batch 17643, Loss: 165.62960815429688\n",
      "Epoch 62, Batch 17644, Loss: 166.99188232421875\n",
      "Epoch 62, Batch 17645, Loss: 173.52882385253906\n",
      "Epoch 62, Batch 17646, Loss: 162.02630615234375\n",
      "Epoch 62, Batch 17647, Loss: 180.9429168701172\n",
      "Epoch 62, Batch 17648, Loss: 172.32162475585938\n",
      "Epoch 62, Batch 17649, Loss: 174.90782165527344\n",
      "Epoch 62, Batch 17650, Loss: 186.47903442382812\n",
      "Epoch 62, Batch 17651, Loss: 177.87460327148438\n",
      "Epoch 62, Batch 17652, Loss: 176.65695190429688\n",
      "Epoch 62, Batch 17653, Loss: 175.36709594726562\n",
      "Epoch 62, Batch 17654, Loss: 188.0421905517578\n",
      "Epoch 62, Batch 17655, Loss: 166.89944458007812\n",
      "Epoch 62, Batch 17656, Loss: 188.284423828125\n",
      "Epoch 62, Batch 17657, Loss: 174.04270935058594\n",
      "Epoch 62, Batch 17658, Loss: 155.42848205566406\n",
      "Epoch 62, Batch 17659, Loss: 179.46420288085938\n",
      "Epoch 62, Batch 17660, Loss: 178.220703125\n",
      "Epoch 62, Batch 17661, Loss: 181.1255645751953\n",
      "Epoch 62, Batch 17662, Loss: 177.5539093017578\n",
      "Epoch 62, Batch 17663, Loss: 178.39126586914062\n",
      "Epoch 62, Batch 17664, Loss: 172.74339294433594\n",
      "Epoch 62, Batch 17665, Loss: 168.8525848388672\n",
      "Epoch 62, Batch 17666, Loss: 182.13768005371094\n",
      "Epoch 62, Batch 17667, Loss: 171.9486846923828\n",
      "Epoch 62, Batch 17668, Loss: 181.59967041015625\n",
      "Epoch 62, Batch 17669, Loss: 169.27987670898438\n",
      "Epoch 62, Batch 17670, Loss: 175.86550903320312\n",
      "Epoch 62, Batch 17671, Loss: 160.67202758789062\n",
      "Epoch 62, Batch 17672, Loss: 175.44227600097656\n",
      "Epoch 62, Batch 17673, Loss: 176.6766357421875\n",
      "Epoch 62, Batch 17674, Loss: 179.76333618164062\n",
      "Epoch 62, Batch 17675, Loss: 181.80287170410156\n",
      "Epoch 62, Batch 17676, Loss: 157.07142639160156\n",
      "Epoch 62, Batch 17677, Loss: 176.8031463623047\n",
      "Epoch 62, Batch 17678, Loss: 168.21987915039062\n",
      "Epoch 62, Batch 17679, Loss: 176.51553344726562\n",
      "Epoch 62, Batch 17680, Loss: 179.78004455566406\n",
      "Epoch 62, Batch 17681, Loss: 167.33103942871094\n",
      "Epoch 62, Batch 17682, Loss: 163.78553771972656\n",
      "Epoch 62, Batch 17683, Loss: 168.673583984375\n",
      "Epoch 62, Batch 17684, Loss: 165.0900115966797\n",
      "Epoch 62, Batch 17685, Loss: 175.90716552734375\n",
      "Epoch 62, Batch 17686, Loss: 186.70803833007812\n",
      "Epoch 62, Batch 17687, Loss: 174.81072998046875\n",
      "Epoch 62, Batch 17688, Loss: 172.63482666015625\n",
      "Epoch 62, Batch 17689, Loss: 172.8690185546875\n",
      "Epoch 62, Batch 17690, Loss: 163.57980346679688\n",
      "Epoch 62, Batch 17691, Loss: 161.76817321777344\n",
      "Epoch 62, Batch 17692, Loss: 167.95492553710938\n",
      "Epoch 62, Batch 17693, Loss: 186.79910278320312\n",
      "Epoch 62, Batch 17694, Loss: 173.42344665527344\n",
      "Epoch 62, Batch 17695, Loss: 180.14019775390625\n",
      "Epoch 62, Batch 17696, Loss: 173.45289611816406\n",
      "Epoch 62, Batch 17697, Loss: 178.48983764648438\n",
      "Epoch 62, Batch 17698, Loss: 169.8630828857422\n",
      "Epoch 62, Batch 17699, Loss: 172.8916015625\n",
      "Epoch 62, Batch 17700, Loss: 171.7615966796875\n",
      "Epoch 62, Batch 17701, Loss: 177.03260803222656\n",
      "Epoch 62, Batch 17702, Loss: 164.9006805419922\n",
      "Epoch 62, Batch 17703, Loss: 172.67733764648438\n",
      "Epoch 62, Batch 17704, Loss: 186.8137969970703\n",
      "Epoch 62, Batch 17705, Loss: 161.5425262451172\n",
      "Epoch 62, Batch 17706, Loss: 170.2498779296875\n",
      "Epoch 62, Batch 17707, Loss: 169.56240844726562\n",
      "Epoch 62, Batch 17708, Loss: 180.98863220214844\n",
      "Epoch 62, Batch 17709, Loss: 177.15032958984375\n",
      "Epoch 62, Batch 17710, Loss: 158.75225830078125\n",
      "Epoch 62, Batch 17711, Loss: 170.9175567626953\n",
      "Epoch 62, Batch 17712, Loss: 164.23155212402344\n",
      "Epoch 62, Batch 17713, Loss: 178.9808349609375\n",
      "Epoch 62, Batch 17714, Loss: 168.74464416503906\n",
      "Epoch 62, Batch 17715, Loss: 158.63694763183594\n",
      "Epoch 62, Batch 17716, Loss: 175.75146484375\n",
      "Epoch 62, Batch 17717, Loss: 202.417724609375\n",
      "Epoch 62, Batch 17718, Loss: 166.42514038085938\n",
      "Epoch 62, Batch 17719, Loss: 179.13479614257812\n",
      "Epoch 62, Batch 17720, Loss: 182.8323974609375\n",
      "Epoch 62, Batch 17721, Loss: 160.2349853515625\n",
      "Epoch 62, Batch 17722, Loss: 181.04367065429688\n",
      "Epoch 62, Batch 17723, Loss: 173.60992431640625\n",
      "Epoch 62, Batch 17724, Loss: 165.26873779296875\n",
      "Epoch 62, Batch 17725, Loss: 189.30770874023438\n",
      "Epoch 62, Batch 17726, Loss: 172.28741455078125\n",
      "Epoch 62, Batch 17727, Loss: 147.6879119873047\n",
      "Epoch 62, Batch 17728, Loss: 173.3057403564453\n",
      "Epoch 62, Batch 17729, Loss: 170.9485321044922\n",
      "Epoch 62, Batch 17730, Loss: 180.9977569580078\n",
      "Epoch 62, Batch 17731, Loss: 183.94610595703125\n",
      "Epoch 62, Batch 17732, Loss: 160.52293395996094\n",
      "Epoch 62, Batch 17733, Loss: 155.02520751953125\n",
      "Epoch 62, Batch 17734, Loss: 162.9617156982422\n",
      "Epoch 62, Batch 17735, Loss: 174.01470947265625\n",
      "Epoch 62, Batch 17736, Loss: 175.0192413330078\n",
      "Epoch 62, Batch 17737, Loss: 166.14305114746094\n",
      "Epoch 62, Batch 17738, Loss: 175.27857971191406\n",
      "Epoch 62, Batch 17739, Loss: 185.4927978515625\n",
      "Epoch 62, Batch 17740, Loss: 168.51339721679688\n",
      "Epoch 62, Batch 17741, Loss: 168.87254333496094\n",
      "Epoch 62, Batch 17742, Loss: 165.12649536132812\n",
      "Epoch 62, Batch 17743, Loss: 195.3971405029297\n",
      "Epoch 62, Batch 17744, Loss: 179.05227661132812\n",
      "Epoch 62, Batch 17745, Loss: 173.71302795410156\n",
      "Epoch 62, Batch 17746, Loss: 177.9095001220703\n",
      "Epoch 62, Batch 17747, Loss: 164.6635284423828\n",
      "Epoch 62, Batch 17748, Loss: 184.93637084960938\n",
      "Epoch 62, Batch 17749, Loss: 180.73414611816406\n",
      "Epoch 62, Batch 17750, Loss: 165.49481201171875\n",
      "Epoch 62, Batch 17751, Loss: 180.9944610595703\n",
      "Epoch 62, Batch 17752, Loss: 163.62806701660156\n",
      "Epoch 62, Batch 17753, Loss: 188.00222778320312\n",
      "Epoch 62, Batch 17754, Loss: 178.44029235839844\n",
      "Epoch 62, Batch 17755, Loss: 172.63002014160156\n",
      "Epoch 62, Batch 17756, Loss: 181.35284423828125\n",
      "Epoch 62, Batch 17757, Loss: 183.63595581054688\n",
      "Epoch 62, Batch 17758, Loss: 159.66995239257812\n",
      "Epoch 62, Batch 17759, Loss: 174.1645050048828\n",
      "Epoch 62, Batch 17760, Loss: 177.1910400390625\n",
      "Epoch 62, Batch 17761, Loss: 185.642578125\n",
      "Epoch 62, Batch 17762, Loss: 171.54867553710938\n",
      "Epoch 62, Batch 17763, Loss: 164.30172729492188\n",
      "Epoch 62, Batch 17764, Loss: 174.07814025878906\n",
      "Epoch 62, Batch 17765, Loss: 162.47645568847656\n",
      "Epoch 62, Batch 17766, Loss: 184.93601989746094\n",
      "Epoch 62, Batch 17767, Loss: 160.42779541015625\n",
      "Epoch 62, Batch 17768, Loss: 170.58331298828125\n",
      "Epoch 62, Batch 17769, Loss: 170.66285705566406\n",
      "Epoch 62, Batch 17770, Loss: 185.32391357421875\n",
      "Epoch 62, Batch 17771, Loss: 163.70018005371094\n",
      "Epoch 62, Batch 17772, Loss: 171.31216430664062\n",
      "Epoch 62, Batch 17773, Loss: 183.76124572753906\n",
      "Epoch 62, Batch 17774, Loss: 165.72409057617188\n",
      "Epoch 62, Batch 17775, Loss: 156.6622314453125\n",
      "Epoch 62, Batch 17776, Loss: 172.87388610839844\n",
      "Epoch 62, Batch 17777, Loss: 167.53076171875\n",
      "Epoch 62, Batch 17778, Loss: 157.14028930664062\n",
      "Epoch 62, Batch 17779, Loss: 184.71682739257812\n",
      "Epoch 62, Batch 17780, Loss: 162.81068420410156\n",
      "Epoch 62, Batch 17781, Loss: 162.9513397216797\n",
      "Epoch 62, Batch 17782, Loss: 173.31558227539062\n",
      "Epoch 62, Batch 17783, Loss: 164.89825439453125\n",
      "Epoch 62, Batch 17784, Loss: 173.41522216796875\n",
      "Epoch 62, Batch 17785, Loss: 191.6121063232422\n",
      "Epoch 62, Batch 17786, Loss: 179.52362060546875\n",
      "Epoch 62, Batch 17787, Loss: 172.5879669189453\n",
      "Epoch 62, Batch 17788, Loss: 186.97666931152344\n",
      "Epoch 62, Batch 17789, Loss: 158.2831573486328\n",
      "Epoch 62, Batch 17790, Loss: 159.94151306152344\n",
      "Epoch 62, Batch 17791, Loss: 186.97157287597656\n",
      "Epoch 62, Batch 17792, Loss: 174.7467041015625\n",
      "Epoch 62, Batch 17793, Loss: 154.71878051757812\n",
      "Epoch 62, Batch 17794, Loss: 182.54132080078125\n",
      "Epoch 62, Batch 17795, Loss: 166.77499389648438\n",
      "Epoch 62, Batch 17796, Loss: 156.79086303710938\n",
      "Epoch 62, Batch 17797, Loss: 169.6499481201172\n",
      "Epoch 62, Batch 17798, Loss: 177.20767211914062\n",
      "Epoch 62, Batch 17799, Loss: 172.7384033203125\n",
      "Epoch 62, Batch 17800, Loss: 178.3590545654297\n",
      "Epoch 62, Batch 17801, Loss: 167.95530700683594\n",
      "Epoch 62, Batch 17802, Loss: 174.85696411132812\n",
      "Epoch 62, Batch 17803, Loss: 188.12088012695312\n",
      "Epoch 62, Batch 17804, Loss: 190.52471923828125\n",
      "Epoch 62, Batch 17805, Loss: 165.40362548828125\n",
      "Epoch 62, Batch 17806, Loss: 168.1268768310547\n",
      "Epoch 62, Batch 17807, Loss: 159.0867462158203\n",
      "Epoch 62, Batch 17808, Loss: 153.6497039794922\n",
      "Epoch 62, Batch 17809, Loss: 169.32797241210938\n",
      "Epoch 62, Batch 17810, Loss: 153.80604553222656\n",
      "Epoch 62, Batch 17811, Loss: 185.915283203125\n",
      "Epoch 62, Batch 17812, Loss: 170.85260009765625\n",
      "Epoch 62, Batch 17813, Loss: 158.3194122314453\n",
      "Epoch 62, Batch 17814, Loss: 170.45887756347656\n",
      "Epoch 62, Batch 17815, Loss: 174.73851013183594\n",
      "Epoch 62, Batch 17816, Loss: 184.6431884765625\n",
      "Epoch 62, Batch 17817, Loss: 167.39205932617188\n",
      "Epoch 62, Batch 17818, Loss: 163.11111450195312\n",
      "Epoch 62, Batch 17819, Loss: 164.23825073242188\n",
      "Epoch 62, Batch 17820, Loss: 167.79637145996094\n",
      "Epoch 62, Batch 17821, Loss: 174.7217254638672\n",
      "Epoch 62, Batch 17822, Loss: 183.5524139404297\n",
      "Epoch 62, Batch 17823, Loss: 167.43661499023438\n",
      "Epoch 62, Batch 17824, Loss: 165.63494873046875\n",
      "Epoch 62, Batch 17825, Loss: 180.45407104492188\n",
      "Epoch 62, Batch 17826, Loss: 190.83465576171875\n",
      "Epoch 62, Batch 17827, Loss: 171.62086486816406\n",
      "Epoch 62, Batch 17828, Loss: 183.77359008789062\n",
      "Epoch 62, Batch 17829, Loss: 172.5928955078125\n",
      "Epoch 62, Batch 17830, Loss: 180.25091552734375\n",
      "Epoch 62, Batch 17831, Loss: 167.09722900390625\n",
      "Epoch 62, Batch 17832, Loss: 185.59107971191406\n",
      "Epoch 62, Batch 17833, Loss: 174.2603759765625\n",
      "Epoch 62, Batch 17834, Loss: 173.143310546875\n",
      "Epoch 62, Batch 17835, Loss: 181.97930908203125\n",
      "Epoch 62, Batch 17836, Loss: 161.09397888183594\n",
      "Epoch 62, Batch 17837, Loss: 179.5705108642578\n",
      "Epoch 62, Batch 17838, Loss: 168.53439331054688\n",
      "Epoch 62, Batch 17839, Loss: 159.5769805908203\n",
      "Epoch 62, Batch 17840, Loss: 186.8646240234375\n",
      "Epoch 62, Batch 17841, Loss: 170.6309356689453\n",
      "Epoch 62, Batch 17842, Loss: 193.78285217285156\n",
      "Epoch 62, Batch 17843, Loss: 166.6239013671875\n",
      "Epoch 62, Batch 17844, Loss: 167.49765014648438\n",
      "Epoch 62, Batch 17845, Loss: 169.12313842773438\n",
      "Epoch 62, Batch 17846, Loss: 164.49530029296875\n",
      "Epoch 62, Batch 17847, Loss: 171.1853485107422\n",
      "Epoch 62, Batch 17848, Loss: 176.61123657226562\n",
      "Epoch 62, Batch 17849, Loss: 176.77789306640625\n",
      "Epoch 62, Batch 17850, Loss: 191.32626342773438\n",
      "Epoch 62, Batch 17851, Loss: 181.637939453125\n",
      "Epoch 62, Batch 17852, Loss: 185.27577209472656\n",
      "Epoch 62, Batch 17853, Loss: 176.25953674316406\n",
      "Epoch 62, Batch 17854, Loss: 189.836669921875\n",
      "Epoch 62, Batch 17855, Loss: 170.53134155273438\n",
      "Epoch 62, Batch 17856, Loss: 180.5029296875\n",
      "Epoch 62, Batch 17857, Loss: 179.2479705810547\n",
      "Epoch 62, Batch 17858, Loss: 166.20558166503906\n",
      "Epoch 62, Batch 17859, Loss: 171.89529418945312\n",
      "Epoch 62, Batch 17860, Loss: 185.32608032226562\n",
      "Epoch 62, Batch 17861, Loss: 155.1290740966797\n",
      "Epoch 62, Batch 17862, Loss: 171.435546875\n",
      "Epoch 62, Batch 17863, Loss: 170.6322021484375\n",
      "Epoch 62, Batch 17864, Loss: 181.2585906982422\n",
      "Epoch 62, Batch 17865, Loss: 158.43592834472656\n",
      "Epoch 62, Batch 17866, Loss: 172.86703491210938\n",
      "Epoch 62, Batch 17867, Loss: 172.76150512695312\n",
      "Epoch 62, Batch 17868, Loss: 178.4442901611328\n",
      "Epoch 62, Batch 17869, Loss: 161.82901000976562\n",
      "Epoch 62, Batch 17870, Loss: 168.656982421875\n",
      "Epoch 62, Batch 17871, Loss: 172.509765625\n",
      "Epoch 62, Batch 17872, Loss: 182.2083740234375\n",
      "Epoch 62, Batch 17873, Loss: 179.33946228027344\n",
      "Epoch 62, Batch 17874, Loss: 189.7528076171875\n",
      "Epoch 62, Batch 17875, Loss: 174.5259246826172\n",
      "Epoch 62, Batch 17876, Loss: 166.44326782226562\n",
      "Epoch 62, Batch 17877, Loss: 172.80126953125\n",
      "Epoch 62, Batch 17878, Loss: 175.53663635253906\n",
      "Epoch 62, Batch 17879, Loss: 163.11819458007812\n",
      "Epoch 62, Batch 17880, Loss: 165.42857360839844\n",
      "Epoch 62, Batch 17881, Loss: 169.44375610351562\n",
      "Epoch 62, Batch 17882, Loss: 180.93960571289062\n",
      "Epoch 62, Batch 17883, Loss: 185.20314025878906\n",
      "Epoch 62, Batch 17884, Loss: 176.15589904785156\n",
      "Epoch 62, Batch 17885, Loss: 157.42709350585938\n",
      "Epoch 62, Batch 17886, Loss: 162.1343536376953\n",
      "Epoch 62, Batch 17887, Loss: 185.129150390625\n",
      "Epoch 62, Batch 17888, Loss: 183.12449645996094\n",
      "Epoch 62, Batch 17889, Loss: 174.03372192382812\n",
      "Epoch 62, Batch 17890, Loss: 187.62765502929688\n",
      "Epoch 62, Batch 17891, Loss: 155.01805114746094\n",
      "Epoch 62, Batch 17892, Loss: 171.6310577392578\n",
      "Epoch 62, Batch 17893, Loss: 164.8725128173828\n",
      "Epoch 62, Batch 17894, Loss: 172.65081787109375\n",
      "Epoch 62, Batch 17895, Loss: 174.6615447998047\n",
      "Epoch 62, Batch 17896, Loss: 198.5801544189453\n",
      "Epoch 62, Batch 17897, Loss: 183.36465454101562\n",
      "Epoch 62, Batch 17898, Loss: 170.7749481201172\n",
      "Epoch 62, Batch 17899, Loss: 169.93846130371094\n",
      "Epoch 62, Batch 17900, Loss: 165.95179748535156\n",
      "Epoch 62, Batch 17901, Loss: 161.25401306152344\n",
      "Epoch 62, Batch 17902, Loss: 167.44618225097656\n",
      "Epoch 62, Batch 17903, Loss: 158.30657958984375\n",
      "Epoch 62, Batch 17904, Loss: 178.91932678222656\n",
      "Epoch 62, Batch 17905, Loss: 175.75234985351562\n",
      "Epoch 62, Batch 17906, Loss: 145.72116088867188\n",
      "Epoch 62, Batch 17907, Loss: 187.7633056640625\n",
      "Epoch 62, Batch 17908, Loss: 177.05787658691406\n",
      "Epoch 62, Batch 17909, Loss: 169.9842987060547\n",
      "Epoch 62, Batch 17910, Loss: 179.18048095703125\n",
      "Epoch 62, Batch 17911, Loss: 173.02362060546875\n",
      "Epoch 62, Batch 17912, Loss: 166.9889373779297\n",
      "Epoch 62, Batch 17913, Loss: 178.87347412109375\n",
      "Epoch 62, Batch 17914, Loss: 183.19557189941406\n",
      "Epoch 62, Batch 17915, Loss: 152.18325805664062\n",
      "Epoch 62, Batch 17916, Loss: 165.10862731933594\n",
      "Epoch 62, Batch 17917, Loss: 153.8431396484375\n",
      "Epoch 62, Batch 17918, Loss: 163.9525909423828\n",
      "Epoch 62, Batch 17919, Loss: 170.2020263671875\n",
      "Epoch 62, Batch 17920, Loss: 177.97238159179688\n",
      "Epoch 62, Batch 17921, Loss: 168.09161376953125\n",
      "Epoch 62, Batch 17922, Loss: 177.34185791015625\n",
      "Epoch 62, Batch 17923, Loss: 184.90184020996094\n",
      "Epoch 62, Batch 17924, Loss: 158.07589721679688\n",
      "Epoch 62, Batch 17925, Loss: 172.15077209472656\n",
      "Epoch 62, Batch 17926, Loss: 157.3845977783203\n",
      "Epoch 62, Batch 17927, Loss: 175.33287048339844\n",
      "Epoch 62, Batch 17928, Loss: 182.04965209960938\n",
      "Epoch 62, Batch 17929, Loss: 173.54434204101562\n",
      "Epoch 62, Batch 17930, Loss: 192.04588317871094\n",
      "Epoch 62, Batch 17931, Loss: 174.7362823486328\n",
      "Epoch 62, Batch 17932, Loss: 162.24581909179688\n",
      "Epoch 62, Batch 17933, Loss: 168.8704833984375\n",
      "Epoch 62, Batch 17934, Loss: 165.53973388671875\n",
      "Epoch 62, Batch 17935, Loss: 175.6211700439453\n",
      "Epoch 62, Batch 17936, Loss: 158.42996215820312\n",
      "Epoch 62, Batch 17937, Loss: 172.5089111328125\n",
      "Epoch 62, Batch 17938, Loss: 184.65408325195312\n",
      "Epoch 62, Batch 17939, Loss: 170.45904541015625\n",
      "Epoch 62, Batch 17940, Loss: 165.0899658203125\n",
      "Epoch 62, Batch 17941, Loss: 174.94847106933594\n",
      "Epoch 62, Batch 17942, Loss: 172.91725158691406\n",
      "Epoch 62, Batch 17943, Loss: 173.6499786376953\n",
      "Epoch 62, Batch 17944, Loss: 163.71395874023438\n",
      "Epoch 62, Batch 17945, Loss: 176.07748413085938\n",
      "Epoch 62, Batch 17946, Loss: 186.0704345703125\n",
      "Epoch 62, Batch 17947, Loss: 161.52896118164062\n",
      "Epoch 62, Batch 17948, Loss: 183.29595947265625\n",
      "Epoch 62, Batch 17949, Loss: 154.2848663330078\n",
      "Epoch 62, Batch 17950, Loss: 171.85662841796875\n",
      "Epoch 62, Batch 17951, Loss: 182.9767608642578\n",
      "Epoch 62, Batch 17952, Loss: 165.76376342773438\n",
      "Epoch 62, Batch 17953, Loss: 171.5597381591797\n",
      "Epoch 62, Batch 17954, Loss: 175.66683959960938\n",
      "Epoch 62, Batch 17955, Loss: 180.16856384277344\n",
      "Epoch 62, Batch 17956, Loss: 163.4793701171875\n",
      "Epoch 62, Batch 17957, Loss: 166.15406799316406\n",
      "Epoch 62, Batch 17958, Loss: 167.62484741210938\n",
      "Epoch 62, Batch 17959, Loss: 181.76878356933594\n",
      "Epoch 62, Batch 17960, Loss: 162.46942138671875\n",
      "Epoch 62, Batch 17961, Loss: 164.5953369140625\n",
      "Epoch 62, Batch 17962, Loss: 187.2042236328125\n",
      "Epoch 62, Batch 17963, Loss: 165.48300170898438\n",
      "Epoch 62, Batch 17964, Loss: 166.86749267578125\n",
      "Epoch 62, Batch 17965, Loss: 166.33493041992188\n",
      "Epoch 62, Batch 17966, Loss: 186.0839080810547\n",
      "Epoch 62, Batch 17967, Loss: 167.73880004882812\n",
      "Epoch 62, Batch 17968, Loss: 192.79022216796875\n",
      "Epoch 62, Batch 17969, Loss: 163.5293426513672\n",
      "Epoch 62, Batch 17970, Loss: 176.1640625\n",
      "Epoch 62, Batch 17971, Loss: 164.80975341796875\n",
      "Epoch 62, Batch 17972, Loss: 171.6792755126953\n",
      "Epoch 62, Batch 17973, Loss: 160.946533203125\n",
      "Epoch 62, Batch 17974, Loss: 160.28199768066406\n",
      "Epoch 62, Batch 17975, Loss: 170.34561157226562\n",
      "Epoch 62, Batch 17976, Loss: 172.57240295410156\n",
      "Epoch 62, Batch 17977, Loss: 167.46063232421875\n",
      "Epoch 62, Batch 17978, Loss: 174.86715698242188\n",
      "Epoch 62, Batch 17979, Loss: 162.89344787597656\n",
      "Epoch 62, Batch 17980, Loss: 179.84927368164062\n",
      "Epoch 62, Batch 17981, Loss: 175.91175842285156\n",
      "Epoch 62, Batch 17982, Loss: 171.1335906982422\n",
      "Epoch 62, Batch 17983, Loss: 160.18771362304688\n",
      "Epoch 62, Batch 17984, Loss: 180.4566192626953\n",
      "Epoch 62, Batch 17985, Loss: 179.08929443359375\n",
      "Epoch 62, Batch 17986, Loss: 155.55889892578125\n",
      "Epoch 62, Batch 17987, Loss: 170.2223358154297\n",
      "Epoch 62, Batch 17988, Loss: 182.08612060546875\n",
      "Epoch 62, Batch 17989, Loss: 174.14590454101562\n",
      "Epoch 62, Batch 17990, Loss: 164.0017852783203\n",
      "Epoch 62, Batch 17991, Loss: 158.68051147460938\n",
      "Epoch 62, Batch 17992, Loss: 174.49766540527344\n",
      "Epoch 62, Batch 17993, Loss: 187.86398315429688\n",
      "Epoch 62, Batch 17994, Loss: 168.13180541992188\n",
      "Epoch 62, Batch 17995, Loss: 171.8169403076172\n",
      "Epoch 62, Batch 17996, Loss: 180.9285125732422\n",
      "Epoch 62, Batch 17997, Loss: 171.90113830566406\n",
      "Epoch 62, Batch 17998, Loss: 171.44625854492188\n",
      "Epoch 62, Batch 17999, Loss: 162.78578186035156\n",
      "Epoch 62, Batch 18000, Loss: 176.0221405029297\n",
      "Epoch 62, Batch 18001, Loss: 169.7592010498047\n",
      "Epoch 62, Batch 18002, Loss: 181.0021514892578\n",
      "Epoch 62, Batch 18003, Loss: 172.041748046875\n",
      "Epoch 62, Batch 18004, Loss: 169.1793212890625\n",
      "Epoch 62, Batch 18005, Loss: 167.01792907714844\n",
      "Epoch 62, Batch 18006, Loss: 167.3384552001953\n",
      "Epoch 62, Batch 18007, Loss: 202.2371826171875\n",
      "Epoch 62, Batch 18008, Loss: 171.13197326660156\n",
      "Epoch 62, Batch 18009, Loss: 184.17965698242188\n",
      "Epoch 62, Batch 18010, Loss: 185.27236938476562\n",
      "Epoch 62, Batch 18011, Loss: 172.67630004882812\n",
      "Epoch 62, Batch 18012, Loss: 170.39276123046875\n",
      "Epoch 62, Batch 18013, Loss: 171.08497619628906\n",
      "Epoch 62, Batch 18014, Loss: 167.8200225830078\n",
      "Epoch 62, Batch 18015, Loss: 172.52694702148438\n",
      "Epoch 62, Batch 18016, Loss: 180.76962280273438\n",
      "Epoch 62, Batch 18017, Loss: 179.0020294189453\n",
      "Epoch 62, Batch 18018, Loss: 168.7713165283203\n",
      "Epoch 62, Batch 18019, Loss: 167.00770568847656\n",
      "Epoch 62, Batch 18020, Loss: 157.2455291748047\n",
      "Epoch 62, Batch 18021, Loss: 181.69212341308594\n",
      "Epoch 62, Batch 18022, Loss: 176.34393310546875\n",
      "Epoch 62, Batch 18023, Loss: 148.9071044921875\n",
      "Epoch 62, Batch 18024, Loss: 162.68780517578125\n",
      "Epoch 62, Batch 18025, Loss: 180.10726928710938\n",
      "Epoch 62, Batch 18026, Loss: 160.27394104003906\n",
      "Epoch 62, Batch 18027, Loss: 162.95999145507812\n",
      "Epoch 62, Batch 18028, Loss: 166.13648986816406\n",
      "Epoch 62, Batch 18029, Loss: 176.1556854248047\n",
      "Epoch 62, Batch 18030, Loss: 172.91661071777344\n",
      "Epoch 62, Batch 18031, Loss: 175.55665588378906\n",
      "Epoch 62, Batch 18032, Loss: 181.7876739501953\n",
      "Epoch 62, Batch 18033, Loss: 185.48793029785156\n",
      "Epoch 62, Batch 18034, Loss: 170.6545867919922\n",
      "Epoch 62, Batch 18035, Loss: 192.38674926757812\n",
      "Epoch 62, Batch 18036, Loss: 167.648193359375\n",
      "Epoch 62, Batch 18037, Loss: 181.3238067626953\n",
      "Epoch 62, Batch 18038, Loss: 176.10980224609375\n",
      "Epoch 62, Batch 18039, Loss: 167.56259155273438\n",
      "Epoch 62, Batch 18040, Loss: 160.7593994140625\n",
      "Epoch 62, Batch 18041, Loss: 179.08848571777344\n",
      "Epoch 62, Batch 18042, Loss: 165.5357208251953\n",
      "Epoch 62, Batch 18043, Loss: 172.5045623779297\n",
      "Epoch 62, Batch 18044, Loss: 168.2552032470703\n",
      "Epoch 62, Batch 18045, Loss: 179.5259552001953\n",
      "Epoch 62, Batch 18046, Loss: 180.59217834472656\n",
      "Epoch 62, Batch 18047, Loss: 168.1780242919922\n",
      "Epoch 62, Batch 18048, Loss: 168.4266357421875\n",
      "Epoch 62, Batch 18049, Loss: 176.33578491210938\n",
      "Epoch 62, Batch 18050, Loss: 166.9925994873047\n",
      "Epoch 62, Batch 18051, Loss: 172.93753051757812\n",
      "Epoch 62, Batch 18052, Loss: 174.2074432373047\n",
      "Epoch 62, Batch 18053, Loss: 178.29864501953125\n",
      "Epoch 62, Batch 18054, Loss: 167.92153930664062\n",
      "Epoch 62, Batch 18055, Loss: 179.87161254882812\n",
      "Epoch 62, Batch 18056, Loss: 165.356689453125\n",
      "Epoch 62, Batch 18057, Loss: 187.62908935546875\n",
      "Epoch 62, Batch 18058, Loss: 166.0320587158203\n",
      "Epoch 62, Batch 18059, Loss: 187.7779083251953\n",
      "Epoch 62, Batch 18060, Loss: 160.916748046875\n",
      "Epoch 62, Batch 18061, Loss: 175.31097412109375\n",
      "Epoch 62, Batch 18062, Loss: 176.39817810058594\n",
      "Epoch 62, Batch 18063, Loss: 183.13168334960938\n",
      "Epoch 62, Batch 18064, Loss: 186.447509765625\n",
      "Epoch 62, Batch 18065, Loss: 179.4942626953125\n",
      "Epoch 62, Batch 18066, Loss: 167.84487915039062\n",
      "Epoch 62, Batch 18067, Loss: 183.6348114013672\n",
      "Epoch 62, Batch 18068, Loss: 182.05152893066406\n",
      "Epoch 62, Batch 18069, Loss: 167.3661346435547\n",
      "Epoch 62, Batch 18070, Loss: 175.0226593017578\n",
      "Epoch 62, Batch 18071, Loss: 178.0600128173828\n",
      "Epoch 62, Batch 18072, Loss: 162.0980987548828\n",
      "Epoch 62, Batch 18073, Loss: 174.1168212890625\n",
      "Epoch 62, Batch 18074, Loss: 177.22067260742188\n",
      "Epoch 62, Batch 18075, Loss: 178.8998260498047\n",
      "Epoch 62, Batch 18076, Loss: 174.03294372558594\n",
      "Epoch 62, Batch 18077, Loss: 163.41946411132812\n",
      "Epoch 62, Batch 18078, Loss: 188.18304443359375\n",
      "Epoch 62, Batch 18079, Loss: 175.7311553955078\n",
      "Epoch 62, Batch 18080, Loss: 160.92929077148438\n",
      "Epoch 62, Batch 18081, Loss: 168.0130615234375\n",
      "Epoch 62, Batch 18082, Loss: 174.53399658203125\n",
      "Epoch 62, Batch 18083, Loss: 184.35816955566406\n",
      "Epoch 62, Batch 18084, Loss: 173.7922821044922\n",
      "Epoch 62, Batch 18085, Loss: 179.30711364746094\n",
      "Epoch 62, Batch 18086, Loss: 188.96214294433594\n",
      "Epoch 62, Batch 18087, Loss: 195.5793914794922\n",
      "Epoch 62, Batch 18088, Loss: 185.73858642578125\n",
      "Epoch 62, Batch 18089, Loss: 189.27215576171875\n",
      "Epoch 62, Batch 18090, Loss: 164.89317321777344\n",
      "Epoch 62, Batch 18091, Loss: 184.69168090820312\n",
      "Epoch 62, Batch 18092, Loss: 170.36825561523438\n",
      "Epoch 62, Batch 18093, Loss: 165.09616088867188\n",
      "Epoch 62, Batch 18094, Loss: 172.30856323242188\n",
      "Epoch 62, Batch 18095, Loss: 168.193359375\n",
      "Epoch 62, Batch 18096, Loss: 160.87413024902344\n",
      "Epoch 62, Batch 18097, Loss: 175.2367401123047\n",
      "Epoch 62, Batch 18098, Loss: 161.8241729736328\n",
      "Epoch 62, Batch 18099, Loss: 165.98190307617188\n",
      "Epoch 62, Batch 18100, Loss: 172.9830780029297\n",
      "Epoch 62, Batch 18101, Loss: 174.50247192382812\n",
      "Epoch 62, Batch 18102, Loss: 188.32916259765625\n",
      "Epoch 62, Batch 18103, Loss: 182.2615966796875\n",
      "Epoch 62, Batch 18104, Loss: 162.12786865234375\n",
      "Epoch 62, Batch 18105, Loss: 164.22352600097656\n",
      "Epoch 62, Batch 18106, Loss: 170.5629119873047\n",
      "Epoch 62, Batch 18107, Loss: 180.361083984375\n",
      "Epoch 62, Batch 18108, Loss: 176.091796875\n",
      "Epoch 62, Batch 18109, Loss: 177.20619201660156\n",
      "Epoch 62, Batch 18110, Loss: 175.6225128173828\n",
      "Epoch 62, Batch 18111, Loss: 165.18902587890625\n",
      "Epoch 62, Batch 18112, Loss: 184.65638732910156\n",
      "Epoch 62, Batch 18113, Loss: 190.26687622070312\n",
      "Epoch 62, Batch 18114, Loss: 187.25\n",
      "Epoch 62, Batch 18115, Loss: 183.45556640625\n",
      "Epoch 62, Batch 18116, Loss: 164.39036560058594\n",
      "Epoch 62, Batch 18117, Loss: 175.82388305664062\n",
      "Epoch 62, Batch 18118, Loss: 174.11065673828125\n",
      "Epoch 62, Batch 18119, Loss: 171.08363342285156\n",
      "Epoch 62, Batch 18120, Loss: 158.40536499023438\n",
      "Epoch 62, Batch 18121, Loss: 170.59246826171875\n",
      "Epoch 62, Batch 18122, Loss: 165.3754425048828\n",
      "Epoch 62, Batch 18123, Loss: 178.98178100585938\n",
      "Epoch 62, Batch 18124, Loss: 164.66539001464844\n",
      "Epoch 62, Batch 18125, Loss: 182.4925994873047\n",
      "Epoch 62, Batch 18126, Loss: 189.01968383789062\n",
      "Epoch 62, Batch 18127, Loss: 177.0468292236328\n",
      "Epoch 62, Batch 18128, Loss: 172.9825439453125\n",
      "Epoch 62, Batch 18129, Loss: 152.80975341796875\n",
      "Epoch 62, Batch 18130, Loss: 168.02035522460938\n",
      "Epoch 62, Batch 18131, Loss: 162.1616973876953\n",
      "Epoch 62, Batch 18132, Loss: 174.37547302246094\n",
      "Epoch 62, Batch 18133, Loss: 172.64886474609375\n",
      "Epoch 62, Batch 18134, Loss: 183.40280151367188\n",
      "Epoch 62, Batch 18135, Loss: 168.5585174560547\n",
      "Epoch 62, Batch 18136, Loss: 173.2548370361328\n",
      "Epoch 62, Batch 18137, Loss: 156.9014434814453\n",
      "Epoch 62, Batch 18138, Loss: 165.8267364501953\n",
      "Epoch 62, Batch 18139, Loss: 178.79701232910156\n",
      "Epoch 62, Batch 18140, Loss: 176.2516632080078\n",
      "Epoch 62, Batch 18141, Loss: 165.54469299316406\n",
      "Epoch 62, Batch 18142, Loss: 164.1809539794922\n",
      "Epoch 62, Batch 18143, Loss: 180.13388061523438\n",
      "Epoch 62, Batch 18144, Loss: 194.3998565673828\n",
      "Epoch 62, Batch 18145, Loss: 169.26792907714844\n",
      "Epoch 62, Batch 18146, Loss: 169.9807891845703\n",
      "Epoch 62, Batch 18147, Loss: 178.269775390625\n",
      "Epoch 62, Batch 18148, Loss: 167.05992126464844\n",
      "Epoch 62, Batch 18149, Loss: 159.9935302734375\n",
      "Epoch 62, Batch 18150, Loss: 191.3209686279297\n",
      "Epoch 62, Batch 18151, Loss: 188.0355987548828\n",
      "Epoch 62, Batch 18152, Loss: 161.84664916992188\n",
      "Epoch 62, Batch 18153, Loss: 180.48716735839844\n",
      "Epoch 62, Batch 18154, Loss: 152.99147033691406\n",
      "Epoch 62, Batch 18155, Loss: 180.85455322265625\n",
      "Epoch 62, Batch 18156, Loss: 177.93251037597656\n",
      "Epoch 62, Batch 18157, Loss: 176.93862915039062\n",
      "Epoch 62, Batch 18158, Loss: 169.34474182128906\n",
      "Epoch 62, Batch 18159, Loss: 179.33985900878906\n",
      "Epoch 62, Batch 18160, Loss: 159.9166259765625\n",
      "Epoch 62, Batch 18161, Loss: 177.0044708251953\n",
      "Epoch 62, Batch 18162, Loss: 165.75924682617188\n",
      "Epoch 62, Batch 18163, Loss: 180.4858856201172\n",
      "Epoch 62, Batch 18164, Loss: 184.27923583984375\n",
      "Epoch 62, Batch 18165, Loss: 167.97640991210938\n",
      "Epoch 62, Batch 18166, Loss: 185.07196044921875\n",
      "Epoch 62, Batch 18167, Loss: 185.69192504882812\n",
      "Epoch 62, Batch 18168, Loss: 172.0174102783203\n",
      "Epoch 62, Batch 18169, Loss: 181.6268768310547\n",
      "Epoch 62, Batch 18170, Loss: 171.80685424804688\n",
      "Epoch 62, Batch 18171, Loss: 193.48867797851562\n",
      "Epoch 62, Batch 18172, Loss: 183.0669708251953\n",
      "Epoch 62, Batch 18173, Loss: 159.80068969726562\n",
      "Epoch 62, Batch 18174, Loss: 184.26718139648438\n",
      "Epoch 62, Batch 18175, Loss: 195.50909423828125\n",
      "Epoch 62, Batch 18176, Loss: 169.34564208984375\n",
      "Epoch 62, Batch 18177, Loss: 165.5251007080078\n",
      "Epoch 62, Batch 18178, Loss: 166.55404663085938\n",
      "Epoch 62, Batch 18179, Loss: 174.8940887451172\n",
      "Epoch 62, Batch 18180, Loss: 170.0778045654297\n",
      "Epoch 62, Batch 18181, Loss: 175.17930603027344\n",
      "Epoch 62, Batch 18182, Loss: 171.0546875\n",
      "Epoch 62, Batch 18183, Loss: 156.77210998535156\n",
      "Epoch 62, Batch 18184, Loss: 160.09068298339844\n",
      "Epoch 62, Batch 18185, Loss: 168.09637451171875\n",
      "Epoch 62, Batch 18186, Loss: 169.62283325195312\n",
      "Epoch 62, Batch 18187, Loss: 176.9657745361328\n",
      "Epoch 62, Batch 18188, Loss: 182.80374145507812\n",
      "Epoch 62, Batch 18189, Loss: 163.7720184326172\n",
      "Epoch 62, Batch 18190, Loss: 179.43960571289062\n",
      "Epoch 62, Batch 18191, Loss: 150.79446411132812\n",
      "Epoch 62, Batch 18192, Loss: 170.67276000976562\n",
      "Epoch 62, Batch 18193, Loss: 168.2908172607422\n",
      "Epoch 62, Batch 18194, Loss: 173.26637268066406\n",
      "Epoch 62, Batch 18195, Loss: 170.9150848388672\n",
      "Epoch 62, Batch 18196, Loss: 152.50437927246094\n",
      "Epoch 62, Batch 18197, Loss: 183.7612762451172\n",
      "Epoch 62, Batch 18198, Loss: 165.70651245117188\n",
      "Epoch 62, Batch 18199, Loss: 162.7228240966797\n",
      "Epoch 62, Batch 18200, Loss: 168.7510223388672\n",
      "Epoch 62, Batch 18201, Loss: 163.50961303710938\n",
      "Epoch 62, Batch 18202, Loss: 180.55255126953125\n",
      "Epoch 62, Batch 18203, Loss: 166.38528442382812\n",
      "Epoch 62, Batch 18204, Loss: 174.80575561523438\n",
      "Epoch 62, Batch 18205, Loss: 175.77500915527344\n",
      "Epoch 62, Batch 18206, Loss: 171.10174560546875\n",
      "Epoch 62, Batch 18207, Loss: 172.7811279296875\n",
      "Epoch 62, Batch 18208, Loss: 169.4182586669922\n",
      "Epoch 62, Batch 18209, Loss: 181.50308227539062\n",
      "Epoch 62, Batch 18210, Loss: 166.0655517578125\n",
      "Epoch 62, Batch 18211, Loss: 176.4476318359375\n",
      "Epoch 62, Batch 18212, Loss: 168.166259765625\n",
      "Epoch 62, Batch 18213, Loss: 158.26048278808594\n",
      "Epoch 62, Batch 18214, Loss: 176.28492736816406\n",
      "Epoch 62, Batch 18215, Loss: 171.04150390625\n",
      "Epoch 62, Batch 18216, Loss: 175.6160430908203\n",
      "Epoch 62, Batch 18217, Loss: 174.82553100585938\n",
      "Epoch 62, Batch 18218, Loss: 154.24945068359375\n",
      "Epoch 62, Batch 18219, Loss: 167.35862731933594\n",
      "Epoch 62, Batch 18220, Loss: 177.93214416503906\n",
      "Epoch 62, Batch 18221, Loss: 180.02330017089844\n",
      "Epoch 62, Batch 18222, Loss: 177.6634521484375\n",
      "Epoch 62, Batch 18223, Loss: 171.11044311523438\n",
      "Epoch 62, Batch 18224, Loss: 172.8033447265625\n",
      "Epoch 62, Batch 18225, Loss: 185.61949157714844\n",
      "Epoch 62, Batch 18226, Loss: 182.1576385498047\n",
      "Epoch 62, Batch 18227, Loss: 159.94216918945312\n",
      "Epoch 62, Batch 18228, Loss: 173.2315673828125\n",
      "Epoch 62, Batch 18229, Loss: 168.11083984375\n",
      "Epoch 62, Batch 18230, Loss: 183.23846435546875\n",
      "Epoch 62, Batch 18231, Loss: 187.3037872314453\n",
      "Epoch 62, Batch 18232, Loss: 167.37086486816406\n",
      "Epoch 62, Batch 18233, Loss: 193.65345764160156\n",
      "Epoch 62, Batch 18234, Loss: 172.18601989746094\n",
      "Epoch 62, Batch 18235, Loss: 177.20431518554688\n",
      "Epoch 62, Batch 18236, Loss: 164.0990447998047\n",
      "Epoch 62, Batch 18237, Loss: 166.5188446044922\n",
      "Epoch 62, Batch 18238, Loss: 166.96226501464844\n",
      "Epoch 62, Batch 18239, Loss: 172.671142578125\n",
      "Epoch 62, Batch 18240, Loss: 156.69215393066406\n",
      "Epoch 62, Batch 18241, Loss: 176.62835693359375\n",
      "Epoch 62, Batch 18242, Loss: 172.25344848632812\n",
      "Epoch 62, Batch 18243, Loss: 174.92327880859375\n",
      "Epoch 62, Batch 18244, Loss: 164.3547821044922\n",
      "Epoch 62, Batch 18245, Loss: 166.01382446289062\n",
      "Epoch 62, Batch 18246, Loss: 153.36717224121094\n",
      "Epoch 62, Batch 18247, Loss: 164.82723999023438\n",
      "Epoch 62, Batch 18248, Loss: 173.34059143066406\n",
      "Epoch 62, Batch 18249, Loss: 176.08668518066406\n",
      "Epoch 62, Batch 18250, Loss: 180.05177307128906\n",
      "Epoch 62, Batch 18251, Loss: 173.15330505371094\n",
      "Epoch 62, Batch 18252, Loss: 172.3858642578125\n",
      "Epoch 62, Batch 18253, Loss: 151.87185668945312\n",
      "Epoch 62, Batch 18254, Loss: 175.42526245117188\n",
      "Epoch 62, Batch 18255, Loss: 165.22340393066406\n",
      "Epoch 62, Batch 18256, Loss: 190.54974365234375\n",
      "Epoch 62, Batch 18257, Loss: 189.6414794921875\n",
      "Epoch 62, Batch 18258, Loss: 178.50692749023438\n",
      "Epoch 62, Batch 18259, Loss: 192.1019744873047\n",
      "Epoch 62, Batch 18260, Loss: 172.05252075195312\n",
      "Epoch 62, Batch 18261, Loss: 181.4687042236328\n",
      "Epoch 62, Batch 18262, Loss: 187.57249450683594\n",
      "Epoch 62, Batch 18263, Loss: 185.31588745117188\n",
      "Epoch 62, Batch 18264, Loss: 172.04086303710938\n",
      "Epoch 62, Batch 18265, Loss: 194.18568420410156\n",
      "Epoch 62, Batch 18266, Loss: 162.91175842285156\n",
      "Epoch 62, Batch 18267, Loss: 165.19998168945312\n",
      "Epoch 62, Batch 18268, Loss: 167.0799560546875\n",
      "Epoch 62, Batch 18269, Loss: 155.29510498046875\n",
      "Epoch 62, Batch 18270, Loss: 157.75344848632812\n",
      "Epoch 62, Batch 18271, Loss: 160.3311004638672\n",
      "Epoch 62, Batch 18272, Loss: 175.32763671875\n",
      "Epoch 62, Batch 18273, Loss: 166.1026153564453\n",
      "Epoch 62, Batch 18274, Loss: 169.0642852783203\n",
      "Epoch 62, Batch 18275, Loss: 172.14686584472656\n",
      "Epoch 62, Batch 18276, Loss: 173.81942749023438\n",
      "Epoch 62, Batch 18277, Loss: 175.95579528808594\n",
      "Epoch 62, Batch 18278, Loss: 187.58213806152344\n",
      "Epoch 62, Batch 18279, Loss: 181.67840576171875\n",
      "Epoch 62, Batch 18280, Loss: 176.3347930908203\n",
      "Epoch 62, Batch 18281, Loss: 177.49591064453125\n",
      "Epoch 62, Batch 18282, Loss: 168.11032104492188\n",
      "Epoch 62, Batch 18283, Loss: 166.099609375\n",
      "Epoch 62, Batch 18284, Loss: 176.68653869628906\n",
      "Epoch 62, Batch 18285, Loss: 163.80514526367188\n",
      "Epoch 62, Batch 18286, Loss: 168.1988067626953\n",
      "Epoch 62, Batch 18287, Loss: 162.1584014892578\n",
      "Epoch 62, Batch 18288, Loss: 182.6669464111328\n",
      "Epoch 62, Batch 18289, Loss: 168.05223083496094\n",
      "Epoch 62, Batch 18290, Loss: 162.620849609375\n",
      "Epoch 62, Batch 18291, Loss: 170.82278442382812\n",
      "Epoch 62, Batch 18292, Loss: 171.7704620361328\n",
      "Epoch 62, Batch 18293, Loss: 158.78546142578125\n",
      "Epoch 62, Batch 18294, Loss: 164.60836791992188\n",
      "Epoch 62, Batch 18295, Loss: 164.14517211914062\n",
      "Epoch 62, Batch 18296, Loss: 174.72113037109375\n",
      "Epoch 62, Batch 18297, Loss: 180.80825805664062\n",
      "Epoch 62, Batch 18298, Loss: 174.49354553222656\n",
      "Epoch 62, Batch 18299, Loss: 166.97250366210938\n",
      "Epoch 62, Batch 18300, Loss: 187.6876220703125\n",
      "Epoch 62, Batch 18301, Loss: 178.10540771484375\n",
      "Epoch 62, Batch 18302, Loss: 158.50477600097656\n",
      "Epoch 62, Batch 18303, Loss: 190.2783203125\n",
      "Epoch 62, Batch 18304, Loss: 166.3938446044922\n",
      "Epoch 62, Batch 18305, Loss: 173.63824462890625\n",
      "Epoch 62, Batch 18306, Loss: 173.974365234375\n",
      "Epoch 62, Batch 18307, Loss: 166.09495544433594\n",
      "Epoch 62, Batch 18308, Loss: 159.4914093017578\n",
      "Epoch 62, Batch 18309, Loss: 177.8385009765625\n",
      "Epoch 62, Batch 18310, Loss: 174.81240844726562\n",
      "Epoch 62, Batch 18311, Loss: 156.40672302246094\n",
      "Epoch 62, Batch 18312, Loss: 177.50074768066406\n",
      "Epoch 62, Batch 18313, Loss: 174.86558532714844\n",
      "Epoch 62, Batch 18314, Loss: 170.1839141845703\n",
      "Epoch 62, Batch 18315, Loss: 179.467529296875\n",
      "Epoch 62, Batch 18316, Loss: 174.22232055664062\n",
      "Epoch 62, Batch 18317, Loss: 165.4208221435547\n",
      "Epoch 62, Batch 18318, Loss: 179.63876342773438\n",
      "Epoch 62, Batch 18319, Loss: 186.56192016601562\n",
      "Epoch 62, Batch 18320, Loss: 166.72451782226562\n",
      "Epoch 62, Batch 18321, Loss: 162.7568359375\n",
      "Epoch 62, Batch 18322, Loss: 173.4702911376953\n",
      "Epoch 62, Batch 18323, Loss: 164.81396484375\n",
      "Epoch 62, Batch 18324, Loss: 166.56434631347656\n",
      "Epoch 62, Batch 18325, Loss: 169.68679809570312\n",
      "Epoch 62, Batch 18326, Loss: 171.64039611816406\n",
      "Epoch 62, Batch 18327, Loss: 159.4119873046875\n",
      "Epoch 62, Batch 18328, Loss: 158.28541564941406\n",
      "Epoch 62, Batch 18329, Loss: 160.63340759277344\n",
      "Epoch 62, Batch 18330, Loss: 185.05215454101562\n",
      "Epoch 62, Batch 18331, Loss: 173.9697723388672\n",
      "Epoch 62, Batch 18332, Loss: 170.74588012695312\n",
      "Epoch 62, Batch 18333, Loss: 166.22915649414062\n",
      "Epoch 62, Batch 18334, Loss: 168.97137451171875\n",
      "Epoch 62, Batch 18335, Loss: 171.44190979003906\n",
      "Epoch 62, Batch 18336, Loss: 183.5212860107422\n",
      "Epoch 62, Batch 18337, Loss: 176.11654663085938\n",
      "Epoch 62, Batch 18338, Loss: 153.11647033691406\n",
      "Epoch 62, Batch 18339, Loss: 172.09005737304688\n",
      "Epoch 62, Batch 18340, Loss: 161.8390350341797\n",
      "Epoch 62, Batch 18341, Loss: 174.4276123046875\n",
      "Epoch 62, Batch 18342, Loss: 156.68479919433594\n",
      "Epoch 62, Batch 18343, Loss: 182.70452880859375\n",
      "Epoch 62, Batch 18344, Loss: 160.57290649414062\n",
      "Epoch 62, Batch 18345, Loss: 177.49020385742188\n",
      "Epoch 62, Batch 18346, Loss: 191.90736389160156\n",
      "Epoch 62, Batch 18347, Loss: 161.92227172851562\n",
      "Epoch 62, Batch 18348, Loss: 154.9085235595703\n",
      "Epoch 62, Batch 18349, Loss: 179.93519592285156\n",
      "Epoch 62, Batch 18350, Loss: 178.06785583496094\n",
      "Epoch 62, Batch 18351, Loss: 185.38670349121094\n",
      "Epoch 62, Batch 18352, Loss: 181.431884765625\n",
      "Epoch 62, Batch 18353, Loss: 156.06100463867188\n",
      "Epoch 62, Batch 18354, Loss: 173.2743377685547\n",
      "Epoch 62, Batch 18355, Loss: 166.23167419433594\n",
      "Epoch 62, Batch 18356, Loss: 163.28347778320312\n",
      "Epoch 62, Batch 18357, Loss: 164.3804473876953\n",
      "Epoch 62, Batch 18358, Loss: 156.21607971191406\n",
      "Epoch 62, Batch 18359, Loss: 185.36044311523438\n",
      "Epoch 62, Batch 18360, Loss: 189.77854919433594\n",
      "Epoch 62, Batch 18361, Loss: 181.75604248046875\n",
      "Epoch 62, Batch 18362, Loss: 175.59149169921875\n",
      "Epoch 62, Batch 18363, Loss: 171.46197509765625\n",
      "Epoch 62, Batch 18364, Loss: 187.44937133789062\n",
      "Epoch 62, Batch 18365, Loss: 173.75262451171875\n",
      "Epoch 62, Batch 18366, Loss: 184.9076385498047\n",
      "Epoch 62, Batch 18367, Loss: 158.25564575195312\n",
      "Epoch 62, Batch 18368, Loss: 149.67019653320312\n",
      "Epoch 62, Batch 18369, Loss: 177.88790893554688\n",
      "Epoch 62, Batch 18370, Loss: 186.72146606445312\n",
      "Epoch 62, Batch 18371, Loss: 169.0556640625\n",
      "Epoch 62, Batch 18372, Loss: 182.74440002441406\n",
      "Epoch 62, Batch 18373, Loss: 166.39566040039062\n",
      "Epoch 62, Batch 18374, Loss: 168.4312286376953\n",
      "Epoch 62, Batch 18375, Loss: 175.09585571289062\n",
      "Epoch 62, Batch 18376, Loss: 169.01702880859375\n",
      "Epoch 62, Batch 18377, Loss: 159.84156799316406\n",
      "Epoch 62, Batch 18378, Loss: 172.90130615234375\n",
      "Epoch 62, Batch 18379, Loss: 185.8339385986328\n",
      "Epoch 62, Batch 18380, Loss: 171.72894287109375\n",
      "Epoch 62, Batch 18381, Loss: 174.3982696533203\n",
      "Epoch 62, Batch 18382, Loss: 179.36050415039062\n",
      "Epoch 62, Batch 18383, Loss: 179.09027099609375\n",
      "Epoch 62, Batch 18384, Loss: 182.84437561035156\n",
      "Epoch 62, Batch 18385, Loss: 157.6363983154297\n",
      "Epoch 62, Batch 18386, Loss: 184.43328857421875\n",
      "Epoch 62, Batch 18387, Loss: 168.614990234375\n",
      "Epoch 62, Batch 18388, Loss: 165.3791046142578\n",
      "Epoch 62, Batch 18389, Loss: 177.46737670898438\n",
      "Epoch 62, Batch 18390, Loss: 170.14456176757812\n",
      "Epoch 62, Batch 18391, Loss: 178.3440399169922\n",
      "Epoch 62, Batch 18392, Loss: 184.06881713867188\n",
      "Epoch 62, Batch 18393, Loss: 191.7855987548828\n",
      "Epoch 62, Batch 18394, Loss: 141.52513122558594\n",
      "Epoch 62, Batch 18395, Loss: 164.80677795410156\n",
      "Epoch 62, Batch 18396, Loss: 163.3761749267578\n",
      "Epoch 62, Batch 18397, Loss: 158.45040893554688\n",
      "Epoch 62, Batch 18398, Loss: 173.08914184570312\n",
      "Epoch 62, Batch 18399, Loss: 167.50653076171875\n",
      "Epoch 62, Batch 18400, Loss: 159.5016326904297\n",
      "Epoch 62, Batch 18401, Loss: 176.34234619140625\n",
      "Epoch 62, Batch 18402, Loss: 193.5370330810547\n",
      "Epoch 62, Batch 18403, Loss: 167.69418334960938\n",
      "Epoch 62, Batch 18404, Loss: 161.24656677246094\n",
      "Epoch 62, Batch 18405, Loss: 168.2154541015625\n",
      "Epoch 62, Batch 18406, Loss: 147.90045166015625\n",
      "Epoch 62, Batch 18407, Loss: 176.2047119140625\n",
      "Epoch 62, Batch 18408, Loss: 168.59681701660156\n",
      "Epoch 62, Batch 18409, Loss: 169.8807373046875\n",
      "Epoch 62, Batch 18410, Loss: 173.95985412597656\n",
      "Epoch 62, Batch 18411, Loss: 147.91879272460938\n",
      "Epoch 62, Batch 18412, Loss: 173.8827667236328\n",
      "Epoch 62, Batch 18413, Loss: 173.1289825439453\n",
      "Epoch 62, Batch 18414, Loss: 163.6427764892578\n",
      "Epoch 62, Batch 18415, Loss: 157.03257751464844\n",
      "Epoch 62, Batch 18416, Loss: 170.51795959472656\n",
      "Epoch 62, Batch 18417, Loss: 158.37045288085938\n",
      "Epoch 62, Batch 18418, Loss: 185.82228088378906\n",
      "Epoch 62, Batch 18419, Loss: 175.98501586914062\n",
      "Epoch 62, Batch 18420, Loss: 180.3963165283203\n",
      "Epoch 62, Batch 18421, Loss: 163.7640380859375\n",
      "Epoch 62, Batch 18422, Loss: 160.97589111328125\n",
      "Epoch 62, Batch 18423, Loss: 164.207763671875\n",
      "Epoch 62, Batch 18424, Loss: 182.85833740234375\n",
      "Epoch 62, Batch 18425, Loss: 168.9420928955078\n",
      "Epoch 62, Batch 18426, Loss: 179.97732543945312\n",
      "Epoch 62, Batch 18427, Loss: 181.88063049316406\n",
      "Epoch 62, Batch 18428, Loss: 182.25894165039062\n",
      "Epoch 62, Batch 18429, Loss: 165.30303955078125\n",
      "Epoch 62, Batch 18430, Loss: 185.0749969482422\n",
      "Epoch 62, Batch 18431, Loss: 181.7473602294922\n",
      "Epoch 62, Batch 18432, Loss: 167.81991577148438\n",
      "Epoch 62, Batch 18433, Loss: 169.39407348632812\n",
      "Epoch 62, Batch 18434, Loss: 178.4760284423828\n",
      "Epoch 62, Batch 18435, Loss: 181.72613525390625\n",
      "Epoch 62, Batch 18436, Loss: 185.27854919433594\n",
      "Epoch 62, Batch 18437, Loss: 172.8875274658203\n",
      "Epoch 62, Batch 18438, Loss: 177.7509002685547\n",
      "Epoch 62, Batch 18439, Loss: 160.4366455078125\n",
      "Epoch 62, Batch 18440, Loss: 178.481201171875\n",
      "Epoch 62, Batch 18441, Loss: 166.38803100585938\n",
      "Epoch 62, Batch 18442, Loss: 179.36192321777344\n",
      "Epoch 62, Batch 18443, Loss: 170.42271423339844\n",
      "Epoch 62, Batch 18444, Loss: 161.50283813476562\n",
      "Epoch 62, Batch 18445, Loss: 179.36537170410156\n",
      "Epoch 62, Batch 18446, Loss: 189.05142211914062\n",
      "Epoch 62, Batch 18447, Loss: 175.45486450195312\n",
      "Epoch 62, Batch 18448, Loss: 183.6502227783203\n",
      "Epoch 62, Batch 18449, Loss: 184.9691925048828\n",
      "Epoch 62, Batch 18450, Loss: 171.5780792236328\n",
      "Epoch 62, Batch 18451, Loss: 186.92417907714844\n",
      "Epoch 62, Batch 18452, Loss: 179.88021850585938\n",
      "Epoch 62, Batch 18453, Loss: 173.69444274902344\n",
      "Epoch 62, Batch 18454, Loss: 177.8993377685547\n",
      "Epoch 62, Batch 18455, Loss: 181.88552856445312\n",
      "Epoch 62, Batch 18456, Loss: 174.64785766601562\n",
      "Epoch 62, Batch 18457, Loss: 170.85365295410156\n",
      "Epoch 62, Batch 18458, Loss: 172.03402709960938\n",
      "Epoch 62, Batch 18459, Loss: 175.3076171875\n",
      "Epoch 62, Batch 18460, Loss: 167.3920440673828\n",
      "Epoch 62, Batch 18461, Loss: 181.81825256347656\n",
      "Epoch 62, Batch 18462, Loss: 188.09951782226562\n",
      "Epoch 62, Batch 18463, Loss: 169.07420349121094\n",
      "Epoch 62, Batch 18464, Loss: 171.38748168945312\n",
      "Epoch 62, Batch 18465, Loss: 171.79563903808594\n",
      "Epoch 62, Batch 18466, Loss: 171.12789916992188\n",
      "Epoch 62, Batch 18467, Loss: 181.28282165527344\n",
      "Epoch 62, Batch 18468, Loss: 166.44110107421875\n",
      "Epoch 62, Batch 18469, Loss: 189.83624267578125\n",
      "Epoch 62, Batch 18470, Loss: 185.99400329589844\n",
      "Epoch 62, Batch 18471, Loss: 170.73341369628906\n",
      "Epoch 62, Batch 18472, Loss: 176.74742126464844\n",
      "Epoch 62, Batch 18473, Loss: 163.01116943359375\n",
      "Epoch 62, Batch 18474, Loss: 165.6671600341797\n",
      "Epoch 62, Batch 18475, Loss: 162.65045166015625\n",
      "Epoch 62, Batch 18476, Loss: 186.4612274169922\n",
      "Epoch 62, Batch 18477, Loss: 190.56149291992188\n",
      "Epoch 62, Batch 18478, Loss: 179.4907684326172\n",
      "Epoch 62, Batch 18479, Loss: 172.08914184570312\n",
      "Epoch 62, Batch 18480, Loss: 173.77081298828125\n",
      "Epoch 62, Batch 18481, Loss: 154.9674530029297\n",
      "Epoch 62, Batch 18482, Loss: 187.1455841064453\n",
      "Epoch 62, Batch 18483, Loss: 168.3142547607422\n",
      "Epoch 62, Batch 18484, Loss: 177.11216735839844\n",
      "Epoch 62, Batch 18485, Loss: 164.50027465820312\n",
      "Epoch 62, Batch 18486, Loss: 168.92572021484375\n",
      "Epoch 62, Batch 18487, Loss: 174.2033233642578\n",
      "Epoch 62, Batch 18488, Loss: 172.4475555419922\n",
      "Epoch 62, Batch 18489, Loss: 173.60581970214844\n",
      "Epoch 62, Batch 18490, Loss: 180.82278442382812\n",
      "Epoch 62, Batch 18491, Loss: 183.1884307861328\n",
      "Epoch 62, Batch 18492, Loss: 194.9319305419922\n",
      "Epoch 62, Batch 18493, Loss: 184.982421875\n",
      "Epoch 62, Batch 18494, Loss: 158.37171936035156\n",
      "Epoch 62, Batch 18495, Loss: 157.33982849121094\n",
      "Epoch 62, Batch 18496, Loss: 155.95542907714844\n",
      "Epoch 62, Batch 18497, Loss: 171.4357452392578\n",
      "Epoch 62, Batch 18498, Loss: 174.11386108398438\n",
      "Epoch 62, Batch 18499, Loss: 158.12197875976562\n",
      "Epoch 62, Batch 18500, Loss: 158.5928955078125\n",
      "Epoch 62, Batch 18501, Loss: 170.88755798339844\n",
      "Epoch 62, Batch 18502, Loss: 188.70693969726562\n",
      "Epoch 62, Batch 18503, Loss: 178.8174285888672\n",
      "Epoch 62, Batch 18504, Loss: 170.6466522216797\n",
      "Epoch 62, Batch 18505, Loss: 174.71290588378906\n",
      "Epoch 62, Batch 18506, Loss: 188.5670623779297\n",
      "Epoch 62, Batch 18507, Loss: 155.72425842285156\n",
      "Epoch 62, Batch 18508, Loss: 182.65318298339844\n",
      "Epoch 62, Batch 18509, Loss: 166.99107360839844\n",
      "Epoch 62, Batch 18510, Loss: 170.99896240234375\n",
      "Epoch 62, Batch 18511, Loss: 170.60708618164062\n",
      "Epoch 62, Batch 18512, Loss: 164.322998046875\n",
      "Epoch 62, Batch 18513, Loss: 171.3431396484375\n",
      "Epoch 62, Batch 18514, Loss: 164.78131103515625\n",
      "Epoch 62, Batch 18515, Loss: 159.58694458007812\n",
      "Epoch 62, Batch 18516, Loss: 186.6094512939453\n",
      "Epoch 62, Batch 18517, Loss: 165.96804809570312\n",
      "Epoch 62, Batch 18518, Loss: 172.16001892089844\n",
      "Epoch 62, Batch 18519, Loss: 173.6850128173828\n",
      "Epoch 62, Batch 18520, Loss: 161.74636840820312\n",
      "Epoch 62, Batch 18521, Loss: 166.84725952148438\n",
      "Epoch 62, Batch 18522, Loss: 149.80645751953125\n",
      "Epoch 62, Batch 18523, Loss: 167.99899291992188\n",
      "Epoch 62, Batch 18524, Loss: 172.44012451171875\n",
      "Epoch 62, Batch 18525, Loss: 195.065673828125\n",
      "Epoch 62, Batch 18526, Loss: 182.7666778564453\n",
      "Epoch 62, Batch 18527, Loss: 163.4898681640625\n",
      "Epoch 62, Batch 18528, Loss: 163.16943359375\n",
      "Epoch 62, Batch 18529, Loss: 178.60073852539062\n",
      "Epoch 62, Batch 18530, Loss: 169.0016632080078\n",
      "Epoch 62, Batch 18531, Loss: 186.13963317871094\n",
      "Epoch 62, Batch 18532, Loss: 158.65478515625\n",
      "Epoch 62, Batch 18533, Loss: 167.01275634765625\n",
      "Epoch 62, Batch 18534, Loss: 178.7329864501953\n",
      "Epoch 62, Batch 18535, Loss: 163.7688446044922\n",
      "Epoch 62, Batch 18536, Loss: 182.98695373535156\n",
      "Epoch 62, Batch 18537, Loss: 192.43275451660156\n",
      "Epoch 62, Batch 18538, Loss: 182.30923461914062\n",
      "Epoch 62, Batch 18539, Loss: 172.2572479248047\n",
      "Epoch 62, Batch 18540, Loss: 163.62445068359375\n",
      "Epoch 62, Batch 18541, Loss: 177.47206115722656\n",
      "Epoch 62, Batch 18542, Loss: 170.39883422851562\n",
      "Epoch 62, Batch 18543, Loss: 163.04502868652344\n",
      "Epoch 62, Batch 18544, Loss: 171.64822387695312\n",
      "Epoch 62, Batch 18545, Loss: 174.4696807861328\n",
      "Epoch 62, Batch 18546, Loss: 168.38462829589844\n",
      "Epoch 62, Batch 18547, Loss: 169.15512084960938\n",
      "Epoch 62, Batch 18548, Loss: 175.33522033691406\n",
      "Epoch 62, Batch 18549, Loss: 169.37901306152344\n",
      "Epoch 62, Batch 18550, Loss: 170.6558837890625\n",
      "Epoch 62, Batch 18551, Loss: 164.52447509765625\n",
      "Epoch 62, Batch 18552, Loss: 185.66555786132812\n",
      "Epoch 62, Batch 18553, Loss: 170.01515197753906\n",
      "Epoch 62, Batch 18554, Loss: 162.21995544433594\n",
      "Epoch 62, Batch 18555, Loss: 173.870849609375\n",
      "Epoch 62, Batch 18556, Loss: 161.3359375\n",
      "Epoch 62, Batch 18557, Loss: 200.2364959716797\n",
      "Epoch 62, Batch 18558, Loss: 175.5295867919922\n",
      "Epoch 62, Batch 18559, Loss: 179.34280395507812\n",
      "Epoch 62, Batch 18560, Loss: 180.8774871826172\n",
      "Epoch 62, Batch 18561, Loss: 175.9697265625\n",
      "Epoch 62, Batch 18562, Loss: 169.4290008544922\n",
      "Epoch 62, Batch 18563, Loss: 173.968505859375\n",
      "Epoch 62, Batch 18564, Loss: 192.12022399902344\n",
      "Epoch 62, Batch 18565, Loss: 200.84335327148438\n",
      "Epoch 62, Batch 18566, Loss: 171.78424072265625\n",
      "Epoch 62, Batch 18567, Loss: 177.46018981933594\n",
      "Epoch 62, Batch 18568, Loss: 166.6363525390625\n",
      "Epoch 62, Batch 18569, Loss: 196.3408660888672\n",
      "Epoch 62, Batch 18570, Loss: 179.29661560058594\n",
      "Epoch 62, Batch 18571, Loss: 183.29002380371094\n",
      "Epoch 62, Batch 18572, Loss: 184.89315795898438\n",
      "Epoch 62, Batch 18573, Loss: 174.4912109375\n",
      "Epoch 62, Batch 18574, Loss: 178.32177734375\n",
      "Epoch 62, Batch 18575, Loss: 157.34190368652344\n",
      "Epoch 62, Batch 18576, Loss: 169.22250366210938\n",
      "Epoch 62, Batch 18577, Loss: 197.23995971679688\n",
      "Epoch 62, Batch 18578, Loss: 184.35006713867188\n",
      "Epoch 62, Batch 18579, Loss: 144.81268310546875\n",
      "Epoch 62, Batch 18580, Loss: 191.92568969726562\n",
      "Epoch 62, Batch 18581, Loss: 183.23626708984375\n",
      "Epoch 62, Batch 18582, Loss: 167.05233764648438\n",
      "Epoch 62, Batch 18583, Loss: 150.13966369628906\n",
      "Epoch 62, Batch 18584, Loss: 174.54318237304688\n",
      "Epoch 62, Batch 18585, Loss: 183.272216796875\n",
      "Epoch 62, Batch 18586, Loss: 169.25250244140625\n",
      "Epoch 62, Batch 18587, Loss: 174.63442993164062\n",
      "Epoch 62, Batch 18588, Loss: 163.91033935546875\n",
      "Epoch 62, Batch 18589, Loss: 181.72506713867188\n",
      "Epoch 62, Batch 18590, Loss: 162.1372833251953\n",
      "Epoch 62, Batch 18591, Loss: 181.8986053466797\n",
      "Epoch 62, Batch 18592, Loss: 157.71340942382812\n",
      "Epoch 62, Batch 18593, Loss: 170.1156768798828\n",
      "Epoch 62, Batch 18594, Loss: 171.7347412109375\n",
      "Epoch 62, Batch 18595, Loss: 163.63095092773438\n",
      "Epoch 62, Batch 18596, Loss: 182.68453979492188\n",
      "Epoch 62, Batch 18597, Loss: 182.48495483398438\n",
      "Epoch 62, Batch 18598, Loss: 165.6196746826172\n",
      "Epoch 62, Batch 18599, Loss: 180.13241577148438\n",
      "Epoch 62, Batch 18600, Loss: 179.69300842285156\n",
      "Epoch 62, Batch 18601, Loss: 187.44461059570312\n",
      "Epoch 62, Batch 18602, Loss: 168.53689575195312\n",
      "Epoch 62, Batch 18603, Loss: 162.77789306640625\n",
      "Epoch 62, Batch 18604, Loss: 173.50767517089844\n",
      "Epoch 62, Batch 18605, Loss: 171.77996826171875\n",
      "Epoch 62, Batch 18606, Loss: 182.27996826171875\n",
      "Epoch 62, Batch 18607, Loss: 183.31431579589844\n",
      "Epoch 62, Batch 18608, Loss: 182.383056640625\n",
      "Epoch 62, Batch 18609, Loss: 172.32171630859375\n",
      "Epoch 62, Batch 18610, Loss: 177.0664520263672\n",
      "Epoch 62, Batch 18611, Loss: 192.0721893310547\n",
      "Epoch 62, Batch 18612, Loss: 177.86549377441406\n",
      "Epoch 62, Batch 18613, Loss: 155.9305877685547\n",
      "Epoch 62, Batch 18614, Loss: 177.51913452148438\n",
      "Epoch 62, Batch 18615, Loss: 156.29092407226562\n",
      "Epoch 62, Batch 18616, Loss: 193.2973175048828\n",
      "Epoch 62, Batch 18617, Loss: 173.1793670654297\n",
      "Epoch 62, Batch 18618, Loss: 173.1234588623047\n",
      "Epoch 62, Batch 18619, Loss: 172.82652282714844\n",
      "Epoch 62, Batch 18620, Loss: 185.5742645263672\n",
      "Epoch 62, Batch 18621, Loss: 180.8430938720703\n",
      "Epoch 62, Batch 18622, Loss: 181.7527313232422\n",
      "Epoch 62, Batch 18623, Loss: 200.03492736816406\n",
      "Epoch 62, Batch 18624, Loss: 170.86849975585938\n",
      "Epoch 62, Batch 18625, Loss: 165.35316467285156\n",
      "Epoch 62, Batch 18626, Loss: 175.19378662109375\n",
      "Epoch 62, Batch 18627, Loss: 176.40684509277344\n",
      "Epoch 62, Batch 18628, Loss: 166.1339569091797\n",
      "Epoch 62, Batch 18629, Loss: 185.63418579101562\n",
      "Epoch 62, Batch 18630, Loss: 167.8487548828125\n",
      "Epoch 62, Batch 18631, Loss: 161.48452758789062\n",
      "Epoch 62, Batch 18632, Loss: 162.99154663085938\n",
      "Epoch 62, Batch 18633, Loss: 188.27444458007812\n",
      "Epoch 62, Batch 18634, Loss: 181.59375\n",
      "Epoch 62, Batch 18635, Loss: 148.71214294433594\n",
      "Epoch 62, Batch 18636, Loss: 181.74098205566406\n",
      "Epoch 62, Batch 18637, Loss: 172.51234436035156\n",
      "Epoch 62, Batch 18638, Loss: 186.01303100585938\n",
      "Epoch 62, Batch 18639, Loss: 185.7117919921875\n",
      "Epoch 62, Batch 18640, Loss: 168.2135772705078\n",
      "Epoch 62, Batch 18641, Loss: 175.95968627929688\n",
      "Epoch 62, Batch 18642, Loss: 174.18223571777344\n",
      "Epoch 62, Batch 18643, Loss: 169.25254821777344\n",
      "Epoch 62, Batch 18644, Loss: 170.86973571777344\n",
      "Epoch 62, Batch 18645, Loss: 177.44178771972656\n",
      "Epoch 62, Batch 18646, Loss: 170.74539184570312\n",
      "Epoch 62, Batch 18647, Loss: 171.0961151123047\n",
      "Epoch 62, Batch 18648, Loss: 147.76693725585938\n",
      "Epoch 62, Batch 18649, Loss: 186.80838012695312\n",
      "Epoch 62, Batch 18650, Loss: 156.04669189453125\n",
      "Epoch 62, Batch 18651, Loss: 167.6266632080078\n",
      "Epoch 62, Batch 18652, Loss: 167.1397247314453\n",
      "Epoch 62, Batch 18653, Loss: 162.87420654296875\n",
      "Epoch 62, Batch 18654, Loss: 159.26828002929688\n",
      "Epoch 62, Batch 18655, Loss: 173.5462188720703\n",
      "Epoch 62, Batch 18656, Loss: 164.62672424316406\n",
      "Epoch 62, Batch 18657, Loss: 163.08758544921875\n",
      "Epoch 62, Batch 18658, Loss: 176.82659912109375\n",
      "Epoch 62, Batch 18659, Loss: 161.91217041015625\n",
      "Epoch 62, Batch 18660, Loss: 188.9889373779297\n",
      "Epoch 62, Batch 18661, Loss: 179.10594177246094\n",
      "Epoch 62, Batch 18662, Loss: 159.1798858642578\n",
      "Epoch 62, Batch 18663, Loss: 173.1394805908203\n",
      "Epoch 62, Batch 18664, Loss: 172.39622497558594\n",
      "Epoch 62, Batch 18665, Loss: 178.97496032714844\n",
      "Epoch 62, Batch 18666, Loss: 176.7296905517578\n",
      "Epoch 62, Batch 18667, Loss: 182.90196228027344\n",
      "Epoch 62, Batch 18668, Loss: 178.03109741210938\n",
      "Epoch 62, Batch 18669, Loss: 172.42831420898438\n",
      "Epoch 62, Batch 18670, Loss: 179.27381896972656\n",
      "Epoch 62, Batch 18671, Loss: 163.82388305664062\n",
      "Epoch 62, Batch 18672, Loss: 175.1236572265625\n",
      "Epoch 62, Batch 18673, Loss: 162.48617553710938\n",
      "Epoch 62, Batch 18674, Loss: 176.72698974609375\n",
      "Epoch 62, Batch 18675, Loss: 167.4631805419922\n",
      "Epoch 62, Batch 18676, Loss: 167.02963256835938\n",
      "Epoch 62, Batch 18677, Loss: 181.159912109375\n",
      "Epoch 62, Batch 18678, Loss: 197.7052459716797\n",
      "Epoch 62, Batch 18679, Loss: 172.7797088623047\n",
      "Epoch 62, Batch 18680, Loss: 172.2202606201172\n",
      "Epoch 62, Batch 18681, Loss: 176.47369384765625\n",
      "Epoch 62, Batch 18682, Loss: 173.53118896484375\n",
      "Epoch 62, Batch 18683, Loss: 160.5913848876953\n",
      "Epoch 62, Batch 18684, Loss: 184.0265655517578\n",
      "Epoch 62, Batch 18685, Loss: 165.74063110351562\n",
      "Epoch 62, Batch 18686, Loss: 179.7932891845703\n",
      "Epoch 62, Batch 18687, Loss: 159.1590576171875\n",
      "Epoch 62, Batch 18688, Loss: 176.7069091796875\n",
      "Epoch 62, Batch 18689, Loss: 169.96006774902344\n",
      "Epoch 62, Batch 18690, Loss: 177.66583251953125\n",
      "Epoch 62, Batch 18691, Loss: 157.9530792236328\n",
      "Epoch 62, Batch 18692, Loss: 165.38023376464844\n",
      "Epoch 62, Batch 18693, Loss: 178.15066528320312\n",
      "Epoch 62, Batch 18694, Loss: 167.6869354248047\n",
      "Epoch 62, Batch 18695, Loss: 179.5699005126953\n",
      "Epoch 62, Batch 18696, Loss: 173.3459930419922\n",
      "Epoch 62, Batch 18697, Loss: 177.40394592285156\n",
      "Epoch 62, Batch 18698, Loss: 151.13479614257812\n",
      "Epoch 62, Batch 18699, Loss: 167.3487091064453\n",
      "Epoch 62, Batch 18700, Loss: 191.61276245117188\n",
      "Epoch 62, Batch 18701, Loss: 177.04470825195312\n",
      "Epoch 62, Batch 18702, Loss: 167.9983673095703\n",
      "Epoch 62, Batch 18703, Loss: 163.93994140625\n",
      "Epoch 62, Batch 18704, Loss: 189.1370849609375\n",
      "Epoch 62, Batch 18705, Loss: 176.5653076171875\n",
      "Epoch 62, Batch 18706, Loss: 197.90309143066406\n",
      "Epoch 62, Batch 18707, Loss: 168.55137634277344\n",
      "Epoch 62, Batch 18708, Loss: 189.32363891601562\n",
      "Epoch 62, Batch 18709, Loss: 168.19911193847656\n",
      "Epoch 62, Batch 18710, Loss: 169.0035858154297\n",
      "Epoch 62, Batch 18711, Loss: 167.8152313232422\n",
      "Epoch 62, Batch 18712, Loss: 169.70098876953125\n",
      "Epoch 62, Batch 18713, Loss: 162.0906524658203\n",
      "Epoch 62, Batch 18714, Loss: 171.8574981689453\n",
      "Epoch 62, Batch 18715, Loss: 170.33206176757812\n",
      "Epoch 62, Batch 18716, Loss: 181.06808471679688\n",
      "Epoch 62, Batch 18717, Loss: 172.2586212158203\n",
      "Epoch 62, Batch 18718, Loss: 166.4651641845703\n",
      "Epoch 62, Batch 18719, Loss: 179.67214965820312\n",
      "Epoch 62, Batch 18720, Loss: 166.1337127685547\n",
      "Epoch 62, Batch 18721, Loss: 175.90782165527344\n",
      "Epoch 62, Batch 18722, Loss: 181.3786163330078\n",
      "Epoch 62, Batch 18723, Loss: 180.64312744140625\n",
      "Epoch 62, Batch 18724, Loss: 189.3408660888672\n",
      "Epoch 62, Batch 18725, Loss: 177.43446350097656\n",
      "Epoch 62, Batch 18726, Loss: 161.51657104492188\n",
      "Epoch 62, Batch 18727, Loss: 183.4805908203125\n",
      "Epoch 62, Batch 18728, Loss: 189.752685546875\n",
      "Epoch 62, Batch 18729, Loss: 174.3876190185547\n",
      "Epoch 62, Batch 18730, Loss: 178.3728790283203\n",
      "Epoch 62, Batch 18731, Loss: 170.48263549804688\n",
      "Epoch 62, Batch 18732, Loss: 181.95567321777344\n",
      "Epoch 62, Batch 18733, Loss: 163.59986877441406\n",
      "Epoch 62, Batch 18734, Loss: 160.47621154785156\n",
      "Epoch 62, Batch 18735, Loss: 157.90231323242188\n",
      "Epoch 62, Batch 18736, Loss: 163.3200225830078\n",
      "Epoch 62, Batch 18737, Loss: 162.11752319335938\n",
      "Epoch 62, Batch 18738, Loss: 177.18951416015625\n",
      "Epoch 62, Batch 18739, Loss: 170.2974090576172\n",
      "Epoch 62, Batch 18740, Loss: 172.1468048095703\n",
      "Epoch 62, Batch 18741, Loss: 161.57688903808594\n",
      "Epoch 62, Batch 18742, Loss: 179.8375701904297\n",
      "Epoch 62, Batch 18743, Loss: 180.38369750976562\n",
      "Epoch 62, Batch 18744, Loss: 178.36471557617188\n",
      "Epoch 62, Batch 18745, Loss: 202.3360595703125\n",
      "Epoch 62, Batch 18746, Loss: 193.67616271972656\n",
      "Epoch 62, Batch 18747, Loss: 188.89613342285156\n",
      "Epoch 62, Batch 18748, Loss: 165.34033203125\n",
      "Epoch 62, Batch 18749, Loss: 200.5233917236328\n",
      "Epoch 62, Batch 18750, Loss: 183.5177764892578\n",
      "Epoch 62, Batch 18751, Loss: 159.7324981689453\n",
      "Epoch 62, Batch 18752, Loss: 153.58639526367188\n",
      "Epoch 62, Batch 18753, Loss: 175.42910766601562\n",
      "Epoch 62, Batch 18754, Loss: 176.1000518798828\n",
      "Epoch 62, Batch 18755, Loss: 176.9718780517578\n",
      "Epoch 62, Batch 18756, Loss: 188.67001342773438\n",
      "Epoch 62, Batch 18757, Loss: 168.63232421875\n",
      "Epoch 62, Batch 18758, Loss: 169.353271484375\n",
      "Epoch 62, Batch 18759, Loss: 178.3370361328125\n",
      "Epoch 62, Batch 18760, Loss: 167.97665405273438\n",
      "Epoch 62, Batch 18761, Loss: 177.2584228515625\n",
      "Epoch 62, Batch 18762, Loss: 179.7039337158203\n",
      "Epoch 62, Batch 18763, Loss: 179.82447814941406\n",
      "Epoch 62, Batch 18764, Loss: 166.6590576171875\n",
      "Epoch 62, Batch 18765, Loss: 171.82530212402344\n",
      "Epoch 62, Batch 18766, Loss: 176.98825073242188\n",
      "Epoch 62, Batch 18767, Loss: 180.35989379882812\n",
      "Epoch 62, Batch 18768, Loss: 186.16552734375\n",
      "Epoch 62, Batch 18769, Loss: 181.78199768066406\n",
      "Epoch 62, Batch 18770, Loss: 184.21527099609375\n",
      "Epoch 62, Batch 18771, Loss: 181.3280029296875\n",
      "Epoch 62, Batch 18772, Loss: 170.5818634033203\n",
      "Epoch 62, Batch 18773, Loss: 163.51004028320312\n",
      "Epoch 62, Batch 18774, Loss: 159.7455291748047\n",
      "Epoch 62, Batch 18775, Loss: 185.5920867919922\n",
      "Epoch 62, Batch 18776, Loss: 185.26852416992188\n",
      "Epoch 62, Batch 18777, Loss: 182.7228240966797\n",
      "Epoch 62, Batch 18778, Loss: 173.56919860839844\n",
      "Epoch 62, Batch 18779, Loss: 164.94769287109375\n",
      "Epoch 62, Batch 18780, Loss: 155.9562530517578\n",
      "Epoch 62, Batch 18781, Loss: 174.62876892089844\n",
      "Epoch 62, Batch 18782, Loss: 151.73814392089844\n",
      "Epoch 62, Batch 18783, Loss: 162.89501953125\n",
      "Epoch 62, Batch 18784, Loss: 185.54624938964844\n",
      "Epoch 62, Batch 18785, Loss: 165.72169494628906\n",
      "Epoch 62, Batch 18786, Loss: 180.78091430664062\n",
      "Epoch 62, Batch 18787, Loss: 181.77096557617188\n",
      "Epoch 62, Batch 18788, Loss: 174.10275268554688\n",
      "Epoch 62, Batch 18789, Loss: 181.51364135742188\n",
      "Epoch 62, Batch 18790, Loss: 175.1937255859375\n",
      "Epoch 62, Batch 18791, Loss: 149.33396911621094\n",
      "Epoch 62, Batch 18792, Loss: 166.5563507080078\n",
      "Epoch 62, Batch 18793, Loss: 153.2617645263672\n",
      "Epoch 62, Batch 18794, Loss: 154.52490234375\n",
      "Epoch 62, Batch 18795, Loss: 154.3979949951172\n",
      "Epoch 62, Batch 18796, Loss: 168.99835205078125\n",
      "Epoch 62, Batch 18797, Loss: 178.68309020996094\n",
      "Epoch 62, Batch 18798, Loss: 169.31553649902344\n",
      "Epoch 62, Batch 18799, Loss: 171.45455932617188\n",
      "Epoch 62, Batch 18800, Loss: 178.73220825195312\n",
      "Epoch 62, Batch 18801, Loss: 182.41168212890625\n",
      "Epoch 62, Batch 18802, Loss: 171.14036560058594\n",
      "Epoch 62, Batch 18803, Loss: 192.19705200195312\n",
      "Epoch 62, Batch 18804, Loss: 185.9590301513672\n",
      "Epoch 62, Batch 18805, Loss: 149.21617126464844\n",
      "Epoch 62, Batch 18806, Loss: 175.3852996826172\n",
      "Epoch 62, Batch 18807, Loss: 188.22715759277344\n",
      "Epoch 62, Batch 18808, Loss: 168.46066284179688\n",
      "Epoch 62, Batch 18809, Loss: 158.60133361816406\n",
      "Epoch 62, Batch 18810, Loss: 177.7554168701172\n",
      "Epoch 62, Batch 18811, Loss: 166.24310302734375\n",
      "Epoch 62, Batch 18812, Loss: 176.79901123046875\n",
      "Epoch 62, Batch 18813, Loss: 176.29124450683594\n",
      "Epoch 62, Batch 18814, Loss: 176.28125\n",
      "Epoch 62, Batch 18815, Loss: 187.2969512939453\n",
      "Epoch 62, Batch 18816, Loss: 166.7878875732422\n",
      "Epoch 62, Batch 18817, Loss: 188.45401000976562\n",
      "Epoch 62, Batch 18818, Loss: 154.2698974609375\n",
      "Epoch 62, Batch 18819, Loss: 165.65921020507812\n",
      "Epoch 62, Batch 18820, Loss: 176.0062255859375\n",
      "Epoch 62, Batch 18821, Loss: 180.95394897460938\n",
      "Epoch 62, Batch 18822, Loss: 173.84983825683594\n",
      "Epoch 62, Batch 18823, Loss: 170.70809936523438\n",
      "Epoch 62, Batch 18824, Loss: 171.0609130859375\n",
      "Epoch 62, Batch 18825, Loss: 176.9473419189453\n",
      "Epoch 62, Batch 18826, Loss: 155.2109375\n",
      "Epoch 62, Batch 18827, Loss: 178.61041259765625\n",
      "Epoch 62, Batch 18828, Loss: 169.62908935546875\n",
      "Epoch 62, Batch 18829, Loss: 174.9962158203125\n",
      "Epoch 62, Batch 18830, Loss: 161.60476684570312\n",
      "Epoch 62, Batch 18831, Loss: 183.4389190673828\n",
      "Epoch 62, Batch 18832, Loss: 170.3626251220703\n",
      "Epoch 62, Batch 18833, Loss: 176.44674682617188\n",
      "Epoch 62, Batch 18834, Loss: 168.8621826171875\n",
      "Epoch 62, Batch 18835, Loss: 168.60415649414062\n",
      "Epoch 62, Batch 18836, Loss: 171.41456604003906\n",
      "Epoch 62, Batch 18837, Loss: 159.1439208984375\n",
      "Epoch 62, Batch 18838, Loss: 157.2489471435547\n",
      "Epoch 62, Batch 18839, Loss: 169.82814025878906\n",
      "Epoch 62, Batch 18840, Loss: 180.52365112304688\n",
      "Epoch 62, Batch 18841, Loss: 159.50015258789062\n",
      "Epoch 62, Batch 18842, Loss: 160.179931640625\n",
      "Epoch 62, Batch 18843, Loss: 159.72010803222656\n",
      "Epoch 62, Batch 18844, Loss: 163.93991088867188\n",
      "Epoch 62, Batch 18845, Loss: 166.56930541992188\n",
      "Epoch 62, Batch 18846, Loss: 183.46348571777344\n",
      "Epoch 62, Batch 18847, Loss: 176.60220336914062\n",
      "Epoch 62, Batch 18848, Loss: 178.17530822753906\n",
      "Epoch 62, Batch 18849, Loss: 163.74166870117188\n",
      "Epoch 62, Batch 18850, Loss: 161.56886291503906\n",
      "Epoch 62, Batch 18851, Loss: 181.09176635742188\n",
      "Epoch 62, Batch 18852, Loss: 182.1486358642578\n",
      "Epoch 62, Batch 18853, Loss: 165.66220092773438\n",
      "Epoch 62, Batch 18854, Loss: 159.22900390625\n",
      "Epoch 62, Batch 18855, Loss: 168.38137817382812\n",
      "Epoch 62, Batch 18856, Loss: 177.81698608398438\n",
      "Epoch 62, Batch 18857, Loss: 150.77658081054688\n",
      "Epoch 62, Batch 18858, Loss: 170.02415466308594\n",
      "Epoch 62, Batch 18859, Loss: 177.58438110351562\n",
      "Epoch 62, Batch 18860, Loss: 170.57884216308594\n",
      "Epoch 62, Batch 18861, Loss: 173.3170928955078\n",
      "Epoch 62, Batch 18862, Loss: 173.2738037109375\n",
      "Epoch 62, Batch 18863, Loss: 181.36656188964844\n",
      "Epoch 62, Batch 18864, Loss: 180.97056579589844\n",
      "Epoch 62, Batch 18865, Loss: 171.37001037597656\n",
      "Epoch 62, Batch 18866, Loss: 185.228759765625\n",
      "Epoch 62, Batch 18867, Loss: 172.47972106933594\n",
      "Epoch 62, Batch 18868, Loss: 165.085693359375\n",
      "Epoch 62, Batch 18869, Loss: 182.33926391601562\n",
      "Epoch 62, Batch 18870, Loss: 168.41043090820312\n",
      "Epoch 62, Batch 18871, Loss: 154.11244201660156\n",
      "Epoch 62, Batch 18872, Loss: 162.25384521484375\n",
      "Epoch 62, Batch 18873, Loss: 154.6754150390625\n",
      "Epoch 62, Batch 18874, Loss: 171.24610900878906\n",
      "Epoch 62, Batch 18875, Loss: 170.29173278808594\n",
      "Epoch 62, Batch 18876, Loss: 173.96163940429688\n",
      "Epoch 62, Batch 18877, Loss: 163.1715850830078\n",
      "Epoch 62, Batch 18878, Loss: 175.15733337402344\n",
      "Epoch 62, Batch 18879, Loss: 172.79495239257812\n",
      "Epoch 62, Batch 18880, Loss: 188.57183837890625\n",
      "Epoch 62, Batch 18881, Loss: 168.9935760498047\n",
      "Epoch 62, Batch 18882, Loss: 185.9287109375\n",
      "Epoch 62, Batch 18883, Loss: 174.210693359375\n",
      "Epoch 62, Batch 18884, Loss: 165.1356201171875\n",
      "Epoch 62, Batch 18885, Loss: 175.4760284423828\n",
      "Epoch 62, Batch 18886, Loss: 186.2686309814453\n",
      "Epoch 62, Batch 18887, Loss: 169.02670288085938\n",
      "Epoch 62, Batch 18888, Loss: 178.99842834472656\n",
      "Epoch 62, Batch 18889, Loss: 170.84732055664062\n",
      "Epoch 62, Batch 18890, Loss: 179.2603759765625\n",
      "Epoch 62, Batch 18891, Loss: 189.86566162109375\n",
      "Epoch 62, Batch 18892, Loss: 176.19869995117188\n",
      "Epoch 62, Batch 18893, Loss: 182.28919982910156\n",
      "Epoch 62, Batch 18894, Loss: 169.38055419921875\n",
      "Epoch 62, Batch 18895, Loss: 161.8774871826172\n",
      "Epoch 62, Batch 18896, Loss: 177.61599731445312\n",
      "Epoch 62, Batch 18897, Loss: 162.283935546875\n",
      "Epoch 62, Batch 18898, Loss: 184.04209899902344\n",
      "Epoch 62, Batch 18899, Loss: 181.05776977539062\n",
      "Epoch 62, Batch 18900, Loss: 171.45498657226562\n",
      "Epoch 62, Batch 18901, Loss: 171.25877380371094\n",
      "Epoch 62, Batch 18902, Loss: 155.28160095214844\n",
      "Epoch 62, Batch 18903, Loss: 168.7765655517578\n",
      "Epoch 62, Batch 18904, Loss: 170.1392059326172\n",
      "Epoch 62, Batch 18905, Loss: 169.55287170410156\n",
      "Epoch 62, Batch 18906, Loss: 165.95364379882812\n",
      "Epoch 62, Batch 18907, Loss: 157.15773010253906\n",
      "Epoch 62, Batch 18908, Loss: 170.38449096679688\n",
      "Epoch 62, Batch 18909, Loss: 157.40985107421875\n",
      "Epoch 62, Batch 18910, Loss: 171.87765502929688\n",
      "Epoch 62, Batch 18911, Loss: 155.85055541992188\n",
      "Epoch 62, Batch 18912, Loss: 181.55311584472656\n",
      "Epoch 62, Batch 18913, Loss: 173.65634155273438\n",
      "Epoch 62, Batch 18914, Loss: 164.0236053466797\n",
      "Epoch 62, Batch 18915, Loss: 163.37977600097656\n",
      "Epoch 62, Batch 18916, Loss: 174.6387176513672\n",
      "Epoch 62, Batch 18917, Loss: 165.0536651611328\n",
      "Epoch 62, Batch 18918, Loss: 175.88409423828125\n",
      "Epoch 62, Batch 18919, Loss: 184.7334442138672\n",
      "Epoch 62, Batch 18920, Loss: 185.16830444335938\n",
      "Epoch 62, Batch 18921, Loss: 173.8078155517578\n",
      "Epoch 62, Batch 18922, Loss: 180.4444580078125\n",
      "Epoch 62, Batch 18923, Loss: 169.87667846679688\n",
      "Epoch 62, Batch 18924, Loss: 184.45648193359375\n",
      "Epoch 62, Batch 18925, Loss: 178.48712158203125\n",
      "Epoch 62, Batch 18926, Loss: 163.92544555664062\n",
      "Epoch 62, Batch 18927, Loss: 169.33685302734375\n",
      "Epoch 62, Batch 18928, Loss: 159.21856689453125\n",
      "Epoch 62, Batch 18929, Loss: 190.18331909179688\n",
      "Epoch 62, Batch 18930, Loss: 182.03541564941406\n",
      "Epoch 62, Batch 18931, Loss: 178.97714233398438\n",
      "Epoch 62, Batch 18932, Loss: 170.99166870117188\n",
      "Epoch 62, Batch 18933, Loss: 158.57347106933594\n",
      "Epoch 62, Batch 18934, Loss: 178.90542602539062\n",
      "Epoch 62, Batch 18935, Loss: 177.43247985839844\n",
      "Epoch 62, Batch 18936, Loss: 167.01893615722656\n",
      "Epoch 62, Batch 18937, Loss: 158.9398193359375\n",
      "Epoch 62, Batch 18938, Loss: 179.1395263671875\n",
      "Epoch 62, Batch 18939, Loss: 175.15890502929688\n",
      "Epoch 62, Batch 18940, Loss: 170.7431182861328\n",
      "Epoch 62, Batch 18941, Loss: 177.2698211669922\n",
      "Epoch 62, Batch 18942, Loss: 178.98583984375\n",
      "Epoch 62, Batch 18943, Loss: 156.35916137695312\n",
      "Epoch 62, Batch 18944, Loss: 188.06724548339844\n",
      "Epoch 62, Batch 18945, Loss: 184.1648406982422\n",
      "Epoch 62, Batch 18946, Loss: 177.6017303466797\n",
      "Epoch 62, Batch 18947, Loss: 178.5540008544922\n",
      "Epoch 62, Batch 18948, Loss: 167.71182250976562\n",
      "Epoch 62, Batch 18949, Loss: 169.07901000976562\n",
      "Epoch 62, Batch 18950, Loss: 178.35276794433594\n",
      "Epoch 62, Batch 18951, Loss: 175.76007080078125\n",
      "Epoch 62, Batch 18952, Loss: 167.69680786132812\n",
      "Epoch 62, Batch 18953, Loss: 169.99859619140625\n",
      "Epoch 62, Batch 18954, Loss: 171.28858947753906\n",
      "Epoch 62, Batch 18955, Loss: 174.35467529296875\n",
      "Epoch 62, Batch 18956, Loss: 194.25611877441406\n",
      "Epoch 62, Batch 18957, Loss: 180.8137969970703\n",
      "Epoch 62, Batch 18958, Loss: 155.66314697265625\n",
      "Epoch 62, Batch 18959, Loss: 165.71575927734375\n",
      "Epoch 62, Batch 18960, Loss: 171.66859436035156\n",
      "Epoch 62, Batch 18961, Loss: 156.85780334472656\n",
      "Epoch 62, Batch 18962, Loss: 179.76731872558594\n",
      "Epoch 62, Batch 18963, Loss: 190.32455444335938\n",
      "Epoch 62, Batch 18964, Loss: 183.09373474121094\n",
      "Epoch 62, Batch 18965, Loss: 169.8465576171875\n",
      "Epoch 62, Batch 18966, Loss: 165.6807098388672\n",
      "Epoch 62, Batch 18967, Loss: 166.20071411132812\n",
      "Epoch 62, Batch 18968, Loss: 179.0668487548828\n",
      "Epoch 62, Batch 18969, Loss: 169.72109985351562\n",
      "Epoch 62, Batch 18970, Loss: 170.7156982421875\n",
      "Epoch 62, Batch 18971, Loss: 176.06414794921875\n",
      "Epoch 62, Batch 18972, Loss: 174.9200439453125\n",
      "Epoch 62, Batch 18973, Loss: 177.9415740966797\n",
      "Epoch 62, Batch 18974, Loss: 169.2601318359375\n",
      "Epoch 62, Batch 18975, Loss: 189.52903747558594\n",
      "Epoch 62, Batch 18976, Loss: 175.72964477539062\n",
      "Epoch 62, Batch 18977, Loss: 161.1018829345703\n",
      "Epoch 62, Batch 18978, Loss: 166.28819274902344\n",
      "Epoch 62, Batch 18979, Loss: 171.0782928466797\n",
      "Epoch 62, Batch 18980, Loss: 166.73524475097656\n",
      "Epoch 62, Batch 18981, Loss: 165.05242919921875\n",
      "Epoch 62, Batch 18982, Loss: 166.41761779785156\n",
      "Epoch 62, Batch 18983, Loss: 168.21820068359375\n",
      "Epoch 62, Batch 18984, Loss: 159.36068725585938\n",
      "Epoch 62, Batch 18985, Loss: 175.4940643310547\n",
      "Epoch 62, Batch 18986, Loss: 157.731689453125\n",
      "Epoch 62, Batch 18987, Loss: 171.47694396972656\n",
      "Epoch 62, Batch 18988, Loss: 165.8459930419922\n",
      "Epoch 62, Batch 18989, Loss: 182.643310546875\n",
      "Epoch 62, Batch 18990, Loss: 179.3515167236328\n",
      "Epoch 62, Batch 18991, Loss: 166.8916015625\n",
      "Epoch 62, Batch 18992, Loss: 162.29190063476562\n",
      "Epoch 62, Batch 18993, Loss: 154.85142517089844\n",
      "Epoch 62, Batch 18994, Loss: 179.04141235351562\n",
      "Epoch 62, Batch 18995, Loss: 187.73489379882812\n",
      "Epoch 62, Batch 18996, Loss: 164.17300415039062\n",
      "Epoch 62, Batch 18997, Loss: 174.1645965576172\n",
      "Epoch 62, Batch 18998, Loss: 173.70970153808594\n",
      "Epoch 62, Batch 18999, Loss: 152.98873901367188\n",
      "Epoch 62, Batch 19000, Loss: 167.2936553955078\n",
      "Epoch 62, Batch 19001, Loss: 182.6787567138672\n",
      "Epoch 62, Batch 19002, Loss: 176.81114196777344\n",
      "Epoch 62, Batch 19003, Loss: 153.1529541015625\n",
      "Epoch 62, Batch 19004, Loss: 163.5098876953125\n",
      "Epoch 62, Batch 19005, Loss: 191.0922088623047\n",
      "Epoch 62, Batch 19006, Loss: 177.38023376464844\n",
      "Epoch 62, Batch 19007, Loss: 162.7311248779297\n",
      "Epoch 62, Batch 19008, Loss: 179.2061309814453\n",
      "Epoch 62, Batch 19009, Loss: 190.67457580566406\n",
      "Epoch 62, Batch 19010, Loss: 178.10861206054688\n",
      "Epoch 62, Batch 19011, Loss: 162.98731994628906\n",
      "Epoch 62, Batch 19012, Loss: 175.53541564941406\n",
      "Epoch 62, Batch 19013, Loss: 158.293701171875\n",
      "Epoch 62, Batch 19014, Loss: 162.343994140625\n",
      "Epoch 62, Batch 19015, Loss: 159.43850708007812\n",
      "Epoch 62, Batch 19016, Loss: 176.67100524902344\n",
      "Epoch 62, Batch 19017, Loss: 191.4115447998047\n",
      "Epoch 62, Batch 19018, Loss: 170.1929473876953\n",
      "Epoch 62, Batch 19019, Loss: 165.8087158203125\n",
      "Epoch 62, Batch 19020, Loss: 160.0683135986328\n",
      "Epoch 62, Batch 19021, Loss: 176.49464416503906\n",
      "Epoch 62, Batch 19022, Loss: 158.3017120361328\n",
      "Epoch 62, Batch 19023, Loss: 173.50128173828125\n",
      "Epoch 62, Batch 19024, Loss: 172.33900451660156\n",
      "Epoch 62, Batch 19025, Loss: 150.83384704589844\n",
      "Epoch 62, Batch 19026, Loss: 167.1536102294922\n",
      "Epoch 62, Batch 19027, Loss: 171.88706970214844\n",
      "Epoch 62, Batch 19028, Loss: 177.61453247070312\n",
      "Epoch 62, Batch 19029, Loss: 163.92233276367188\n",
      "Epoch 62, Batch 19030, Loss: 173.77732849121094\n",
      "Epoch 62, Batch 19031, Loss: 183.3199005126953\n",
      "Epoch 62, Batch 19032, Loss: 160.92013549804688\n",
      "Epoch 62, Batch 19033, Loss: 168.6957550048828\n",
      "Epoch 62, Batch 19034, Loss: 156.29501342773438\n",
      "Epoch 62, Batch 19035, Loss: 169.95721435546875\n",
      "Epoch 62, Batch 19036, Loss: 174.48333740234375\n",
      "Epoch 62, Batch 19037, Loss: 167.28756713867188\n",
      "Epoch 62, Batch 19038, Loss: 151.09085083007812\n",
      "Epoch 62, Batch 19039, Loss: 167.75146484375\n",
      "Epoch 62, Batch 19040, Loss: 161.29005432128906\n",
      "Epoch 62, Batch 19041, Loss: 176.01995849609375\n",
      "Epoch 62, Batch 19042, Loss: 171.43746948242188\n",
      "Epoch 62, Batch 19043, Loss: 179.51991271972656\n",
      "Epoch 62, Batch 19044, Loss: 162.4971160888672\n",
      "Epoch 62, Batch 19045, Loss: 168.9603729248047\n",
      "Epoch 62, Batch 19046, Loss: 174.22825622558594\n",
      "Epoch 62, Batch 19047, Loss: 190.50135803222656\n",
      "Epoch 62, Batch 19048, Loss: 191.78602600097656\n",
      "Epoch 62, Batch 19049, Loss: 174.2704315185547\n",
      "Epoch 62, Batch 19050, Loss: 173.781005859375\n",
      "Epoch 62, Batch 19051, Loss: 179.21115112304688\n",
      "Epoch 62, Batch 19052, Loss: 186.9441375732422\n",
      "Epoch 62, Batch 19053, Loss: 176.687744140625\n",
      "Epoch 62, Batch 19054, Loss: 180.8452606201172\n",
      "Epoch 62, Batch 19055, Loss: 161.5878143310547\n",
      "Epoch 62, Batch 19056, Loss: 165.15814208984375\n",
      "Epoch 62, Batch 19057, Loss: 166.31787109375\n",
      "Epoch 62, Batch 19058, Loss: 185.0255889892578\n",
      "Epoch 62, Batch 19059, Loss: 166.3039093017578\n",
      "Epoch 62, Batch 19060, Loss: 170.79098510742188\n",
      "Epoch 62, Batch 19061, Loss: 158.35736083984375\n",
      "Epoch 62, Batch 19062, Loss: 166.29405212402344\n",
      "Epoch 62, Batch 19063, Loss: 173.1394500732422\n",
      "Epoch 62, Batch 19064, Loss: 181.3727264404297\n",
      "Epoch 62, Batch 19065, Loss: 169.35169982910156\n",
      "Epoch 62, Batch 19066, Loss: 184.95474243164062\n",
      "Epoch 62, Batch 19067, Loss: 173.86329650878906\n",
      "Epoch 62, Batch 19068, Loss: 174.48377990722656\n",
      "Epoch 62, Batch 19069, Loss: 169.2674560546875\n",
      "Epoch 62, Batch 19070, Loss: 154.2058868408203\n",
      "Epoch 62, Batch 19071, Loss: 171.70066833496094\n",
      "Epoch 62, Batch 19072, Loss: 158.20069885253906\n",
      "Epoch 62, Batch 19073, Loss: 161.85653686523438\n",
      "Epoch 62, Batch 19074, Loss: 167.70529174804688\n",
      "Epoch 62, Batch 19075, Loss: 175.34181213378906\n",
      "Epoch 62, Batch 19076, Loss: 171.5178985595703\n",
      "Epoch 62, Batch 19077, Loss: 166.65367126464844\n",
      "Epoch 62, Batch 19078, Loss: 161.60813903808594\n",
      "Epoch 62, Batch 19079, Loss: 189.4775848388672\n",
      "Epoch 62, Batch 19080, Loss: 186.721435546875\n",
      "Epoch 62, Batch 19081, Loss: 181.3008270263672\n",
      "Epoch 62, Batch 19082, Loss: 178.80152893066406\n",
      "Epoch 62, Batch 19083, Loss: 188.0758056640625\n",
      "Epoch 62, Batch 19084, Loss: 174.4592742919922\n",
      "Epoch 62, Batch 19085, Loss: 153.64132690429688\n",
      "Epoch 62, Batch 19086, Loss: 167.972900390625\n",
      "Epoch 62, Batch 19087, Loss: 168.22421264648438\n",
      "Epoch 62, Batch 19088, Loss: 167.45306396484375\n",
      "Epoch 62, Batch 19089, Loss: 173.84707641601562\n",
      "Epoch 62, Batch 19090, Loss: 173.9800262451172\n",
      "Epoch 62, Batch 19091, Loss: 183.1410675048828\n",
      "Epoch 62, Batch 19092, Loss: 176.01988220214844\n",
      "Epoch 62, Batch 19093, Loss: 167.01527404785156\n",
      "Epoch 62, Batch 19094, Loss: 163.75637817382812\n",
      "Epoch 62, Batch 19095, Loss: 166.2181396484375\n",
      "Epoch 62, Batch 19096, Loss: 183.9931182861328\n",
      "Epoch 62, Batch 19097, Loss: 174.77833557128906\n",
      "Epoch 62, Batch 19098, Loss: 180.09510803222656\n",
      "Epoch 62, Batch 19099, Loss: 172.11343383789062\n",
      "Epoch 62, Batch 19100, Loss: 175.6279754638672\n",
      "Epoch 62, Batch 19101, Loss: 162.26669311523438\n",
      "Epoch 62, Batch 19102, Loss: 188.00613403320312\n",
      "Epoch 62, Batch 19103, Loss: 177.03414916992188\n",
      "Epoch 62, Batch 19104, Loss: 178.7964630126953\n",
      "Epoch 62, Batch 19105, Loss: 183.92965698242188\n",
      "Epoch 62, Batch 19106, Loss: 164.17791748046875\n",
      "Epoch 62, Batch 19107, Loss: 172.87356567382812\n",
      "Epoch 62, Batch 19108, Loss: 171.0428466796875\n",
      "Epoch 62, Batch 19109, Loss: 177.46961975097656\n",
      "Epoch 62, Batch 19110, Loss: 179.3344268798828\n",
      "Epoch 62, Batch 19111, Loss: 173.04171752929688\n",
      "Epoch 62, Batch 19112, Loss: 180.9774169921875\n",
      "Epoch 62, Batch 19113, Loss: 192.78466796875\n",
      "Epoch 62, Batch 19114, Loss: 159.93568420410156\n",
      "Epoch 62, Batch 19115, Loss: 161.64524841308594\n",
      "Epoch 62, Batch 19116, Loss: 166.60520935058594\n",
      "Epoch 62, Batch 19117, Loss: 172.08914184570312\n",
      "Epoch 62, Batch 19118, Loss: 168.1324462890625\n",
      "Epoch 62, Batch 19119, Loss: 180.0331573486328\n",
      "Epoch 62, Batch 19120, Loss: 176.42184448242188\n",
      "Epoch 62, Batch 19121, Loss: 175.87115478515625\n",
      "Epoch 62, Batch 19122, Loss: 161.95016479492188\n",
      "Epoch 62, Batch 19123, Loss: 173.4244842529297\n",
      "Epoch 62, Batch 19124, Loss: 193.07540893554688\n",
      "Epoch 62, Batch 19125, Loss: 181.6966552734375\n",
      "Epoch 62, Batch 19126, Loss: 169.67103576660156\n",
      "Epoch 62, Batch 19127, Loss: 177.18038940429688\n",
      "Epoch 62, Batch 19128, Loss: 170.24070739746094\n",
      "Epoch 62, Batch 19129, Loss: 179.50439453125\n",
      "Epoch 62, Batch 19130, Loss: 166.9872283935547\n",
      "Epoch 62, Batch 19131, Loss: 190.96038818359375\n",
      "Epoch 62, Batch 19132, Loss: 170.665283203125\n",
      "Epoch 62, Batch 19133, Loss: 181.81468200683594\n",
      "Epoch 62, Batch 19134, Loss: 166.62661743164062\n",
      "Epoch 62, Batch 19135, Loss: 167.95387268066406\n",
      "Epoch 62, Batch 19136, Loss: 161.91085815429688\n",
      "Epoch 62, Batch 19137, Loss: 182.51512145996094\n",
      "Epoch 62, Batch 19138, Loss: 176.0929718017578\n",
      "Epoch 62, Batch 19139, Loss: 178.7194366455078\n",
      "Epoch 62, Batch 19140, Loss: 179.03358459472656\n",
      "Epoch 62, Batch 19141, Loss: 176.78257751464844\n",
      "Epoch 62, Batch 19142, Loss: 176.69581604003906\n",
      "Epoch 62, Batch 19143, Loss: 166.58709716796875\n",
      "Epoch 62, Batch 19144, Loss: 165.95130920410156\n",
      "Epoch 62, Batch 19145, Loss: 182.0952911376953\n",
      "Epoch 62, Batch 19146, Loss: 150.30006408691406\n",
      "Epoch 62, Batch 19147, Loss: 168.80841064453125\n",
      "Epoch 62, Batch 19148, Loss: 195.00672912597656\n",
      "Epoch 62, Batch 19149, Loss: 158.22984313964844\n",
      "Epoch 62, Batch 19150, Loss: 198.97842407226562\n",
      "Epoch 62, Batch 19151, Loss: 167.33106994628906\n",
      "Epoch 62, Batch 19152, Loss: 166.21351623535156\n",
      "Epoch 62, Batch 19153, Loss: 171.63951110839844\n",
      "Epoch 62, Batch 19154, Loss: 168.3432159423828\n",
      "Epoch 62, Batch 19155, Loss: 194.6577911376953\n",
      "Epoch 62, Batch 19156, Loss: 191.95115661621094\n",
      "Epoch 62, Batch 19157, Loss: 159.7649383544922\n",
      "Epoch 62, Batch 19158, Loss: 167.60964965820312\n",
      "Epoch 62, Batch 19159, Loss: 152.87042236328125\n",
      "Epoch 62, Batch 19160, Loss: 159.20144653320312\n",
      "Epoch 62, Batch 19161, Loss: 178.76150512695312\n",
      "Epoch 62, Batch 19162, Loss: 164.37945556640625\n",
      "Epoch 62, Batch 19163, Loss: 170.62777709960938\n",
      "Epoch 62, Batch 19164, Loss: 182.34637451171875\n",
      "Epoch 62, Batch 19165, Loss: 177.5859832763672\n",
      "Epoch 62, Batch 19166, Loss: 169.7131805419922\n",
      "Epoch 62, Batch 19167, Loss: 183.6501922607422\n",
      "Epoch 62, Batch 19168, Loss: 165.29122924804688\n",
      "Epoch 62, Batch 19169, Loss: 173.06231689453125\n",
      "Epoch 62, Batch 19170, Loss: 151.60166931152344\n",
      "Epoch 62, Batch 19171, Loss: 180.5540313720703\n",
      "Epoch 62, Batch 19172, Loss: 164.2078857421875\n",
      "Epoch 62, Batch 19173, Loss: 174.97593688964844\n",
      "Epoch 62, Batch 19174, Loss: 162.8877410888672\n",
      "Epoch 62, Batch 19175, Loss: 153.03424072265625\n",
      "Epoch 62, Batch 19176, Loss: 176.13023376464844\n",
      "Epoch 62, Batch 19177, Loss: 176.08883666992188\n",
      "Epoch 62, Batch 19178, Loss: 171.0388946533203\n",
      "Epoch 62, Batch 19179, Loss: 166.35816955566406\n",
      "Epoch 62, Batch 19180, Loss: 170.22950744628906\n",
      "Epoch 62, Batch 19181, Loss: 167.90667724609375\n",
      "Epoch 62, Batch 19182, Loss: 167.5700225830078\n",
      "Epoch 62, Batch 19183, Loss: 172.30413818359375\n",
      "Epoch 62, Batch 19184, Loss: 174.03433227539062\n",
      "Epoch 62, Batch 19185, Loss: 166.76539611816406\n",
      "Epoch 62, Batch 19186, Loss: 163.87252807617188\n",
      "Epoch 62, Batch 19187, Loss: 171.72518920898438\n",
      "Epoch 62, Batch 19188, Loss: 167.25767517089844\n",
      "Epoch 62, Batch 19189, Loss: 168.2714080810547\n",
      "Epoch 62, Batch 19190, Loss: 161.02659606933594\n",
      "Epoch 62, Batch 19191, Loss: 165.5258026123047\n",
      "Epoch 62, Batch 19192, Loss: 171.23399353027344\n",
      "Epoch 62, Batch 19193, Loss: 179.89663696289062\n",
      "Epoch 62, Batch 19194, Loss: 165.8820343017578\n",
      "Epoch 62, Batch 19195, Loss: 169.27130126953125\n",
      "Epoch 62, Batch 19196, Loss: 150.62667846679688\n",
      "Epoch 62, Batch 19197, Loss: 175.1102294921875\n",
      "Epoch 62, Batch 19198, Loss: 177.99740600585938\n",
      "Epoch 62, Batch 19199, Loss: 162.99954223632812\n",
      "Epoch 62, Batch 19200, Loss: 170.79766845703125\n",
      "Epoch 62, Batch 19201, Loss: 178.4936981201172\n",
      "Epoch 62, Batch 19202, Loss: 161.93528747558594\n",
      "Epoch 62, Batch 19203, Loss: 174.5599365234375\n",
      "Epoch 62, Batch 19204, Loss: 168.19508361816406\n",
      "Epoch 62, Batch 19205, Loss: 185.07432556152344\n",
      "Epoch 62, Batch 19206, Loss: 150.42019653320312\n",
      "Epoch 62, Batch 19207, Loss: 178.7345428466797\n",
      "Epoch 62, Batch 19208, Loss: 153.2993621826172\n",
      "Epoch 62, Batch 19209, Loss: 169.2462615966797\n",
      "Epoch 62, Batch 19210, Loss: 159.0426483154297\n",
      "Epoch 62, Batch 19211, Loss: 170.81336975097656\n",
      "Epoch 62, Batch 19212, Loss: 169.02476501464844\n",
      "Epoch 62, Batch 19213, Loss: 190.66065979003906\n",
      "Epoch 62, Batch 19214, Loss: 183.3834686279297\n",
      "Epoch 62, Batch 19215, Loss: 176.2174072265625\n",
      "Epoch 62, Batch 19216, Loss: 153.16958618164062\n",
      "Epoch 62, Batch 19217, Loss: 150.98104858398438\n",
      "Epoch 62, Batch 19218, Loss: 169.12538146972656\n",
      "Epoch 62, Batch 19219, Loss: 173.67747497558594\n",
      "Epoch 62, Batch 19220, Loss: 173.10215759277344\n",
      "Epoch 62, Batch 19221, Loss: 167.2994842529297\n",
      "Epoch 62, Batch 19222, Loss: 162.85804748535156\n",
      "Epoch 62, Batch 19223, Loss: 159.24400329589844\n",
      "Epoch 62, Batch 19224, Loss: 157.7544708251953\n",
      "Epoch 62, Batch 19225, Loss: 195.60386657714844\n",
      "Epoch 62, Batch 19226, Loss: 171.65841674804688\n",
      "Epoch 62, Batch 19227, Loss: 178.5359649658203\n",
      "Epoch 62, Batch 19228, Loss: 178.4423370361328\n",
      "Epoch 62, Batch 19229, Loss: 188.63282775878906\n",
      "Epoch 62, Batch 19230, Loss: 158.09295654296875\n",
      "Epoch 62, Batch 19231, Loss: 166.0382080078125\n",
      "Epoch 62, Batch 19232, Loss: 171.9736785888672\n",
      "Epoch 62, Batch 19233, Loss: 188.43487548828125\n",
      "Epoch 62, Batch 19234, Loss: 172.57960510253906\n",
      "Epoch 62, Batch 19235, Loss: 178.15176391601562\n",
      "Epoch 62, Batch 19236, Loss: 183.39849853515625\n",
      "Epoch 62, Batch 19237, Loss: 180.30149841308594\n",
      "Epoch 62, Batch 19238, Loss: 167.5272216796875\n",
      "Epoch 62, Batch 19239, Loss: 150.40927124023438\n",
      "Epoch 62, Batch 19240, Loss: 174.081298828125\n",
      "Epoch 62, Batch 19241, Loss: 174.7213134765625\n",
      "Epoch 62, Batch 19242, Loss: 193.02197265625\n",
      "Epoch 62, Batch 19243, Loss: 175.73768615722656\n",
      "Epoch 62, Batch 19244, Loss: 179.08258056640625\n",
      "Epoch 62, Batch 19245, Loss: 174.14158630371094\n",
      "Epoch 62, Batch 19246, Loss: 157.936279296875\n",
      "Epoch 62, Batch 19247, Loss: 175.5742950439453\n",
      "Epoch 62, Batch 19248, Loss: 168.9534912109375\n",
      "Epoch 62, Batch 19249, Loss: 174.2183380126953\n",
      "Epoch 62, Batch 19250, Loss: 187.5956573486328\n",
      "Epoch 62, Batch 19251, Loss: 180.4766082763672\n",
      "Epoch 62, Batch 19252, Loss: 164.47984313964844\n",
      "Epoch 62, Batch 19253, Loss: 165.2680206298828\n",
      "Epoch 62, Batch 19254, Loss: 175.31735229492188\n",
      "Epoch 62, Batch 19255, Loss: 188.6734619140625\n",
      "Epoch 62, Batch 19256, Loss: 182.66136169433594\n",
      "Epoch 62, Batch 19257, Loss: 163.57908630371094\n",
      "Epoch 62, Batch 19258, Loss: 175.63885498046875\n",
      "Epoch 62, Batch 19259, Loss: 184.39166259765625\n",
      "Epoch 62, Batch 19260, Loss: 177.62257385253906\n",
      "Epoch 62, Batch 19261, Loss: 171.46127319335938\n",
      "Epoch 62, Batch 19262, Loss: 181.38150024414062\n",
      "Epoch 62, Batch 19263, Loss: 162.4496307373047\n",
      "Epoch 62, Batch 19264, Loss: 181.36007690429688\n",
      "Epoch 62, Batch 19265, Loss: 179.968994140625\n",
      "Epoch 62, Batch 19266, Loss: 177.29281616210938\n",
      "Epoch 62, Batch 19267, Loss: 166.37722778320312\n",
      "Epoch 62, Batch 19268, Loss: 148.91351318359375\n",
      "Epoch 62, Batch 19269, Loss: 168.52871704101562\n",
      "Epoch 62, Batch 19270, Loss: 181.17587280273438\n",
      "Epoch 62, Batch 19271, Loss: 177.42503356933594\n",
      "Epoch 62, Batch 19272, Loss: 180.88377380371094\n",
      "Epoch 62, Batch 19273, Loss: 174.80908203125\n",
      "Epoch 62, Batch 19274, Loss: 157.87010192871094\n",
      "Epoch 62, Batch 19275, Loss: 163.8999786376953\n",
      "Epoch 62, Batch 19276, Loss: 167.76014709472656\n",
      "Epoch 62, Batch 19277, Loss: 171.55177307128906\n",
      "Epoch 62, Batch 19278, Loss: 190.4204559326172\n",
      "Epoch 62, Batch 19279, Loss: 157.08203125\n",
      "Epoch 62, Batch 19280, Loss: 180.5263671875\n",
      "Epoch 62, Batch 19281, Loss: 160.28396606445312\n",
      "Epoch 62, Batch 19282, Loss: 159.3167724609375\n",
      "Epoch 62, Batch 19283, Loss: 162.85025024414062\n",
      "Epoch 62, Batch 19284, Loss: 174.85633850097656\n",
      "Epoch 62, Batch 19285, Loss: 181.599365234375\n",
      "Epoch 62, Batch 19286, Loss: 175.3792724609375\n",
      "Epoch 62, Batch 19287, Loss: 172.06845092773438\n",
      "Epoch 62, Batch 19288, Loss: 163.59991455078125\n",
      "Epoch 62, Batch 19289, Loss: 182.47430419921875\n",
      "Epoch 62, Batch 19290, Loss: 174.96226501464844\n",
      "Epoch 62, Batch 19291, Loss: 169.69114685058594\n",
      "Epoch 62, Batch 19292, Loss: 173.33990478515625\n",
      "Epoch 62, Batch 19293, Loss: 168.20965576171875\n",
      "Epoch 62, Batch 19294, Loss: 184.1934814453125\n",
      "Epoch 62, Batch 19295, Loss: 186.5283966064453\n",
      "Epoch 62, Batch 19296, Loss: 185.96560668945312\n",
      "Epoch 62, Batch 19297, Loss: 186.8065948486328\n",
      "Epoch 62, Batch 19298, Loss: 164.7802276611328\n",
      "Epoch 62, Batch 19299, Loss: 160.3745880126953\n",
      "Epoch 62, Batch 19300, Loss: 167.50363159179688\n",
      "Epoch 62, Batch 19301, Loss: 173.5565643310547\n",
      "Epoch 62, Batch 19302, Loss: 189.52703857421875\n",
      "Epoch 62, Batch 19303, Loss: 184.98516845703125\n",
      "Epoch 62, Batch 19304, Loss: 174.44415283203125\n",
      "Epoch 62, Batch 19305, Loss: 185.8109588623047\n",
      "Epoch 62, Batch 19306, Loss: 172.8355712890625\n",
      "Epoch 62, Batch 19307, Loss: 165.69175720214844\n",
      "Epoch 62, Batch 19308, Loss: 175.1317138671875\n",
      "Epoch 62, Batch 19309, Loss: 180.24095153808594\n",
      "Epoch 62, Batch 19310, Loss: 159.63963317871094\n",
      "Epoch 62, Batch 19311, Loss: 171.06040954589844\n",
      "Epoch 62, Batch 19312, Loss: 175.22630310058594\n",
      "Epoch 62, Batch 19313, Loss: 168.8237762451172\n",
      "Epoch 62, Batch 19314, Loss: 159.53121948242188\n",
      "Epoch 62, Batch 19315, Loss: 172.45240783691406\n",
      "Epoch 62, Batch 19316, Loss: 182.91790771484375\n",
      "Epoch 62, Batch 19317, Loss: 167.7574920654297\n",
      "Epoch 62, Batch 19318, Loss: 186.1986541748047\n",
      "Epoch 62, Batch 19319, Loss: 177.74960327148438\n",
      "Epoch 62, Batch 19320, Loss: 156.93572998046875\n",
      "Epoch 62, Batch 19321, Loss: 172.0355224609375\n",
      "Epoch 62, Batch 19322, Loss: 166.4225311279297\n",
      "Epoch 62, Batch 19323, Loss: 174.199951171875\n",
      "Epoch 62, Batch 19324, Loss: 173.8044891357422\n",
      "Epoch 62, Batch 19325, Loss: 163.35488891601562\n",
      "Epoch 62, Batch 19326, Loss: 171.47042846679688\n",
      "Epoch 62, Batch 19327, Loss: 177.85740661621094\n",
      "Epoch 62, Batch 19328, Loss: 165.2892303466797\n",
      "Epoch 62, Batch 19329, Loss: 164.54322814941406\n",
      "Epoch 62, Batch 19330, Loss: 193.81533813476562\n",
      "Epoch 62, Batch 19331, Loss: 175.39474487304688\n",
      "Epoch 62, Batch 19332, Loss: 164.6278076171875\n",
      "Epoch 62, Batch 19333, Loss: 176.3807830810547\n",
      "Epoch 62, Batch 19334, Loss: 170.1046905517578\n",
      "Epoch 62, Batch 19335, Loss: 159.36489868164062\n",
      "Epoch 62, Batch 19336, Loss: 182.044677734375\n",
      "Epoch 62, Batch 19337, Loss: 174.14175415039062\n",
      "Epoch 62, Batch 19338, Loss: 165.3408966064453\n",
      "Epoch 62, Batch 19339, Loss: 181.22476196289062\n",
      "Epoch 62, Batch 19340, Loss: 159.1990509033203\n",
      "Epoch 62, Batch 19341, Loss: 178.9384002685547\n",
      "Epoch 62, Batch 19342, Loss: 166.3775634765625\n",
      "Epoch 62, Batch 19343, Loss: 167.06463623046875\n",
      "Epoch 62, Batch 19344, Loss: 157.03916931152344\n",
      "Epoch 62, Batch 19345, Loss: 173.51730346679688\n",
      "Epoch 62, Batch 19346, Loss: 167.64553833007812\n",
      "Epoch 62, Batch 19347, Loss: 171.71360778808594\n",
      "Epoch 62, Batch 19348, Loss: 168.00485229492188\n",
      "Epoch 62, Batch 19349, Loss: 173.48597717285156\n",
      "Epoch 62, Batch 19350, Loss: 161.73721313476562\n",
      "Epoch 62, Batch 19351, Loss: 183.73162841796875\n",
      "Epoch 62, Batch 19352, Loss: 174.61512756347656\n",
      "Epoch 62, Batch 19353, Loss: 173.81871032714844\n",
      "Epoch 62, Batch 19354, Loss: 166.6245574951172\n",
      "Epoch 62, Batch 19355, Loss: 178.9781494140625\n",
      "Epoch 62, Batch 19356, Loss: 166.24807739257812\n",
      "Epoch 62, Batch 19357, Loss: 174.34530639648438\n",
      "Epoch 62, Batch 19358, Loss: 172.12550354003906\n",
      "Epoch 62, Batch 19359, Loss: 167.99258422851562\n",
      "Epoch 62, Batch 19360, Loss: 186.5877227783203\n",
      "Epoch 62, Batch 19361, Loss: 171.6517333984375\n",
      "Epoch 62, Batch 19362, Loss: 163.2674102783203\n",
      "Epoch 62, Batch 19363, Loss: 181.9138641357422\n",
      "Epoch 62, Batch 19364, Loss: 170.0718994140625\n",
      "Epoch 62, Batch 19365, Loss: 164.9420623779297\n",
      "Epoch 62, Batch 19366, Loss: 177.49673461914062\n",
      "Epoch 62, Batch 19367, Loss: 189.17164611816406\n",
      "Epoch 62, Batch 19368, Loss: 183.66213989257812\n",
      "Epoch 62, Batch 19369, Loss: 187.73855590820312\n",
      "Epoch 62, Batch 19370, Loss: 161.3024139404297\n",
      "Epoch 62, Batch 19371, Loss: 170.5184326171875\n",
      "Epoch 62, Batch 19372, Loss: 169.03713989257812\n",
      "Epoch 62, Batch 19373, Loss: 186.81944274902344\n",
      "Epoch 62, Batch 19374, Loss: 185.25912475585938\n",
      "Epoch 62, Batch 19375, Loss: 177.5041961669922\n",
      "Epoch 62, Batch 19376, Loss: 190.64402770996094\n",
      "Epoch 62, Batch 19377, Loss: 175.99606323242188\n",
      "Epoch 62, Batch 19378, Loss: 160.4413604736328\n",
      "Epoch 62, Batch 19379, Loss: 166.2138214111328\n",
      "Epoch 62, Batch 19380, Loss: 194.08135986328125\n",
      "Epoch 62, Batch 19381, Loss: 181.98822021484375\n",
      "Epoch 62, Batch 19382, Loss: 166.01675415039062\n",
      "Epoch 62, Batch 19383, Loss: 167.51316833496094\n",
      "Epoch 62, Batch 19384, Loss: 174.2729949951172\n",
      "Epoch 62, Batch 19385, Loss: 172.20391845703125\n",
      "Epoch 62, Batch 19386, Loss: 171.7173309326172\n",
      "Epoch 62, Batch 19387, Loss: 178.35189819335938\n",
      "Epoch 62, Batch 19388, Loss: 163.14578247070312\n",
      "Epoch 62, Batch 19389, Loss: 164.64646911621094\n",
      "Epoch 62, Batch 19390, Loss: 162.13316345214844\n",
      "Epoch 62, Batch 19391, Loss: 168.39202880859375\n",
      "Epoch 62, Batch 19392, Loss: 163.04888916015625\n",
      "Epoch 62, Batch 19393, Loss: 172.15847778320312\n",
      "Epoch 62, Batch 19394, Loss: 161.29615783691406\n",
      "Epoch 62, Batch 19395, Loss: 180.16363525390625\n",
      "Epoch 62, Batch 19396, Loss: 171.85499572753906\n",
      "Epoch 62, Batch 19397, Loss: 181.26065063476562\n",
      "Epoch 62, Batch 19398, Loss: 162.10897827148438\n",
      "Epoch 62, Batch 19399, Loss: 185.08071899414062\n",
      "Epoch 62, Batch 19400, Loss: 157.7220001220703\n",
      "Epoch 62, Batch 19401, Loss: 183.51756286621094\n",
      "Epoch 62, Batch 19402, Loss: 181.08181762695312\n",
      "Epoch 62, Batch 19403, Loss: 172.65182495117188\n",
      "Epoch 62, Batch 19404, Loss: 178.69534301757812\n",
      "Epoch 62, Batch 19405, Loss: 200.54530334472656\n",
      "Epoch 62, Batch 19406, Loss: 184.62403869628906\n",
      "Epoch 62, Batch 19407, Loss: 179.19622802734375\n",
      "Epoch 62, Batch 19408, Loss: 186.89138793945312\n",
      "Epoch 62, Batch 19409, Loss: 185.43026733398438\n",
      "Epoch 62, Batch 19410, Loss: 170.7212371826172\n",
      "Epoch 62, Batch 19411, Loss: 167.61192321777344\n",
      "Epoch 62, Batch 19412, Loss: 153.6947021484375\n",
      "Epoch 62, Batch 19413, Loss: 171.46371459960938\n",
      "Epoch 62, Batch 19414, Loss: 180.1942138671875\n",
      "Epoch 62, Batch 19415, Loss: 171.39857482910156\n",
      "Epoch 62, Batch 19416, Loss: 169.6434326171875\n",
      "Epoch 62, Batch 19417, Loss: 195.69139099121094\n",
      "Epoch 62, Batch 19418, Loss: 168.78152465820312\n",
      "Epoch 62, Batch 19419, Loss: 177.25225830078125\n",
      "Epoch 62, Batch 19420, Loss: 171.67852783203125\n",
      "Epoch 62, Batch 19421, Loss: 185.9265594482422\n",
      "Epoch 62, Batch 19422, Loss: 169.7157745361328\n",
      "Epoch 62, Batch 19423, Loss: 168.093017578125\n",
      "Epoch 62, Batch 19424, Loss: 180.1434783935547\n",
      "Epoch 62, Batch 19425, Loss: 165.8212890625\n",
      "Epoch 62, Batch 19426, Loss: 163.3642578125\n",
      "Epoch 62, Batch 19427, Loss: 197.49319458007812\n",
      "Epoch 62, Batch 19428, Loss: 181.6595001220703\n",
      "Epoch 62, Batch 19429, Loss: 182.12521362304688\n",
      "Epoch 62, Batch 19430, Loss: 161.76309204101562\n",
      "Epoch 62, Batch 19431, Loss: 166.18235778808594\n",
      "Epoch 62, Batch 19432, Loss: 157.0454864501953\n",
      "Epoch 62, Batch 19433, Loss: 184.02879333496094\n",
      "Epoch 62, Batch 19434, Loss: 184.7021484375\n",
      "Epoch 62, Batch 19435, Loss: 162.4766082763672\n",
      "Epoch 62, Batch 19436, Loss: 178.59713745117188\n",
      "Epoch 62, Batch 19437, Loss: 155.53732299804688\n",
      "Epoch 62, Batch 19438, Loss: 182.14138793945312\n",
      "Epoch 62, Batch 19439, Loss: 187.93740844726562\n",
      "Epoch 62, Batch 19440, Loss: 161.01780700683594\n",
      "Epoch 62, Batch 19441, Loss: 156.45285034179688\n",
      "Epoch 62, Batch 19442, Loss: 163.0736541748047\n",
      "Epoch 62, Batch 19443, Loss: 172.34083557128906\n",
      "Epoch 62, Batch 19444, Loss: 174.11819458007812\n",
      "Epoch 62, Batch 19445, Loss: 171.1385955810547\n",
      "Epoch 62, Batch 19446, Loss: 183.7545166015625\n",
      "Epoch 62, Batch 19447, Loss: 177.43043518066406\n",
      "Epoch 62, Batch 19448, Loss: 188.88140869140625\n",
      "Epoch 62, Batch 19449, Loss: 190.4346466064453\n",
      "Epoch 62, Batch 19450, Loss: 173.97177124023438\n",
      "Epoch 62, Batch 19451, Loss: 168.99949645996094\n",
      "Epoch 62, Batch 19452, Loss: 168.36407470703125\n",
      "Epoch 62, Batch 19453, Loss: 165.350830078125\n",
      "Epoch 62, Batch 19454, Loss: 160.22134399414062\n",
      "Epoch 62, Batch 19455, Loss: 169.84849548339844\n",
      "Epoch 62, Batch 19456, Loss: 178.5857391357422\n",
      "Epoch 62, Batch 19457, Loss: 190.6217041015625\n",
      "Epoch 62, Batch 19458, Loss: 170.14266967773438\n",
      "Epoch 62, Batch 19459, Loss: 176.78317260742188\n",
      "Epoch 62, Batch 19460, Loss: 176.6675262451172\n",
      "Epoch 62, Batch 19461, Loss: 169.47752380371094\n",
      "Epoch 62, Batch 19462, Loss: 166.59213256835938\n",
      "Epoch 62, Batch 19463, Loss: 155.70289611816406\n",
      "Epoch 62, Batch 19464, Loss: 156.71685791015625\n",
      "Epoch 62, Batch 19465, Loss: 155.01513671875\n",
      "Epoch 62, Batch 19466, Loss: 174.19076538085938\n",
      "Epoch 62, Batch 19467, Loss: 174.9365997314453\n",
      "Epoch 62, Batch 19468, Loss: 157.6506805419922\n",
      "Epoch 62, Batch 19469, Loss: 171.32302856445312\n",
      "Epoch 62, Batch 19470, Loss: 178.16156005859375\n",
      "Epoch 62, Batch 19471, Loss: 199.22848510742188\n",
      "Epoch 62, Batch 19472, Loss: 167.44790649414062\n",
      "Epoch 62, Batch 19473, Loss: 168.3118896484375\n",
      "Epoch 62, Batch 19474, Loss: 171.86793518066406\n",
      "Epoch 62, Batch 19475, Loss: 164.0430908203125\n",
      "Epoch 62, Batch 19476, Loss: 156.46351623535156\n",
      "Epoch 62, Batch 19477, Loss: 186.59542846679688\n",
      "Epoch 62, Batch 19478, Loss: 171.90634155273438\n",
      "Epoch 62, Batch 19479, Loss: 164.76181030273438\n",
      "Epoch 62, Batch 19480, Loss: 187.3553009033203\n",
      "Epoch 62, Batch 19481, Loss: 193.413330078125\n",
      "Epoch 62, Batch 19482, Loss: 171.5345458984375\n",
      "Epoch 62, Batch 19483, Loss: 174.95326232910156\n",
      "Epoch 62, Batch 19484, Loss: 169.23159790039062\n",
      "Epoch 62, Batch 19485, Loss: 182.12091064453125\n",
      "Epoch 62, Batch 19486, Loss: 196.4634246826172\n",
      "Epoch 62, Batch 19487, Loss: 170.56085205078125\n",
      "Epoch 62, Batch 19488, Loss: 178.23838806152344\n",
      "Epoch 62, Batch 19489, Loss: 173.7032012939453\n",
      "Epoch 62, Batch 19490, Loss: 170.9403533935547\n",
      "Epoch 62, Batch 19491, Loss: 183.71006774902344\n",
      "Epoch 62, Batch 19492, Loss: 187.4020233154297\n",
      "Epoch 62, Batch 19493, Loss: 175.03929138183594\n",
      "Epoch 62, Batch 19494, Loss: 177.00344848632812\n",
      "Epoch 62, Batch 19495, Loss: 171.96490478515625\n",
      "Epoch 62, Batch 19496, Loss: 175.4043731689453\n",
      "Epoch 62, Batch 19497, Loss: 177.01454162597656\n",
      "Epoch 62, Batch 19498, Loss: 185.1963653564453\n",
      "Epoch 62, Batch 19499, Loss: 177.2459716796875\n",
      "Epoch 62, Batch 19500, Loss: 162.40615844726562\n",
      "Epoch 62, Batch 19501, Loss: 165.64987182617188\n",
      "Epoch 62, Batch 19502, Loss: 170.65896606445312\n",
      "Epoch 62, Batch 19503, Loss: 179.849609375\n",
      "Epoch 62, Batch 19504, Loss: 180.41180419921875\n",
      "Epoch 62, Batch 19505, Loss: 159.07196044921875\n",
      "Epoch 62, Batch 19506, Loss: 191.8533172607422\n",
      "Epoch 62, Batch 19507, Loss: 179.98728942871094\n",
      "Epoch 62, Batch 19508, Loss: 171.53172302246094\n",
      "Epoch 62, Batch 19509, Loss: 179.46115112304688\n",
      "Epoch 62, Batch 19510, Loss: 172.2510223388672\n",
      "Epoch 62, Batch 19511, Loss: 171.4871826171875\n",
      "Epoch 62, Batch 19512, Loss: 170.9562225341797\n",
      "Epoch 62, Batch 19513, Loss: 187.0507354736328\n",
      "Epoch 62, Batch 19514, Loss: 175.4348907470703\n",
      "Epoch 62, Batch 19515, Loss: 172.358154296875\n",
      "Epoch 62, Batch 19516, Loss: 178.2788543701172\n",
      "Epoch 62, Batch 19517, Loss: 186.86032104492188\n",
      "Epoch 62, Batch 19518, Loss: 184.3160858154297\n",
      "Epoch 62, Batch 19519, Loss: 184.8996124267578\n",
      "Epoch 62, Batch 19520, Loss: 176.14817810058594\n",
      "Epoch 62, Batch 19521, Loss: 174.0728759765625\n",
      "Epoch 62, Batch 19522, Loss: 188.9869384765625\n",
      "Epoch 62, Batch 19523, Loss: 165.44586181640625\n",
      "Epoch 62, Batch 19524, Loss: 169.92503356933594\n",
      "Epoch 62, Batch 19525, Loss: 169.8691864013672\n",
      "Epoch 62, Batch 19526, Loss: 166.15061950683594\n",
      "Epoch 62, Batch 19527, Loss: 164.58575439453125\n",
      "Epoch 62, Batch 19528, Loss: 173.7330780029297\n",
      "Epoch 62, Batch 19529, Loss: 170.22695922851562\n",
      "Epoch 62, Batch 19530, Loss: 169.91995239257812\n",
      "Epoch 62, Batch 19531, Loss: 183.04867553710938\n",
      "Epoch 62, Batch 19532, Loss: 172.19760131835938\n",
      "Epoch 62, Batch 19533, Loss: 173.80319213867188\n",
      "Epoch 62, Batch 19534, Loss: 160.92007446289062\n",
      "Epoch 62, Batch 19535, Loss: 160.5143585205078\n",
      "Epoch 62, Batch 19536, Loss: 168.14501953125\n",
      "Epoch 62, Batch 19537, Loss: 167.15342712402344\n",
      "Epoch 62, Batch 19538, Loss: 191.01182556152344\n",
      "Epoch 62, Batch 19539, Loss: 192.37857055664062\n",
      "Epoch 62, Batch 19540, Loss: 179.21946716308594\n",
      "Epoch 62, Batch 19541, Loss: 167.80426025390625\n",
      "Epoch 62, Batch 19542, Loss: 184.2378692626953\n",
      "Epoch 62, Batch 19543, Loss: 160.80690002441406\n",
      "Epoch 62, Batch 19544, Loss: 174.32337951660156\n",
      "Epoch 62, Batch 19545, Loss: 174.8689727783203\n",
      "Epoch 62, Batch 19546, Loss: 168.9494171142578\n",
      "Epoch 62, Batch 19547, Loss: 162.43177795410156\n",
      "Epoch 62, Batch 19548, Loss: 188.6851043701172\n",
      "Epoch 62, Batch 19549, Loss: 161.04351806640625\n",
      "Epoch 62, Batch 19550, Loss: 176.5700225830078\n",
      "Epoch 62, Batch 19551, Loss: 170.6566925048828\n",
      "Epoch 62, Batch 19552, Loss: 170.55641174316406\n",
      "Epoch 62, Batch 19553, Loss: 163.3386993408203\n",
      "Epoch 62, Batch 19554, Loss: 177.8182373046875\n",
      "Epoch 62, Batch 19555, Loss: 167.5374755859375\n",
      "Epoch 62, Batch 19556, Loss: 184.0795440673828\n",
      "Epoch 62, Batch 19557, Loss: 172.53306579589844\n",
      "Epoch 62, Batch 19558, Loss: 186.0133514404297\n",
      "Epoch 62, Batch 19559, Loss: 163.06948852539062\n",
      "Epoch 62, Batch 19560, Loss: 185.86187744140625\n",
      "Epoch 62, Batch 19561, Loss: 171.85214233398438\n",
      "Epoch 62, Batch 19562, Loss: 168.16358947753906\n",
      "Epoch 62, Batch 19563, Loss: 189.3357696533203\n",
      "Epoch 62, Batch 19564, Loss: 168.08926391601562\n",
      "Epoch 62, Batch 19565, Loss: 180.20004272460938\n",
      "Epoch 62, Batch 19566, Loss: 192.03224182128906\n",
      "Epoch 62, Batch 19567, Loss: 179.64874267578125\n",
      "Epoch 62, Batch 19568, Loss: 174.07098388671875\n",
      "Epoch 62, Batch 19569, Loss: 173.84027099609375\n",
      "Epoch 62, Batch 19570, Loss: 177.96444702148438\n",
      "Epoch 62, Batch 19571, Loss: 177.3540496826172\n",
      "Epoch 62, Batch 19572, Loss: 171.76856994628906\n",
      "Epoch 62, Batch 19573, Loss: 179.9420623779297\n",
      "Epoch 62, Batch 19574, Loss: 172.82823181152344\n",
      "Epoch 62, Batch 19575, Loss: 151.29132080078125\n",
      "Epoch 62, Batch 19576, Loss: 183.70034790039062\n",
      "Epoch 62, Batch 19577, Loss: 180.99781799316406\n",
      "Epoch 62, Batch 19578, Loss: 172.2017364501953\n",
      "Epoch 62, Batch 19579, Loss: 175.97230529785156\n",
      "Epoch 62, Batch 19580, Loss: 171.00770568847656\n",
      "Epoch 62, Batch 19581, Loss: 173.41050720214844\n",
      "Epoch 62, Batch 19582, Loss: 167.6053924560547\n",
      "Epoch 62, Batch 19583, Loss: 170.17941284179688\n",
      "Epoch 62, Batch 19584, Loss: 175.05567932128906\n",
      "Epoch 62, Batch 19585, Loss: 166.17263793945312\n",
      "Epoch 62, Batch 19586, Loss: 191.05686950683594\n",
      "Epoch 62, Batch 19587, Loss: 178.15548706054688\n",
      "Epoch 62, Batch 19588, Loss: 181.6223907470703\n",
      "Epoch 62, Batch 19589, Loss: 165.6807098388672\n",
      "Epoch 62, Batch 19590, Loss: 162.4447784423828\n",
      "Epoch 62, Batch 19591, Loss: 178.5473175048828\n",
      "Epoch 62, Batch 19592, Loss: 173.42330932617188\n",
      "Epoch 62, Batch 19593, Loss: 163.2576446533203\n",
      "Epoch 62, Batch 19594, Loss: 185.79495239257812\n",
      "Epoch 62, Batch 19595, Loss: 161.34542846679688\n",
      "Epoch 62, Batch 19596, Loss: 172.83763122558594\n",
      "Epoch 62, Batch 19597, Loss: 189.61788940429688\n",
      "Epoch 62, Batch 19598, Loss: 160.6216278076172\n",
      "Epoch 62, Batch 19599, Loss: 166.00775146484375\n",
      "Epoch 62, Batch 19600, Loss: 168.57962036132812\n",
      "Epoch 62, Batch 19601, Loss: 175.49722290039062\n",
      "Epoch 62, Batch 19602, Loss: 172.58261108398438\n",
      "Epoch 62, Batch 19603, Loss: 177.39674377441406\n",
      "Epoch 62, Batch 19604, Loss: 159.79043579101562\n",
      "Epoch 62, Batch 19605, Loss: 170.23023986816406\n",
      "Epoch 62, Batch 19606, Loss: 166.225830078125\n",
      "Epoch 62, Batch 19607, Loss: 174.3508758544922\n",
      "Epoch 62, Batch 19608, Loss: 185.5967254638672\n",
      "Epoch 62, Batch 19609, Loss: 168.8551483154297\n",
      "Epoch 62, Batch 19610, Loss: 165.2087860107422\n",
      "Epoch 62, Batch 19611, Loss: 172.93023681640625\n",
      "Epoch 62, Batch 19612, Loss: 173.44105529785156\n",
      "Epoch 62, Batch 19613, Loss: 167.4227294921875\n",
      "Epoch 62, Batch 19614, Loss: 167.43785095214844\n",
      "Epoch 62, Batch 19615, Loss: 168.37608337402344\n",
      "Epoch 62, Batch 19616, Loss: 186.0189971923828\n",
      "Epoch 62, Batch 19617, Loss: 163.3737030029297\n",
      "Epoch 62, Batch 19618, Loss: 173.12989807128906\n",
      "Epoch 62, Batch 19619, Loss: 196.0276336669922\n",
      "Epoch 62, Batch 19620, Loss: 176.26052856445312\n",
      "Epoch 62, Batch 19621, Loss: 161.13851928710938\n",
      "Epoch 62, Batch 19622, Loss: 180.8963165283203\n",
      "Epoch 62, Batch 19623, Loss: 170.3465118408203\n",
      "Epoch 62, Batch 19624, Loss: 156.0488739013672\n",
      "Epoch 62, Batch 19625, Loss: 185.66671752929688\n",
      "Epoch 62, Batch 19626, Loss: 182.5413055419922\n",
      "Epoch 62, Batch 19627, Loss: 183.08119201660156\n",
      "Epoch 62, Batch 19628, Loss: 170.9518585205078\n",
      "Epoch 62, Batch 19629, Loss: 181.15985107421875\n",
      "Epoch 62, Batch 19630, Loss: 170.21234130859375\n",
      "Epoch 62, Batch 19631, Loss: 165.12681579589844\n",
      "Epoch 62, Batch 19632, Loss: 178.3395538330078\n",
      "Epoch 62, Batch 19633, Loss: 157.8209228515625\n",
      "Epoch 62, Batch 19634, Loss: 183.48060607910156\n",
      "Epoch 62, Batch 19635, Loss: 167.05934143066406\n",
      "Epoch 62, Batch 19636, Loss: 187.3773193359375\n",
      "Epoch 62, Batch 19637, Loss: 164.61871337890625\n",
      "Epoch 62, Batch 19638, Loss: 173.38352966308594\n",
      "Epoch 62, Batch 19639, Loss: 154.64437866210938\n",
      "Epoch 62, Batch 19640, Loss: 176.31204223632812\n",
      "Epoch 62, Batch 19641, Loss: 167.58705139160156\n",
      "Epoch 62, Batch 19642, Loss: 177.5646514892578\n",
      "Epoch 62, Batch 19643, Loss: 171.6063232421875\n",
      "Epoch 62, Batch 19644, Loss: 159.3450164794922\n",
      "Epoch 62, Batch 19645, Loss: 182.0186004638672\n",
      "Epoch 62, Batch 19646, Loss: 172.40701293945312\n",
      "Epoch 62, Batch 19647, Loss: 179.1498565673828\n",
      "Epoch 62, Batch 19648, Loss: 171.22555541992188\n",
      "Epoch 62, Batch 19649, Loss: 181.52163696289062\n",
      "Epoch 62, Batch 19650, Loss: 172.3875732421875\n",
      "Epoch 62, Batch 19651, Loss: 158.25253295898438\n",
      "Epoch 62, Batch 19652, Loss: 188.0753173828125\n",
      "Epoch 62, Batch 19653, Loss: 185.87933349609375\n",
      "Epoch 62, Batch 19654, Loss: 167.14242553710938\n",
      "Epoch 62, Batch 19655, Loss: 185.6631317138672\n",
      "Epoch 62, Batch 19656, Loss: 155.5850372314453\n",
      "Epoch 62, Batch 19657, Loss: 178.60940551757812\n",
      "Epoch 62, Batch 19658, Loss: 170.9600067138672\n",
      "Epoch 62, Batch 19659, Loss: 160.96949768066406\n",
      "Epoch 62, Batch 19660, Loss: 182.7211456298828\n",
      "Epoch 62, Batch 19661, Loss: 163.60699462890625\n",
      "Epoch 62, Batch 19662, Loss: 175.88174438476562\n",
      "Epoch 62, Batch 19663, Loss: 177.71548461914062\n",
      "Epoch 62, Batch 19664, Loss: 158.2230682373047\n",
      "Epoch 62, Batch 19665, Loss: 171.87538146972656\n",
      "Epoch 62, Batch 19666, Loss: 188.53347778320312\n",
      "Epoch 62, Batch 19667, Loss: 184.74488830566406\n",
      "Epoch 62, Batch 19668, Loss: 172.1321563720703\n",
      "Epoch 62, Batch 19669, Loss: 178.26100158691406\n",
      "Epoch 62, Batch 19670, Loss: 174.55654907226562\n",
      "Epoch 62, Batch 19671, Loss: 188.71327209472656\n",
      "Epoch 62, Batch 19672, Loss: 173.6678009033203\n",
      "Epoch 62, Batch 19673, Loss: 170.06690979003906\n",
      "Epoch 62, Batch 19674, Loss: 169.7028350830078\n",
      "Epoch 62, Batch 19675, Loss: 160.9617156982422\n",
      "Epoch 62, Batch 19676, Loss: 172.3970184326172\n",
      "Epoch 62, Batch 19677, Loss: 157.30194091796875\n",
      "Epoch 62, Batch 19678, Loss: 167.1737823486328\n",
      "Epoch 62, Batch 19679, Loss: 176.6739044189453\n",
      "Epoch 62, Batch 19680, Loss: 186.26954650878906\n",
      "Epoch 62, Batch 19681, Loss: 182.35421752929688\n",
      "Epoch 62, Batch 19682, Loss: 187.9027099609375\n",
      "Epoch 62, Batch 19683, Loss: 178.38818359375\n",
      "Epoch 62, Batch 19684, Loss: 171.1399688720703\n",
      "Epoch 62, Batch 19685, Loss: 165.8548583984375\n",
      "Epoch 62, Batch 19686, Loss: 156.5455780029297\n",
      "Epoch 62, Batch 19687, Loss: 176.352783203125\n",
      "Epoch 62, Batch 19688, Loss: 172.6063232421875\n",
      "Epoch 62, Batch 19689, Loss: 181.8830108642578\n",
      "Epoch 62, Batch 19690, Loss: 188.662841796875\n",
      "Epoch 62, Batch 19691, Loss: 178.9528045654297\n",
      "Epoch 62, Batch 19692, Loss: 164.38351440429688\n",
      "Epoch 62, Batch 19693, Loss: 170.44793701171875\n",
      "Epoch 62, Batch 19694, Loss: 195.45262145996094\n",
      "Epoch 62, Batch 19695, Loss: 175.8472900390625\n",
      "Epoch 62, Batch 19696, Loss: 176.81752014160156\n",
      "Epoch 62, Batch 19697, Loss: 172.37986755371094\n",
      "Epoch 62, Batch 19698, Loss: 197.66009521484375\n",
      "Epoch 62, Batch 19699, Loss: 159.28318786621094\n",
      "Epoch 62, Batch 19700, Loss: 186.08984375\n",
      "Epoch 62, Batch 19701, Loss: 168.1458740234375\n",
      "Epoch 62, Batch 19702, Loss: 166.35023498535156\n",
      "Epoch 62, Batch 19703, Loss: 169.56167602539062\n",
      "Epoch 62, Batch 19704, Loss: 191.81155395507812\n",
      "Epoch 62, Batch 19705, Loss: 160.08261108398438\n",
      "Epoch 62, Batch 19706, Loss: 165.564453125\n",
      "Epoch 62, Batch 19707, Loss: 184.74835205078125\n",
      "Epoch 62, Batch 19708, Loss: 178.44696044921875\n",
      "Epoch 62, Batch 19709, Loss: 177.44879150390625\n",
      "Epoch 62, Batch 19710, Loss: 183.4075927734375\n",
      "Epoch 62, Batch 19711, Loss: 184.96250915527344\n",
      "Epoch 62, Batch 19712, Loss: 161.4718017578125\n",
      "Epoch 62, Batch 19713, Loss: 167.99794006347656\n",
      "Epoch 62, Batch 19714, Loss: 158.28564453125\n",
      "Epoch 62, Batch 19715, Loss: 164.97706604003906\n",
      "Epoch 62, Batch 19716, Loss: 169.33218383789062\n",
      "Epoch 62, Batch 19717, Loss: 169.2034912109375\n",
      "Epoch 62, Batch 19718, Loss: 166.8118438720703\n",
      "Epoch 62, Batch 19719, Loss: 171.5481414794922\n",
      "Epoch 62, Batch 19720, Loss: 150.88897705078125\n",
      "Epoch 62, Batch 19721, Loss: 174.28366088867188\n",
      "Epoch 62, Batch 19722, Loss: 194.31483459472656\n",
      "Epoch 62, Batch 19723, Loss: 180.7703399658203\n",
      "Epoch 62, Batch 19724, Loss: 166.36668395996094\n",
      "Epoch 62, Batch 19725, Loss: 187.5498504638672\n",
      "Epoch 62, Batch 19726, Loss: 160.70323181152344\n",
      "Epoch 62, Batch 19727, Loss: 175.44810485839844\n",
      "Epoch 62, Batch 19728, Loss: 154.7969207763672\n",
      "Epoch 62, Batch 19729, Loss: 170.9259033203125\n",
      "Epoch 62, Batch 19730, Loss: 179.28408813476562\n",
      "Epoch 62, Batch 19731, Loss: 169.27349853515625\n",
      "Epoch 62, Batch 19732, Loss: 181.55494689941406\n",
      "Epoch 62, Batch 19733, Loss: 159.6120147705078\n",
      "Epoch 62, Batch 19734, Loss: 174.80641174316406\n",
      "Epoch 62, Batch 19735, Loss: 179.12435913085938\n",
      "Epoch 62, Batch 19736, Loss: 156.93821716308594\n",
      "Epoch 62, Batch 19737, Loss: 186.30215454101562\n",
      "Epoch 62, Batch 19738, Loss: 183.76687622070312\n",
      "Epoch 62, Batch 19739, Loss: 172.31228637695312\n",
      "Epoch 62, Batch 19740, Loss: 162.61758422851562\n",
      "Epoch 62, Batch 19741, Loss: 161.5106201171875\n",
      "Epoch 62, Batch 19742, Loss: 187.3512725830078\n",
      "Epoch 62, Batch 19743, Loss: 184.4872589111328\n",
      "Epoch 62, Batch 19744, Loss: 182.986328125\n",
      "Epoch 62, Batch 19745, Loss: 172.7530517578125\n",
      "Epoch 62, Batch 19746, Loss: 159.3346405029297\n",
      "Epoch 62, Batch 19747, Loss: 172.7240447998047\n",
      "Epoch 62, Batch 19748, Loss: 174.127685546875\n",
      "Epoch 62, Batch 19749, Loss: 183.53091430664062\n",
      "Epoch 62, Batch 19750, Loss: 177.63409423828125\n",
      "Epoch 62, Batch 19751, Loss: 163.28680419921875\n",
      "Epoch 62, Batch 19752, Loss: 168.9261016845703\n",
      "Epoch 62, Batch 19753, Loss: 191.92660522460938\n",
      "Epoch 62, Batch 19754, Loss: 173.50328063964844\n",
      "Epoch 62, Batch 19755, Loss: 177.6273651123047\n",
      "Epoch 62, Batch 19756, Loss: 172.9362335205078\n",
      "Epoch 62, Batch 19757, Loss: 176.5009002685547\n",
      "Epoch 62, Batch 19758, Loss: 173.599853515625\n",
      "Epoch 62, Batch 19759, Loss: 192.07479858398438\n",
      "Epoch 62, Batch 19760, Loss: 179.40985107421875\n",
      "Epoch 62, Batch 19761, Loss: 167.50701904296875\n",
      "Epoch 62, Batch 19762, Loss: 170.46266174316406\n",
      "Epoch 62, Batch 19763, Loss: 188.40769958496094\n",
      "Epoch 62, Batch 19764, Loss: 180.47589111328125\n",
      "Epoch 62, Batch 19765, Loss: 163.3076171875\n",
      "Epoch 62, Batch 19766, Loss: 183.5341796875\n",
      "Epoch 62, Batch 19767, Loss: 164.26487731933594\n",
      "Epoch 62, Batch 19768, Loss: 193.4247283935547\n",
      "Epoch 62, Batch 19769, Loss: 167.4593963623047\n",
      "Epoch 62, Batch 19770, Loss: 160.571533203125\n",
      "Epoch 62, Batch 19771, Loss: 174.89801025390625\n",
      "Epoch 62, Batch 19772, Loss: 163.25794982910156\n",
      "Epoch 62, Batch 19773, Loss: 166.20098876953125\n",
      "Epoch 62, Batch 19774, Loss: 156.06942749023438\n",
      "Epoch 62, Batch 19775, Loss: 185.0808868408203\n",
      "Epoch 62, Batch 19776, Loss: 170.2301483154297\n",
      "Epoch 62, Batch 19777, Loss: 166.7353515625\n",
      "Epoch 62, Batch 19778, Loss: 167.66392517089844\n",
      "Epoch 62, Batch 19779, Loss: 177.07638549804688\n",
      "Epoch 62, Batch 19780, Loss: 169.54513549804688\n",
      "Epoch 62, Batch 19781, Loss: 178.37220764160156\n",
      "Epoch 62, Batch 19782, Loss: 173.92205810546875\n",
      "Epoch 62, Batch 19783, Loss: 185.95364379882812\n",
      "Epoch 62, Batch 19784, Loss: 176.51705932617188\n",
      "Epoch 62, Batch 19785, Loss: 163.9603729248047\n",
      "Epoch 62, Batch 19786, Loss: 166.93463134765625\n",
      "Epoch 62, Batch 19787, Loss: 198.5425262451172\n",
      "Epoch 62, Batch 19788, Loss: 174.34197998046875\n",
      "Epoch 62, Batch 19789, Loss: 174.34523010253906\n",
      "Epoch 62, Batch 19790, Loss: 181.33297729492188\n",
      "Epoch 62, Batch 19791, Loss: 169.2696075439453\n",
      "Epoch 62, Batch 19792, Loss: 165.3956756591797\n",
      "Epoch 62, Batch 19793, Loss: 156.3478546142578\n",
      "Epoch 62, Batch 19794, Loss: 193.80218505859375\n",
      "Epoch 62, Batch 19795, Loss: 157.6761474609375\n",
      "Epoch 62, Batch 19796, Loss: 157.4318084716797\n",
      "Epoch 62, Batch 19797, Loss: 164.5274200439453\n",
      "Epoch 62, Batch 19798, Loss: 171.85031127929688\n",
      "Epoch 62, Batch 19799, Loss: 184.10572814941406\n",
      "Epoch 62, Batch 19800, Loss: 154.29380798339844\n",
      "Epoch 62, Batch 19801, Loss: 181.0435333251953\n",
      "Epoch 62, Batch 19802, Loss: 162.35629272460938\n",
      "Epoch 62, Batch 19803, Loss: 163.09446716308594\n",
      "Epoch 62, Batch 19804, Loss: 163.29385375976562\n",
      "Epoch 62, Batch 19805, Loss: 177.6022491455078\n",
      "Epoch 62, Batch 19806, Loss: 184.1848602294922\n",
      "Epoch 62, Batch 19807, Loss: 174.36900329589844\n",
      "Epoch 62, Batch 19808, Loss: 178.25535583496094\n",
      "Epoch 62, Batch 19809, Loss: 155.7234649658203\n",
      "Epoch 62, Batch 19810, Loss: 168.08322143554688\n",
      "Epoch 62, Batch 19811, Loss: 179.25091552734375\n",
      "Epoch 62, Batch 19812, Loss: 172.30494689941406\n",
      "Epoch 62, Batch 19813, Loss: 184.6522216796875\n",
      "Epoch 62, Batch 19814, Loss: 167.3717803955078\n",
      "Epoch 62, Batch 19815, Loss: 178.54135131835938\n",
      "Epoch 62, Batch 19816, Loss: 184.44696044921875\n",
      "Epoch 62, Batch 19817, Loss: 191.06398010253906\n",
      "Epoch 62, Batch 19818, Loss: 184.37884521484375\n",
      "Epoch 62, Batch 19819, Loss: 182.55662536621094\n",
      "Epoch 62, Batch 19820, Loss: 159.43006896972656\n",
      "Epoch 62, Batch 19821, Loss: 179.2124481201172\n",
      "Epoch 62, Batch 19822, Loss: 168.53367614746094\n",
      "Epoch 62, Batch 19823, Loss: 186.61773681640625\n",
      "Epoch 62, Batch 19824, Loss: 167.41578674316406\n",
      "Epoch 62, Batch 19825, Loss: 182.591552734375\n",
      "Epoch 62, Batch 19826, Loss: 168.24362182617188\n",
      "Epoch 62, Batch 19827, Loss: 169.59458923339844\n",
      "Epoch 62, Batch 19828, Loss: 186.94094848632812\n",
      "Epoch 62, Batch 19829, Loss: 166.05702209472656\n",
      "Epoch 62, Batch 19830, Loss: 180.70809936523438\n",
      "Epoch 62, Batch 19831, Loss: 165.4294891357422\n",
      "Epoch 62, Batch 19832, Loss: 167.32232666015625\n",
      "Epoch 62, Batch 19833, Loss: 173.7596893310547\n",
      "Epoch 62, Batch 19834, Loss: 191.92384338378906\n",
      "Epoch 62, Batch 19835, Loss: 180.98770141601562\n",
      "Epoch 62, Batch 19836, Loss: 181.22235107421875\n",
      "Epoch 62, Batch 19837, Loss: 172.1484832763672\n",
      "Epoch 62, Batch 19838, Loss: 169.7775421142578\n",
      "Epoch 62, Batch 19839, Loss: 161.6820068359375\n",
      "Epoch 62, Batch 19840, Loss: 180.24024963378906\n",
      "Epoch 62, Batch 19841, Loss: 165.3726043701172\n",
      "Epoch 62, Batch 19842, Loss: 172.5413818359375\n",
      "Epoch 62, Batch 19843, Loss: 173.01478576660156\n",
      "Epoch 62, Batch 19844, Loss: 165.8825225830078\n",
      "Epoch 62, Batch 19845, Loss: 177.23890686035156\n",
      "Epoch 62, Batch 19846, Loss: 179.9619140625\n",
      "Epoch 62, Batch 19847, Loss: 179.17347717285156\n",
      "Epoch 62, Batch 19848, Loss: 205.5375518798828\n",
      "Epoch 62, Batch 19849, Loss: 176.29978942871094\n",
      "Epoch 62, Batch 19850, Loss: 171.07302856445312\n",
      "Epoch 62, Batch 19851, Loss: 184.69097900390625\n",
      "Epoch 62, Batch 19852, Loss: 201.93829345703125\n",
      "Epoch 62, Batch 19853, Loss: 199.0333251953125\n",
      "Epoch 62, Batch 19854, Loss: 183.81393432617188\n",
      "Epoch 62, Batch 19855, Loss: 191.95816040039062\n",
      "Epoch 62, Batch 19856, Loss: 171.33242797851562\n",
      "Epoch 62, Batch 19857, Loss: 178.2551727294922\n",
      "Epoch 62, Batch 19858, Loss: 171.739013671875\n",
      "Epoch 62, Batch 19859, Loss: 175.37164306640625\n",
      "Epoch 62, Batch 19860, Loss: 155.2157440185547\n",
      "Epoch 62, Batch 19861, Loss: 185.85426330566406\n",
      "Epoch 62, Batch 19862, Loss: 185.77993774414062\n",
      "Epoch 62, Batch 19863, Loss: 154.07691955566406\n",
      "Epoch 62, Batch 19864, Loss: 157.43975830078125\n",
      "Epoch 62, Batch 19865, Loss: 173.30538940429688\n",
      "Epoch 62, Batch 19866, Loss: 196.98477172851562\n",
      "Epoch 62, Batch 19867, Loss: 171.70108032226562\n",
      "Epoch 62, Batch 19868, Loss: 177.87432861328125\n",
      "Epoch 62, Batch 19869, Loss: 161.05918884277344\n",
      "Epoch 62, Batch 19870, Loss: 160.5750732421875\n",
      "Epoch 62, Batch 19871, Loss: 167.5220184326172\n",
      "Epoch 62, Batch 19872, Loss: 174.5928192138672\n",
      "Epoch 62, Batch 19873, Loss: 156.11343383789062\n",
      "Epoch 62, Batch 19874, Loss: 165.6213836669922\n",
      "Epoch 62, Batch 19875, Loss: 183.8892364501953\n",
      "Epoch 62, Batch 19876, Loss: 192.62022399902344\n",
      "Epoch 62, Batch 19877, Loss: 162.45050048828125\n",
      "Epoch 62, Batch 19878, Loss: 169.98789978027344\n",
      "Epoch 62, Batch 19879, Loss: 166.85977172851562\n",
      "Epoch 62, Batch 19880, Loss: 177.13925170898438\n",
      "Epoch 62, Batch 19881, Loss: 175.81727600097656\n",
      "Epoch 62, Batch 19882, Loss: 163.51808166503906\n",
      "Epoch 62, Batch 19883, Loss: 177.2529296875\n",
      "Epoch 62, Batch 19884, Loss: 186.87359619140625\n",
      "Epoch 62, Batch 19885, Loss: 176.7509002685547\n",
      "Epoch 62, Batch 19886, Loss: 174.95591735839844\n",
      "Epoch 62, Batch 19887, Loss: 192.1984405517578\n",
      "Epoch 62, Batch 19888, Loss: 177.88238525390625\n",
      "Epoch 62, Batch 19889, Loss: 172.7171173095703\n",
      "Epoch 62, Batch 19890, Loss: 174.58343505859375\n",
      "Epoch 62, Batch 19891, Loss: 184.4889678955078\n",
      "Epoch 62, Batch 19892, Loss: 172.57215881347656\n",
      "Epoch 62, Batch 19893, Loss: 189.15106201171875\n",
      "Epoch 62, Batch 19894, Loss: 190.41683959960938\n",
      "Epoch 62, Batch 19895, Loss: 166.30081176757812\n",
      "Epoch 62, Batch 19896, Loss: 160.24037170410156\n",
      "Epoch 62, Batch 19897, Loss: 165.98414611816406\n",
      "Epoch 62, Batch 19898, Loss: 169.96315002441406\n",
      "Epoch 62, Batch 19899, Loss: 171.57647705078125\n",
      "Epoch 62, Batch 19900, Loss: 170.33485412597656\n",
      "Epoch 62, Batch 19901, Loss: 164.69439697265625\n",
      "Epoch 62, Batch 19902, Loss: 169.81118774414062\n",
      "Epoch 62, Batch 19903, Loss: 183.14060974121094\n",
      "Epoch 62, Batch 19904, Loss: 197.479248046875\n",
      "Epoch 62, Batch 19905, Loss: 179.6565399169922\n",
      "Epoch 62, Batch 19906, Loss: 171.18800354003906\n",
      "Epoch 62, Batch 19907, Loss: 176.25563049316406\n",
      "Epoch 62, Batch 19908, Loss: 162.5164337158203\n",
      "Epoch 62, Batch 19909, Loss: 159.284423828125\n",
      "Epoch 62, Batch 19910, Loss: 170.50662231445312\n",
      "Epoch 62, Batch 19911, Loss: 174.87799072265625\n",
      "Epoch 62, Batch 19912, Loss: 181.57467651367188\n",
      "Epoch 62, Batch 19913, Loss: 172.78713989257812\n",
      "Epoch 62, Batch 19914, Loss: 169.09754943847656\n",
      "Epoch 62, Batch 19915, Loss: 172.2511749267578\n",
      "Epoch 62, Batch 19916, Loss: 174.1474151611328\n",
      "Epoch 62, Batch 19917, Loss: 203.6695098876953\n",
      "Epoch 62, Batch 19918, Loss: 171.2262725830078\n",
      "Epoch 62, Batch 19919, Loss: 171.36041259765625\n",
      "Epoch 62, Batch 19920, Loss: 184.2932891845703\n",
      "Epoch 62, Batch 19921, Loss: 188.21185302734375\n",
      "Epoch 62, Batch 19922, Loss: 176.33465576171875\n",
      "Epoch 62, Batch 19923, Loss: 153.54708862304688\n",
      "Epoch 62, Batch 19924, Loss: 177.50494384765625\n",
      "Epoch 62, Batch 19925, Loss: 184.08999633789062\n",
      "Epoch 62, Batch 19926, Loss: 168.60116577148438\n",
      "Epoch 62, Batch 19927, Loss: 161.3900909423828\n",
      "Epoch 62, Batch 19928, Loss: 181.72198486328125\n",
      "Epoch 62, Batch 19929, Loss: 164.23947143554688\n",
      "Epoch 62, Batch 19930, Loss: 174.78497314453125\n",
      "Epoch 62, Batch 19931, Loss: 167.9219970703125\n",
      "Epoch 62, Batch 19932, Loss: 166.033447265625\n",
      "Epoch 62, Batch 19933, Loss: 177.25283813476562\n",
      "Epoch 62, Batch 19934, Loss: 159.72256469726562\n",
      "Epoch 62, Batch 19935, Loss: 169.3499755859375\n",
      "Epoch 62, Batch 19936, Loss: 164.63929748535156\n",
      "Epoch 62, Batch 19937, Loss: 184.13853454589844\n",
      "Epoch 62, Batch 19938, Loss: 169.32223510742188\n",
      "Epoch 62, Batch 19939, Loss: 170.2585906982422\n",
      "Epoch 62, Batch 19940, Loss: 157.063720703125\n",
      "Epoch 62, Batch 19941, Loss: 163.45828247070312\n",
      "Epoch 62, Batch 19942, Loss: 179.96669006347656\n",
      "Epoch 62, Batch 19943, Loss: 176.97967529296875\n",
      "Epoch 62, Batch 19944, Loss: 161.2275848388672\n",
      "Epoch 62, Batch 19945, Loss: 168.40037536621094\n",
      "Epoch 62, Batch 19946, Loss: 187.4865264892578\n",
      "Epoch 62, Batch 19947, Loss: 158.65176391601562\n",
      "Epoch 62, Batch 19948, Loss: 170.52593994140625\n",
      "Epoch 62, Batch 19949, Loss: 182.81924438476562\n",
      "Epoch 62, Batch 19950, Loss: 188.67123413085938\n",
      "Epoch 62, Batch 19951, Loss: 168.5326690673828\n",
      "Epoch 62, Batch 19952, Loss: 163.95709228515625\n",
      "Epoch 62, Batch 19953, Loss: 170.2129364013672\n",
      "Epoch 62, Batch 19954, Loss: 175.5411834716797\n",
      "Epoch 62, Batch 19955, Loss: 197.32676696777344\n",
      "Epoch 62, Batch 19956, Loss: 183.9315643310547\n",
      "Epoch 62, Batch 19957, Loss: 191.7800750732422\n",
      "Epoch 62, Batch 19958, Loss: 175.52749633789062\n",
      "Epoch 62, Batch 19959, Loss: 157.4734649658203\n",
      "Epoch 62, Batch 19960, Loss: 172.2124786376953\n",
      "Epoch 62, Batch 19961, Loss: 169.3683319091797\n",
      "Epoch 62, Batch 19962, Loss: 147.08116149902344\n",
      "Epoch 62, Batch 19963, Loss: 183.96017456054688\n",
      "Epoch 62, Batch 19964, Loss: 189.44158935546875\n",
      "Epoch 62, Batch 19965, Loss: 172.322021484375\n",
      "Epoch 62, Batch 19966, Loss: 165.5326690673828\n",
      "Epoch 62, Batch 19967, Loss: 173.3470001220703\n",
      "Epoch 62, Batch 19968, Loss: 162.6470489501953\n",
      "Epoch 62, Batch 19969, Loss: 168.47396850585938\n",
      "Epoch 62, Batch 19970, Loss: 182.26419067382812\n",
      "Epoch 62, Batch 19971, Loss: 188.46463012695312\n",
      "Epoch 62, Batch 19972, Loss: 166.88160705566406\n",
      "Epoch 62, Batch 19973, Loss: 166.94117736816406\n",
      "Epoch 62, Batch 19974, Loss: 181.4210662841797\n",
      "Epoch 62, Batch 19975, Loss: 177.53436279296875\n",
      "Epoch 62, Batch 19976, Loss: 172.4324188232422\n",
      "Epoch 62, Batch 19977, Loss: 167.4364776611328\n",
      "Epoch 62, Batch 19978, Loss: 184.3568572998047\n",
      "Epoch 62, Batch 19979, Loss: 170.1573944091797\n",
      "Epoch 62, Batch 19980, Loss: 162.5755615234375\n",
      "Epoch 62, Batch 19981, Loss: 158.97396850585938\n",
      "Epoch 62, Batch 19982, Loss: 173.40048217773438\n",
      "Epoch 62, Batch 19983, Loss: 177.5211639404297\n",
      "Epoch 62, Batch 19984, Loss: 159.171142578125\n",
      "Epoch 62, Batch 19985, Loss: 163.7129364013672\n",
      "Epoch 62, Batch 19986, Loss: 189.36940002441406\n",
      "Epoch 62, Batch 19987, Loss: 168.31884765625\n",
      "Epoch 62, Batch 19988, Loss: 157.0956268310547\n",
      "Epoch 62, Batch 19989, Loss: 164.08944702148438\n",
      "Epoch 62, Batch 19990, Loss: 177.7049560546875\n",
      "Epoch 62, Batch 19991, Loss: 177.07247924804688\n",
      "Epoch 62, Batch 19992, Loss: 162.73194885253906\n",
      "Epoch 62, Batch 19993, Loss: 169.1160125732422\n",
      "Epoch 62, Batch 19994, Loss: 175.1167755126953\n",
      "Epoch 62, Batch 19995, Loss: 176.39517211914062\n",
      "Epoch 62, Batch 19996, Loss: 194.53250122070312\n",
      "Epoch 62, Batch 19997, Loss: 165.68443298339844\n",
      "Epoch 62, Batch 19998, Loss: 161.85971069335938\n",
      "Epoch 62, Batch 19999, Loss: 174.07949829101562\n",
      "Epoch 62, Batch 20000, Loss: 164.3572998046875\n",
      "Epoch 62, Batch 20001, Loss: 173.9902801513672\n",
      "Epoch 62, Batch 20002, Loss: 179.51190185546875\n",
      "Epoch 62, Batch 20003, Loss: 159.98284912109375\n",
      "Epoch 62, Batch 20004, Loss: 169.03993225097656\n",
      "Epoch 62, Batch 20005, Loss: 183.54769897460938\n",
      "Epoch 62, Batch 20006, Loss: 167.9500274658203\n",
      "Epoch 62, Batch 20007, Loss: 168.0457000732422\n",
      "Epoch 62, Batch 20008, Loss: 168.13143920898438\n",
      "Epoch 62, Batch 20009, Loss: 190.8922576904297\n",
      "Epoch 62, Batch 20010, Loss: 168.90573120117188\n",
      "Epoch 62, Batch 20011, Loss: 160.9498748779297\n",
      "Epoch 62, Batch 20012, Loss: 177.03904724121094\n",
      "Epoch 62, Batch 20013, Loss: 182.16856384277344\n",
      "Epoch 62, Batch 20014, Loss: 150.93800354003906\n",
      "Epoch 62, Batch 20015, Loss: 168.6156768798828\n",
      "Epoch 62, Batch 20016, Loss: 171.7645263671875\n",
      "Epoch 62, Batch 20017, Loss: 170.2519073486328\n",
      "Epoch 62, Batch 20018, Loss: 176.8502655029297\n",
      "Epoch 62, Batch 20019, Loss: 173.11338806152344\n",
      "Epoch 62, Batch 20020, Loss: 162.95513916015625\n",
      "Epoch 62, Batch 20021, Loss: 177.91090393066406\n",
      "Epoch 62, Batch 20022, Loss: 185.78529357910156\n",
      "Epoch 62, Batch 20023, Loss: 154.63426208496094\n",
      "Epoch 62, Batch 20024, Loss: 173.1939697265625\n",
      "Epoch 62, Batch 20025, Loss: 173.7083282470703\n",
      "Epoch 62, Batch 20026, Loss: 169.6880645751953\n",
      "Epoch 62, Batch 20027, Loss: 179.15684509277344\n",
      "Epoch 62, Batch 20028, Loss: 174.9617919921875\n",
      "Epoch 62, Batch 20029, Loss: 178.35342407226562\n",
      "Epoch 62, Batch 20030, Loss: 192.27125549316406\n",
      "Epoch 62, Batch 20031, Loss: 178.66561889648438\n",
      "Epoch 62, Batch 20032, Loss: 181.93141174316406\n",
      "Epoch 62, Batch 20033, Loss: 170.44483947753906\n",
      "Epoch 62, Batch 20034, Loss: 174.73599243164062\n",
      "Epoch 62, Batch 20035, Loss: 172.64683532714844\n",
      "Epoch 62, Batch 20036, Loss: 174.2516326904297\n",
      "Epoch 62, Batch 20037, Loss: 179.37928771972656\n",
      "Epoch 62, Batch 20038, Loss: 192.61473083496094\n",
      "Epoch 62, Batch 20039, Loss: 183.71739196777344\n",
      "Epoch 62, Batch 20040, Loss: 191.78639221191406\n",
      "Epoch 62, Batch 20041, Loss: 169.48251342773438\n",
      "Epoch 62, Batch 20042, Loss: 169.41104125976562\n",
      "Epoch 62, Batch 20043, Loss: 177.1659698486328\n",
      "Epoch 62, Batch 20044, Loss: 177.23568725585938\n",
      "Epoch 62, Batch 20045, Loss: 177.21253967285156\n",
      "Epoch 62, Batch 20046, Loss: 185.45545959472656\n",
      "Epoch 62, Batch 20047, Loss: 165.53829956054688\n",
      "Epoch 62, Batch 20048, Loss: 169.06643676757812\n",
      "Epoch 62, Batch 20049, Loss: 172.50125122070312\n",
      "Epoch 62, Batch 20050, Loss: 185.464111328125\n",
      "Epoch 62, Batch 20051, Loss: 173.3348846435547\n",
      "Epoch 62, Batch 20052, Loss: 169.28115844726562\n",
      "Epoch 62, Batch 20053, Loss: 186.47171020507812\n",
      "Epoch 62, Batch 20054, Loss: 163.50181579589844\n",
      "Epoch 62, Batch 20055, Loss: 185.31398010253906\n",
      "Epoch 62, Batch 20056, Loss: 156.87527465820312\n",
      "Epoch 62, Batch 20057, Loss: 174.26222229003906\n",
      "Epoch 62, Batch 20058, Loss: 180.47647094726562\n",
      "Epoch 62, Batch 20059, Loss: 174.1328887939453\n",
      "Epoch 62, Batch 20060, Loss: 162.45819091796875\n",
      "Epoch 62, Batch 20061, Loss: 179.9361114501953\n",
      "Epoch 62, Batch 20062, Loss: 173.974365234375\n",
      "Epoch 62, Batch 20063, Loss: 165.77664184570312\n",
      "Epoch 62, Batch 20064, Loss: 182.6815948486328\n",
      "Epoch 62, Batch 20065, Loss: 180.8070526123047\n",
      "Epoch 62, Batch 20066, Loss: 158.15606689453125\n",
      "Epoch 62, Batch 20067, Loss: 163.3172149658203\n",
      "Epoch 62, Batch 20068, Loss: 158.9627227783203\n",
      "Epoch 62, Batch 20069, Loss: 163.8726043701172\n",
      "Epoch 62, Batch 20070, Loss: 163.13339233398438\n",
      "Epoch 62, Batch 20071, Loss: 164.42494201660156\n",
      "Epoch 62, Batch 20072, Loss: 148.22303771972656\n",
      "Epoch 62, Batch 20073, Loss: 173.03726196289062\n",
      "Epoch 62, Batch 20074, Loss: 183.00338745117188\n",
      "Epoch 62, Batch 20075, Loss: 165.7410430908203\n",
      "Epoch 62, Batch 20076, Loss: 176.3024444580078\n",
      "Epoch 62, Batch 20077, Loss: 176.10983276367188\n",
      "Epoch 62, Batch 20078, Loss: 174.650146484375\n",
      "Epoch 62, Batch 20079, Loss: 182.68173217773438\n",
      "Epoch 62, Batch 20080, Loss: 167.5877227783203\n",
      "Epoch 62, Batch 20081, Loss: 178.15841674804688\n",
      "Epoch 62, Batch 20082, Loss: 171.8036346435547\n",
      "Epoch 62, Batch 20083, Loss: 164.94363403320312\n",
      "Epoch 62, Batch 20084, Loss: 179.47305297851562\n",
      "Epoch 62, Batch 20085, Loss: 160.71737670898438\n",
      "Epoch 62, Batch 20086, Loss: 167.62290954589844\n",
      "Epoch 62, Batch 20087, Loss: 166.50732421875\n",
      "Epoch 62, Batch 20088, Loss: 173.61444091796875\n",
      "Epoch 62, Batch 20089, Loss: 183.03439331054688\n",
      "Epoch 62, Batch 20090, Loss: 179.90768432617188\n",
      "Epoch 62, Batch 20091, Loss: 177.3310546875\n",
      "Epoch 62, Batch 20092, Loss: 160.55018615722656\n",
      "Epoch 62, Batch 20093, Loss: 161.59080505371094\n",
      "Epoch 62, Batch 20094, Loss: 169.58453369140625\n",
      "Epoch 62, Batch 20095, Loss: 173.60159301757812\n",
      "Epoch 62, Batch 20096, Loss: 162.5745391845703\n",
      "Epoch 62, Batch 20097, Loss: 185.46173095703125\n",
      "Epoch 62, Batch 20098, Loss: 172.66891479492188\n",
      "Epoch 62, Batch 20099, Loss: 167.5374755859375\n",
      "Epoch 62, Batch 20100, Loss: 177.45327758789062\n",
      "Epoch 62, Batch 20101, Loss: 156.0463409423828\n",
      "Epoch 62, Batch 20102, Loss: 178.5897674560547\n",
      "Epoch 62, Batch 20103, Loss: 172.4683380126953\n",
      "Epoch 62, Batch 20104, Loss: 167.0478515625\n",
      "Epoch 62, Batch 20105, Loss: 170.7681121826172\n",
      "Epoch 62, Batch 20106, Loss: 176.34671020507812\n",
      "Epoch 62, Batch 20107, Loss: 190.38882446289062\n",
      "Epoch 62, Batch 20108, Loss: 176.648193359375\n",
      "Epoch 62, Batch 20109, Loss: 182.01576232910156\n",
      "Epoch 62, Batch 20110, Loss: 169.9437255859375\n",
      "Epoch 62, Batch 20111, Loss: 172.56398010253906\n",
      "Epoch 62, Batch 20112, Loss: 175.05020141601562\n",
      "Epoch 62, Batch 20113, Loss: 186.14337158203125\n",
      "Epoch 62, Batch 20114, Loss: 171.12916564941406\n",
      "Epoch 62, Batch 20115, Loss: 175.98980712890625\n",
      "Epoch 62, Batch 20116, Loss: 177.7929229736328\n",
      "Epoch 62, Batch 20117, Loss: 180.26612854003906\n",
      "Epoch 62, Batch 20118, Loss: 164.1851348876953\n",
      "Epoch 62, Batch 20119, Loss: 175.08111572265625\n",
      "Epoch 62, Batch 20120, Loss: 181.52447509765625\n",
      "Epoch 62, Batch 20121, Loss: 160.1193389892578\n",
      "Epoch 62, Batch 20122, Loss: 170.3809814453125\n",
      "Epoch 62, Batch 20123, Loss: 167.93907165527344\n",
      "Epoch 62, Batch 20124, Loss: 167.76797485351562\n",
      "Epoch 62, Batch 20125, Loss: 177.9838409423828\n",
      "Epoch 62, Batch 20126, Loss: 193.88858032226562\n",
      "Epoch 62, Batch 20127, Loss: 170.13345336914062\n",
      "Epoch 62, Batch 20128, Loss: 178.00669860839844\n",
      "Epoch 62, Batch 20129, Loss: 167.6332244873047\n",
      "Epoch 62, Batch 20130, Loss: 178.06869506835938\n",
      "Epoch 62, Batch 20131, Loss: 173.96763610839844\n",
      "Epoch 62, Batch 20132, Loss: 171.48263549804688\n",
      "Epoch 62, Batch 20133, Loss: 188.13356018066406\n",
      "Epoch 62, Batch 20134, Loss: 166.3649444580078\n",
      "Epoch 62, Batch 20135, Loss: 170.27337646484375\n",
      "Epoch 62, Batch 20136, Loss: 196.05618286132812\n",
      "Epoch 62, Batch 20137, Loss: 163.68096923828125\n",
      "Epoch 62, Batch 20138, Loss: 161.8966064453125\n",
      "Epoch 62, Batch 20139, Loss: 171.22817993164062\n",
      "Epoch 62, Batch 20140, Loss: 164.71890258789062\n",
      "Epoch 62, Batch 20141, Loss: 166.0636444091797\n",
      "Epoch 62, Batch 20142, Loss: 184.09695434570312\n",
      "Epoch 62, Batch 20143, Loss: 182.05072021484375\n",
      "Epoch 62, Batch 20144, Loss: 177.95159912109375\n",
      "Epoch 62, Batch 20145, Loss: 158.08950805664062\n",
      "Epoch 62, Batch 20146, Loss: 175.93927001953125\n",
      "Epoch 62, Batch 20147, Loss: 179.24685668945312\n",
      "Epoch 62, Batch 20148, Loss: 180.93478393554688\n",
      "Epoch 62, Batch 20149, Loss: 169.22097778320312\n",
      "Epoch 62, Batch 20150, Loss: 171.60594177246094\n",
      "Epoch 62, Batch 20151, Loss: 188.49705505371094\n",
      "Epoch 62, Batch 20152, Loss: 180.47506713867188\n",
      "Epoch 62, Batch 20153, Loss: 161.0849151611328\n",
      "Epoch 62, Batch 20154, Loss: 167.1539764404297\n",
      "Epoch 62, Batch 20155, Loss: 189.7510223388672\n",
      "Epoch 62, Batch 20156, Loss: 164.28369140625\n",
      "Epoch 62, Batch 20157, Loss: 171.02734375\n",
      "Epoch 62, Batch 20158, Loss: 169.46185302734375\n",
      "Epoch 62, Batch 20159, Loss: 168.4004669189453\n",
      "Epoch 62, Batch 20160, Loss: 185.03636169433594\n",
      "Epoch 62, Batch 20161, Loss: 177.10838317871094\n",
      "Epoch 62, Batch 20162, Loss: 180.0077362060547\n",
      "Epoch 62, Batch 20163, Loss: 185.02090454101562\n",
      "Epoch 62, Batch 20164, Loss: 169.01853942871094\n",
      "Epoch 62, Batch 20165, Loss: 176.18014526367188\n",
      "Epoch 62, Batch 20166, Loss: 183.52452087402344\n",
      "Epoch 62, Batch 20167, Loss: 173.7422332763672\n",
      "Epoch 62, Batch 20168, Loss: 178.84291076660156\n",
      "Epoch 62, Batch 20169, Loss: 178.81808471679688\n",
      "Epoch 62, Batch 20170, Loss: 184.0881805419922\n",
      "Epoch 62, Batch 20171, Loss: 177.16665649414062\n",
      "Epoch 62, Batch 20172, Loss: 164.59967041015625\n",
      "Epoch 62, Batch 20173, Loss: 190.9579315185547\n",
      "Epoch 62, Batch 20174, Loss: 175.29409790039062\n",
      "Epoch 62, Batch 20175, Loss: 177.20516967773438\n",
      "Epoch 62, Batch 20176, Loss: 168.71005249023438\n",
      "Epoch 62, Batch 20177, Loss: 185.3039093017578\n",
      "Epoch 62, Batch 20178, Loss: 169.31150817871094\n",
      "Epoch 62, Batch 20179, Loss: 168.64781188964844\n",
      "Epoch 62, Batch 20180, Loss: 162.4208221435547\n",
      "Epoch 62, Batch 20181, Loss: 172.4534454345703\n",
      "Epoch 62, Batch 20182, Loss: 174.07296752929688\n",
      "Epoch 62, Batch 20183, Loss: 171.58316040039062\n",
      "Epoch 62, Batch 20184, Loss: 172.33303833007812\n",
      "Epoch 62, Batch 20185, Loss: 176.64797973632812\n",
      "Epoch 62, Batch 20186, Loss: 166.26364135742188\n",
      "Epoch 62, Batch 20187, Loss: 179.66758728027344\n",
      "Epoch 62, Batch 20188, Loss: 183.58624267578125\n",
      "Epoch 62, Batch 20189, Loss: 158.2170867919922\n",
      "Epoch 62, Batch 20190, Loss: 165.1700439453125\n",
      "Epoch 62, Batch 20191, Loss: 171.66632080078125\n",
      "Epoch 62, Batch 20192, Loss: 173.99945068359375\n",
      "Epoch 62, Batch 20193, Loss: 162.88555908203125\n",
      "Epoch 62, Batch 20194, Loss: 181.71055603027344\n",
      "Epoch 62, Batch 20195, Loss: 183.82102966308594\n",
      "Epoch 62, Batch 20196, Loss: 180.07960510253906\n",
      "Epoch 62, Batch 20197, Loss: 172.62945556640625\n",
      "Epoch 62, Batch 20198, Loss: 159.11170959472656\n",
      "Epoch 62, Batch 20199, Loss: 162.52734375\n",
      "Epoch 62, Batch 20200, Loss: 165.69581604003906\n",
      "Epoch 62, Batch 20201, Loss: 169.8323516845703\n",
      "Epoch 62, Batch 20202, Loss: 160.6688232421875\n",
      "Epoch 62, Batch 20203, Loss: 170.01556396484375\n",
      "Epoch 62, Batch 20204, Loss: 159.55113220214844\n",
      "Epoch 62, Batch 20205, Loss: 166.0452117919922\n",
      "Epoch 62, Batch 20206, Loss: 168.95465087890625\n",
      "Epoch 62, Batch 20207, Loss: 182.43060302734375\n",
      "Epoch 62, Batch 20208, Loss: 159.70875549316406\n",
      "Epoch 62, Batch 20209, Loss: 173.145263671875\n",
      "Epoch 62, Batch 20210, Loss: 186.59239196777344\n",
      "Epoch 62, Batch 20211, Loss: 187.64076232910156\n",
      "Epoch 62, Batch 20212, Loss: 186.66983032226562\n",
      "Epoch 62, Batch 20213, Loss: 177.76040649414062\n",
      "Epoch 62, Batch 20214, Loss: 166.7923126220703\n",
      "Epoch 62, Batch 20215, Loss: 167.93032836914062\n",
      "Epoch 62, Batch 20216, Loss: 167.29083251953125\n",
      "Epoch 62, Batch 20217, Loss: 162.0010223388672\n",
      "Epoch 62, Batch 20218, Loss: 180.7332000732422\n",
      "Epoch 62, Batch 20219, Loss: 169.43922424316406\n",
      "Epoch 62, Batch 20220, Loss: 158.54888916015625\n",
      "Epoch 62, Batch 20221, Loss: 167.14385986328125\n",
      "Epoch 62, Batch 20222, Loss: 179.31478881835938\n",
      "Epoch 62, Batch 20223, Loss: 162.5104522705078\n",
      "Epoch 62, Batch 20224, Loss: 183.92971801757812\n",
      "Epoch 62, Batch 20225, Loss: 171.8766632080078\n",
      "Epoch 62, Batch 20226, Loss: 165.36892700195312\n",
      "Epoch 62, Batch 20227, Loss: 182.10316467285156\n",
      "Epoch 62, Batch 20228, Loss: 169.9788055419922\n",
      "Epoch 62, Batch 20229, Loss: 171.57740783691406\n",
      "Epoch 62, Batch 20230, Loss: 173.4353485107422\n",
      "Epoch 62, Batch 20231, Loss: 162.1790313720703\n",
      "Epoch 62, Batch 20232, Loss: 160.09378051757812\n",
      "Epoch 62, Batch 20233, Loss: 166.8215789794922\n",
      "Epoch 62, Batch 20234, Loss: 166.333251953125\n",
      "Epoch 62, Batch 20235, Loss: 174.5144805908203\n",
      "Epoch 62, Batch 20236, Loss: 182.1501007080078\n",
      "Epoch 62, Batch 20237, Loss: 161.4205780029297\n",
      "Epoch 62, Batch 20238, Loss: 185.93148803710938\n",
      "Epoch 62, Batch 20239, Loss: 168.15162658691406\n",
      "Epoch 62, Batch 20240, Loss: 154.27110290527344\n",
      "Epoch 62, Batch 20241, Loss: 165.9207305908203\n",
      "Epoch 62, Batch 20242, Loss: 176.2370147705078\n",
      "Epoch 62, Batch 20243, Loss: 146.3585662841797\n",
      "Epoch 62, Batch 20244, Loss: 161.312744140625\n",
      "Epoch 62, Batch 20245, Loss: 177.34861755371094\n",
      "Epoch 62, Batch 20246, Loss: 160.06288146972656\n",
      "Epoch 62, Batch 20247, Loss: 154.94757080078125\n",
      "Epoch 62, Batch 20248, Loss: 178.93719482421875\n",
      "Epoch 62, Batch 20249, Loss: 173.93963623046875\n",
      "Epoch 62, Batch 20250, Loss: 165.10743713378906\n",
      "Epoch 62, Batch 20251, Loss: 177.39874267578125\n",
      "Epoch 62, Batch 20252, Loss: 164.12254333496094\n",
      "Epoch 62, Batch 20253, Loss: 179.20492553710938\n",
      "Epoch 62, Batch 20254, Loss: 160.1891632080078\n",
      "Epoch 62, Batch 20255, Loss: 175.98944091796875\n",
      "Epoch 62, Batch 20256, Loss: 179.00518798828125\n",
      "Epoch 62, Batch 20257, Loss: 171.0354461669922\n",
      "Epoch 62, Batch 20258, Loss: 175.59815979003906\n",
      "Epoch 62, Batch 20259, Loss: 160.51829528808594\n",
      "Epoch 62, Batch 20260, Loss: 186.3948974609375\n",
      "Epoch 62, Batch 20261, Loss: 162.8850555419922\n",
      "Epoch 62, Batch 20262, Loss: 193.49400329589844\n",
      "Epoch 62, Batch 20263, Loss: 170.3596954345703\n",
      "Epoch 62, Batch 20264, Loss: 163.09718322753906\n",
      "Epoch 62, Batch 20265, Loss: 170.8064727783203\n",
      "Epoch 62, Batch 20266, Loss: 178.01792907714844\n",
      "Epoch 62, Batch 20267, Loss: 156.43838500976562\n",
      "Epoch 62, Batch 20268, Loss: 182.96641540527344\n",
      "Epoch 62, Batch 20269, Loss: 153.75634765625\n",
      "Epoch 62, Batch 20270, Loss: 163.67318725585938\n",
      "Epoch 62, Batch 20271, Loss: 164.19898986816406\n",
      "Epoch 62, Batch 20272, Loss: 189.29823303222656\n",
      "Epoch 62, Batch 20273, Loss: 177.2202606201172\n",
      "Epoch 62, Batch 20274, Loss: 165.40013122558594\n",
      "Epoch 62, Batch 20275, Loss: 191.6026611328125\n",
      "Epoch 62, Batch 20276, Loss: 178.56422424316406\n",
      "Epoch 62, Batch 20277, Loss: 176.86849975585938\n",
      "Epoch 62, Batch 20278, Loss: 169.0709228515625\n",
      "Epoch 62, Batch 20279, Loss: 177.9822998046875\n",
      "Epoch 62, Batch 20280, Loss: 169.87045288085938\n",
      "Epoch 62, Batch 20281, Loss: 179.1647491455078\n",
      "Epoch 62, Batch 20282, Loss: 193.79501342773438\n",
      "Epoch 62, Batch 20283, Loss: 171.09378051757812\n",
      "Epoch 62, Batch 20284, Loss: 188.7597198486328\n",
      "Epoch 62, Batch 20285, Loss: 161.79957580566406\n",
      "Epoch 62, Batch 20286, Loss: 170.75955200195312\n",
      "Epoch 62, Batch 20287, Loss: 175.9628448486328\n",
      "Epoch 62, Batch 20288, Loss: 169.12063598632812\n",
      "Epoch 62, Batch 20289, Loss: 192.07191467285156\n",
      "Epoch 62, Batch 20290, Loss: 170.5581817626953\n",
      "Epoch 62, Batch 20291, Loss: 185.82493591308594\n",
      "Epoch 62, Batch 20292, Loss: 183.7519989013672\n",
      "Epoch 62, Batch 20293, Loss: 177.49050903320312\n",
      "Epoch 62, Batch 20294, Loss: 172.13818359375\n",
      "Epoch 62, Batch 20295, Loss: 197.3302764892578\n",
      "Epoch 62, Batch 20296, Loss: 173.92919921875\n",
      "Epoch 62, Batch 20297, Loss: 174.68328857421875\n",
      "Epoch 62, Batch 20298, Loss: 159.03981018066406\n",
      "Epoch 62, Batch 20299, Loss: 168.28109741210938\n",
      "Epoch 62, Batch 20300, Loss: 185.691162109375\n",
      "Epoch 62, Batch 20301, Loss: 187.04322814941406\n",
      "Epoch 62, Batch 20302, Loss: 174.9946746826172\n",
      "Epoch 62, Batch 20303, Loss: 181.33489990234375\n",
      "Epoch 62, Batch 20304, Loss: 174.37608337402344\n",
      "Epoch 62, Batch 20305, Loss: 160.3662567138672\n",
      "Epoch 62, Batch 20306, Loss: 165.5480499267578\n",
      "Epoch 62, Batch 20307, Loss: 177.9619903564453\n",
      "Epoch 62, Batch 20308, Loss: 159.2677459716797\n",
      "Epoch 62, Batch 20309, Loss: 160.37106323242188\n",
      "Epoch 62, Batch 20310, Loss: 175.5397491455078\n",
      "Epoch 62, Batch 20311, Loss: 170.0619659423828\n",
      "Epoch 62, Batch 20312, Loss: 187.8160400390625\n",
      "Epoch 62, Batch 20313, Loss: 189.9330596923828\n",
      "Epoch 62, Batch 20314, Loss: 175.9594268798828\n",
      "Epoch 62, Batch 20315, Loss: 195.44879150390625\n",
      "Epoch 62, Batch 20316, Loss: 169.5357666015625\n",
      "Epoch 62, Batch 20317, Loss: 176.0230712890625\n",
      "Epoch 62, Batch 20318, Loss: 163.03485107421875\n",
      "Epoch 62, Batch 20319, Loss: 199.58387756347656\n",
      "Epoch 62, Batch 20320, Loss: 168.80499267578125\n",
      "Epoch 62, Batch 20321, Loss: 154.1314239501953\n",
      "Epoch 62, Batch 20322, Loss: 181.87294006347656\n",
      "Epoch 62, Batch 20323, Loss: 167.31898498535156\n",
      "Epoch 62, Batch 20324, Loss: 169.7840576171875\n",
      "Epoch 62, Batch 20325, Loss: 163.7574920654297\n",
      "Epoch 62, Batch 20326, Loss: 189.4647674560547\n",
      "Epoch 62, Batch 20327, Loss: 168.66954040527344\n",
      "Epoch 62, Batch 20328, Loss: 178.39578247070312\n",
      "Epoch 62, Batch 20329, Loss: 181.9445037841797\n",
      "Epoch 62, Batch 20330, Loss: 172.3549346923828\n",
      "Epoch 62, Batch 20331, Loss: 174.57150268554688\n",
      "Epoch 62, Batch 20332, Loss: 169.1470489501953\n",
      "Epoch 62, Batch 20333, Loss: 176.228759765625\n",
      "Epoch 62, Batch 20334, Loss: 171.10450744628906\n",
      "Epoch 62, Batch 20335, Loss: 173.18115234375\n",
      "Epoch 62, Batch 20336, Loss: 175.78485107421875\n",
      "Epoch 62, Batch 20337, Loss: 168.45379638671875\n",
      "Epoch 62, Batch 20338, Loss: 158.3842010498047\n",
      "Epoch 62, Batch 20339, Loss: 171.0625\n",
      "Epoch 62, Batch 20340, Loss: 176.92376708984375\n",
      "Epoch 62, Batch 20341, Loss: 185.9440155029297\n",
      "Epoch 62, Batch 20342, Loss: 173.8484649658203\n",
      "Epoch 62, Batch 20343, Loss: 151.3972930908203\n",
      "Epoch 62, Batch 20344, Loss: 159.11439514160156\n",
      "Epoch 62, Batch 20345, Loss: 189.1259002685547\n",
      "Epoch 62, Batch 20346, Loss: 151.4976806640625\n",
      "Epoch 62, Batch 20347, Loss: 169.58802795410156\n",
      "Epoch 62, Batch 20348, Loss: 162.14620971679688\n",
      "Epoch 62, Batch 20349, Loss: 164.5074920654297\n",
      "Epoch 62, Batch 20350, Loss: 181.822021484375\n",
      "Epoch 62, Batch 20351, Loss: 171.3616180419922\n",
      "Epoch 62, Batch 20352, Loss: 167.74911499023438\n",
      "Epoch 62, Batch 20353, Loss: 182.4996337890625\n",
      "Epoch 62, Batch 20354, Loss: 167.48794555664062\n",
      "Epoch 62, Batch 20355, Loss: 183.87185668945312\n",
      "Epoch 62, Batch 20356, Loss: 190.5700225830078\n",
      "Epoch 62, Batch 20357, Loss: 198.44496154785156\n",
      "Epoch 62, Batch 20358, Loss: 183.7927703857422\n",
      "Epoch 62, Batch 20359, Loss: 191.70028686523438\n",
      "Epoch 62, Batch 20360, Loss: 163.63307189941406\n",
      "Epoch 62, Batch 20361, Loss: 167.20404052734375\n",
      "Epoch 62, Batch 20362, Loss: 170.71636962890625\n",
      "Epoch 62, Batch 20363, Loss: 173.4877471923828\n",
      "Epoch 62, Batch 20364, Loss: 173.1399688720703\n",
      "Epoch 62, Batch 20365, Loss: 164.03427124023438\n",
      "Epoch 62, Batch 20366, Loss: 171.4033203125\n",
      "Epoch 62, Batch 20367, Loss: 164.67555236816406\n",
      "Epoch 62, Batch 20368, Loss: 164.20053100585938\n",
      "Epoch 62, Batch 20369, Loss: 176.30995178222656\n",
      "Epoch 62, Batch 20370, Loss: 168.92100524902344\n",
      "Epoch 62, Batch 20371, Loss: 195.23045349121094\n",
      "Epoch 62, Batch 20372, Loss: 176.3180694580078\n",
      "Epoch 62, Batch 20373, Loss: 183.1724090576172\n",
      "Epoch 62, Batch 20374, Loss: 168.55738830566406\n",
      "Epoch 62, Batch 20375, Loss: 158.4890594482422\n",
      "Epoch 62, Batch 20376, Loss: 163.62342834472656\n",
      "Epoch 62, Batch 20377, Loss: 166.58316040039062\n",
      "Epoch 62, Batch 20378, Loss: 154.98367309570312\n",
      "Epoch 62, Batch 20379, Loss: 151.99993896484375\n",
      "Epoch 62, Batch 20380, Loss: 176.04588317871094\n",
      "Epoch 62, Batch 20381, Loss: 159.27011108398438\n",
      "Epoch 62, Batch 20382, Loss: 180.26272583007812\n",
      "Epoch 62, Batch 20383, Loss: 183.877197265625\n",
      "Epoch 62, Batch 20384, Loss: 164.4840087890625\n",
      "Epoch 62, Batch 20385, Loss: 176.859375\n",
      "Epoch 62, Batch 20386, Loss: 198.3698272705078\n",
      "Epoch 62, Batch 20387, Loss: 170.82676696777344\n",
      "Epoch 62, Batch 20388, Loss: 183.30247497558594\n",
      "Epoch 62, Batch 20389, Loss: 175.5895538330078\n",
      "Epoch 62, Batch 20390, Loss: 168.70436096191406\n",
      "Epoch 62, Batch 20391, Loss: 158.36814880371094\n",
      "Epoch 62, Batch 20392, Loss: 174.5249786376953\n",
      "Epoch 62, Batch 20393, Loss: 179.255126953125\n",
      "Epoch 62, Batch 20394, Loss: 178.16995239257812\n",
      "Epoch 62, Batch 20395, Loss: 170.4047393798828\n",
      "Epoch 62, Batch 20396, Loss: 171.4654541015625\n",
      "Epoch 62, Batch 20397, Loss: 159.5143585205078\n",
      "Epoch 62, Batch 20398, Loss: 177.4374237060547\n",
      "Epoch 62, Batch 20399, Loss: 188.80841064453125\n",
      "Epoch 62, Batch 20400, Loss: 169.30250549316406\n",
      "Epoch 62, Batch 20401, Loss: 164.04649353027344\n",
      "Epoch 62, Batch 20402, Loss: 167.28077697753906\n",
      "Epoch 62, Batch 20403, Loss: 174.7933349609375\n",
      "Epoch 62, Batch 20404, Loss: 181.1507568359375\n",
      "Epoch 62, Batch 20405, Loss: 175.45582580566406\n",
      "Epoch 62, Batch 20406, Loss: 173.37164306640625\n",
      "Epoch 62, Batch 20407, Loss: 181.1993865966797\n",
      "Epoch 62, Batch 20408, Loss: 194.34552001953125\n",
      "Epoch 62, Batch 20409, Loss: 155.5778045654297\n",
      "Epoch 62, Batch 20410, Loss: 164.31996154785156\n",
      "Epoch 62, Batch 20411, Loss: 158.1458740234375\n",
      "Epoch 62, Batch 20412, Loss: 187.9539794921875\n",
      "Epoch 62, Batch 20413, Loss: 178.5344696044922\n",
      "Epoch 62, Batch 20414, Loss: 173.8308563232422\n",
      "Epoch 62, Batch 20415, Loss: 163.7960662841797\n",
      "Epoch 62, Batch 20416, Loss: 164.85067749023438\n",
      "Epoch 62, Batch 20417, Loss: 171.00453186035156\n",
      "Epoch 62, Batch 20418, Loss: 166.63491821289062\n",
      "Epoch 62, Batch 20419, Loss: 177.0603790283203\n",
      "Epoch 62, Batch 20420, Loss: 170.87765502929688\n",
      "Epoch 62, Batch 20421, Loss: 166.5723419189453\n",
      "Epoch 62, Batch 20422, Loss: 167.86842346191406\n",
      "Epoch 62, Batch 20423, Loss: 185.14430236816406\n",
      "Epoch 62, Batch 20424, Loss: 176.12014770507812\n",
      "Epoch 62, Batch 20425, Loss: 168.0787353515625\n",
      "Epoch 62, Batch 20426, Loss: 165.248291015625\n",
      "Epoch 62, Batch 20427, Loss: 179.20877075195312\n",
      "Epoch 62, Batch 20428, Loss: 183.66653442382812\n",
      "Epoch 62, Batch 20429, Loss: 152.97122192382812\n",
      "Epoch 62, Batch 20430, Loss: 153.4103546142578\n",
      "Epoch 62, Batch 20431, Loss: 191.06056213378906\n",
      "Epoch 62, Batch 20432, Loss: 161.83615112304688\n",
      "Epoch 62, Batch 20433, Loss: 173.58346557617188\n",
      "Epoch 62, Batch 20434, Loss: 174.72799682617188\n",
      "Epoch 62, Batch 20435, Loss: 179.9615478515625\n",
      "Epoch 62, Batch 20436, Loss: 197.8094482421875\n",
      "Epoch 62, Batch 20437, Loss: 176.8989715576172\n",
      "Epoch 62, Batch 20438, Loss: 176.63926696777344\n",
      "Epoch 62, Batch 20439, Loss: 173.39297485351562\n",
      "Epoch 62, Batch 20440, Loss: 182.78106689453125\n",
      "Epoch 62, Batch 20441, Loss: 168.63888549804688\n",
      "Epoch 62, Batch 20442, Loss: 164.04562377929688\n",
      "Epoch 62, Batch 20443, Loss: 181.62571716308594\n",
      "Epoch 62, Batch 20444, Loss: 186.74134826660156\n",
      "Epoch 62, Batch 20445, Loss: 171.38204956054688\n",
      "Epoch 62, Batch 20446, Loss: 170.2775115966797\n",
      "Epoch 62, Batch 20447, Loss: 182.7787322998047\n",
      "Epoch 62, Batch 20448, Loss: 178.082763671875\n",
      "Epoch 62, Batch 20449, Loss: 172.35670471191406\n",
      "Epoch 62, Batch 20450, Loss: 179.45335388183594\n",
      "Epoch 62, Batch 20451, Loss: 190.40480041503906\n",
      "Epoch 62, Batch 20452, Loss: 181.49539184570312\n",
      "Epoch 62, Batch 20453, Loss: 166.45419311523438\n",
      "Epoch 62, Batch 20454, Loss: 166.70941162109375\n",
      "Epoch 62, Batch 20455, Loss: 161.72189331054688\n",
      "Epoch 62, Batch 20456, Loss: 173.48934936523438\n",
      "Epoch 62, Batch 20457, Loss: 186.66998291015625\n",
      "Epoch 62, Batch 20458, Loss: 174.52325439453125\n",
      "Epoch 62, Batch 20459, Loss: 167.98635864257812\n",
      "Epoch 62, Batch 20460, Loss: 167.2992706298828\n",
      "Epoch 62, Batch 20461, Loss: 173.6377410888672\n",
      "Epoch 62, Batch 20462, Loss: 173.34703063964844\n",
      "Epoch 62, Batch 20463, Loss: 177.84083557128906\n",
      "Epoch 62, Batch 20464, Loss: 165.41099548339844\n",
      "Epoch 62, Batch 20465, Loss: 166.28042602539062\n",
      "Epoch 62, Batch 20466, Loss: 162.5621337890625\n",
      "Epoch 62, Batch 20467, Loss: 173.0050811767578\n",
      "Epoch 62, Batch 20468, Loss: 172.59722900390625\n",
      "Epoch 62, Batch 20469, Loss: 162.93959045410156\n",
      "Epoch 62, Batch 20470, Loss: 155.72183227539062\n",
      "Epoch 62, Batch 20471, Loss: 158.4021453857422\n",
      "Epoch 62, Batch 20472, Loss: 167.44252014160156\n",
      "Epoch 62, Batch 20473, Loss: 163.5543670654297\n",
      "Epoch 62, Batch 20474, Loss: 171.81863403320312\n",
      "Epoch 62, Batch 20475, Loss: 163.87342834472656\n",
      "Epoch 62, Batch 20476, Loss: 159.58242797851562\n",
      "Epoch 62, Batch 20477, Loss: 178.4663848876953\n",
      "Epoch 62, Batch 20478, Loss: 167.30726623535156\n",
      "Epoch 62, Batch 20479, Loss: 170.4207305908203\n",
      "Epoch 62, Batch 20480, Loss: 181.2749481201172\n",
      "Epoch 62, Batch 20481, Loss: 170.21206665039062\n",
      "Epoch 62, Batch 20482, Loss: 160.9568634033203\n",
      "Epoch 62, Batch 20483, Loss: 159.8848419189453\n",
      "Epoch 62, Batch 20484, Loss: 172.05462646484375\n",
      "Epoch 62, Batch 20485, Loss: 173.20758056640625\n",
      "Epoch 62, Batch 20486, Loss: 168.480224609375\n",
      "Epoch 62, Batch 20487, Loss: 170.97579956054688\n",
      "Epoch 62, Batch 20488, Loss: 175.7718505859375\n",
      "Epoch 62, Batch 20489, Loss: 178.1793212890625\n",
      "Epoch 62, Batch 20490, Loss: 164.5522918701172\n",
      "Epoch 62, Batch 20491, Loss: 168.77256774902344\n",
      "Epoch 62, Batch 20492, Loss: 171.92063903808594\n",
      "Epoch 62, Batch 20493, Loss: 177.08291625976562\n",
      "Epoch 62, Batch 20494, Loss: 150.5494842529297\n",
      "Epoch 62, Batch 20495, Loss: 180.89093017578125\n",
      "Epoch 62, Batch 20496, Loss: 175.75814819335938\n",
      "Epoch 62, Batch 20497, Loss: 178.1133270263672\n",
      "Epoch 62, Batch 20498, Loss: 187.62493896484375\n",
      "Epoch 62, Batch 20499, Loss: 159.41355895996094\n",
      "Epoch 62, Batch 20500, Loss: 171.53433227539062\n",
      "Epoch 62, Batch 20501, Loss: 181.30662536621094\n",
      "Epoch 62, Batch 20502, Loss: 179.42562866210938\n",
      "Epoch 62, Batch 20503, Loss: 173.4630126953125\n",
      "Epoch 62, Batch 20504, Loss: 192.43418884277344\n",
      "Epoch 62, Batch 20505, Loss: 172.5470428466797\n",
      "Epoch 62, Batch 20506, Loss: 173.63771057128906\n",
      "Epoch 62, Batch 20507, Loss: 168.8904571533203\n",
      "Epoch 62, Batch 20508, Loss: 170.99758911132812\n",
      "Epoch 62, Batch 20509, Loss: 167.34210205078125\n",
      "Epoch 62, Batch 20510, Loss: 163.13609313964844\n",
      "Epoch 62, Batch 20511, Loss: 172.6816864013672\n",
      "Epoch 62, Batch 20512, Loss: 166.0308074951172\n",
      "Epoch 62, Batch 20513, Loss: 182.5638427734375\n",
      "Epoch 62, Batch 20514, Loss: 165.03421020507812\n",
      "Epoch 62, Batch 20515, Loss: 167.04698181152344\n",
      "Epoch 62, Batch 20516, Loss: 192.76499938964844\n",
      "Epoch 62, Batch 20517, Loss: 180.63815307617188\n",
      "Epoch 62, Batch 20518, Loss: 175.8786163330078\n",
      "Epoch 62, Batch 20519, Loss: 170.77960205078125\n",
      "Epoch 62, Batch 20520, Loss: 170.2653350830078\n",
      "Epoch 62, Batch 20521, Loss: 195.8151397705078\n",
      "Epoch 62, Batch 20522, Loss: 160.9633331298828\n",
      "Epoch 62, Batch 20523, Loss: 186.0119171142578\n",
      "Epoch 62, Batch 20524, Loss: 176.58726501464844\n",
      "Epoch 62, Batch 20525, Loss: 175.78350830078125\n",
      "Epoch 62, Batch 20526, Loss: 171.5436553955078\n",
      "Epoch 62, Batch 20527, Loss: 170.92897033691406\n",
      "Epoch 62, Batch 20528, Loss: 164.94300842285156\n",
      "Epoch 62, Batch 20529, Loss: 162.24862670898438\n",
      "Epoch 62, Batch 20530, Loss: 173.1551513671875\n",
      "Epoch 62, Batch 20531, Loss: 173.79042053222656\n",
      "Epoch 62, Batch 20532, Loss: 170.15625\n",
      "Epoch 62, Batch 20533, Loss: 172.23768615722656\n",
      "Epoch 62, Batch 20534, Loss: 191.55357360839844\n",
      "Epoch 62, Batch 20535, Loss: 172.36187744140625\n",
      "Epoch 62, Batch 20536, Loss: 166.8023223876953\n",
      "Epoch 62, Batch 20537, Loss: 174.18345642089844\n",
      "Epoch 62, Batch 20538, Loss: 169.242431640625\n",
      "Epoch 62, Batch 20539, Loss: 174.06549072265625\n",
      "Epoch 62, Batch 20540, Loss: 164.88784790039062\n",
      "Epoch 62, Batch 20541, Loss: 175.41629028320312\n",
      "Epoch 62, Batch 20542, Loss: 172.3245086669922\n",
      "Epoch 62, Batch 20543, Loss: 189.84068298339844\n",
      "Epoch 62, Batch 20544, Loss: 172.19882202148438\n",
      "Epoch 62, Batch 20545, Loss: 175.6808624267578\n",
      "Epoch 62, Batch 20546, Loss: 184.1568145751953\n",
      "Epoch 62, Batch 20547, Loss: 188.84970092773438\n",
      "Epoch 62, Batch 20548, Loss: 179.3470916748047\n",
      "Epoch 62, Batch 20549, Loss: 180.11407470703125\n",
      "Epoch 62, Batch 20550, Loss: 178.01036071777344\n",
      "Epoch 62, Batch 20551, Loss: 175.66111755371094\n",
      "Epoch 62, Batch 20552, Loss: 186.51693725585938\n",
      "Epoch 62, Batch 20553, Loss: 164.74827575683594\n",
      "Epoch 62, Batch 20554, Loss: 165.40892028808594\n",
      "Epoch 62, Batch 20555, Loss: 169.73094177246094\n",
      "Epoch 62, Batch 20556, Loss: 148.73585510253906\n",
      "Epoch 62, Batch 20557, Loss: 179.35536193847656\n",
      "Epoch 62, Batch 20558, Loss: 170.30880737304688\n",
      "Epoch 62, Batch 20559, Loss: 186.811279296875\n",
      "Epoch 62, Batch 20560, Loss: 168.5611572265625\n",
      "Epoch 62, Batch 20561, Loss: 181.761474609375\n",
      "Epoch 62, Batch 20562, Loss: 178.9918212890625\n",
      "Epoch 62, Batch 20563, Loss: 172.75930786132812\n",
      "Epoch 62, Batch 20564, Loss: 168.1945343017578\n",
      "Epoch 62, Batch 20565, Loss: 170.7544708251953\n",
      "Epoch 62, Batch 20566, Loss: 163.62672424316406\n",
      "Epoch 62, Batch 20567, Loss: 163.80740356445312\n",
      "Epoch 62, Batch 20568, Loss: 164.80520629882812\n",
      "Epoch 62, Batch 20569, Loss: 178.2863311767578\n",
      "Epoch 62, Batch 20570, Loss: 175.07269287109375\n",
      "Epoch 62, Batch 20571, Loss: 170.4585418701172\n",
      "Epoch 62, Batch 20572, Loss: 168.54071044921875\n",
      "Epoch 62, Batch 20573, Loss: 159.0839385986328\n",
      "Epoch 62, Batch 20574, Loss: 168.0852813720703\n",
      "Epoch 62, Batch 20575, Loss: 179.565185546875\n",
      "Epoch 62, Batch 20576, Loss: 168.718017578125\n",
      "Epoch 62, Batch 20577, Loss: 159.8608856201172\n",
      "Epoch 62, Batch 20578, Loss: 195.08255004882812\n",
      "Epoch 62, Batch 20579, Loss: 160.75558471679688\n",
      "Epoch 62, Batch 20580, Loss: 187.6537628173828\n",
      "Epoch 62, Batch 20581, Loss: 161.21774291992188\n",
      "Epoch 62, Batch 20582, Loss: 183.16207885742188\n",
      "Epoch 62, Batch 20583, Loss: 163.59400939941406\n",
      "Epoch 62, Batch 20584, Loss: 171.1645050048828\n",
      "Epoch 62, Batch 20585, Loss: 166.43385314941406\n",
      "Epoch 62, Batch 20586, Loss: 160.4017333984375\n",
      "Epoch 62, Batch 20587, Loss: 153.42454528808594\n",
      "Epoch 62, Batch 20588, Loss: 180.63661193847656\n",
      "Epoch 62, Batch 20589, Loss: 166.63255310058594\n",
      "Epoch 62, Batch 20590, Loss: 182.6941375732422\n",
      "Epoch 62, Batch 20591, Loss: 173.71005249023438\n",
      "Epoch 62, Batch 20592, Loss: 171.97531127929688\n",
      "Epoch 62, Batch 20593, Loss: 186.29144287109375\n",
      "Epoch 62, Batch 20594, Loss: 169.2744140625\n",
      "Epoch 62, Batch 20595, Loss: 179.21534729003906\n",
      "Epoch 62, Batch 20596, Loss: 165.25804138183594\n",
      "Epoch 62, Batch 20597, Loss: 168.2730255126953\n",
      "Epoch 62, Batch 20598, Loss: 167.5452117919922\n",
      "Epoch 62, Batch 20599, Loss: 168.30551147460938\n",
      "Epoch 62, Batch 20600, Loss: 177.859130859375\n",
      "Epoch 62, Batch 20601, Loss: 157.36048889160156\n",
      "Epoch 62, Batch 20602, Loss: 165.76675415039062\n",
      "Epoch 62, Batch 20603, Loss: 191.8006591796875\n",
      "Epoch 62, Batch 20604, Loss: 177.88499450683594\n",
      "Epoch 62, Batch 20605, Loss: 179.89218139648438\n",
      "Epoch 62, Batch 20606, Loss: 160.6120147705078\n",
      "Epoch 62, Batch 20607, Loss: 168.69686889648438\n",
      "Epoch 62, Batch 20608, Loss: 174.69049072265625\n",
      "Epoch 62, Batch 20609, Loss: 180.09210205078125\n",
      "Epoch 62, Batch 20610, Loss: 169.83055114746094\n",
      "Epoch 62, Batch 20611, Loss: 170.81582641601562\n",
      "Epoch 62, Batch 20612, Loss: 161.78836059570312\n",
      "Epoch 62, Batch 20613, Loss: 168.71749877929688\n",
      "Epoch 62, Batch 20614, Loss: 167.2711944580078\n",
      "Epoch 62, Batch 20615, Loss: 172.42898559570312\n",
      "Epoch 62, Batch 20616, Loss: 170.9921112060547\n",
      "Epoch 62, Batch 20617, Loss: 176.32806396484375\n",
      "Epoch 62, Batch 20618, Loss: 169.18663024902344\n",
      "Epoch 62, Batch 20619, Loss: 173.68553161621094\n",
      "Epoch 62, Batch 20620, Loss: 176.4737091064453\n",
      "Epoch 62, Batch 20621, Loss: 177.28964233398438\n",
      "Epoch 62, Batch 20622, Loss: 157.4873504638672\n",
      "Epoch 62, Batch 20623, Loss: 163.94970703125\n",
      "Epoch 62, Batch 20624, Loss: 168.70423889160156\n",
      "Epoch 62, Batch 20625, Loss: 174.7399139404297\n",
      "Epoch 62, Batch 20626, Loss: 173.55184936523438\n",
      "Epoch 62, Batch 20627, Loss: 181.6538543701172\n",
      "Epoch 62, Batch 20628, Loss: 178.19375610351562\n",
      "Epoch 62, Batch 20629, Loss: 159.71115112304688\n",
      "Epoch 62, Batch 20630, Loss: 176.09786987304688\n",
      "Epoch 62, Batch 20631, Loss: 183.34835815429688\n",
      "Epoch 62, Batch 20632, Loss: 172.22592163085938\n",
      "Epoch 62, Batch 20633, Loss: 191.7229461669922\n",
      "Epoch 62, Batch 20634, Loss: 154.78976440429688\n",
      "Epoch 62, Batch 20635, Loss: 165.37648010253906\n",
      "Epoch 62, Batch 20636, Loss: 174.7852325439453\n",
      "Epoch 62, Batch 20637, Loss: 169.98281860351562\n",
      "Epoch 62, Batch 20638, Loss: 169.95066833496094\n",
      "Epoch 62, Batch 20639, Loss: 192.25\n",
      "Epoch 62, Batch 20640, Loss: 173.60745239257812\n",
      "Epoch 62, Batch 20641, Loss: 178.6666259765625\n",
      "Epoch 62, Batch 20642, Loss: 177.95477294921875\n",
      "Epoch 62, Batch 20643, Loss: 176.27972412109375\n",
      "Epoch 62, Batch 20644, Loss: 160.3315887451172\n",
      "Epoch 62, Batch 20645, Loss: 175.54942321777344\n",
      "Epoch 62, Batch 20646, Loss: 187.2395477294922\n",
      "Epoch 62, Batch 20647, Loss: 179.8690948486328\n",
      "Epoch 62, Batch 20648, Loss: 162.46200561523438\n",
      "Epoch 62, Batch 20649, Loss: 149.3978271484375\n",
      "Epoch 62, Batch 20650, Loss: 172.48602294921875\n",
      "Epoch 62, Batch 20651, Loss: 173.1694793701172\n",
      "Epoch 62, Batch 20652, Loss: 181.86424255371094\n",
      "Epoch 62, Batch 20653, Loss: 187.70115661621094\n",
      "Epoch 62, Batch 20654, Loss: 180.902587890625\n",
      "Epoch 62, Batch 20655, Loss: 156.23414611816406\n",
      "Epoch 62, Batch 20656, Loss: 177.5490264892578\n",
      "Epoch 62, Batch 20657, Loss: 187.20252990722656\n",
      "Epoch 62, Batch 20658, Loss: 177.87583923339844\n",
      "Epoch 62, Batch 20659, Loss: 171.27755737304688\n",
      "Epoch 62, Batch 20660, Loss: 167.3475799560547\n",
      "Epoch 62, Batch 20661, Loss: 177.2418975830078\n",
      "Epoch 62, Batch 20662, Loss: 167.08734130859375\n",
      "Epoch 62, Batch 20663, Loss: 175.57083129882812\n",
      "Epoch 62, Batch 20664, Loss: 175.80508422851562\n",
      "Epoch 62, Batch 20665, Loss: 167.88824462890625\n",
      "Epoch 62, Batch 20666, Loss: 175.02847290039062\n",
      "Epoch 62, Batch 20667, Loss: 162.3203125\n",
      "Epoch 62, Batch 20668, Loss: 171.3995819091797\n",
      "Epoch 62, Batch 20669, Loss: 166.27317810058594\n",
      "Epoch 62, Batch 20670, Loss: 156.38409423828125\n",
      "Epoch 62, Batch 20671, Loss: 178.6469268798828\n",
      "Epoch 62, Batch 20672, Loss: 174.69818115234375\n",
      "Epoch 62, Batch 20673, Loss: 183.90933227539062\n",
      "Epoch 62, Batch 20674, Loss: 183.9557342529297\n",
      "Epoch 62, Batch 20675, Loss: 176.4051513671875\n",
      "Epoch 62, Batch 20676, Loss: 168.13490295410156\n",
      "Epoch 62, Batch 20677, Loss: 192.81796264648438\n",
      "Epoch 62, Batch 20678, Loss: 172.1702117919922\n",
      "Epoch 62, Batch 20679, Loss: 164.87921142578125\n",
      "Epoch 62, Batch 20680, Loss: 165.21311950683594\n",
      "Epoch 62, Batch 20681, Loss: 176.1778106689453\n",
      "Epoch 62, Batch 20682, Loss: 195.3369903564453\n",
      "Epoch 62, Batch 20683, Loss: 177.3631591796875\n",
      "Epoch 62, Batch 20684, Loss: 174.88238525390625\n",
      "Epoch 62, Batch 20685, Loss: 179.87118530273438\n",
      "Epoch 62, Batch 20686, Loss: 163.15609741210938\n",
      "Epoch 62, Batch 20687, Loss: 167.1114501953125\n",
      "Epoch 62, Batch 20688, Loss: 175.94618225097656\n",
      "Epoch 62, Batch 20689, Loss: 183.4880828857422\n",
      "Epoch 62, Batch 20690, Loss: 174.58258056640625\n",
      "Epoch 62, Batch 20691, Loss: 167.26795959472656\n",
      "Epoch 62, Batch 20692, Loss: 163.92221069335938\n",
      "Epoch 62, Batch 20693, Loss: 178.35304260253906\n",
      "Epoch 62, Batch 20694, Loss: 183.52456665039062\n",
      "Epoch 62, Batch 20695, Loss: 193.32395935058594\n",
      "Epoch 62, Batch 20696, Loss: 178.34861755371094\n",
      "Epoch 62, Batch 20697, Loss: 162.87838745117188\n",
      "Epoch 62, Batch 20698, Loss: 189.72129821777344\n",
      "Epoch 62, Batch 20699, Loss: 187.83465576171875\n",
      "Epoch 62, Batch 20700, Loss: 178.43026733398438\n",
      "Epoch 62, Batch 20701, Loss: 171.3490753173828\n",
      "Epoch 62, Batch 20702, Loss: 185.9778289794922\n",
      "Epoch 62, Batch 20703, Loss: 167.6122283935547\n",
      "Epoch 62, Batch 20704, Loss: 175.43069458007812\n",
      "Epoch 62, Batch 20705, Loss: 176.1051788330078\n",
      "Epoch 62, Batch 20706, Loss: 173.2924041748047\n",
      "Epoch 62, Batch 20707, Loss: 162.82791137695312\n",
      "Epoch 62, Batch 20708, Loss: 174.21214294433594\n",
      "Epoch 62, Batch 20709, Loss: 175.870849609375\n",
      "Epoch 62, Batch 20710, Loss: 164.6866455078125\n",
      "Epoch 62, Batch 20711, Loss: 169.16929626464844\n",
      "Epoch 62, Batch 20712, Loss: 198.95645141601562\n",
      "Epoch 62, Batch 20713, Loss: 178.1361541748047\n",
      "Epoch 62, Batch 20714, Loss: 185.31643676757812\n",
      "Epoch 62, Batch 20715, Loss: 169.90609741210938\n",
      "Epoch 62, Batch 20716, Loss: 166.95574951171875\n",
      "Epoch 62, Batch 20717, Loss: 162.58993530273438\n",
      "Epoch 62, Batch 20718, Loss: 180.85330200195312\n",
      "Epoch 62, Batch 20719, Loss: 176.54122924804688\n",
      "Epoch 62, Batch 20720, Loss: 167.07504272460938\n",
      "Epoch 62, Batch 20721, Loss: 152.78109741210938\n",
      "Epoch 62, Batch 20722, Loss: 183.06053161621094\n",
      "Epoch 62, Batch 20723, Loss: 189.22019958496094\n",
      "Epoch 62, Batch 20724, Loss: 184.03802490234375\n",
      "Epoch 62, Batch 20725, Loss: 169.71316528320312\n",
      "Epoch 62, Batch 20726, Loss: 184.41075134277344\n",
      "Epoch 62, Batch 20727, Loss: 179.9756622314453\n",
      "Epoch 62, Batch 20728, Loss: 180.26077270507812\n",
      "Epoch 62, Batch 20729, Loss: 175.2816162109375\n",
      "Epoch 62, Batch 20730, Loss: 168.1876220703125\n",
      "Epoch 62, Batch 20731, Loss: 162.63003540039062\n",
      "Epoch 62, Batch 20732, Loss: 162.97445678710938\n",
      "Epoch 62, Batch 20733, Loss: 164.3250732421875\n",
      "Epoch 62, Batch 20734, Loss: 178.4501953125\n",
      "Epoch 62, Batch 20735, Loss: 187.68882751464844\n",
      "Epoch 62, Batch 20736, Loss: 159.58766174316406\n",
      "Epoch 62, Batch 20737, Loss: 170.18064880371094\n",
      "Epoch 62, Batch 20738, Loss: 168.3496551513672\n",
      "Epoch 62, Batch 20739, Loss: 173.411865234375\n",
      "Epoch 62, Batch 20740, Loss: 169.52491760253906\n",
      "Epoch 62, Batch 20741, Loss: 163.76687622070312\n",
      "Epoch 62, Batch 20742, Loss: 168.8107147216797\n",
      "Epoch 62, Batch 20743, Loss: 154.78004455566406\n",
      "Epoch 62, Batch 20744, Loss: 171.08120727539062\n",
      "Epoch 62, Batch 20745, Loss: 169.87710571289062\n",
      "Epoch 62, Batch 20746, Loss: 181.70941162109375\n",
      "Epoch 62, Batch 20747, Loss: 174.0572052001953\n",
      "Epoch 62, Batch 20748, Loss: 173.24749755859375\n",
      "Epoch 62, Batch 20749, Loss: 177.56668090820312\n",
      "Epoch 62, Batch 20750, Loss: 164.52574157714844\n",
      "Epoch 62, Batch 20751, Loss: 167.36988830566406\n",
      "Epoch 62, Batch 20752, Loss: 187.25692749023438\n",
      "Epoch 62, Batch 20753, Loss: 156.99508666992188\n",
      "Epoch 62, Batch 20754, Loss: 178.2061767578125\n",
      "Epoch 62, Batch 20755, Loss: 154.34194946289062\n",
      "Epoch 62, Batch 20756, Loss: 186.65130615234375\n",
      "Epoch 62, Batch 20757, Loss: 176.7521514892578\n",
      "Epoch 62, Batch 20758, Loss: 160.5457000732422\n",
      "Epoch 62, Batch 20759, Loss: 172.26422119140625\n",
      "Epoch 62, Batch 20760, Loss: 165.87982177734375\n",
      "Epoch 62, Batch 20761, Loss: 189.5591278076172\n",
      "Epoch 62, Batch 20762, Loss: 175.01397705078125\n",
      "Epoch 62, Batch 20763, Loss: 181.3374786376953\n",
      "Epoch 62, Batch 20764, Loss: 172.0889434814453\n",
      "Epoch 62, Batch 20765, Loss: 161.75396728515625\n",
      "Epoch 62, Batch 20766, Loss: 167.252197265625\n",
      "Epoch 62, Batch 20767, Loss: 163.40513610839844\n",
      "Epoch 62, Batch 20768, Loss: 165.13328552246094\n",
      "Epoch 62, Batch 20769, Loss: 185.60775756835938\n",
      "Epoch 62, Batch 20770, Loss: 164.5535125732422\n",
      "Epoch 62, Batch 20771, Loss: 171.60276794433594\n",
      "Epoch 62, Batch 20772, Loss: 155.72605895996094\n",
      "Epoch 62, Batch 20773, Loss: 155.65257263183594\n",
      "Epoch 62, Batch 20774, Loss: 165.80181884765625\n",
      "Epoch 62, Batch 20775, Loss: 167.697021484375\n",
      "Epoch 62, Batch 20776, Loss: 179.03184509277344\n",
      "Epoch 62, Batch 20777, Loss: 177.3598175048828\n",
      "Epoch 62, Batch 20778, Loss: 169.13897705078125\n",
      "Epoch 62, Batch 20779, Loss: 170.92001342773438\n",
      "Epoch 62, Batch 20780, Loss: 155.684326171875\n",
      "Epoch 62, Batch 20781, Loss: 172.87657165527344\n",
      "Epoch 62, Batch 20782, Loss: 163.69906616210938\n",
      "Epoch 62, Batch 20783, Loss: 148.02932739257812\n",
      "Epoch 62, Batch 20784, Loss: 172.61346435546875\n",
      "Epoch 62, Batch 20785, Loss: 167.5488739013672\n",
      "Epoch 62, Batch 20786, Loss: 154.50650024414062\n",
      "Epoch 62, Batch 20787, Loss: 159.53292846679688\n",
      "Epoch 62, Batch 20788, Loss: 183.33619689941406\n",
      "Epoch 62, Batch 20789, Loss: 174.8477783203125\n",
      "Epoch 62, Batch 20790, Loss: 180.29751586914062\n",
      "Epoch 62, Batch 20791, Loss: 154.55462646484375\n",
      "Epoch 62, Batch 20792, Loss: 171.00376892089844\n",
      "Epoch 62, Batch 20793, Loss: 175.09007263183594\n",
      "Epoch 62, Batch 20794, Loss: 157.96200561523438\n",
      "Epoch 62, Batch 20795, Loss: 182.61669921875\n",
      "Epoch 62, Batch 20796, Loss: 167.2650146484375\n",
      "Epoch 62, Batch 20797, Loss: 183.13104248046875\n",
      "Epoch 62, Batch 20798, Loss: 179.08106994628906\n",
      "Epoch 62, Batch 20799, Loss: 178.94729614257812\n",
      "Epoch 62, Batch 20800, Loss: 158.64239501953125\n",
      "Epoch 62, Batch 20801, Loss: 169.1570587158203\n",
      "Epoch 62, Batch 20802, Loss: 170.2151641845703\n",
      "Epoch 62, Batch 20803, Loss: 167.92510986328125\n",
      "Epoch 62, Batch 20804, Loss: 183.7799072265625\n",
      "Epoch 62, Batch 20805, Loss: 178.773193359375\n",
      "Epoch 62, Batch 20806, Loss: 159.1835479736328\n",
      "Epoch 62, Batch 20807, Loss: 164.74888610839844\n",
      "Epoch 62, Batch 20808, Loss: 167.2929229736328\n",
      "Epoch 62, Batch 20809, Loss: 192.4742431640625\n",
      "Epoch 62, Batch 20810, Loss: 168.0429229736328\n",
      "Epoch 62, Batch 20811, Loss: 195.23358154296875\n",
      "Epoch 62, Batch 20812, Loss: 177.6905517578125\n",
      "Epoch 62, Batch 20813, Loss: 178.1843719482422\n",
      "Epoch 62, Batch 20814, Loss: 168.5312957763672\n",
      "Epoch 62, Batch 20815, Loss: 176.53842163085938\n",
      "Epoch 62, Batch 20816, Loss: 190.17503356933594\n",
      "Epoch 62, Batch 20817, Loss: 180.27200317382812\n",
      "Epoch 62, Batch 20818, Loss: 173.4657745361328\n",
      "Epoch 62, Batch 20819, Loss: 177.1062774658203\n",
      "Epoch 62, Batch 20820, Loss: 174.78196716308594\n",
      "Epoch 62, Batch 20821, Loss: 163.455810546875\n",
      "Epoch 62, Batch 20822, Loss: 161.90992736816406\n",
      "Epoch 62, Batch 20823, Loss: 166.2030487060547\n",
      "Epoch 62, Batch 20824, Loss: 153.18922424316406\n",
      "Epoch 62, Batch 20825, Loss: 172.1823272705078\n",
      "Epoch 62, Batch 20826, Loss: 177.55490112304688\n",
      "Epoch 62, Batch 20827, Loss: 186.4613037109375\n",
      "Epoch 62, Batch 20828, Loss: 183.75511169433594\n",
      "Epoch 62, Batch 20829, Loss: 169.73193359375\n",
      "Epoch 62, Batch 20830, Loss: 156.76968383789062\n",
      "Epoch 62, Batch 20831, Loss: 156.3184814453125\n",
      "Epoch 62, Batch 20832, Loss: 176.98165893554688\n",
      "Epoch 62, Batch 20833, Loss: 175.57260131835938\n",
      "Epoch 62, Batch 20834, Loss: 166.441650390625\n",
      "Epoch 62, Batch 20835, Loss: 159.39125061035156\n",
      "Epoch 62, Batch 20836, Loss: 167.2789764404297\n",
      "Epoch 62, Batch 20837, Loss: 167.74566650390625\n",
      "Epoch 62, Batch 20838, Loss: 187.8188018798828\n",
      "Epoch 62, Batch 20839, Loss: 172.5906219482422\n",
      "Epoch 62, Batch 20840, Loss: 168.62937927246094\n",
      "Epoch 62, Batch 20841, Loss: 170.21490478515625\n",
      "Epoch 62, Batch 20842, Loss: 174.09381103515625\n",
      "Epoch 62, Batch 20843, Loss: 160.79840087890625\n",
      "Epoch 62, Batch 20844, Loss: 168.4371795654297\n",
      "Epoch 62, Batch 20845, Loss: 188.03143310546875\n",
      "Epoch 62, Batch 20846, Loss: 179.33522033691406\n",
      "Epoch 62, Batch 20847, Loss: 171.5789794921875\n",
      "Epoch 62, Batch 20848, Loss: 163.4085693359375\n",
      "Epoch 62, Batch 20849, Loss: 173.7209014892578\n",
      "Epoch 62, Batch 20850, Loss: 177.36428833007812\n",
      "Epoch 62, Batch 20851, Loss: 172.6742706298828\n",
      "Epoch 62, Batch 20852, Loss: 162.8459930419922\n",
      "Epoch 62, Batch 20853, Loss: 181.9695587158203\n",
      "Epoch 62, Batch 20854, Loss: 176.26974487304688\n",
      "Epoch 62, Batch 20855, Loss: 170.95840454101562\n",
      "Epoch 62, Batch 20856, Loss: 184.6727294921875\n",
      "Epoch 62, Batch 20857, Loss: 174.7610626220703\n",
      "Epoch 62, Batch 20858, Loss: 166.52374267578125\n",
      "Epoch 62, Batch 20859, Loss: 181.18614196777344\n",
      "Epoch 62, Batch 20860, Loss: 168.8685760498047\n",
      "Epoch 62, Batch 20861, Loss: 163.426513671875\n",
      "Epoch 62, Batch 20862, Loss: 166.49790954589844\n",
      "Epoch 62, Batch 20863, Loss: 180.142578125\n",
      "Epoch 62, Batch 20864, Loss: 186.0070343017578\n",
      "Epoch 62, Batch 20865, Loss: 201.7101593017578\n",
      "Epoch 62, Batch 20866, Loss: 180.26541137695312\n",
      "Epoch 62, Batch 20867, Loss: 171.83079528808594\n",
      "Epoch 62, Batch 20868, Loss: 181.45350646972656\n",
      "Epoch 62, Batch 20869, Loss: 175.65756225585938\n",
      "Epoch 62, Batch 20870, Loss: 174.27745056152344\n",
      "Epoch 62, Batch 20871, Loss: 165.8707275390625\n",
      "Epoch 62, Batch 20872, Loss: 170.09585571289062\n",
      "Epoch 62, Batch 20873, Loss: 171.3311309814453\n",
      "Epoch 62, Batch 20874, Loss: 180.01715087890625\n",
      "Epoch 62, Batch 20875, Loss: 164.76400756835938\n",
      "Epoch 62, Batch 20876, Loss: 177.59007263183594\n",
      "Epoch 62, Batch 20877, Loss: 178.65310668945312\n",
      "Epoch 62, Batch 20878, Loss: 176.59800720214844\n",
      "Epoch 62, Batch 20879, Loss: 168.60629272460938\n",
      "Epoch 62, Batch 20880, Loss: 181.3346710205078\n",
      "Epoch 62, Batch 20881, Loss: 179.98251342773438\n",
      "Epoch 62, Batch 20882, Loss: 157.40809631347656\n",
      "Epoch 62, Batch 20883, Loss: 188.8651580810547\n",
      "Epoch 62, Batch 20884, Loss: 170.79701232910156\n",
      "Epoch 62, Batch 20885, Loss: 175.32891845703125\n",
      "Epoch 62, Batch 20886, Loss: 185.00450134277344\n",
      "Epoch 62, Batch 20887, Loss: 174.80184936523438\n",
      "Epoch 62, Batch 20888, Loss: 171.84889221191406\n",
      "Epoch 62, Batch 20889, Loss: 156.73336791992188\n",
      "Epoch 62, Batch 20890, Loss: 163.76678466796875\n",
      "Epoch 62, Batch 20891, Loss: 186.88694763183594\n",
      "Epoch 62, Batch 20892, Loss: 177.0426483154297\n",
      "Epoch 62, Batch 20893, Loss: 163.04405212402344\n",
      "Epoch 62, Batch 20894, Loss: 174.67254638671875\n",
      "Epoch 62, Batch 20895, Loss: 175.77340698242188\n",
      "Epoch 62, Batch 20896, Loss: 178.13748168945312\n",
      "Epoch 62, Batch 20897, Loss: 173.38587951660156\n",
      "Epoch 62, Batch 20898, Loss: 179.32354736328125\n",
      "Epoch 62, Batch 20899, Loss: 165.28054809570312\n",
      "Epoch 62, Batch 20900, Loss: 184.87681579589844\n",
      "Epoch 62, Batch 20901, Loss: 178.48870849609375\n",
      "Epoch 62, Batch 20902, Loss: 173.3970489501953\n",
      "Epoch 62, Batch 20903, Loss: 171.58950805664062\n",
      "Epoch 62, Batch 20904, Loss: 184.21485900878906\n",
      "Epoch 62, Batch 20905, Loss: 166.5919647216797\n",
      "Epoch 62, Batch 20906, Loss: 183.66017150878906\n",
      "Epoch 62, Batch 20907, Loss: 181.99501037597656\n",
      "Epoch 62, Batch 20908, Loss: 167.6475067138672\n",
      "Epoch 62, Batch 20909, Loss: 180.041015625\n",
      "Epoch 62, Batch 20910, Loss: 187.8258056640625\n",
      "Epoch 62, Batch 20911, Loss: 171.1831817626953\n",
      "Epoch 62, Batch 20912, Loss: 169.2781524658203\n",
      "Epoch 62, Batch 20913, Loss: 168.6017303466797\n",
      "Epoch 62, Batch 20914, Loss: 161.96775817871094\n",
      "Epoch 62, Batch 20915, Loss: 167.17190551757812\n",
      "Epoch 62, Batch 20916, Loss: 158.31890869140625\n",
      "Epoch 62, Batch 20917, Loss: 159.715087890625\n",
      "Epoch 62, Batch 20918, Loss: 175.66749572753906\n",
      "Epoch 62, Batch 20919, Loss: 164.14646911621094\n",
      "Epoch 62, Batch 20920, Loss: 162.64102172851562\n",
      "Epoch 62, Batch 20921, Loss: 180.5281219482422\n",
      "Epoch 62, Batch 20922, Loss: 168.79957580566406\n",
      "Epoch 62, Batch 20923, Loss: 177.94308471679688\n",
      "Epoch 62, Batch 20924, Loss: 173.0399932861328\n",
      "Epoch 62, Batch 20925, Loss: 155.63442993164062\n",
      "Epoch 62, Batch 20926, Loss: 171.98190307617188\n",
      "Epoch 62, Batch 20927, Loss: 175.077392578125\n",
      "Epoch 62, Batch 20928, Loss: 175.66839599609375\n",
      "Epoch 62, Batch 20929, Loss: 196.45603942871094\n",
      "Epoch 62, Batch 20930, Loss: 161.0576934814453\n",
      "Epoch 62, Batch 20931, Loss: 159.75234985351562\n",
      "Epoch 62, Batch 20932, Loss: 176.19625854492188\n",
      "Epoch 62, Batch 20933, Loss: 165.77967834472656\n",
      "Epoch 62, Batch 20934, Loss: 176.6717071533203\n",
      "Epoch 62, Batch 20935, Loss: 171.49742126464844\n",
      "Epoch 62, Batch 20936, Loss: 169.05186462402344\n",
      "Epoch 62, Batch 20937, Loss: 163.09957885742188\n",
      "Epoch 62, Batch 20938, Loss: 180.27249145507812\n",
      "Epoch 62, Batch 20939, Loss: 168.57565307617188\n",
      "Epoch 62, Batch 20940, Loss: 167.8525848388672\n",
      "Epoch 62, Batch 20941, Loss: 155.6575164794922\n",
      "Epoch 62, Batch 20942, Loss: 165.6079864501953\n",
      "Epoch 62, Batch 20943, Loss: 173.0792694091797\n",
      "Epoch 62, Batch 20944, Loss: 167.45274353027344\n",
      "Epoch 62, Batch 20945, Loss: 157.13595581054688\n",
      "Epoch 62, Batch 20946, Loss: 154.44668579101562\n",
      "Epoch 62, Batch 20947, Loss: 159.29916381835938\n",
      "Epoch 62, Batch 20948, Loss: 172.09768676757812\n",
      "Epoch 62, Batch 20949, Loss: 176.60951232910156\n",
      "Epoch 62, Batch 20950, Loss: 151.42599487304688\n",
      "Epoch 62, Batch 20951, Loss: 182.96327209472656\n",
      "Epoch 62, Batch 20952, Loss: 162.55857849121094\n",
      "Epoch 62, Batch 20953, Loss: 169.72174072265625\n",
      "Epoch 62, Batch 20954, Loss: 160.2587127685547\n",
      "Epoch 62, Batch 20955, Loss: 158.8441619873047\n",
      "Epoch 62, Batch 20956, Loss: 156.44192504882812\n",
      "Epoch 62, Batch 20957, Loss: 176.19012451171875\n",
      "Epoch 62, Batch 20958, Loss: 165.76768493652344\n",
      "Epoch 62, Batch 20959, Loss: 165.1677703857422\n",
      "Epoch 62, Batch 20960, Loss: 177.18701171875\n",
      "Epoch 62, Batch 20961, Loss: 169.0334930419922\n",
      "Epoch 62, Batch 20962, Loss: 187.3732147216797\n",
      "Epoch 62, Batch 20963, Loss: 178.82870483398438\n",
      "Epoch 62, Batch 20964, Loss: 182.6288604736328\n",
      "Epoch 62, Batch 20965, Loss: 162.9604034423828\n",
      "Epoch 62, Batch 20966, Loss: 163.1349334716797\n",
      "Epoch 62, Batch 20967, Loss: 175.81846618652344\n",
      "Epoch 62, Batch 20968, Loss: 176.3407440185547\n",
      "Epoch 62, Batch 20969, Loss: 173.35231018066406\n",
      "Epoch 62, Batch 20970, Loss: 157.22877502441406\n",
      "Epoch 62, Batch 20971, Loss: 200.50819396972656\n",
      "Epoch 62, Batch 20972, Loss: 166.25477600097656\n",
      "Epoch 62, Batch 20973, Loss: 172.72171020507812\n",
      "Epoch 62, Batch 20974, Loss: 149.62786865234375\n",
      "Epoch 62, Batch 20975, Loss: 175.18849182128906\n",
      "Epoch 62, Batch 20976, Loss: 191.36492919921875\n",
      "Epoch 62, Batch 20977, Loss: 172.2914276123047\n",
      "Epoch 62, Batch 20978, Loss: 169.43597412109375\n",
      "Epoch 62, Batch 20979, Loss: 180.48623657226562\n",
      "Epoch 62, Batch 20980, Loss: 164.0848388671875\n",
      "Epoch 62, Batch 20981, Loss: 177.1583251953125\n",
      "Epoch 62, Batch 20982, Loss: 157.6631622314453\n",
      "Epoch 62, Batch 20983, Loss: 171.6233673095703\n",
      "Epoch 62, Batch 20984, Loss: 180.61495971679688\n",
      "Epoch 62, Batch 20985, Loss: 175.31915283203125\n",
      "Epoch 62, Batch 20986, Loss: 173.0527801513672\n",
      "Epoch 62, Batch 20987, Loss: 171.29640197753906\n",
      "Epoch 62, Batch 20988, Loss: 175.6791534423828\n",
      "Epoch 62, Batch 20989, Loss: 171.63865661621094\n",
      "Epoch 62, Batch 20990, Loss: 194.5500030517578\n",
      "Epoch 62, Batch 20991, Loss: 183.1511993408203\n",
      "Epoch 62, Batch 20992, Loss: 182.01364135742188\n",
      "Epoch 62, Batch 20993, Loss: 164.62733459472656\n",
      "Epoch 62, Batch 20994, Loss: 175.70387268066406\n",
      "Epoch 62, Batch 20995, Loss: 167.1058349609375\n",
      "Epoch 62, Batch 20996, Loss: 156.40187072753906\n",
      "Epoch 62, Batch 20997, Loss: 174.04356384277344\n",
      "Epoch 62, Batch 20998, Loss: 183.32232666015625\n",
      "Epoch 62, Batch 20999, Loss: 166.7307891845703\n",
      "Epoch 62, Batch 21000, Loss: 173.0801544189453\n",
      "Epoch 62, Batch 21001, Loss: 191.9149932861328\n",
      "Epoch 62, Batch 21002, Loss: 166.21157836914062\n",
      "Epoch 62, Batch 21003, Loss: 170.44717407226562\n",
      "Epoch 62, Batch 21004, Loss: 158.35855102539062\n",
      "Epoch 62, Batch 21005, Loss: 170.1515350341797\n",
      "Epoch 62, Batch 21006, Loss: 172.3057098388672\n",
      "Epoch 62, Batch 21007, Loss: 180.28359985351562\n",
      "Epoch 62, Batch 21008, Loss: 169.5723876953125\n",
      "Epoch 62, Batch 21009, Loss: 184.65695190429688\n",
      "Epoch 62, Batch 21010, Loss: 167.93414306640625\n",
      "Epoch 62, Batch 21011, Loss: 182.25218200683594\n",
      "Epoch 62, Batch 21012, Loss: 188.674072265625\n",
      "Epoch 62, Batch 21013, Loss: 178.24200439453125\n",
      "Epoch 62, Batch 21014, Loss: 161.35520935058594\n",
      "Epoch 62, Batch 21015, Loss: 163.47048950195312\n",
      "Epoch 62, Batch 21016, Loss: 188.6603546142578\n",
      "Epoch 62, Batch 21017, Loss: 168.87399291992188\n",
      "Epoch 62, Batch 21018, Loss: 174.06173706054688\n",
      "Epoch 62, Batch 21019, Loss: 188.36721801757812\n",
      "Epoch 62, Batch 21020, Loss: 166.9217987060547\n",
      "Epoch 62, Batch 21021, Loss: 162.11044311523438\n",
      "Epoch 62, Batch 21022, Loss: 175.54701232910156\n",
      "Epoch 62, Batch 21023, Loss: 163.930419921875\n",
      "Epoch 62, Batch 21024, Loss: 184.1713104248047\n",
      "Epoch 62, Batch 21025, Loss: 178.202880859375\n",
      "Epoch 62, Batch 21026, Loss: 160.4871368408203\n",
      "Epoch 62, Batch 21027, Loss: 178.96739196777344\n",
      "Epoch 62, Batch 21028, Loss: 166.23727416992188\n",
      "Epoch 62, Batch 21029, Loss: 203.39926147460938\n",
      "Epoch 62, Batch 21030, Loss: 181.5054168701172\n",
      "Epoch 62, Batch 21031, Loss: 189.17173767089844\n",
      "Epoch 62, Batch 21032, Loss: 173.19378662109375\n",
      "Epoch 62, Batch 21033, Loss: 171.98532104492188\n",
      "Epoch 62, Batch 21034, Loss: 184.9209747314453\n",
      "Epoch 62, Batch 21035, Loss: 174.9146270751953\n",
      "Epoch 62, Batch 21036, Loss: 168.7510223388672\n",
      "Epoch 62, Batch 21037, Loss: 179.7376251220703\n",
      "Epoch 62, Batch 21038, Loss: 166.5140380859375\n",
      "Epoch 62, Batch 21039, Loss: 166.6828155517578\n",
      "Epoch 62, Batch 21040, Loss: 174.48355102539062\n",
      "Epoch 62, Batch 21041, Loss: 164.86383056640625\n",
      "Epoch 62, Batch 21042, Loss: 180.6123504638672\n",
      "Epoch 62, Batch 21043, Loss: 172.3585968017578\n",
      "Epoch 62, Batch 21044, Loss: 172.2559051513672\n",
      "Epoch 62, Batch 21045, Loss: 176.9167938232422\n",
      "Epoch 62, Batch 21046, Loss: 185.4311065673828\n",
      "Epoch 62, Batch 21047, Loss: 180.47348022460938\n",
      "Epoch 62, Batch 21048, Loss: 177.005615234375\n",
      "Epoch 62, Batch 21049, Loss: 178.08409118652344\n",
      "Epoch 62, Batch 21050, Loss: 172.9931640625\n",
      "Epoch 62, Batch 21051, Loss: 173.22271728515625\n",
      "Epoch 62, Batch 21052, Loss: 174.1615447998047\n",
      "Epoch 62, Batch 21053, Loss: 161.4311981201172\n",
      "Epoch 62, Batch 21054, Loss: 177.41383361816406\n",
      "Epoch 62, Batch 21055, Loss: 182.4460906982422\n",
      "Epoch 62, Batch 21056, Loss: 174.26895141601562\n",
      "Epoch 62, Batch 21057, Loss: 167.56961059570312\n",
      "Epoch 62, Batch 21058, Loss: 176.18362426757812\n",
      "Epoch 62, Batch 21059, Loss: 173.8133087158203\n",
      "Epoch 62, Batch 21060, Loss: 169.81399536132812\n",
      "Epoch 62, Batch 21061, Loss: 169.479736328125\n",
      "Epoch 62, Batch 21062, Loss: 173.30477905273438\n",
      "Epoch 62, Batch 21063, Loss: 179.02737426757812\n",
      "Epoch 62, Batch 21064, Loss: 172.6195526123047\n",
      "Epoch 62, Batch 21065, Loss: 173.9687042236328\n",
      "Epoch 62, Batch 21066, Loss: 177.14280700683594\n",
      "Epoch 62, Batch 21067, Loss: 167.58328247070312\n",
      "Epoch 62, Batch 21068, Loss: 146.67164611816406\n",
      "Epoch 62, Batch 21069, Loss: 175.3238525390625\n",
      "Epoch 62, Batch 21070, Loss: 178.762451171875\n",
      "Epoch 62, Batch 21071, Loss: 179.98277282714844\n",
      "Epoch 62, Batch 21072, Loss: 171.2684783935547\n",
      "Epoch 62, Batch 21073, Loss: 166.02891540527344\n",
      "Epoch 62, Batch 21074, Loss: 185.52099609375\n",
      "Epoch 62, Batch 21075, Loss: 184.6095733642578\n",
      "Epoch 62, Batch 21076, Loss: 172.76097106933594\n",
      "Epoch 62, Batch 21077, Loss: 170.60108947753906\n",
      "Epoch 62, Batch 21078, Loss: 188.5531463623047\n",
      "Epoch 62, Batch 21079, Loss: 158.4905242919922\n",
      "Epoch 62, Batch 21080, Loss: 185.5347900390625\n",
      "Epoch 62, Batch 21081, Loss: 200.66659545898438\n",
      "Epoch 62, Batch 21082, Loss: 168.0800323486328\n",
      "Epoch 62, Batch 21083, Loss: 180.97215270996094\n",
      "Epoch 62, Batch 21084, Loss: 157.05419921875\n",
      "Epoch 62, Batch 21085, Loss: 159.0299835205078\n",
      "Epoch 62, Batch 21086, Loss: 179.5422821044922\n",
      "Epoch 62, Batch 21087, Loss: 169.26409912109375\n",
      "Epoch 62, Batch 21088, Loss: 150.4825439453125\n",
      "Epoch 62, Batch 21089, Loss: 165.16481018066406\n",
      "Epoch 62, Batch 21090, Loss: 175.91592407226562\n",
      "Epoch 62, Batch 21091, Loss: 167.68467712402344\n",
      "Epoch 62, Batch 21092, Loss: 168.4159698486328\n",
      "Epoch 62, Batch 21093, Loss: 161.65383911132812\n",
      "Epoch 62, Batch 21094, Loss: 170.15370178222656\n",
      "Epoch 62, Batch 21095, Loss: 180.83094787597656\n",
      "Epoch 62, Batch 21096, Loss: 182.7994842529297\n",
      "Epoch 62, Batch 21097, Loss: 181.10711669921875\n",
      "Epoch 62, Batch 21098, Loss: 158.27517700195312\n",
      "Epoch 62, Batch 21099, Loss: 139.27317810058594\n",
      "Epoch 62, Batch 21100, Loss: 173.11935424804688\n",
      "Epoch 62, Batch 21101, Loss: 182.7667694091797\n",
      "Epoch 62, Batch 21102, Loss: 169.50296020507812\n",
      "Epoch 62, Batch 21103, Loss: 180.90872192382812\n",
      "Epoch 62, Batch 21104, Loss: 160.7587127685547\n",
      "Epoch 62, Batch 21105, Loss: 189.47930908203125\n",
      "Epoch 62, Batch 21106, Loss: 162.5200653076172\n",
      "Epoch 62, Batch 21107, Loss: 182.3543701171875\n",
      "Epoch 62, Batch 21108, Loss: 144.3684844970703\n",
      "Epoch 62, Batch 21109, Loss: 159.0698699951172\n",
      "Epoch 62, Batch 21110, Loss: 185.83177185058594\n",
      "Epoch 62, Batch 21111, Loss: 188.00682067871094\n",
      "Epoch 62, Batch 21112, Loss: 187.67880249023438\n",
      "Epoch 62, Batch 21113, Loss: 202.81459045410156\n",
      "Epoch 62, Batch 21114, Loss: 172.9443359375\n",
      "Epoch 62, Batch 21115, Loss: 167.51162719726562\n",
      "Epoch 62, Batch 21116, Loss: 191.12623596191406\n",
      "Epoch 62, Batch 21117, Loss: 168.92498779296875\n",
      "Epoch 62, Batch 21118, Loss: 181.0018768310547\n",
      "Epoch 62, Batch 21119, Loss: 160.05987548828125\n",
      "Epoch 62, Batch 21120, Loss: 186.36264038085938\n",
      "Epoch 62, Batch 21121, Loss: 166.3199005126953\n",
      "Epoch 62, Batch 21122, Loss: 174.24514770507812\n",
      "Epoch 62, Batch 21123, Loss: 174.45716857910156\n",
      "Epoch 62, Batch 21124, Loss: 172.55589294433594\n",
      "Epoch 62, Batch 21125, Loss: 168.2816925048828\n",
      "Epoch 62, Batch 21126, Loss: 185.0746612548828\n",
      "Epoch 62, Batch 21127, Loss: 175.18919372558594\n",
      "Epoch 62, Batch 21128, Loss: 175.39852905273438\n",
      "Epoch 62, Batch 21129, Loss: 173.81021118164062\n",
      "Epoch 62, Batch 21130, Loss: 183.75502014160156\n",
      "Epoch 62, Batch 21131, Loss: 191.40370178222656\n",
      "Epoch 62, Batch 21132, Loss: 172.52854919433594\n",
      "Epoch 62, Batch 21133, Loss: 155.6197509765625\n",
      "Epoch 62, Batch 21134, Loss: 178.55496215820312\n",
      "Epoch 62, Batch 21135, Loss: 175.7776336669922\n",
      "Epoch 62, Batch 21136, Loss: 183.6279296875\n",
      "Epoch 62, Batch 21137, Loss: 181.6397247314453\n",
      "Epoch 62, Batch 21138, Loss: 170.16087341308594\n",
      "Epoch 62, Batch 21139, Loss: 178.1547088623047\n",
      "Epoch 62, Batch 21140, Loss: 173.08175659179688\n",
      "Epoch 62, Batch 21141, Loss: 171.12356567382812\n",
      "Epoch 62, Batch 21142, Loss: 170.8202667236328\n",
      "Epoch 62, Batch 21143, Loss: 171.5001678466797\n",
      "Epoch 62, Batch 21144, Loss: 169.61045837402344\n",
      "Epoch 62, Batch 21145, Loss: 170.54286193847656\n",
      "Epoch 62, Batch 21146, Loss: 158.1188201904297\n",
      "Epoch 62, Batch 21147, Loss: 162.50868225097656\n",
      "Epoch 62, Batch 21148, Loss: 160.66201782226562\n",
      "Epoch 62, Batch 21149, Loss: 177.27305603027344\n",
      "Epoch 62, Batch 21150, Loss: 185.72010803222656\n",
      "Epoch 62, Batch 21151, Loss: 175.2298583984375\n",
      "Epoch 62, Batch 21152, Loss: 185.80235290527344\n",
      "Epoch 62, Batch 21153, Loss: 176.50941467285156\n",
      "Epoch 62, Batch 21154, Loss: 163.0803680419922\n",
      "Epoch 62, Batch 21155, Loss: 182.52011108398438\n",
      "Epoch 62, Batch 21156, Loss: 173.23194885253906\n",
      "Epoch 62, Batch 21157, Loss: 174.05142211914062\n",
      "Epoch 62, Batch 21158, Loss: 180.3681640625\n",
      "Epoch 62, Batch 21159, Loss: 185.74066162109375\n",
      "Epoch 62, Batch 21160, Loss: 183.50074768066406\n",
      "Epoch 62, Batch 21161, Loss: 153.3520050048828\n",
      "Epoch 62, Batch 21162, Loss: 168.2002410888672\n",
      "Epoch 62, Batch 21163, Loss: 174.90467834472656\n",
      "Epoch 62, Batch 21164, Loss: 173.5636444091797\n",
      "Epoch 62, Batch 21165, Loss: 181.12899780273438\n",
      "Epoch 62, Batch 21166, Loss: 168.16983032226562\n",
      "Epoch 62, Batch 21167, Loss: 180.0112762451172\n",
      "Epoch 62, Batch 21168, Loss: 174.89718627929688\n",
      "Epoch 62, Batch 21169, Loss: 169.82936096191406\n",
      "Epoch 62, Batch 21170, Loss: 166.09373474121094\n",
      "Epoch 62, Batch 21171, Loss: 176.32644653320312\n",
      "Epoch 62, Batch 21172, Loss: 162.08343505859375\n",
      "Epoch 62, Batch 21173, Loss: 164.34068298339844\n",
      "Epoch 62, Batch 21174, Loss: 183.8376007080078\n",
      "Epoch 62, Batch 21175, Loss: 174.0027313232422\n",
      "Epoch 62, Batch 21176, Loss: 181.25704956054688\n",
      "Epoch 62, Batch 21177, Loss: 180.8474884033203\n",
      "Epoch 62, Batch 21178, Loss: 186.75546264648438\n",
      "Epoch 62, Batch 21179, Loss: 184.34776306152344\n",
      "Epoch 62, Batch 21180, Loss: 183.0598907470703\n",
      "Epoch 62, Batch 21181, Loss: 187.72085571289062\n",
      "Epoch 62, Batch 21182, Loss: 156.5481414794922\n",
      "Epoch 62, Batch 21183, Loss: 188.17324829101562\n",
      "Epoch 62, Batch 21184, Loss: 166.79225158691406\n",
      "Epoch 62, Batch 21185, Loss: 186.31890869140625\n",
      "Epoch 62, Batch 21186, Loss: 159.6282196044922\n",
      "Epoch 62, Batch 21187, Loss: 187.17376708984375\n",
      "Epoch 62, Batch 21188, Loss: 183.2198028564453\n",
      "Epoch 62, Batch 21189, Loss: 165.00369262695312\n",
      "Epoch 62, Batch 21190, Loss: 182.00096130371094\n",
      "Epoch 62, Batch 21191, Loss: 172.05018615722656\n",
      "Epoch 62, Batch 21192, Loss: 158.38685607910156\n",
      "Epoch 62, Batch 21193, Loss: 162.6632080078125\n",
      "Epoch 62, Batch 21194, Loss: 171.7161102294922\n",
      "Epoch 62, Batch 21195, Loss: 167.16287231445312\n",
      "Epoch 62, Batch 21196, Loss: 169.213134765625\n",
      "Epoch 62, Batch 21197, Loss: 171.1428680419922\n",
      "Epoch 62, Batch 21198, Loss: 181.11851501464844\n",
      "Epoch 62, Batch 21199, Loss: 177.39535522460938\n",
      "Epoch 62, Batch 21200, Loss: 182.51866149902344\n",
      "Epoch 62, Batch 21201, Loss: 168.72708129882812\n",
      "Epoch 62, Batch 21202, Loss: 185.2404022216797\n",
      "Epoch 62, Batch 21203, Loss: 192.79913330078125\n",
      "Epoch 62, Batch 21204, Loss: 173.52195739746094\n",
      "Epoch 62, Batch 21205, Loss: 163.9252471923828\n",
      "Epoch 62, Batch 21206, Loss: 159.76475524902344\n",
      "Epoch 62, Batch 21207, Loss: 194.53102111816406\n",
      "Epoch 62, Batch 21208, Loss: 166.0970458984375\n",
      "Epoch 62, Batch 21209, Loss: 175.57879638671875\n",
      "Epoch 62, Batch 21210, Loss: 161.87632751464844\n",
      "Epoch 62, Batch 21211, Loss: 168.22137451171875\n",
      "Epoch 62, Batch 21212, Loss: 162.50755310058594\n",
      "Epoch 62, Batch 21213, Loss: 184.62205505371094\n",
      "Epoch 62, Batch 21214, Loss: 182.3544464111328\n",
      "Epoch 62, Batch 21215, Loss: 164.5399932861328\n",
      "Epoch 62, Batch 21216, Loss: 172.98876953125\n",
      "Epoch 62, Batch 21217, Loss: 172.64622497558594\n",
      "Epoch 62, Batch 21218, Loss: 162.9670867919922\n",
      "Epoch 62, Batch 21219, Loss: 169.30625915527344\n",
      "Epoch 62, Batch 21220, Loss: 170.17257690429688\n",
      "Epoch 62, Batch 21221, Loss: 164.03453063964844\n",
      "Epoch 62, Batch 21222, Loss: 173.5262908935547\n",
      "Epoch 62, Batch 21223, Loss: 162.2089385986328\n",
      "Epoch 62, Batch 21224, Loss: 184.48968505859375\n",
      "Epoch 62, Batch 21225, Loss: 169.46542358398438\n",
      "Epoch 62, Batch 21226, Loss: 182.76502990722656\n",
      "Epoch 62, Batch 21227, Loss: 176.88400268554688\n",
      "Epoch 62, Batch 21228, Loss: 163.9186553955078\n",
      "Epoch 62, Batch 21229, Loss: 163.953857421875\n",
      "Epoch 62, Batch 21230, Loss: 173.2266845703125\n",
      "Epoch 62, Batch 21231, Loss: 159.03321838378906\n",
      "Epoch 62, Batch 21232, Loss: 175.50450134277344\n",
      "Epoch 62, Batch 21233, Loss: 185.391357421875\n",
      "Epoch 62, Batch 21234, Loss: 192.30628967285156\n",
      "Epoch 62, Batch 21235, Loss: 178.67922973632812\n",
      "Epoch 62, Batch 21236, Loss: 166.26290893554688\n",
      "Epoch 62, Batch 21237, Loss: 162.6177978515625\n",
      "Epoch 62, Batch 21238, Loss: 166.15821838378906\n",
      "Epoch 62, Batch 21239, Loss: 173.4785919189453\n",
      "Epoch 62, Batch 21240, Loss: 183.57894897460938\n",
      "Epoch 62, Batch 21241, Loss: 190.83001708984375\n",
      "Epoch 62, Batch 21242, Loss: 188.37193298339844\n",
      "Epoch 62, Batch 21243, Loss: 175.25645446777344\n",
      "Epoch 62, Batch 21244, Loss: 172.59231567382812\n",
      "Epoch 62, Batch 21245, Loss: 164.14950561523438\n",
      "Epoch 62, Batch 21246, Loss: 177.2035675048828\n",
      "Epoch 62, Batch 21247, Loss: 177.5432586669922\n",
      "Epoch 62, Batch 21248, Loss: 185.5212860107422\n",
      "Epoch 62, Batch 21249, Loss: 176.76156616210938\n",
      "Epoch 62, Batch 21250, Loss: 169.0529327392578\n",
      "Epoch 62, Batch 21251, Loss: 161.48951721191406\n",
      "Epoch 62, Batch 21252, Loss: 191.2096405029297\n",
      "Epoch 62, Batch 21253, Loss: 167.62930297851562\n",
      "Epoch 62, Batch 21254, Loss: 164.218994140625\n",
      "Epoch 62, Batch 21255, Loss: 162.9540252685547\n",
      "Epoch 62, Batch 21256, Loss: 179.2119140625\n",
      "Epoch 62, Batch 21257, Loss: 185.16664123535156\n",
      "Epoch 62, Batch 21258, Loss: 176.71556091308594\n",
      "Epoch 62, Batch 21259, Loss: 170.28269958496094\n",
      "Epoch 62, Batch 21260, Loss: 168.92909240722656\n",
      "Epoch 62, Batch 21261, Loss: 167.6630096435547\n",
      "Epoch 62, Batch 21262, Loss: 167.64088439941406\n",
      "Epoch 62, Batch 21263, Loss: 195.35012817382812\n",
      "Epoch 62, Batch 21264, Loss: 170.78614807128906\n",
      "Epoch 62, Batch 21265, Loss: 160.04620361328125\n",
      "Epoch 62, Batch 21266, Loss: 173.36260986328125\n",
      "Epoch 62, Batch 21267, Loss: 169.9078826904297\n",
      "Epoch 62, Batch 21268, Loss: 171.82470703125\n",
      "Epoch 62, Batch 21269, Loss: 172.99945068359375\n",
      "Epoch 62, Batch 21270, Loss: 172.79759216308594\n",
      "Epoch 62, Batch 21271, Loss: 178.04409790039062\n",
      "Epoch 62, Batch 21272, Loss: 167.5256805419922\n",
      "Epoch 62, Batch 21273, Loss: 174.94357299804688\n",
      "Epoch 62, Batch 21274, Loss: 177.44769287109375\n",
      "Epoch 62, Batch 21275, Loss: 183.8972930908203\n",
      "Epoch 62, Batch 21276, Loss: 184.0865478515625\n",
      "Epoch 62, Batch 21277, Loss: 177.89596557617188\n",
      "Epoch 62, Batch 21278, Loss: 172.30850219726562\n",
      "Epoch 62, Batch 21279, Loss: 167.87672424316406\n",
      "Epoch 62, Batch 21280, Loss: 186.80723571777344\n",
      "Epoch 62, Batch 21281, Loss: 158.41175842285156\n",
      "Epoch 62, Batch 21282, Loss: 179.81597900390625\n",
      "Epoch 62, Batch 21283, Loss: 194.38174438476562\n",
      "Epoch 62, Batch 21284, Loss: 181.8481903076172\n",
      "Epoch 62, Batch 21285, Loss: 174.082275390625\n",
      "Epoch 62, Batch 21286, Loss: 174.95806884765625\n",
      "Epoch 62, Batch 21287, Loss: 193.78541564941406\n",
      "Epoch 62, Batch 21288, Loss: 182.16928100585938\n",
      "Epoch 62, Batch 21289, Loss: 165.57545471191406\n",
      "Epoch 62, Batch 21290, Loss: 174.8299560546875\n",
      "Epoch 62, Batch 21291, Loss: 163.57020568847656\n",
      "Epoch 62, Batch 21292, Loss: 161.41690063476562\n",
      "Epoch 62, Batch 21293, Loss: 178.15647888183594\n",
      "Epoch 62, Batch 21294, Loss: 173.1905975341797\n",
      "Epoch 62, Batch 21295, Loss: 176.8706512451172\n",
      "Epoch 62, Batch 21296, Loss: 187.96530151367188\n",
      "Epoch 62, Batch 21297, Loss: 177.1395721435547\n",
      "Epoch 62, Batch 21298, Loss: 187.00062561035156\n",
      "Epoch 62, Batch 21299, Loss: 185.56573486328125\n",
      "Epoch 62, Batch 21300, Loss: 186.52484130859375\n",
      "Epoch 62, Batch 21301, Loss: 173.48919677734375\n",
      "Epoch 62, Batch 21302, Loss: 189.0523223876953\n",
      "Epoch 62, Batch 21303, Loss: 172.80859375\n",
      "Epoch 62, Batch 21304, Loss: 171.6308135986328\n",
      "Epoch 62, Batch 21305, Loss: 174.04287719726562\n",
      "Epoch 62, Batch 21306, Loss: 175.13658142089844\n",
      "Epoch 62, Batch 21307, Loss: 173.54808044433594\n",
      "Epoch 62, Batch 21308, Loss: 174.83193969726562\n",
      "Epoch 62, Batch 21309, Loss: 175.06349182128906\n",
      "Epoch 62, Batch 21310, Loss: 169.22784423828125\n",
      "Epoch 62, Batch 21311, Loss: 150.50076293945312\n",
      "Epoch 62, Batch 21312, Loss: 178.7203369140625\n",
      "Epoch 62, Batch 21313, Loss: 190.61373901367188\n",
      "Epoch 62, Batch 21314, Loss: 168.33615112304688\n",
      "Epoch 62, Batch 21315, Loss: 145.35076904296875\n",
      "Epoch 62, Batch 21316, Loss: 164.76596069335938\n",
      "Epoch 62, Batch 21317, Loss: 174.7649688720703\n",
      "Epoch 62, Batch 21318, Loss: 183.60400390625\n",
      "Epoch 62, Batch 21319, Loss: 164.32904052734375\n",
      "Epoch 62, Batch 21320, Loss: 183.06698608398438\n",
      "Epoch 62, Batch 21321, Loss: 170.45391845703125\n",
      "Epoch 62, Batch 21322, Loss: 161.65293884277344\n",
      "Epoch 62, Batch 21323, Loss: 170.10614013671875\n",
      "Epoch 62, Batch 21324, Loss: 172.70248413085938\n",
      "Epoch 62, Batch 21325, Loss: 172.90615844726562\n",
      "Epoch 62, Batch 21326, Loss: 178.2336883544922\n",
      "Epoch 62, Batch 21327, Loss: 173.31149291992188\n",
      "Epoch 62, Batch 21328, Loss: 195.8529510498047\n",
      "Epoch 62, Batch 21329, Loss: 197.74034118652344\n",
      "Epoch 62, Batch 21330, Loss: 173.78482055664062\n",
      "Epoch 62, Batch 21331, Loss: 166.557861328125\n",
      "Epoch 62, Batch 21332, Loss: 163.23239135742188\n",
      "Epoch 62, Batch 21333, Loss: 164.13479614257812\n",
      "Epoch 62, Batch 21334, Loss: 151.56224060058594\n",
      "Epoch 62, Batch 21335, Loss: 169.2930908203125\n",
      "Epoch 62, Batch 21336, Loss: 171.31546020507812\n",
      "Epoch 62, Batch 21337, Loss: 177.8283233642578\n",
      "Epoch 62, Batch 21338, Loss: 175.5628662109375\n",
      "Epoch 62, Batch 21339, Loss: 175.31918334960938\n",
      "Epoch 62, Batch 21340, Loss: 167.3094024658203\n",
      "Epoch 62, Batch 21341, Loss: 185.94203186035156\n",
      "Epoch 62, Batch 21342, Loss: 162.2161102294922\n",
      "Epoch 62, Batch 21343, Loss: 198.1359405517578\n",
      "Epoch 62, Batch 21344, Loss: 174.18894958496094\n",
      "Epoch 62, Batch 21345, Loss: 184.35130310058594\n",
      "Epoch 62, Batch 21346, Loss: 173.17684936523438\n",
      "Epoch 62, Batch 21347, Loss: 170.9600372314453\n",
      "Epoch 62, Batch 21348, Loss: 171.91395568847656\n",
      "Epoch 62, Batch 21349, Loss: 179.67742919921875\n",
      "Epoch 62, Batch 21350, Loss: 195.60302734375\n",
      "Epoch 62, Batch 21351, Loss: 169.78712463378906\n",
      "Epoch 62, Batch 21352, Loss: 172.9247283935547\n",
      "Epoch 62, Batch 21353, Loss: 171.8629608154297\n",
      "Epoch 62, Batch 21354, Loss: 185.59390258789062\n",
      "Epoch 62, Batch 21355, Loss: 180.1067657470703\n",
      "Epoch 62, Batch 21356, Loss: 175.59400939941406\n",
      "Epoch 62, Batch 21357, Loss: 178.37783813476562\n",
      "Epoch 62, Batch 21358, Loss: 169.67266845703125\n",
      "Epoch 62, Batch 21359, Loss: 167.8972930908203\n",
      "Epoch 62, Batch 21360, Loss: 178.68670654296875\n",
      "Epoch 62, Batch 21361, Loss: 171.71084594726562\n",
      "Epoch 62, Batch 21362, Loss: 158.79428100585938\n",
      "Epoch 62, Batch 21363, Loss: 171.10313415527344\n",
      "Epoch 62, Batch 21364, Loss: 176.5150604248047\n",
      "Epoch 62, Batch 21365, Loss: 161.43812561035156\n",
      "Epoch 62, Batch 21366, Loss: 152.25892639160156\n",
      "Epoch 62, Batch 21367, Loss: 177.03494262695312\n",
      "Epoch 62, Batch 21368, Loss: 188.90072631835938\n",
      "Epoch 62, Batch 21369, Loss: 156.02310180664062\n",
      "Epoch 62, Batch 21370, Loss: 185.80809020996094\n",
      "Epoch 62, Batch 21371, Loss: 164.9827423095703\n",
      "Epoch 62, Batch 21372, Loss: 173.06802368164062\n",
      "Epoch 62, Batch 21373, Loss: 162.86111450195312\n",
      "Epoch 62, Batch 21374, Loss: 185.27420043945312\n",
      "Epoch 62, Batch 21375, Loss: 166.59683227539062\n",
      "Epoch 62, Batch 21376, Loss: 161.45567321777344\n",
      "Epoch 62, Batch 21377, Loss: 199.67562866210938\n",
      "Epoch 62, Batch 21378, Loss: 169.82498168945312\n",
      "Epoch 62, Batch 21379, Loss: 164.82601928710938\n",
      "Epoch 62, Batch 21380, Loss: 169.09437561035156\n",
      "Epoch 62, Batch 21381, Loss: 194.3190460205078\n",
      "Epoch 62, Batch 21382, Loss: 175.461669921875\n",
      "Epoch 62, Batch 21383, Loss: 170.19659423828125\n",
      "Epoch 62, Batch 21384, Loss: 164.23178100585938\n",
      "Epoch 62, Batch 21385, Loss: 190.95071411132812\n",
      "Epoch 62, Batch 21386, Loss: 164.26303100585938\n",
      "Epoch 62, Batch 21387, Loss: 175.33901977539062\n",
      "Epoch 62, Batch 21388, Loss: 174.24481201171875\n",
      "Epoch 62, Batch 21389, Loss: 204.2833251953125\n",
      "Epoch 62, Batch 21390, Loss: 177.77413940429688\n",
      "Epoch 62, Batch 21391, Loss: 176.5013885498047\n",
      "Epoch 62, Batch 21392, Loss: 175.97618103027344\n",
      "Epoch 62, Batch 21393, Loss: 180.93031311035156\n",
      "Epoch 62, Batch 21394, Loss: 170.2047119140625\n",
      "Epoch 62, Batch 21395, Loss: 185.696533203125\n",
      "Epoch 62, Batch 21396, Loss: 173.75619506835938\n",
      "Epoch 62, Batch 21397, Loss: 170.09097290039062\n",
      "Epoch 62, Batch 21398, Loss: 184.67782592773438\n",
      "Epoch 62, Batch 21399, Loss: 165.28334045410156\n",
      "Epoch 62, Batch 21400, Loss: 157.520751953125\n",
      "Epoch 62, Batch 21401, Loss: 182.90402221679688\n",
      "Epoch 62, Batch 21402, Loss: 177.52964782714844\n",
      "Epoch 62, Batch 21403, Loss: 175.12014770507812\n",
      "Epoch 62, Batch 21404, Loss: 168.9948272705078\n",
      "Epoch 62, Batch 21405, Loss: 171.5277099609375\n",
      "Epoch 62, Batch 21406, Loss: 179.00978088378906\n",
      "Epoch 62, Batch 21407, Loss: 172.43699645996094\n",
      "Epoch 62, Batch 21408, Loss: 157.98785400390625\n",
      "Epoch 62, Batch 21409, Loss: 181.07894897460938\n",
      "Epoch 62, Batch 21410, Loss: 170.1876220703125\n",
      "Epoch 62, Batch 21411, Loss: 186.294677734375\n",
      "Epoch 62, Batch 21412, Loss: 184.80218505859375\n",
      "Epoch 62, Batch 21413, Loss: 173.28297424316406\n",
      "Epoch 62, Batch 21414, Loss: 171.34107971191406\n",
      "Epoch 62, Batch 21415, Loss: 179.2037811279297\n",
      "Epoch 62, Batch 21416, Loss: 162.21859741210938\n",
      "Epoch 62, Batch 21417, Loss: 181.01974487304688\n",
      "Epoch 62, Batch 21418, Loss: 173.8525848388672\n",
      "Epoch 62, Batch 21419, Loss: 182.6326904296875\n",
      "Epoch 62, Batch 21420, Loss: 157.71517944335938\n",
      "Epoch 62, Batch 21421, Loss: 186.8906707763672\n",
      "Epoch 62, Batch 21422, Loss: 176.7376251220703\n",
      "Epoch 62, Batch 21423, Loss: 179.85073852539062\n",
      "Epoch 62, Batch 21424, Loss: 166.0104522705078\n",
      "Epoch 62, Batch 21425, Loss: 172.26084899902344\n",
      "Epoch 62, Batch 21426, Loss: 179.59280395507812\n",
      "Epoch 62, Batch 21427, Loss: 166.3076171875\n",
      "Epoch 62, Batch 21428, Loss: 196.2509307861328\n",
      "Epoch 62, Batch 21429, Loss: 175.5895538330078\n",
      "Epoch 62, Batch 21430, Loss: 172.3466796875\n",
      "Epoch 62, Batch 21431, Loss: 173.73501586914062\n",
      "Epoch 62, Batch 21432, Loss: 159.23583984375\n",
      "Epoch 62, Batch 21433, Loss: 172.1324005126953\n",
      "Epoch 62, Batch 21434, Loss: 167.41986083984375\n",
      "Epoch 62, Batch 21435, Loss: 186.92100524902344\n",
      "Epoch 62, Batch 21436, Loss: 198.87867736816406\n",
      "Epoch 62, Batch 21437, Loss: 177.3520050048828\n",
      "Epoch 62, Batch 21438, Loss: 170.7130126953125\n",
      "Epoch 62, Batch 21439, Loss: 164.80224609375\n",
      "Epoch 62, Batch 21440, Loss: 191.9864501953125\n",
      "Epoch 62, Batch 21441, Loss: 168.46072387695312\n",
      "Epoch 62, Batch 21442, Loss: 173.3585662841797\n",
      "Epoch 62, Batch 21443, Loss: 169.99424743652344\n",
      "Epoch 62, Batch 21444, Loss: 155.31228637695312\n",
      "Epoch 62, Batch 21445, Loss: 161.99603271484375\n",
      "Epoch 62, Batch 21446, Loss: 165.5327606201172\n",
      "Epoch 62, Batch 21447, Loss: 177.54490661621094\n",
      "Epoch 62, Batch 21448, Loss: 169.82664489746094\n",
      "Epoch 62, Batch 21449, Loss: 162.18531799316406\n",
      "Epoch 62, Batch 21450, Loss: 157.04132080078125\n",
      "Epoch 62, Batch 21451, Loss: 167.6636199951172\n",
      "Epoch 62, Batch 21452, Loss: 172.47927856445312\n",
      "Epoch 62, Batch 21453, Loss: 176.62620544433594\n",
      "Epoch 62, Batch 21454, Loss: 163.14511108398438\n",
      "Epoch 62, Batch 21455, Loss: 168.91513061523438\n",
      "Epoch 62, Batch 21456, Loss: 191.0257110595703\n",
      "Epoch 62, Batch 21457, Loss: 168.81417846679688\n",
      "Epoch 62, Batch 21458, Loss: 164.5899658203125\n",
      "Epoch 62, Batch 21459, Loss: 155.08895874023438\n",
      "Epoch 62, Batch 21460, Loss: 172.3311309814453\n",
      "Epoch 62, Batch 21461, Loss: 162.5616455078125\n",
      "Epoch 62, Batch 21462, Loss: 154.37130737304688\n",
      "Epoch 62, Batch 21463, Loss: 176.68222045898438\n",
      "Epoch 62, Batch 21464, Loss: 157.5756378173828\n",
      "Epoch 62, Batch 21465, Loss: 176.31092834472656\n",
      "Epoch 62, Batch 21466, Loss: 155.05087280273438\n",
      "Epoch 62, Batch 21467, Loss: 170.1703338623047\n",
      "Epoch 62, Batch 21468, Loss: 160.8733673095703\n",
      "Epoch 62, Batch 21469, Loss: 173.37889099121094\n",
      "Epoch 62, Batch 21470, Loss: 186.63479614257812\n",
      "Epoch 62, Batch 21471, Loss: 174.0841064453125\n",
      "Epoch 62, Batch 21472, Loss: 188.8150634765625\n",
      "Epoch 62, Batch 21473, Loss: 167.9034881591797\n",
      "Epoch 62, Batch 21474, Loss: 174.8584442138672\n",
      "Epoch 62, Batch 21475, Loss: 177.96630859375\n",
      "Epoch 62, Batch 21476, Loss: 177.05101013183594\n",
      "Epoch 62, Batch 21477, Loss: 188.15927124023438\n",
      "Epoch 62, Batch 21478, Loss: 164.9133758544922\n",
      "Epoch 62, Batch 21479, Loss: 178.39744567871094\n",
      "Epoch 62, Batch 21480, Loss: 165.05099487304688\n",
      "Epoch 62, Batch 21481, Loss: 169.24185180664062\n",
      "Epoch 62, Batch 21482, Loss: 181.81944274902344\n",
      "Epoch 62, Batch 21483, Loss: 185.36915588378906\n",
      "Epoch 62, Batch 21484, Loss: 151.67527770996094\n",
      "Epoch 62, Batch 21485, Loss: 183.769775390625\n",
      "Epoch 62, Batch 21486, Loss: 169.92161560058594\n",
      "Epoch 62, Batch 21487, Loss: 180.2340545654297\n",
      "Epoch 62, Batch 21488, Loss: 170.0071258544922\n",
      "Epoch 62, Batch 21489, Loss: 167.38888549804688\n",
      "Epoch 62, Batch 21490, Loss: 178.99034118652344\n",
      "Epoch 62, Batch 21491, Loss: 176.77821350097656\n",
      "Epoch 62, Batch 21492, Loss: 185.44097900390625\n",
      "Epoch 62, Batch 21493, Loss: 167.9508819580078\n",
      "Epoch 62, Batch 21494, Loss: 179.32923889160156\n",
      "Epoch 62, Batch 21495, Loss: 188.46963500976562\n",
      "Epoch 62, Batch 21496, Loss: 154.26657104492188\n",
      "Epoch 62, Batch 21497, Loss: 176.5181121826172\n",
      "Epoch 62, Batch 21498, Loss: 160.13092041015625\n",
      "Epoch 62, Batch 21499, Loss: 175.71798706054688\n",
      "Epoch 62, Batch 21500, Loss: 170.60487365722656\n",
      "Epoch 62, Batch 21501, Loss: 185.29591369628906\n",
      "Epoch 62, Batch 21502, Loss: 190.38092041015625\n",
      "Epoch 62, Batch 21503, Loss: 155.97039794921875\n",
      "Epoch 62, Batch 21504, Loss: 181.41885375976562\n",
      "Epoch 62, Batch 21505, Loss: 166.22509765625\n",
      "Epoch 62, Batch 21506, Loss: 176.4807586669922\n",
      "Epoch 62, Batch 21507, Loss: 172.23150634765625\n",
      "Epoch 62, Batch 21508, Loss: 178.907958984375\n",
      "Epoch 62, Batch 21509, Loss: 181.503662109375\n",
      "Epoch 62, Batch 21510, Loss: 184.1475067138672\n",
      "Epoch 62, Batch 21511, Loss: 184.39248657226562\n",
      "Epoch 62, Batch 21512, Loss: 168.81690979003906\n",
      "Epoch 62, Batch 21513, Loss: 172.39720153808594\n",
      "Epoch 62, Batch 21514, Loss: 183.10342407226562\n",
      "Epoch 62, Batch 21515, Loss: 177.29965209960938\n",
      "Epoch 62, Batch 21516, Loss: 174.7269744873047\n",
      "Epoch 62, Batch 21517, Loss: 182.531005859375\n",
      "Epoch 62, Batch 21518, Loss: 167.8707275390625\n",
      "Epoch 62, Batch 21519, Loss: 196.0658721923828\n",
      "Epoch 62, Batch 21520, Loss: 182.8413848876953\n",
      "Epoch 62, Batch 21521, Loss: 158.39349365234375\n",
      "Epoch 62, Batch 21522, Loss: 175.3262939453125\n",
      "Epoch 62, Batch 21523, Loss: 188.2781219482422\n",
      "Epoch 62, Batch 21524, Loss: 171.21780395507812\n",
      "Epoch 62, Batch 21525, Loss: 176.21339416503906\n",
      "Epoch 62, Batch 21526, Loss: 155.3314208984375\n",
      "Epoch 62, Batch 21527, Loss: 189.6573028564453\n",
      "Epoch 62, Batch 21528, Loss: 172.8424072265625\n",
      "Epoch 62, Batch 21529, Loss: 166.09335327148438\n",
      "Epoch 62, Batch 21530, Loss: 154.77395629882812\n",
      "Epoch 62, Batch 21531, Loss: 174.0671844482422\n",
      "Epoch 62, Batch 21532, Loss: 168.55072021484375\n",
      "Epoch 62, Batch 21533, Loss: 173.48159790039062\n",
      "Epoch 62, Batch 21534, Loss: 166.85853576660156\n",
      "Epoch 62, Batch 21535, Loss: 169.47264099121094\n",
      "Epoch 62, Batch 21536, Loss: 176.1517333984375\n",
      "Epoch 62, Batch 21537, Loss: 173.89892578125\n",
      "Epoch 62, Batch 21538, Loss: 174.08506774902344\n",
      "Epoch 62, Batch 21539, Loss: 158.0960693359375\n",
      "Epoch 62, Batch 21540, Loss: 189.38162231445312\n",
      "Epoch 62, Batch 21541, Loss: 179.55250549316406\n",
      "Epoch 62, Batch 21542, Loss: 161.26844787597656\n",
      "Epoch 62, Batch 21543, Loss: 167.0634765625\n",
      "Epoch 62, Batch 21544, Loss: 177.05276489257812\n",
      "Epoch 62, Batch 21545, Loss: 162.54232788085938\n",
      "Epoch 62, Batch 21546, Loss: 187.5643310546875\n",
      "Epoch 62, Batch 21547, Loss: 189.65443420410156\n",
      "Epoch 62, Batch 21548, Loss: 192.06951904296875\n",
      "Epoch 62, Batch 21549, Loss: 179.10206604003906\n",
      "Epoch 62, Batch 21550, Loss: 168.57920837402344\n",
      "Epoch 62, Batch 21551, Loss: 172.3230743408203\n",
      "Epoch 62, Batch 21552, Loss: 180.13522338867188\n",
      "Epoch 62, Batch 21553, Loss: 185.59959411621094\n",
      "Epoch 62, Batch 21554, Loss: 178.9273681640625\n",
      "Epoch 62, Batch 21555, Loss: 168.5043182373047\n",
      "Epoch 62, Batch 21556, Loss: 191.32220458984375\n",
      "Epoch 62, Batch 21557, Loss: 178.71975708007812\n",
      "Epoch 62, Batch 21558, Loss: 165.22438049316406\n",
      "Epoch 62, Batch 21559, Loss: 177.7808837890625\n",
      "Epoch 62, Batch 21560, Loss: 186.63137817382812\n",
      "Epoch 62, Batch 21561, Loss: 168.60520935058594\n",
      "Epoch 62, Batch 21562, Loss: 168.07940673828125\n",
      "Epoch 62, Batch 21563, Loss: 174.54391479492188\n",
      "Epoch 62, Batch 21564, Loss: 162.63926696777344\n",
      "Epoch 62, Batch 21565, Loss: 181.6151123046875\n",
      "Epoch 62, Batch 21566, Loss: 175.02540588378906\n",
      "Epoch 62, Batch 21567, Loss: 172.05169677734375\n",
      "Epoch 62, Batch 21568, Loss: 160.92013549804688\n",
      "Epoch 62, Batch 21569, Loss: 188.93116760253906\n",
      "Epoch 62, Batch 21570, Loss: 177.1298065185547\n",
      "Epoch 62, Batch 21571, Loss: 166.79483032226562\n",
      "Epoch 62, Batch 21572, Loss: 169.16798400878906\n",
      "Epoch 62, Batch 21573, Loss: 174.6716766357422\n",
      "Epoch 62, Batch 21574, Loss: 168.8255157470703\n",
      "Epoch 62, Batch 21575, Loss: 189.708251953125\n",
      "Epoch 62, Batch 21576, Loss: 181.95437622070312\n",
      "Epoch 62, Batch 21577, Loss: 187.07046508789062\n",
      "Epoch 62, Batch 21578, Loss: 156.6201171875\n",
      "Epoch 62, Batch 21579, Loss: 172.7688446044922\n",
      "Epoch 62, Batch 21580, Loss: 169.9503173828125\n",
      "Epoch 62, Batch 21581, Loss: 168.9734344482422\n",
      "Epoch 62, Batch 21582, Loss: 164.87655639648438\n",
      "Epoch 62, Batch 21583, Loss: 184.80154418945312\n",
      "Epoch 62, Batch 21584, Loss: 165.22430419921875\n",
      "Epoch 62, Batch 21585, Loss: 171.7469940185547\n",
      "Epoch 62, Batch 21586, Loss: 166.51670837402344\n",
      "Epoch 62, Batch 21587, Loss: 159.4036102294922\n",
      "Epoch 62, Batch 21588, Loss: 197.4663848876953\n",
      "Epoch 62, Batch 21589, Loss: 170.10513305664062\n",
      "Epoch 62, Batch 21590, Loss: 182.2442169189453\n",
      "Epoch 62, Batch 21591, Loss: 188.44024658203125\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(input_size=len(feature_names), hidden_size=50, num_layers=3, output_size=1, dropout=0.5).to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, log_interval=10, checkpoint_dir='./logs/LSTM_v2/',TBlog_dir='./runs/LSTM_v2/'):\n",
    "    # 指定使用 GPU\n",
    "    device = torch.device('cuda:1')\n",
    "    model.to(device)\n",
    "    \n",
    "    # 初始化 TensorBoard 记录器\n",
    "    writer = SummaryWriter(TBlog_dir)\n",
    "    start_epoch = 1\n",
    "    flag = True\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')  # 指定检查点文件名\n",
    "    \n",
    "    # 确保检查点目录存在\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    # 检查是否存在检查点\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1  # 从下一个周期开始\n",
    "        \n",
    "    model.train()\n",
    "    for epoch in range(start_epoch, num_epochs + start_epoch):\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if flag:\n",
    "                print(next(model.parameters()).is_cuda)  # 确认模型参数是否在 GPU 上\n",
    "                print(inputs.is_cuda)  # 确认输入是否在 GPU 上\n",
    "                flag = False\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 在 TensorBoard 中记录损失\n",
    "            if batch_idx % log_interval == 0:\n",
    "                writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "            \n",
    "            print(f'Epoch {epoch}, Batch {batch_idx+1}, Loss: {loss.item()}')\n",
    "            # 定期保存检查点\n",
    "            if batch_idx % 100 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                }, checkpoint_path)\n",
    "    # 关闭 TensorBoard 记录器\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE on evaluation data: 313.5852\n"
     ]
    }
   ],
   "source": [
    "def calculate_mae(model, data_loader, device):\n",
    "    model.eval()  \n",
    "    total_mae = 0.0\n",
    "    total_count = 0\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            mae = l1_loss(outputs, targets, reduction='sum')\n",
    "            total_mae += mae.item()\n",
    "            total_count += targets.size(0)\n",
    "    \n",
    "    average_mae = total_mae / total_count\n",
    "    return average_mae\n",
    "\n",
    "\n",
    "average_mae = calculate_mae(model, eval_loader, device)\n",
    "print(f'Average MAE on evaluation data: {average_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439298\n",
      "Predictions saved to 'LSTM_v2_predictions.csv', with IDs starting from 1.\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features in loader:\n",
    "            features = features[0].to(device)\n",
    "            outputs = model(features)\n",
    "            # predictions.extend(outputs.cpu().numpy())\n",
    "            predictions.extend(outputs.round().cpu().numpy())\n",
    "    return predictions\n",
    "\n",
    "predictions = evaluate_model(model, test_loader)\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions, columns=['estimate_q'])\n",
    "predictions_df.index = predictions_df.index + 1  # Adjust index to start from 1\n",
    "print(len(predictions_df))\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv(f'{resultdir}LSTM_v2_new_predictions.csv', index_label='id')\n",
    "\n",
    "print(\"Predictions saved to 'LSTM_v2_predictions.csv', with IDs starting from 1.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
