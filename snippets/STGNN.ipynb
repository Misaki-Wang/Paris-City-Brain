{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as l1_loss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CUDA devices: 2\n",
      "Device 0: NVIDIA GeForce RTX 4090\n",
      "Device 1: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "inputdir = '../data/constructed/'\n",
    "resultdir = '../data/result/'\n",
    "num_nodes = 4634\n",
    "batchsize = 1024\n",
    "# [53, 1730], [7006,1500]\n",
    "total_num_timesteps = 7006\n",
    "num_timesteps =  2\n",
    "\n",
    "feature_names = [\n",
    "    'iu_ac',\n",
    "    'hour_sin', 'hour_cos', \n",
    "    'day_of_week_sin', 'day_of_week_cos', \n",
    "    'month_sin', 'month_cos',\n",
    "    'etat_barre_0', 'etat_barre_1', 'etat_barre_2', 'etat_barre_3',\n",
    "    'constructed'\n",
    "]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Available CUDA devices:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "if not os.path.exists(resultdir):\n",
    "    os.makedirs(resultdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath, feature_names, target_name='target', batch_size=batchsize, shuffle=False, train=True):\n",
    "    # features = load_time_series_data(filepath, feature_names)\n",
    "    data = pd.read_csv(filepath)\n",
    "    features = data[feature_names].values\n",
    "    # 假设每个样本都是按时间步骤连续的，此处可能需要根据实际情况调整\n",
    "    features = features.reshape(-1, num_timesteps, num_nodes,  len(feature_names))\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "    print(features_tensor.shape)\n",
    "\n",
    "    if train:\n",
    "        targets = data[target_name].values\n",
    "        targets = targets.reshape(-1, num_timesteps, num_nodes, 1)\n",
    "        targets_tensor = torch.tensor(targets, dtype=torch.float32)\n",
    "        dataset = TensorDataset(features_tensor, targets_tensor)\n",
    "    else:\n",
    "        dataset = TensorDataset(features_tensor)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_data(filename):\n",
    "    data = np.load(filename)\n",
    "    adjacency_matrix = data['adjacency_matrix']\n",
    "    keys = data['keys']\n",
    "    values = data['values']\n",
    "    index_to_iu_ac = {key: value for key, value in zip(keys, values)}\n",
    "    return adjacency_matrix, index_to_iu_ac\n",
    "\n",
    "def symmetric_normalize_adjacency(adjacency_matrix):\n",
    "    D = np.diag(np.sqrt(np.sum(adjacency_matrix, axis=1)) + 1e-6)\n",
    "    D_inv = np.linalg.inv(D)\n",
    "    normalized_adjacency = np.dot(np.dot(D_inv, adjacency_matrix), D_inv)\n",
    "    return normalized_adjacency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3503, 2, 4634, 12])\n"
     ]
    }
   ],
   "source": [
    "# train_loader = load_dataset(f'{inputdir}train_dataset_constructed.csv', feature_names)\n",
    "train_loader = load_dataset(f'{inputdir}train_dataset_constructed_1500.csv', feature_names)\n",
    "# eval_loader = load_dataset(f'{inputdir}eval_dataset_constructed.csv', feature_names)\n",
    "# test_loader = load_dataset(f'{inputdir}test_dataset_constructed_x.csv', feature_names, train=False)\n",
    "\n",
    "# 加载图数据\n",
    "adjacency_matrix, index_to_iu_ac = load_graph_data(f\"{inputdir}graph_data.npz\")\n",
    "# 归一化邻接矩阵\n",
    "normalized_adjacency = symmetric_normalize_adjacency(adjacency_matrix)\n",
    "adjacency_tensor = torch.from_numpy(normalized_adjacency)\n",
    "adjacency_tensor = adjacency_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / np.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        output = torch.matmul(adj, support)\n",
    "        return output + self.bias\n",
    "\n",
    "class STGNN(nn.Module):\n",
    "    def __init__(self, num_features, num_nodes, hidden_size, num_layers, dropout=0.5):\n",
    "        super(STGNN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(num_features, hidden_size)\n",
    "        # Now the input_size to LSTM should consider all nodes as part of the sequence\n",
    "        self.lstm = nn.LSTM(input_size=num_nodes * hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_nodes * num_timesteps * 1)  # 修改输出大小\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        # Apply the graph convolution to each time step\n",
    "        batch_size, num_timesteps, num_nodes, num_features = x.shape\n",
    "        x = x.view(batch_size * num_timesteps, num_nodes, num_features)\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = x.view(batch_size, num_timesteps, -1)  # Flatten all node features into the feature dimension\n",
    "        # Process sequences using LSTM\n",
    "        x, _ = self.lstm(x)  # LSTM expects [batch_size, seq_length, input_size]\n",
    "        # Take the output from the last time step and reshape it\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        x = x.view(batch_size, num_timesteps, num_nodes, 1)  # Reshape to desired output shape\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, adjacency_matrix, criterion, optimizer, num_epochs, log_interval=10, checkpoint_dir='./logs/stgnn/',TBlog_dir='./runs/stgnn/'):\n",
    "    model.to(device)\n",
    "    \n",
    "    # 初始化 TensorBoard 记录器\n",
    "    if not os.path.exists(TBlog_dir):\n",
    "        os.makedirs(TBlog_dir)\n",
    "    writer = SummaryWriter(TBlog_dir)\n",
    "    start_epoch = 1\n",
    "    flag = True\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')  # 指定检查点文件名\n",
    "    \n",
    "    # 确保检查点目录存在\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    # 检查是否存在检查点\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1  # 从下一个周期开始\n",
    "        \n",
    "    model.train()\n",
    "    for epoch in range(start_epoch, num_epochs + start_epoch):\n",
    "        # for batch_idx, (inputs, targets) in tqdm(enumerate(train_loader), desc=\"Training\"):\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if flag:\n",
    "                print(next(model.parameters()).is_cuda)  # 确认模型参数是否在 GPU 上\n",
    "                print(inputs.is_cuda)  # 确认输入是否在 GPU 上\n",
    "                flag = False\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs, adjacency_matrix)\n",
    "            # print(f'output: {output.shape}, target:{targets.shape}')\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 在 TensorBoard 中记录损失\n",
    "            if batch_idx % log_interval == 0:\n",
    "                writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "            \n",
    "            # print(f'Epoch {epoch}, Batch {batch_idx+1}, Loss: {loss.item()}')\n",
    "            # 定期保存检查点\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx+1}, Loss: {loss.item()}')\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                }, checkpoint_path)\n",
    "    # 关闭 TensorBoard 记录器\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = STGNN(len(feature_names), num_nodes=num_nodes, hidden_size=64, num_layers=2).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "Epoch 1, Batch 1, Loss: 243.07518005371094\n",
      "Epoch 2, Batch 1, Loss: 242.43093872070312\n",
      "Epoch 3, Batch 1, Loss: 242.03390502929688\n",
      "Epoch 4, Batch 1, Loss: 241.29771423339844\n",
      "Epoch 5, Batch 1, Loss: 240.2039337158203\n",
      "Epoch 6, Batch 1, Loss: 239.1455078125\n",
      "Epoch 7, Batch 1, Loss: 238.21034240722656\n",
      "Epoch 8, Batch 1, Loss: 237.3214874267578\n",
      "Epoch 9, Batch 1, Loss: 236.4547576904297\n",
      "Epoch 10, Batch 1, Loss: 235.60665893554688\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, adjacency_tensor, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 17071656 into shape (2,4634,12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_loader \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43minputdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43meval_dataset_constructed.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# # test_loader = load_dataset(f'{inputdir}test_dataset_constructed_x.csv', feature_names, train=False)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(filepath, feature_names, target_name, batch_size, shuffle, train)\u001b[0m\n\u001b[1;32m      4\u001b[0m features \u001b[38;5;241m=\u001b[39m data[feature_names]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 假设每个样本都是按时间步骤连续的，此处可能需要根据实际情况调整\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m features_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(features, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(features_tensor\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 17071656 into shape (2,4634,12)"
     ]
    }
   ],
   "source": [
    "eval_loader = load_dataset(f'{inputdir}eval_dataset_constructed.csv', feature_names)\n",
    "# # test_loader = load_dataset(f'{inputdir}test_dataset_constructed_x.csv', feature_names, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_mae(model, data_loader,adjacency_matrix= adjacency_matrix, device=device):\n",
    "#     model.eval()  \n",
    "#     total_mae = 0.0\n",
    "#     total_count = 0\n",
    "    \n",
    "#     with torch.no_grad():  \n",
    "#         for inputs, targets in data_loader:\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             output = model(inputs, adjacency_matrix)\n",
    "#             mae = l1_loss(output, targets, reduction='sum')\n",
    "#             total_mae += mae.item()\n",
    "#             total_count += targets.size(0)\n",
    "    \n",
    "#     average_mae = total_mae / total_count\n",
    "#     return average_mae\n",
    "\n",
    "\n",
    "# average_mae = calculate_mae(model, eval_loader)\n",
    "# print(f'Average MAE on evaluation data: {average_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, loader, adjacency_matrix= adjacency_matrix, device=device):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     with torch.no_grad():\n",
    "#         for features, _ in loader:\n",
    "#             features = features[0].to(device)\n",
    "#             output = model(features, adjacency_matrix)\n",
    "#             # predictions.extend(outputs.cpu().numpy())\n",
    "#             predictions.extend(output.round().cpu().numpy())\n",
    "#     return predictions\n",
    "\n",
    "# predictions = evaluate_model(model, eval_loader)\n",
    "\n",
    "# predictions_df = pd.DataFrame(predictions)\n",
    "# predictions_df.index = predictions_df.index + 1  # Adjust index to start from 1\n",
    "# print(len(predictions_df))\n",
    "# # Save the predictions to a CSV file\n",
    "# predictions_df.to_csv(f'{resultdir}stgnn_predictions.csv', index_label='id')\n",
    "\n",
    "# print(\"Predictions saved to 'stgnn_predictions.csv', with IDs starting from 1.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
